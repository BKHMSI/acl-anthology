<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jiajun Chen - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jiajun</span> <span class=font-weight-bold>Chen</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.138/><span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>TIIMT</span>: A Bilingual Text-infilling Method for Interactive Machine Translation</a></strong><br><a href=/people/y/yanling-xiao/>Yanling Xiao</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/g/guoping-huang/>Guoping Huang</a>
|
<a href=/people/q/qu-cui/>Qu Cui</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--138><div class="card-body p-3 small">Interactive neural machine translation (INMT) is able to guarantee high-quality translations by taking human interactions into account. Existing IMT systems relying on lexical constrained decoding (LCD) enable humans to translate in a flexible translation order beyond the left-to-right. However, they typically suffer from two significant limitations in translation efficiency and quality due to the reliance on LCD. In this work, we propose a novel BiTIIMT system, Bilingual Text-Infilling for Interactive Neural Machine Translation. The key idea to BiTIIMT is Bilingual Text-infilling (BiTI) which aims to fill missing segments in a manually revised translation for a given source sentence. We propose a simple yet effective solution by casting this task as a sequence-to-sequence task. In this way, our system performs decoding without explicit constraints and makes full use of revised words for better translation prediction. Experiment results show that BiTiIMT performs significantly better and faster than state-of-the-art LCD-based IMT on three translation tasks.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1086.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1086/>Dynamic Past and Future for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xinyu-dai/>Xin-Yu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1086><div class="card-body p-3 small">Previous studies have shown that neural machine translation (NMT) models can benefit from explicitly modeling translated () and untranslated () source contents as recurrent states (CITATION). However, this less interpretable recurrent process hinders its power to model the dynamic updating of and contents during decoding. In this paper, we propose to model the dynamic principles by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely Guided Dynamic Routing, where the translating status at each decoding step guides the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both <a href=https://en.wikipedia.org/wiki/Reverse_Polish_notation>Rnmt</a> and <a href=https://en.wikipedia.org/wiki/Reverse_Polish_notation>Transformer</a> by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.<i>dynamic principles</i> by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely <i>Guided Dynamic Routing</i>, where the translating status at each decoding step <i>guides</i> the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both Rnmt and Transformer by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1192/>Online Distilling from Checkpoints for Neural Machine Translation</a></strong><br><a href=/people/h/hao-ran-wei/>Hao-Ran Wei</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/r/ran-wang/>Ran Wang</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1192><div class="card-body p-3 small">Current predominant neural machine translation (NMT) models often have a deep structure with large amounts of parameters, making these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> hard to train and easily suffering from <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. A common practice is to utilize a validation set to evaluate the training process and select the best checkpoint. Average and ensemble techniques on checkpoints can lead to further performance improvement. However, as these methods do not affect the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training process</a>, the <a href=https://en.wikipedia.org/wiki/System>system</a> performance is restricted to the checkpoints generated in original <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a>. In contrast, we propose an online knowledge distillation method. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on-the-fly generates a teacher model from <a href=https://en.wikipedia.org/wiki/Checkpoint>checkpoints</a>, guiding the <a href=https://en.wikipedia.org/wiki/Training>training process</a> to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. Furthermore, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> leads to an improvement in a machine reading experiment as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1325/>Exploiting Noisy Data in Distant Supervision Relation Classification</a></strong><br><a href=/people/k/kaijia-yang/>Kaijia Yang</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1325><div class="card-body p-3 small">Distant supervision has obtained great progress on relation classification task. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> still suffers from noisy labeling problem. Different from previous works that underutilize <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a> which inherently characterize the property of classification, in this paper, we propose RCEND, a novel framework to enhance Relation Classification by Exploiting <a href=https://en.wikipedia.org/wiki/Noisy_data>Noisy Data</a>. First, an instance discriminator with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> is designed to split the noisy data into correctly labeled data and incorrectly labeled data. Second, we learn a robust relation classifier in semi-supervised learning way, whereby the correctly and incorrectly labeled data are treated as labeled and unlabeled data respectively. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1616 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1616" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1616/>Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering</a></strong><br><a href=/people/p/peng-wu/>Peng Wu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/r/rongxiang-weng/>Rongxiang Weng</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/j/jianbing-zhang/>Jianbing Zhang</a>
|
<a href=/people/x/xiaohui-yan/>Xiaohui Yan</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1616><div class="card-body p-3 small">Relation detection is a core step in many <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language process applications</a> including <a href=https://en.wikipedia.org/wiki/Question_answering>knowledge base question answering</a>. Previous efforts show that single-fact questions could be answered with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. However, one critical problem is that current approaches only get high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for questions whose relations have been seen in the training data. But for unseen relations, the performance will drop rapidly. The main reason for this problem is that the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for unseen relations are missing. In this paper, we propose a simple mapping method, named representation adapter, to learn the <a href=https://en.wikipedia.org/wiki/Representation_theory>representation mapping</a> for both seen and unseen relations based on previously learned relation embedding. We employ the adversarial objective and the reconstruction objective to improve the <a href=https://en.wikipedia.org/wiki/Cartography>mapping</a> performance. We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations. Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q18-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/Q18-1011/>Modeling Past and Future for Neural Machine Translation</a></strong><br><a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1011><div class="card-body p-3 small">Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts : translated Past contents and untranslated Future contents, which are modeled by two additional recurrent layers. The Past and Future contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1116/>Combining Character and Word Information in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using a Multi-Level Attention</a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1116><div class="card-body p-3 small">Natural language sentences, being hierarchical, can be represented at different levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, like <a href=https://en.wikipedia.org/wiki/Word>words</a>, <a href=https://en.wikipedia.org/wiki/Subword>subwords</a>, or <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>. But most neural machine translation systems require the sentence to be represented as a sequence at a single level of granularity. It can be difficult to determine which <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a> is better for a particular translation task. In this paper, we improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information ; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model, and a strong character-based model.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1011/>Top-Rank Enhanced Listwise Optimization for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a></a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1011><div class="card-body p-3 small">Pairwise ranking methods are the most widely used discriminative training approaches for structure prediction problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Decomposing the problem of ranking hypotheses into <a href=https://en.wikipedia.org/wiki/Pairwise_comparisons>pairwise comparisons</a> enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder <a href=https://en.wikipedia.org/wiki/Learning>learning</a>. We propose a listwise learning framework for structure prediction problems such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our framework directly models the entire translation list&#8217;s ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1177/>Improved <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with a Syntax-Aware Encoder and Decoder</a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1177><div class="card-body p-3 small">Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. In this paper, we improve this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations ; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2092/>Chunk-Based Bi-Scale Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xiaohua-liu/>Xiaohua Liu</a>
|
<a href=/people/h/hang-li/>Hang Li</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2092><div class="card-body p-3 small">In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all <a href=https://en.wikipedia.org/wiki/Granularity>linguistic granularities</a> in the same time-scale of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a>. In this paper, we propose a new type of <a href=https://en.wikipedia.org/wiki/Code>decoder</a> for NMT, which splits the decode state into two parts and updates them in two different <a href=https://en.wikipedia.org/wiki/Time_complexity>time-scales</a>. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance over the state-of-the-art NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1013/>Neural Machine Translation with Word Predictions</a></strong><br><a href=/people/r/rongxiang-weng/>Rongxiang Weng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1013><div class="card-body p-3 small">In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Decoder>decoder</a> carry the crucial information about the sentence. These <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vectors</a> are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use <a href=https://en.wikipedia.org/wiki/Word_prediction>word predictions</a> as a mechanism for direct supervision. More specifically, we require these <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vectors</a> to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English machine translation task show an average BLEU improvement by 4.53, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1079/>Word-Context Character Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhenting-yu/>Zhenting Yu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1079><div class="card-body p-3 small">Neural parsers have benefited from automatically labeled data via dependency-context word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method improves state-of-the-art neural word segmentation models significantly, beating tri-training baselines for leveraging auto-segmented data.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jiajun+Chen" title="Search for 'Jiajun Chen' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/s/shujian-huang/ class=align-middle>Shujian Huang</a>
<span class="badge badge-secondary align-middle ml-2">12</span></li><li class=list-group-item><a href=/people/x/xinyu-dai/ class=align-middle>Xinyu Dai</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/z/zaixiang-zheng/ class=align-middle>Zaixiang Zheng</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/h/huadong-chen/ class=align-middle>Huadong Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/d/david-chiang/ class=align-middle>David Chiang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/h/hao-zhou/ class=align-middle>Hao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/z/zhaopeng-tu/ class=align-middle>Zhaopeng Tu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/rongxiang-weng/ class=align-middle>Rongxiang Weng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xiaohua-liu/ class=align-middle>Xiaohua Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hang-li/ class=align-middle>Hang Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanling-xiao/ class=align-middle>Yanling Xiao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lemao-liu/ class=align-middle>Lemao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guoping-huang/ class=align-middle>Guoping Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qu-cui/ class=align-middle>Qu Cui</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuming-shi/ class=align-middle>Shuming Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lili-mou/ class=align-middle>Lili Mou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhenting-yu/ class=align-middle>Zhenting Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-zhang/ class=align-middle>Yue Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-ran-wei/ class=align-middle>Hao-Ran Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ran-wang/ class=align-middle>Ran Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaijia-yang/ class=align-middle>Kaijia Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liang-he/ class=align-middle>Liang He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-wu/ class=align-middle>Peng Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jianbing-zhang/ class=align-middle>Jianbing Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaohui-yan/ class=align-middle>Xiaohui Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>