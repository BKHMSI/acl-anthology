<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jordan Boyd-Graber - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jordan</span> <span class=font-weight-bold>Boyd-Graber</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--519 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.519" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.519/>Adapting Coreference Resolution Models through <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a></a></strong><br><a href=/people/m/michelle-yuan/>Michelle Yuan</a>
|
<a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/c/chandler-may/>Chandler May</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--519><div class="card-body p-3 small">Neural coreference resolution models trained on one dataset may not transfer to new low resource domains Active learning mitigates this problem by sampling a small subset of data for annotators to label While <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is well defined for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification tasks</a> its application to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> is neither well defined nor fully understood This paper explores how to actively label coreference examining sources of model uncertainty and document reading costs We compare uncertainty sampling strategies and their advantages through thorough error analysis In both synthetic and human experiments labeling spans within the same document is more effective than annotating spans across documents The findings contribute to a more realistic development of coreference resolution models</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mrqa-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mrqa-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mrqa-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mrqa-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mrqa-1.9/>Eliciting Bias in Question Answering Models through Ambiguity</a></strong><br><a href=/people/a/andrew-mao/>Andrew Mao</a>
|
<a href=/people/n/naveen-raman/>Naveen Raman</a>
|
<a href=/people/m/matthew-shu/>Matthew Shu</a>
|
<a href=/people/e/eric-li/>Eric Li</a>
|
<a href=/people/f/franklin-yang/>Franklin Yang</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/2021.mrqa-1/ class=text-muted>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mrqa-1--9><div class="card-body p-3 small">Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a> can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a> can inform methods to mitigate <a href=https://en.wikipedia.org/wiki/Equity_(economics)>inequity</a>. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for <a href=https://en.wikipedia.org/wiki/Bias>bias</a>. We feed three deep-learning-based QA systems with our question sets and evaluate responses for <a href=https://en.wikipedia.org/wiki/Bias>bias</a> via the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater <a href=https://en.wikipedia.org/wiki/Freedom_of_choice>freedom of choice</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.756.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--756 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.756 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.756" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.756/>Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation</a></strong><br><a href=/people/c/chen-zhao/>Chen Zhao</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--756><div class="card-body p-3 small">Open-domain question answering answers a question based on evidence retrieved from a <a href=https://en.wikipedia.org/wiki/Text_corpus>large corpus</a>. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> that rely on them can not transfer to the more common setting, where only questionanswer pairs are available. This paper investigates whether <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.637.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--637 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938687 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.637" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.637/>Cold-start Active Learning through Self-supervised Language Modeling</a></strong><br><a href=/people/m/michelle-yuan/>Michelle Yuan</a>
|
<a href=/people/h/hsuan-tien-lin/>Hsuan-Tien Lin</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--637><div class="card-body p-3 small">Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a>. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is impractical because of model instability and data scarcity. Fortunately, modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> provides an additional source of information : pre-trained language models. The pre-training loss can find examples that surprise the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and should be labeled for efficient <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Compared to other baselines, our approach reaches higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> within less <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling iterations</a> and <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.353.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--353 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.353 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928719 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.353/>It Takes Two to Lie : One to Lie, and One to Listen</a></strong><br><a href=/people/d/denis-peskov/>Denis Peskov</a>
|
<a href=/people/b/benny-cheng/>Benny Cheng</a>
|
<a href=/people/a/ahmed-elgohary/>Ahmed Elgohary</a>
|
<a href=/people/j/joe-barrow/>Joe Barrow</a>
|
<a href=/people/c/cristian-danescu-niculescu-mizil/>Cristian Danescu-Niculescu-Mizil</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--353><div class="card-body p-3 small">Trust is implicit in many online text conversationsstriking up new friendships, or asking for tech support. But <a href=https://en.wikipedia.org/wiki/Trust_(social_science)>trust</a> can be betrayed through <a href=https://en.wikipedia.org/wiki/Deception>deception</a>. We study the language and dynamics of <a href=https://en.wikipedia.org/wiki/Deception>deception</a> in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures <a href=https://en.wikipedia.org/wiki/Deception>deception</a> in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1076/>Automatic Evaluation of Local Topic Quality</a></strong><br><a href=/people/j/jeffrey-lund/>Jeffrey Lund</a>
|
<a href=/people/p/piper-armstrong/>Piper Armstrong</a>
|
<a href=/people/w/wilson-fearn/>Wilson Fearn</a>
|
<a href=/people/s/stephen-cowley/>Stephen Cowley</a>
|
<a href=/people/c/courtni-byun/>Courtni Byun</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/k/kevin-seppi/>Kevin Seppi</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1076><div class="card-body p-3 small">Topic models are typically evaluated with respect to the global topic distributions that they generate, using <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> such as <a href=https://en.wikipedia.org/wiki/Coherence_(statistics)>coherence</a>, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Even recent <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, which aim to improve the quality of these token-level topic assignments, have been evaluated only with respect to <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>global metrics</a>. We propose a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> designed to elicit human judgments of token-level topic assignments. We use a variety of <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a> types and parameters and discover that global metrics agree poorly with human assignments. Since <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> is expensive we propose a variety of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automated metrics</a> to evaluate <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> at a local level. Finally, we correlate our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> from the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> on several datasets. We show that an <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> based on the percent of topic switches correlates most strongly with human judgment of local topic quality. We suggest that this new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, which we call <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a>, be adopted alongside global metrics such as topic coherence when evaluating new topic models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1307 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1307/>Are Girls Neko or Shjo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization</a></strong><br><a href=/people/m/mozhi-zhang/>Mozhi Zhang</a>
|
<a href=/people/k/keyulu-xu/>Keyulu Xu</a>
|
<a href=/people/k/ken-ichi-kawarabayashi/>Ken-ichi Kawarabayashi</a>
|
<a href=/people/s/stefanie-jegelka/>Stefanie Jegelka</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1307><div class="card-body p-3 small">Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language&#8217;s average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2 % to 44 % test accuracy).</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1144 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1144/>Learning from Measurements in Crowdsourcing Models : Inferring Ground Truth from Diverse Annotation Types</a></strong><br><a href=/people/p/paul-felt/>Paul Felt</a>
|
<a href=/people/e/eric-ringger/>Eric Ringger</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/k/kevin-seppi/>Kevin Seppi</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1144><div class="card-body p-3 small">Annotated corpora enable <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> and <a href=https://en.wikipedia.org/wiki/Data_analysis>data analysis</a>. To reduce the cost of manual annotation, tasks are often assigned to internet workers whose judgments are reconciled by crowdsourcing models. We approach the problem of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> using a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for learning from rich prior knowledge, and we identify a family of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing models</a> with the novel ability to combine <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> with differing structures : e.g., document labels and word labels. Annotator judgments are given in the form of the predicted expected value of <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>measurement functions</a> computed over <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> and the data, unifying annotation models. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, a specific instance of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>, compares favorably with previous work. Furthermore, it enables active sample selection, jointly selecting annotator, data item, and annotation structure to reduce annotation effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/277669906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1099/>Lessons from the Bible on Modern Topics : Low-Resource Multilingual Topic Model Evaluation<span class=acl-fixed-case>B</span>ible on Modern Topics: Low-Resource Multilingual Topic Model Evaluation</a></strong><br><a href=/people/s/shudong-hao/>Shudong Hao</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/m/michael-paul/>Michael J. Paul</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1099><div class="card-body p-3 small">Multilingual topic models enable <a href=https://en.wikipedia.org/wiki/Document_analysis>document analysis</a> across languages through coherent multilingual summaries of the data. However, there is no standard and effective <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> in low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2120 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-2120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-2120/>Learning to Color from Language</a></strong><br><a href=/people/v/varun-manjunatha/>Varun Manjunatha</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/l/larry-davis/>Larry Davis</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2120><div class="card-body p-3 small">Automatic colorization is the process of adding <a href=https://en.wikipedia.org/wiki/Color>color</a> to <a href=https://en.wikipedia.org/wiki/Grayscale>greyscale images</a>. We condition this <a href=https://en.wikipedia.org/wiki/Process_(computing)>process</a> on <a href=https://en.wikipedia.org/wiki/Language>language</a>, allowing end users to manipulate a colorized image by feeding in different captions. We present two different architectures for language-conditioned colorization, both of which produce more accurate and plausible colorizations than a language-agnostic version. Furthermore, we demonstrate through crowdsourced experiments that we can dramatically alter <a href=https://en.wikipedia.org/wiki/Colorization>colorizations</a> simply by manipulating descriptive color words in captions.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956951 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1083/>Tandem Anchoring : a Multiword Anchor Approach for Interactive Topic Modeling</a></strong><br><a href=/people/j/jeffrey-lund/>Jeffrey Lund</a>
|
<a href=/people/c/connor-cook/>Connor Cook</a>
|
<a href=/people/k/kevin-seppi/>Kevin Seppi</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1083><div class="card-body p-3 small">Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as <a href=https://en.wikipedia.org/wiki/News_anchor>anchors</a>, going beyond existing single word anchor algorithmsan approach we call Tandem Anchors. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/m/maja-popovic/>Maja Popović</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/P17-5/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957075 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1001/>Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels</a></strong><br><a href=/people/a/alison-smith/>Alison Smith</a>
|
<a href=/people/t/tak-yeon-lee/>Tak Yeon Lee</a>
|
<a href=/people/f/forough-poursabzi-sangdeh/>Forough Poursabzi-Sangdeh</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/n/niklas-elmqvist/>Niklas Elmqvist</a>
|
<a href=/people/l/leah-findlater/>Leah Findlater</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1001><div class="card-body p-3 small">Probabilistic topic models are important tools for <a href=https://en.wikipedia.org/wiki/Index_(publishing)>indexing</a>, summarizing, and analyzing large document collections by their themes. However, promoting end-user understanding of topics remains an open research problem. We compare labels generated by users given four topic visualization techniquesword lists, word lists with bars, word clouds, and network graphsagainst each other and against automatically generated labels. Our basis of comparison is participant ratings of how well labels describe documents from the topic. Our study has two phases : a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics&#8217; documents. Although all <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualizations</a> produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure. Automatic labels lag behind user-created labels, but our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of manually labeled topics highlights <a href=https://en.wikipedia.org/wiki/Pattern>linguistic patterns</a> (e.g., <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernyms</a>, phrases) that can be used to improve automatic topic labeling algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1153.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1153/>Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback</a></strong><br><a href=/people/k/khanh-nguyen/>Khanh Nguyen</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1153><div class="card-body p-3 small">Machine translation is a natural candidate problem for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> from human feedback : users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning algorithm</a> that improves neural machine translation systems from simulated human feedback. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1203/>Adapting Topic Models using <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Associations</a> with Tree Priors</a></strong><br><a href=/people/w/weiwei-yang/>Weiwei Yang</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1203><div class="card-body p-3 small">Models work best when they are optimized taking into account the evaluation criteria that people care about. For <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>, people often care about <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting <a href=https://en.wikipedia.org/wiki/Intrinsic_and_extrinsic_properties>extrinsic performance</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jordan+Boyd-Graber" title="Search for 'Jordan Boyd-Graber' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kevin-seppi/ class=align-middle>Kevin Seppi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/michelle-yuan/ class=align-middle>Michelle Yuan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jeffrey-lund/ class=align-middle>Jeffrey Lund</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hal-daume-iii/ class=align-middle>Hal Daumé III</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/andrew-mao/ class=align-middle>Andrew Mao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/n/naveen-raman/ class=align-middle>Naveen Raman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-shu/ class=align-middle>Matthew Shu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eric-li/ class=align-middle>Eric Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/franklin-yang/ class=align-middle>Franklin Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-felt/ class=align-middle>Paul Felt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eric-ringger/ class=align-middle>Eric Ringger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hsuan-tien-lin/ class=align-middle>Hsuan-Tien Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/denis-peskov/ class=align-middle>Denis Peskov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benny-cheng/ class=align-middle>Benny Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmed-elgohary/ class=align-middle>Ahmed Elgohary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joe-barrow/ class=align-middle>Joe Barrow</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cristian-danescu-niculescu-mizil/ class=align-middle>Cristian Danescu-Niculescu-Mizil</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/connor-cook/ class=align-middle>Connor Cook</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maja-popovic/ class=align-middle>Maja Popović</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-xia/ class=align-middle>Patrick Xia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chandler-may/ class=align-middle>Chandler May</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/benjamin-van-durme/ class=align-middle>Benjamin Van Durme</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alison-smith/ class=align-middle>Alison Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tak-yeon-lee/ class=align-middle>Tak Yeon Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/forough-poursabzi-sangdeh/ class=align-middle>Forough Poursabzi-Sangdeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/niklas-elmqvist/ class=align-middle>Niklas Elmqvist</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leah-findlater/ class=align-middle>Leah Findlater</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-zhao/ class=align-middle>Chen Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenyan-xiong/ class=align-middle>Chenyan Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khanh-nguyen/ class=align-middle>Khanh Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weiwei-yang/ class=align-middle>Weiwei Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philip-resnik/ class=align-middle>Philip Resnik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shudong-hao/ class=align-middle>Shudong Hao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-paul/ class=align-middle>Michael Paul</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/varun-manjunatha/ class=align-middle>Varun Manjunatha</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-iyyer/ class=align-middle>Mohit Iyyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/larry-davis/ class=align-middle>Larry Davis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/piper-armstrong/ class=align-middle>Piper Armstrong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wilson-fearn/ class=align-middle>Wilson Fearn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephen-cowley/ class=align-middle>Stephen Cowley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/courtni-byun/ class=align-middle>Courtni Byun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mozhi-zhang/ class=align-middle>Mozhi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keyulu-xu/ class=align-middle>Keyulu Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/ken-ichi-kawarabayashi/ class=align-middle>Ken-ichi Kawarabayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefanie-jegelka/ class=align-middle>Stefanie Jegelka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/mrqa/ class=align-middle>MRQA</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>