<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>John Kelleher - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>John</span> <span class=font-weight-bold>Kelleher</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
John D. <span class=font-weight-normal>Kelleher</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.648.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--648 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.648" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.648/>Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods</a></strong><br><a href=/people/p/peru-bhardwaj/>Peru Bhardwaj</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a>
|
<a href=/people/l/luca-costabello/>Luca Costabello</a>
|
<a href=/people/d/declan-osullivan/>Declan O’Sullivan</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--648><div class="card-body p-3 small">Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the <a href=https://en.wikipedia.org/wiki/Vulnerability_(computing)>security vulnerabilities</a> that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model&#8217;s predictions on test instances. We use these influential triples as adversarial deletions. We further propose a <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic method</a> to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62 % over the baselines.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--spnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.spnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940154 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.5/>Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking</a></strong><br><a href=/people/a/anh-duong-trinh/>Anh Duong Trinh</a>
|
<a href=/people/r/robert-j-ross/>Robert J. Ross</a>
|
<a href=/people/j/john-kelleher/>John D. Kelleher</a><br><a href=/volumes/2020.spnlp-1/ class=text-muted>Proceedings of the Fourth Workshop on Structured Prediction for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--spnlp-1--5><div class="card-body p-3 small">Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that : (i) modelling variable dependencies yields better results ; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.174/>Language-Driven Region Pointer Advancement for Controllable Image Captioning</a></strong><br><a href=/people/a/annika-lindh/>Annika Lindh</a>
|
<a href=/people/r/robert-ross/>Robert Ross</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--174><div class="card-body p-3 small">Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of 86.55 % and a recall of 97.92 %. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.197/>Style versus Content : A distinction without a (learnable) difference?</a></strong><br><a href=/people/s/somayeh-jafaritazehjani/>Somayeh Jafaritazehjani</a>
|
<a href=/people/g/gwenole-lecorve/>Gwénolé Lecorvé</a>
|
<a href=/people/d/damien-lolive/>Damien Lolive</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--197><div class="card-body p-3 small">Textual style transfer involves modifying the style of a text while preserving its content. This assumes that it is possible to separate <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a> from content. This paper investigates whether this separation is possible. We use sentiment transfer as our case study for style transfer analysis. Our experimental methodology frames style transfer as a multi-objective problem, balancing style shift with content preservation and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Due to the lack of parallel data for style transfer we employ a variety of adversarial encoder-decoder networks in our experiments. Also, we use of a probing methodology to analyse how these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> encode style-related features in their latent spaces. The results of our experiments which are further confirmed by a human evaluation reveal the inherent trade-off between the multiple style transfer objectives which indicates that style can not be usefully separated from content within these style-transfer systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.0/>Proceedings of the 13th International Conference on Natural Language Generation</a></strong><br><a href=/people/b/brian-davis/>Brian Davis</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a>
|
<a href=/people/y/yaji-sripada/>Yaji Sripada</a><br><a href=/volumes/2020.inlg-1/ class=text-muted>Proceedings of the 13th International Conference on Natural Language Generation</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.gwc-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--gwc-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.gwc-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.gwc-1.18/>Synthetic, yet natural : Properties of WordNet random walk corpora and the impact of rare words on embedding performance<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et random walk corpora and the impact of rare words on embedding performance</a></strong><br><a href=/people/f/filip-klubicka/>Filip Klubička</a>
|
<a href=/people/a/alfredo-maldonado/>Alfredo Maldonado</a>
|
<a href=/people/a/abhijit-mahalunkar/>Abhijit Mahalunkar</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a><br><a href=/volumes/2019.gwc-1/ class=text-muted>Proceedings of the 10th Global Wordnet Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--gwc-1--18><div class="card-body p-3 small">Creating <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> that reflect semantic relationships encoded in lexical knowledge resources is an open challenge. One approach is to use a random walk over a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> to generate a pseudo-corpus and use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to train embeddings. However, the effect of the shape of the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> on the generated pseudo-corpora, and on the resulting <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, has not been studied. To explore this, we use <a href=https://en.wikipedia.org/wiki/WordNet>English WordNet</a>, constrained to the taxonomic (tree-like) portion of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, as a case study. We investigate the properties of the generated pseudo-corpora, and their impact on the resulting <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. We find that the distributions in the psuedo-corpora exhibit properties found in natural corpora, such as Zipf&#8217;s and Heaps&#8217; law, and also observe that the proportion of rare words in a pseudo-corpus affects the performance of its embeddings on word similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3904/>Multi-Element Long Distance Dependencies : Using SPk Languages to Explore the Characteristics of Long-Distance Dependencies<span class=acl-fixed-case>SP</span>k Languages to Explore the Characteristics of Long-Distance Dependencies</a></strong><br><a href=/people/a/abhijit-mahalunkar/>Abhijit Mahalunkar</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a><br><a href=/volumes/W19-39/ class=text-muted>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3904><div class="card-body p-3 small">In order to successfully model Long Distance Dependencies (LDDs) it is necessary to under-stand the full-range of the characteristics of the LDDs exhibited in a target dataset. In this paper, we use Strictly k-Piecewise languages to generate <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with various properties. We then compute the characteristics of the LDDs in these datasets using <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> and analyze the impact of factors such as (i) k, (ii) length of LDDs, (iii) vocabulary size, (iv) forbidden strings, and (v) dataset size. This analysis reveal that the number of interacting elements in a dependency is an important characteristic of LDDs. This leads us to the challenge of modelling multi-element long-distance dependencies. Our results suggest that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> may aide in modeling datasets with multi-element long-distance dependencies. However, we conclude that there is a need to develop more efficient attention mechanisms to address this issue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/R19-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-R19-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-R19-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/R19-1121/>Persistence pays off : Paying Attention to What the LSTM Gating Mechanism Persists<span class=acl-fixed-case>LSTM</span> Gating Mechanism Persists</a></strong><br><a href=/people/g/giancarlo-salton/>Giancarlo Salton</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a><br><a href=/volumes/R19-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-R19-1121><div class="card-body p-3 small">Recurrent Neural Network Language Models composed of LSTM units, especially those augmented with an external memory, have achieved state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a>. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> still struggle to process long sequences which are more likely to contain long-distance dependencies because of information fading. In this paper we demonstrate an effective <a href=https://en.wikipedia.org/wiki/Mechanism_design>mechanism</a> for retrieving information in a memory augmented LSTM LM based on attending to information in memory in proportion to the number of timesteps the LSTM gating mechanism persisted the information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/R19-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-R19-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-R19-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/R19-1150/>Bigger versus Similar : Selecting a Background Corpus for First Story Detection Based on Distributional Similarity</a></strong><br><a href=/people/f/fei-wang/>Fei Wang</a>
|
<a href=/people/r/robert-j-ross/>Robert J. Ross</a>
|
<a href=/people/j/john-kelleher/>John D. Kelleher</a><br><a href=/volumes/R19-1/ class=text-muted>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-R19-1150><div class="card-body p-3 small">The current state of the art for First Story Detection (FSD) are nearest neighbour-based models with traditional term vector representations ; however, one challenge faced by FSD models is that the document representation is usually defined by the vocabulary and term frequency from a background corpus. Consequently, the ideal background corpus should arguably be both large-scale to ensure adequate term coverage, and similar to the target domain in terms of the <a href=https://en.wikipedia.org/wiki/Frequency_distribution>language distribution</a>. However, given these two factors can not always be mutually satisfied, in this paper we examine whether the distributional similarity of common terms is more important than the scale of common terms for FSD. As a basis for our analysis we propose a set of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to quantitatively measure the scale of common terms and the distributional similarity between corpora. Using these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> we rank different background corpora relative to a target corpus. We also apply <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> based on different background corpora to the FSD task. Our results show that term distributional similarity is more predictive of good FSD performance than the scale of common terms ; and, thus we demonstrate that a smaller recent domain-related corpus will be more suitable than a very large-scale general corpus for FSD.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=John+Kelleher" title="Search for 'John Kelleher' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/a/abhijit-mahalunkar/ class=align-middle>Abhijit Mahalunkar</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/r/robert-j-ross/ class=align-middle>Robert J. Ross</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/peru-bhardwaj/ class=align-middle>Peru Bhardwaj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luca-costabello/ class=align-middle>Luca Costabello</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/declan-osullivan/ class=align-middle>Declan O’Sullivan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/f/filip-klubicka/ class=align-middle>Filip Klubička</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alfredo-maldonado/ class=align-middle>Alfredo Maldonado</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anh-duong-trinh/ class=align-middle>Anh Duong Trinh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/annika-lindh/ class=align-middle>Annika Lindh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-ross/ class=align-middle>Robert Ross</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/somayeh-jafaritazehjani/ class=align-middle>Somayeh Jafaritazehjani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gwenole-lecorve/ class=align-middle>Gwénolé Lecorvé</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/damien-lolive/ class=align-middle>Damien Lolive</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brian-davis/ class=align-middle>Brian Davis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yvette-graham/ class=align-middle>Yvette Graham</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yaji-sripada/ class=align-middle>Yaji Sripada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/giancarlo-salton/ class=align-middle>Giancarlo Salton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fei-wang/ class=align-middle>Fei Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ranlp/ class=align-middle>RANLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/gwc/ class=align-middle>GWC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/spnlp/ class=align-middle>spnlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/inlg/ class=align-middle>INLG</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>