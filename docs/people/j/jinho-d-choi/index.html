<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jinho D. Choi - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jinho D.</span> <span class=font-weight-bold>Choi</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4convai-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4convai-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4convai-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4convai-1.9/>What Went Wrong? Explaining Overall Dialogue Quality through Utterance-Level Impacts</a></strong><br><a href=/people/j/james-d-finch/>James D. Finch</a>
|
<a href=/people/s/sarah-e-finch/>Sarah E. Finch</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/2021.nlp4convai-1/ class=text-muted>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4convai-1--9><div class="card-body p-3 small">Improving user experience of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> identifies <a href=https://en.wikipedia.org/wiki/Interaction>interactions</a> that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.2/>View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data</a></strong><br><a href=/people/p/payam-karisani/>Payam Karisani</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a>
|
<a href=/people/l/li-xiong/>Li Xiong</a><br><a href=/volumes/2021.smm4h-1/ class=text-muted>Proceedings of the Sixth Social Media Mining for Health (#SMM4H) Workshop and Shared Task</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--2><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on multi-layer transformers for identifying Adverse Drug Reactions (ADR) in social media data. Our model relies on the properties of the problem and the characteristics of contextual word embeddings to extract two views from documents. Then a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> is trained on each view to label a set of unlabeled documents to be used as an initializer for a new <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> in the other view. Finally, the initialized classifier in each view is further trained using the initial training examples. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in the largest publicly available ADR dataset. The experiments testify that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the transformer-based models pretrained on domain-specific data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--538 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.538" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.538/>Boosting Cross-Lingual Transfer via <a href=https://en.wikipedia.org/wiki/Self-learning>Self-Learning</a> with Uncertainty Estimation</a></strong><br><a href=/people/l/liyan-xu/>Liyan Xu</a>
|
<a href=/people/x/xuchao-zhang/>Xuchao Zhang</a>
|
<a href=/people/x/xujiang-zhao/>Xujiang Zhao</a>
|
<a href=/people/h/haifeng-chen/>Haifeng Chen</a>
|
<a href=/people/f/feng-chen/>Feng Chen</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--538><div class="card-body p-3 small">Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer : Language Heteroscedastic / Homoscedastic Uncertainty (LEU / LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 for NER on average and 2.5 accuracy for NLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-demo.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-demo--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-demo.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-demo.19/>UMR-Writer : A Web Application for Annotating Uniform Meaning Representations<span class=acl-fixed-case>UMR</span>-Writer: A Web Application for Annotating Uniform Meaning Representations</a></strong><br><a href=/people/j/jin-zhao/>Jin Zhao</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/j/jens-van-gysel/>Jens Van Gysel</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/2021.emnlp-demo/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-demo--19><div class="card-body p-3 small">We present UMR-Writer, a web-based application for annotating Uniform Meaning Representations (UMR), a graph-based, cross-linguistically applicable semantic representation developed recently to support the development of interpretable natural language applications that require deep semantic analysis of texts. We present the functionalities of UMR-Writer and discuss the challenges in developing such a <a href=https://en.wikipedia.org/wiki/Tool>tool</a> and how they are addressed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cmcl-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.18/>Enhancing Cognitive Models of Emotions with <a href=https://en.wikipedia.org/wiki/Representation_learning>Representation Learning</a></a></strong><br><a href=/people/y/yuting-guo/>Yuting Guo</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/2021.cmcl-1/ class=text-muted>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--18><div class="card-body p-3 small">We present a novel deep learning-based framework to generate embedding representations of fine-grained emotions that can be used to computationally describe psychological models of emotions. Our framework integrates a contextualized embedding encoder with a multi-head probing model that enables to interpret dynamically learned representations optimized for an emotion classification task. Our model is evaluated on the Empathetic Dialogue dataset and shows the state-of-the-art result for classifying 32 emotions. Our layer analysis can derive an emotion graph to depict hierarchical relations among the emotions. Our emotion representations can be used to generate an emotion wheel directly comparable to the one from Plutchik&#8217;s model, and also augment the values of missing emotions in the <a href=https://en.wikipedia.org/wiki/PAD_emotional_state_model>PAD emotional state model</a>.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwpt-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwpt-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwpt-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929686 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwpt-1.19/>Adaptation of Multilingual Transformer Encoder for Robust Enhanced Universal Dependency Parsing<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing</a></strong><br><a href=/people/h/han-he/>Han He</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/2020.iwpt-1/ class=text-muted>Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwpt-1--19><div class="card-body p-3 small">This paper presents our enhanced dependency parsing approach using transformer encoders, coupled with a simple yet powerful ensemble algorithm that takes advantage of both tree and graph dependency parsing. Two types of transformer encoders are compared, a multilingual encoder and language-specific encoders. Our dependency tree parsing (DTP) approach generates only primary dependencies to form trees whereas our dependency graph parsing (DGP) approach handles both primary and secondary dependencies to form graphs. Since DGP does not guarantee the generated graphs are acyclic, the ensemble algorithm is designed to add secondary arcs predicted by DGP to primary arcs predicted by DTP. Our results show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using the multilingual encoder outperform ones using the language specific encoders for most languages. The ensemble models generally show higher labeled attachment score on enhanced dependencies (ELAS) than the DTP and DGP models. As the result, our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> rank the third place on the macro-average ELAS over 17 languages.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5923.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5923 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5923 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5923/>FriendsQA : Open-Domain Question Answering on TV Show Transcripts<span class=acl-fixed-case>F</span>riends<span class=acl-fixed-case>QA</span>: Open-Domain Question Answering on <span class=acl-fixed-case>TV</span> Show Transcripts</a></strong><br><a href=/people/z/zhengzhe-yang/>Zhengzhe Yang</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/W19-59/ class=text-muted>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5923><div class="card-body p-3 small">This paper presents FriendsQA, a challenging question answering dataset that contains 1,222 dialogues and 10,610 open-domain questions, to tackle machine comprehension on everyday conversations. Each dialogue, involving multiple speakers, is annotated with several types of questions regarding the dialogue contexts, and the answers are annotated with certain spans in the dialogue. A series of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing tasks</a> are conducted to ensure good annotation quality, resulting a high <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> of 81.82 %. A comprehensive annotation analytics is provided for a deeper understanding in this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Three state-of-the-art QA systems are experimented, R-Net, QANet, and BERT, and evaluated on this dataset. BERT in particular depicts promising results, an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 74.2 % for answer utterance selection and an F1-score of 64.2 % for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1003/>They Exist ! Introducing Plural Mentions to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> and Entity Linking</a></strong><br><a href=/people/e/ethan-zhou/>Ethan Zhou</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1003><div class="card-body p-3 small">This paper analyzes arguably the most challenging yet under-explored aspect of resolution tasks such as <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, that is the resolution of plural mentions. Unlike <a href=https://en.wikipedia.org/wiki/Grammatical_number>singular mentions</a> each of which represents one entity, <a href=https://en.wikipedia.org/wiki/Grammatical_number>plural mentions</a> stand for multiple entities. To tackle this aspect, we take the character identification corpus from the SemEval 2018 shared task that consists of entity annotation for singular mentions, and expand it by adding <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> for plural mentions. We then introduce a novel coreference resolution algorithm that selectively creates clusters to handle both singular and plural mentions, and also a deep learning-based entity linking model that jointly handles both types of mentions through <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Adjusted evaluation metrics are proposed for these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> as well to handle the uniqueness of plural mentions. Our experiments show that the new coreference resolution and entity linking models significantly outperform traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> designed only for singular mentions. To the best of our knowledge, this is the first time that <a href=https://en.wikipedia.org/wiki/Plural>plural mentions</a> are thoroughly analyzed for these two resolution tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1007/>SemEval 2018 Task 4 : Character Identification on Multiparty Dialogues<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2018 Task 4: Character Identification on Multiparty Dialogues</a></strong><br><a href=/people/j/jinho-d-choi/>Jinho D. Choi</a>
|
<a href=/people/h/henry-y-chen/>Henry Y. Chen</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1007><div class="card-body p-3 small">Character identification is a task of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> that finds the global entity of each personal mention in multiparty dialogue. For this task, the first two seasons of the popular TV show Friends are annotated, comprising a total of 448 dialogues, 15,709 mentions, and 401 entities. The personal mentions are detected from <a href=https://en.wikipedia.org/wiki/Character_(arts)>nominals</a> referring to certain characters in the show, and the <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a> are collected from the list of all characters in those two seasons of the show. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging because it requires the identification of characters that are mentioned but may not be active during the conversation. Among 90 + participants, four of them submitted their system outputs and showed strengths in different aspects about the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Thorough analyses of the <a href=https://en.wikipedia.org/wiki/Distributed_database>distributed datasets</a>, <a href=https://en.wikipedia.org/wiki/System>system outputs</a>, and comparative studies are also provided. To facilitate the momentum, we create an open-source project for this task and publicly release a larger and cleaner dataset, hoping to support researchers for more enhanced modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-1185/>Challenging Reading Comprehension on Daily Conversation : Passage Completion on Multiparty Dialog</a></strong><br><a href=/people/k/kaixin-ma/>Kaixin Ma</a>
|
<a href=/people/t/tomasz-jurczyk/>Tomasz Jurczyk</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1185><div class="card-body p-3 small">This paper presents a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and a robust deep learning architecture for a task in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, passage completion, on multiparty dialog. Given a dialog in text and a passage containing factual descriptions about the dialog where mentions of the characters are replaced by blanks, the task is to fill the blanks with the most appropriate character names that reflect the contexts in the dialog. Since there is no dataset that challenges the task of passage completion in this <a href=https://en.wikipedia.org/wiki/Genre>genre</a>, we create a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by selecting transcripts from a <a href=https://en.wikipedia.org/wiki/Television_show>TV show</a> that comprise 1,681 dialogs, generating passages for each dialog through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, and annotating mentions of characters in both the dialog and the passages. Given this dataset, we build a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> that integrates rich feature extraction from <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> into sequence modeling in <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, optimized by utterance and dialog level attentions. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> outperforms the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on this task in a different genre using bidirectional LSTM, showing a 13.0+% improvement for longer dialogs. Our analysis shows the effectiveness of the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> and suggests a direction to machine comprehension on multiparty dialog.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4416/>Improving Document Clustering by Removing Unnatural Language</a></strong><br><a href=/people/m/myungha-jang/>Myungha Jang</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a>
|
<a href=/people/j/james-allan/>James Allan</a><br><a href=/volumes/W17-44/ class=text-muted>Proceedings of the 3rd Workshop on Noisy User-generated Text</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4416><div class="card-body p-3 small">Technical documents contain a fair amount of unnatural language, such as tables, <a href=https://en.wikipedia.org/wiki/Formula>formulas</a>, and <a href=https://en.wikipedia.org/wiki/Pseudo-code>pseudo-code</a>. Unnatural language can bean important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, and evaluates the impact of un-natural language detection on NLP tasks such as <a href=https://en.wikipedia.org/wiki/Document_clustering>document clustering</a>. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various for-mats, <a href=https://en.wikipedia.org/wiki/PDF>PPT</a>, <a href=https://en.wikipedia.org/wiki/PDF>PDF</a>, and <a href=https://en.wikipedia.org/wiki/HTML>HTML</a>, where unnatural language components are annotated into four categories. We then explore features available from <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a> to build a <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical model</a> that can handle any format as long as it is converted into plain text. Our experiments show that re-moving unnatural language components gives an absolute improvement in document cluster-ing by up to 15 %. Our corpus and tool are publicly available</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5220/>Lexicon Integrated CNN Models with Attention for Sentiment Analysis<span class=acl-fixed-case>CNN</span> Models with Attention for Sentiment Analysis</a></strong><br><a href=/people/b/bonggun-shin/>Bonggun Shin</a>
|
<a href=/people/t/timothy-lee/>Timothy Lee</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/W17-52/ class=text-muted>Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5220><div class="card-body p-3 small">With the advent of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> are no longer fully utilized for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> although they still provide important features in the traditional setting. This paper introduces a novel approach to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> that integrates lexicon embeddings and an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> into <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a>. Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. Our models are experimented on both the SemEval&#8217;16 Task 4 dataset and the Stanford Sentiment Treebank and show comparative or better results against the existing state-of-the-art systems. Our analysis shows that lexicon embeddings allow building high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5407/>Cross-genre Document Retrieval : Matching between Conversational and Formal Writings</a></strong><br><a href=/people/t/tomasz-jurczyk/>Tomasz Jurczyk</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a><br><a href=/volumes/W17-54/ class=text-muted>Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5407><div class="card-body p-3 small">This paper challenges a cross-genre document retrieval task, where the queries are in <a href=https://en.wikipedia.org/wiki/Formal_writing>formal writing</a> and the target documents are in conversational writing. In this task, a query, is a sentence extracted from either a summary or a plot of an episode in a TV show, and the target document consists of transcripts from the corresponding episode. To establish a strong baseline, we employ the current state-of-the-art <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> to perform <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> collected for this work. We then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tools</a>. Our evaluation shows an improvement of more than 4 % when the structure reranking is applied, which is very promising.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jinho+D.+Choi" title="Search for 'Jinho D. Choi' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/t/tomasz-jurczyk/ class=align-middle>Tomasz Jurczyk</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/e/ethan-zhou/ class=align-middle>Ethan Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/han-he/ class=align-middle>Han He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/myungha-jang/ class=align-middle>Myungha Jang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-allan/ class=align-middle>James Allan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/b/bonggun-shin/ class=align-middle>Bonggun Shin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-lee/ class=align-middle>Timothy Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-d-finch/ class=align-middle>James D. Finch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sarah-e-finch/ class=align-middle>Sarah E. Finch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/payam-karisani/ class=align-middle>Payam Karisani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-xiong/ class=align-middle>Li Xiong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liyan-xu/ class=align-middle>Liyan Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xuchao-zhang/ class=align-middle>Xuchao Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xujiang-zhao/ class=align-middle>Xujiang Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haifeng-chen/ class=align-middle>Haifeng Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/feng-chen/ class=align-middle>Feng Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jin-zhao/ class=align-middle>Jin Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nianwen-xue/ class=align-middle>Nianwen Xue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jens-van-gysel/ class=align-middle>Jens Van Gysel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/henry-y-chen/ class=align-middle>Henry Y. Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuting-guo/ class=align-middle>Yuting Guo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhengzhe-yang/ class=align-middle>Zhengzhe Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaixin-ma/ class=align-middle>Kaixin Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwpt/ class=align-middle>IWPT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/nlp4convai/ class=align-middle>NLP4ConvAI</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/smm4h/ class=align-middle>SMM4H</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cmcl/ class=align-middle>CMCL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>