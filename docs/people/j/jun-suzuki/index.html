<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jun Suzuki - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jun</span> <span class=font-weight-bold>Suzuki</span></h2><hr><div class=row><div class=col-lg-9><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-srw.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-srw--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-srw.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928673 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-srw.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-srw.30/>Embeddings of Label Components for Sequence Labeling : A Case Study of Fine-grained Named Entity Recognition</a></strong><br><a href=/people/t/takuma-kato/>Takuma Kato</a>
|
<a href=/people/k/kaori-abe/>Kaori Abe</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/s/shumpei-miyawaki/>Shumpei Miyawaki</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/2020.acl-srw/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-srw--30><div class="card-body p-3 small">In general, the labels used in <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> consist of different types of elements. For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person). However, while most sequence labeling models do not consider such label components, the shared components across labels, such as <a href=https://en.wikipedia.org/wiki/Person_(disambiguation)>Person</a>, can be beneficial for label prediction. In this work, we propose to integrate label component information as <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> into <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Through experiments on English and Japanese fine-grained named entity recognition, we demonstrate that the proposed method improves performance, especially for instances with low-frequency labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939545 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.12/>Tohoku-AIP-NTT at WMT 2020 News Translation Task<span class=acl-fixed-case>AIP</span>-<span class=acl-fixed-case>NTT</span> at <span class=acl-fixed-case>WMT</span> 2020 News Translation Task</a></strong><br><a href=/people/s/shun-kiyono/>Shun Kiyono</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/r/ryuto-konno/>Ryuto Konno</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--12><div class="card-body p-3 small">In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT&#8217;20 news translation task. We participated in this task in two language pairs and four language directions : <a href=https://en.wikipedia.org/wiki/German_language>English German</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>English Japanese</a>. Our system consists of techniques such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, which are already widely adopted in translation tasks. We attempted to develop new <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for both synthetic data filtering and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>. However, the <a href=https://en.wikipedia.org/wiki/Scientific_method>methods</a> turned out to be ineffective, and they provided us with no significant improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>. We analyze these negative results to provide insights for future studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.443.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--443 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.443 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.443/>JParaCrawl : A Large Scale Web-Based English-Japanese Parallel Corpus<span class=acl-fixed-case>JP</span>ara<span class=acl-fixed-case>C</span>rawl: A Large Scale Web-Based <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>J</span>apanese Parallel Corpus</a></strong><br><a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--443><div class="card-body p-3 small">Recent machine translation algorithms mainly rely on <a href=https://en.wikipedia.org/wiki/Parallel_corpora>parallel corpora</a>. However, since the availability of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> remains limited, only some resource-rich language pairs can benefit from them. We constructed a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> for <a href=https://en.wikipedia.org/wiki/Standard_Japanese>English-Japanese</a>, for which the amount of publicly available parallel corpora is still limited. We constructed the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> by broadly crawling the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and automatically aligning parallel sentences. Our collected <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, called JParaCrawl, amassed over 8.7 million sentence pairs. We show how it includes a broader range of domains and how a neural machine translation model trained with it works as a good pre-trained model for fine-tuning specific domains. The pre-training and fine-tuning approaches achieved or surpassed performance comparable to model training from the initial state and reduced the training time. Additionally, we trained the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with an in-domain dataset and JParaCrawl to show how we achieved the best performance with them. JParaCrawl and the pre-trained models are freely available online for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.6.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939427 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.6/>Efficient Estimation of Influence of a Training Instance</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/2020.sustainlp-1/ class=text-muted>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--6><div class="card-body p-3 small">Understanding the influence of a training instance on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> leads to improving <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. However, it is difficult and inefficient to evaluate the influence, which shows how a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s prediction would be changed if a training instance were not used. In this paper, we propose an efficient <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for estimating the influence. Our method is inspired by dropout, which zero-masks a <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> and prevents the <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1379 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1379.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1379/>Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis</a></strong><br><a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1379><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a>, an unlabeled test set is used for <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>. Although this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> is underexplored in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Here we conduct an empirical study of <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3039 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3039/>TEASPN : Framework and Protocol for Integrated Writing Assistance Environments<span class=acl-fixed-case>TEASPN</span>: Framework and Protocol for Integrated Writing Assistance Environments</a></strong><br><a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/D19-3/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3039><div class="card-body p-3 small">Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with <a href=https://en.wikipedia.org/wiki/Writing_system>writing software</a>. We propose TEASPN, a <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> and an open-source framework for achieving integrated writing assistance environments. The <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> standardizes the way <a href=https://en.wikipedia.org/wiki/Computer-aided_software_engineering>writing software</a> communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> with low cost. As a result, users can enjoy the integrated experience in their favorite <a href=https://en.wikipedia.org/wiki/Writing_system>writing software</a>. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2185 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2185/>The Sally Smedley Hyperpartisan News Detector at SemEval-2019 Task 4<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 4</a></strong><br><a href=/people/k/kazuaki-hanawa/>Kazuaki Hanawa</a>
|
<a href=/people/s/shota-sasaki/>Shota Sasaki</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2185><div class="card-body p-3 small">This paper describes our system submitted to the formal run of SemEval-2019 Task 4 : Hyperpartisan news detection. Our system is based on a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> using several <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, i.e., 1) embedding features based on the pre-trained BERT embeddings, 2) article length features, and 3) embedding features of informative phrases extracted from by-publisher dataset. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved 80.9 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the test set for the formal run and got the 3rd place out of 42 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2605" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2605/>Annotating with Pros and Cons of Technologies in Computer Science Papers</a></strong><br><a href=/people/h/hono-shirai/>Hono Shirai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a><br><a href=/volumes/W19-26/ class=text-muted>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2605><div class="card-body p-3 small">This paper explores a task for extracting a technological expression and its pros / cons from computer science papers. We report ongoing efforts on an annotated corpus of pros / cons and an analysis of the nature of the automatic extraction task. Specifically, we show how to adapt the targeted sentiment analysis task for pros / cons extraction in computer science papers and conduct an annotation study. In order to identify the challenges of the automatic extraction task, we construct a strong baseline model and conduct an error analysis. The experiments show that pros / cons can be consistently annotated by several annotators, and that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging due to domain-specific knowledge. The annotated dataset is made publicly available for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.iwslt-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--iwslt-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.iwslt-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.iwslt-1.4/>ESPnet How2 Speech Translation System for IWSLT 2019 : Pre-training, Knowledge Distillation, and Going Deeper<span class=acl-fixed-case>ESP</span>net How2 Speech Translation System for <span class=acl-fixed-case>IWSLT</span> 2019: Pre-training, Knowledge Distillation, and Going Deeper</a></strong><br><a href=/people/h/hirofumi-inaguma/>Hirofumi Inaguma</a>
|
<a href=/people/s/shun-kiyono/>Shun Kiyono</a>
|
<a href=/people/n/nelson-enrique-yalta-soplin/>Nelson Enrique Yalta Soplin</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/s/shinji-watanabe/>Shinji Watanabe</a><br><a href=/volumes/2019.iwslt-1/ class=text-muted>Proceedings of the 16th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--iwslt-1--4><div class="card-body p-3 small">This paper describes the ESPnet submissions to the How2 Speech Translation task at IWSLT2019. In this year, we mainly build our systems based on Transformer architectures in all tasks and focus on the end-to-end speech translation (E2E-ST). We first compare RNN-based models and Transformer, and then confirm Transformer models significantly and consistently outperform RNN models in all tasks and corpora. Next, we investigate pre-training of E2E-ST models with the ASR and MT tasks. On top of the pre-training, we further explore knowledge distillation from the NMT model and the deeper speech encoder, and confirm drastic improvements over the baseline model. All of our codes are publicly available in ESPnet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1020.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383968561 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1020/>Effective Adversarial Regularization for Neural Machine Translation</a></strong><br><a href=/people/m/motoki-sato/>Motoki Sato</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/s/shun-kiyono/>Shun Kiyono</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1020><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization technique</a> based on adversarial perturbation, which was initially developed in the field of <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a>, has been successfully applied to text classification tasks and has yielded attractive improvements. We aim to further leverage this promising <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> into more sophisticated and critical neural models in the natural language processing field, i.e., <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT) models</a>. However, it is not trivial to apply this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Thus, this paper investigates the effectiveness of several possible configurations of applying the adversarial perturbation and reveals that the adversarial regularization technique can significantly and consistently improve the performance of widely used NMT models, such as LSTM-based and Transformer-based models.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1052/>Improving <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> by Incorporating Hierarchical Subword Features</a></strong><br><a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1052><div class="card-body p-3 small">This paper focuses on subword-based Neural Machine Translation (NMT). We hypothesize that in the NMT model, the appropriate subword units for the following three modules (layers) can differ : (1) the encoder embedding layer, (2) the decoder embedding layer, and (3) the decoder output layer. We find the <a href=https://en.wikipedia.org/wiki/Subword>subword</a> based on Sennrich et al. (2016) has a feature that a large vocabulary is a superset of a small vocabulary and modify the NMT model enables the incorporation of several different subword units in a single embedding layer. We refer these small subword features as hierarchical subword features. To empirically investigate our assumption, we compare the performance of several different subword units and hierarchical subword features for both the encoder and decoder embedding layers. We confirmed that incorporating hierarchical subword features in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> consistently improves BLEU scores on the IWSLT evaluation datasets.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-2008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-2008/>Input-to-Output Gate to Improve RNN Language Models<span class=acl-fixed-case>RNN</span> Language Models</a></strong><br><a href=/people/s/sho-takase/>Sho Takase</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2008><div class="card-body p-3 small">This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a> and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2044/>Improving Neural Text Normalization with Data Augmentation at Character- and Morphological Levels</a></strong><br><a href=/people/i/itsumi-saito/>Itsumi Saito</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/k/kugatsu-sadamitsu/>Kugatsu Sadamitsu</a>
|
<a href=/people/s/satoshi-kobashikawa/>Satoshi Kobashikawa</a>
|
<a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2044><div class="card-body p-3 small">In this study, we investigated the effectiveness of augmented data for encoder-decoder-based neural normalization models. Attention based encoder-decoder models are greatly effective in generating many natural languages. % such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> or machine summarization. In general, we have to prepare for a large amount of training data to train an encoder-decoder model. Unlike <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, there are few training data for text-normalization tasks. In this paper, we propose two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for generating augmented data. The experimental results with Japanese dialect normalization indicate that our methods are effective for an encoder-decoder model and achieve higher BLEU score than that of <a href=https://en.wikipedia.org/wiki/Baseline_(typography)>baselines</a>. We also investigated the oracle performance and revealed that there is sufficient room for improving an encoder-decoder model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1037/>Enumeration of Extractive Oracle Summaries</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1037><div class="card-body p-3 small">To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle summaries</a> for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following : (1) room still exists to improve the performance of extractive summarization ; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2047/>Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization</a></strong><br><a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2047><div class="card-body p-3 small">This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and control the output words based on the estimation in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jun+Suzuki" title="Search for 'Jun Suzuki' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kentaro-inui/ class=align-middle>Kentaro Inui</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/m/masaaki-nagata/ class=align-middle>Masaaki Nagata</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/m/makoto-morishita/ class=align-middle>Makoto Morishita</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hiroki-ouchi/ class=align-middle>Hiroki Ouchi</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/s/shun-kiyono/ class=align-middle>Shun Kiyono</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/t/takumi-ito/ class=align-middle>Takumi Ito</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sho-takase/ class=align-middle>Sho Takase</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/itsumi-saito/ class=align-middle>Itsumi Saito</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kyosuke-nishida/ class=align-middle>Kyosuke Nishida</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kugatsu-sadamitsu/ class=align-middle>Kugatsu Sadamitsu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satoshi-kobashikawa/ class=align-middle>Satoshi Kobashikawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryo-masumura/ class=align-middle>Ryo Masumura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuji-matsumoto/ class=align-middle>Yuji Matsumoto</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junji-tomita/ class=align-middle>Junji Tomita</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/takuma-kato/ class=align-middle>Takuma Kato</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaori-abe/ class=align-middle>Kaori Abe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shumpei-miyawaki/ class=align-middle>Shumpei Miyawaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masato-hagiwara/ class=align-middle>Masato Hagiwara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tatsuki-kuribayashi/ class=align-middle>Tatsuki Kuribayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kazuaki-hanawa/ class=align-middle>Kazuaki Hanawa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shota-sasaki/ class=align-middle>Shota Sasaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hono-shirai/ class=align-middle>Hono Shirai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/naoya-inoue/ class=align-middle>Naoya Inoue</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryuto-konno/ class=align-middle>Ryuto Konno</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hirofumi-inaguma/ class=align-middle>Hirofumi Inaguma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nelson-enrique-yalta-soplin/ class=align-middle>Nelson Enrique Yalta Soplin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-duh/ class=align-middle>Kevin Duh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shinji-watanabe/ class=align-middle>Shinji Watanabe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tsutomu-hirao/ class=align-middle>Tsutomu Hirao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/masaaki-nishino/ class=align-middle>Masaaki Nishino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/motoki-sato/ class=align-middle>Motoki Sato</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sosuke-kobayashi/ class=align-middle>Sosuke Kobayashi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sho-yokoi/ class=align-middle>Sho Yokoi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/sustainlp/ class=align-middle>sustainlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>