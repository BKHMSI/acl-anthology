<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Junchi Yan - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Junchi</span> <span class=font-weight-bold>Yan</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.19/>UniRE : A Unified Label Space for Entity Relation Extraction<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>RE</span>: A Unified Label Space for Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--19><div class="card-body p-3 small">Many joint entity relation extraction models setup two separated label spaces for the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity detection</a> and relation classification). We argue that this setting may hinder the <a href=https://en.wikipedia.org/wiki/Information_exchange>information interaction</a> between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks&#8217; label spaces. The input of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell&#8217;s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.251" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.251/>ENPAR : Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction<span class=acl-fixed-case>ENPAR</span>:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--251><div class="card-body p-3 small">Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019 ; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for these additional tasks such as <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity information</a> into the sentence encoder, we further utilize the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair information</a>. Specifically, we devise four novel <a href=https://en.wikipedia.org/wiki/Goal>objectives</a>, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity encoder</a> and an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair encoder</a>. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6011/>Pingan Smart Health and SJTU at COIN-Shared Task : utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks<span class=acl-fixed-case>SJTU</span> at <span class=acl-fixed-case>COIN</span> - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks</a></strong><br><a href=/people/x/xiepeng-li/>Xiepeng Li</a>
|
<a href=/people/z/zhexi-zhang/>Zhexi Zhang</a>
|
<a href=/people/w/wei-zhu/>Wei Zhu</a>
|
<a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/y/yuan-ni/>Yuan Ni</a>
|
<a href=/people/p/peng-gao/>Peng Gao</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a>
|
<a href=/people/g/guotong-xie/>Guotong Xie</a><br><a href=/volumes/D19-60/ class=text-muted>Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6011><div class="card-body p-3 small">To solve the shared tasks of COIN : COmmonsense INference in Natural Language Processing) Workshop in, we need explore the impact of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The <a href=https://en.wikipedia.org/wiki/First_law_of_thermodynamics>first</a> is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks ; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a> or multi-head attention. We find out that : (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly ; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1128/>Transductive Non-linear Learning for Chinese Hypernym Prediction<span class=acl-fixed-case>C</span>hinese Hypernym Prediction</a></strong><br><a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a>
|
<a href=/people/a/aoying-zhou/>Aoying Zhou</a>
|
<a href=/people/x/xiaofeng-he/>Xiaofeng He</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1128><div class="card-body p-3 small">Finding the correct hypernyms for entities is essential for <a href=https://en.wikipedia.org/wiki/Taxonomy_learning>taxonomy learning</a>, fine-grained entity categorization, <a href=https://en.wikipedia.org/wiki/Query_understanding>query understanding</a>, etc. Due to the flexibility of the <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese language</a>, it is challenging to identify <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> accurately. Rather than extracting <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> from texts, in this paper, we present a transductive learning approach to establish <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> from entities to <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> in the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Junchi+Yan" title="Search for 'Junchi Yan' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yijun-wang/ class=align-middle>Yijun Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/changzhi-sun/ class=align-middle>Changzhi Sun</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yuanbin-wu/ class=align-middle>Yuanbin Wu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hao-zhou/ class=align-middle>Hao Zhou</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lei-li/ class=align-middle>Lei Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/c/chengyu-wang/ class=align-middle>Chengyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aoying-zhou/ class=align-middle>Aoying Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaofeng-he/ class=align-middle>Xiaofeng He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiepeng-li/ class=align-middle>Xiepeng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhexi-zhang/ class=align-middle>Zhexi Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-zhu/ class=align-middle>Wei Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-li/ class=align-middle>Zheng Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-ni/ class=align-middle>Yuan Ni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/peng-gao/ class=align-middle>Peng Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guotong-xie/ class=align-middle>Guotong Xie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>