<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jonathan Berant - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jonathan</span> <span class=font-weight-bold>Berant</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.25/>Evaluating the Evaluation of Diversity in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>Natural Language Generation</a></a></strong><br><a href=/people/g/guy-tevet/>Guy Tevet</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--25><div class="card-body p-3 small">Despite growing interest in natural language generation (NLG) models that produce diverse outputs, there is currently no principled method for evaluating the diversity of an NLG system. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for evaluating diversity metrics. The framework measures the correlation between a proposed <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity metric</a> and a <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity parameter</a>, a single parameter that controls some aspect of <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> in generated text. For example, a diversity parameter might be a <a href=https://en.wikipedia.org/wiki/Binary_variable>binary variable</a> used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by : (a) establishing best practices for eliciting <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity judgments</a> from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> by tuning a decoding parameter mostly affect form but not meaning. Our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.<i>metrics</i>. The framework measures the correlation between a proposed diversity metric and a <i>diversity parameter</i>, a single parameter that controls some aspect of diversity in generated text. For example, a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by: (a) establishing best practices for eliciting diversity judgments from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling diversity by tuning a &#8220;decoding parameter&#8221; mostly affect form but not meaning. Our framework can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.115/>Achieving Model Robustness through Discrete Adversarial Training</a></strong><br><a href=/people/m/maor-ivgi/>Maor Ivgi</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--115><div class="card-body p-3 small">Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> has been limited to offline augmentation only. Concretely, given a trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, attacks are used to generate perturbed (adversarial) examples, and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every training step, adapting to the changing nature of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We propose (i) a new discrete attack, based on <a href=https://en.wikipedia.org/wiki/Best-first_search>best-first search</a>, and (ii) random sampling attacks that unlike prior work are not based on expensive search-based procedures. Surprisingly, we find that <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a> leads to impressive gains in <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training cost</a>, significantly improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> on three datasets. Last, we show that our new <a href=https://en.wikipedia.org/wiki/Attack_(computing)>attack</a> substantially improves <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> compared to <a href=https://en.wikipedia.org/wiki/Attack_(computing)>prior methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.646.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--646 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.646/>What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models<span class=acl-fixed-case>W</span>hat’s in Your Head? <span class=acl-fixed-case>E</span>mergent Behaviour in Multi-Task Transformer Models</a></strong><br><a href=/people/m/mor-geva/>Mor Geva</a>
|
<a href=/people/u/uri-katz/>Uri Katz</a>
|
<a href=/people/a/aviv-ben-arie/>Aviv Ben-Arie</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--646><div class="card-body p-3 small">The primary paradigm for multi-task training in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit <a href=https://en.wikipedia.org/wiki/Emergence>emergent behaviour</a>, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This <a href=https://en.wikipedia.org/wiki/Emergence>emergent behaviour</a> suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--spnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.spnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.spnlp-1.2.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.spnlp-1.2/>SmBoP : Semi-autoregressive Bottom-up Semantic Parsing<span class=acl-fixed-case>S</span>m<span class=acl-fixed-case>B</span>o<span class=acl-fixed-case>P</span>: Semi-autoregressive Bottom-up Semantic Parsing</a></strong><br><a href=/people/o/ohad-rubin/>Ohad Rubin</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/2021.spnlp-1/ class=text-muted>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--spnlp-1--2><div class="card-body p-3 small">The de-facto standard decoding method for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> in recent years has been to autoregressively decode the <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>abstract syntax tree</a> of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach : a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, <a href=https://en.wikipedia.org/wiki/Bottom-up_parsing>bottom-up parsing</a> allows to decode all <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>sub-trees</a> of a certain height in parallel, leading to <a href=https://en.wikipedia.org/wiki/Time_complexity>logarithmic runtime complexity</a> rather than <a href=https://en.wikipedia.org/wiki/Time_complexity>linear</a>. From a modeling perspective, a <a href=https://en.wikipedia.org/wiki/Bottom-up_parsing>bottom-up parser</a> learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+Grappa.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.225/>Improving Compositional Generalization in Semantic Parsing</a></strong><br><a href=/people/i/inbar-oren/>Inbar Oren</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/n/nitish-gupta/>Nitish Gupta</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--225><div class="card-body p-3 small">Generalization of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the <a href=https://en.wikipedia.org/wiki/Attention>attention module</a> of the <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>, aiming to improve compositional generalization. We find that the following factors improve compositional generalization : (a) using contextual representations, such as ELMo and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1378 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1378.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1378/>Global Reasoning over Database Structures for Text-to-SQL Parsing<span class=acl-fixed-case>SQL</span> Parsing</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1378><div class="card-body p-3 small">State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. % since their decisions are based on weak, local information only. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use <a href=https://en.wikipedia.org/wiki/Message_passing>message-passing</a> through a graph neural network to softly select a subset of <a href=https://en.wikipedia.org/wiki/Constant_(computer_programming)>database constants</a> for the output query, conditioned on the question. Moreover, we train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from 39.4 % to 47.4 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1139/>White-to-Black : Efficient Distillation of Black-Box Adversarial Attacks</a></strong><br><a href=/people/y/yotam-gil/>Yotam Gil</a>
|
<a href=/people/y/yoav-chai/>Yoav Chai</a>
|
<a href=/people/o/or-gorodissky/>Or Gorodissky</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1139><div class="card-body p-3 small">Adversarial examples are important for understanding the behavior of neural models, and can improve their <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> through adversarial training. Recent work in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization procedure</a> can be distilled into another more efficient <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability. Our attack flips the API-predicted label in 42 % of the generated examples, while humans maintain high-accuracy in predicting the gold label.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1193/>Value-based Search in Execution Space for Mapping Instructions to Programs</a></strong><br><a href=/people/d/dor-muhlgay/>Dor Muhlgay</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1193><div class="card-body p-3 small">Training models to map natural language instructions to <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a>, given target world supervision only, requires searching for good programs at training time. Search is commonly done using <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> in the space of partial programs or program trees, but as the length of the instructions grows finding a good program becomes difficult. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a> that uses the target world state, known at training time, to train a critic network that predicts the expected reward of every search state. We then score search states on the beam by interpolating their expected reward with the likelihood of programs represented by the search state. Moreover, we search not in the space of programs but in a more compressed state of program executions, augmented with recent entities and actions. On the SCONE dataset, we show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> dramatically improves performance on all three domains compared to standard <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1448 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1448.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1448" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1448/>Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing<span class=acl-fixed-case>SQL</span> Parsing</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1448><div class="card-body p-3 small">Research on <a href=https://en.wikipedia.org/wiki/Parsing>parsing language</a> to <a href=https://en.wikipedia.org/wiki/SQL>SQL</a> has largely ignored the structure of the <a href=https://en.wikipedia.org/wiki/Database_schema>database (DB) schema</a>, either because the <a href=https://en.wikipedia.org/wiki/Database>DB</a> was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the <a href=https://en.wikipedia.org/wiki/Database_schema>DB schema</a> can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the <a href=https://en.wikipedia.org/wiki/Database_schema>DB schema</a> is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our <a href=https://en.wikipedia.org/wiki/Parsing>parser accuracy</a> from 33.8 % to 39.4 %, dramatically above the current state of the art, which is at 19.7 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K19-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K19-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-K19-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K19-1042/>On the Limits of Learning to Actively Learn Semantic Representations</a></strong><br><a href=/people/o/omri-koshorek/>Omri Koshorek</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/y/yichu-zhou/>Yichu Zhou</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/K19-1/ class=text-muted>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K19-1042><div class="card-body p-3 small">One of the goals of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> is to develop <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that map sentences into meaning representations. However, training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> requires expensive annotation of complex structures, which hinders their adoption. Learning to actively-learn(LTAL) is a recent paradigm for reducing the amount of labeled data by learning a policy that selects which samples should be labeled. In this work, we examine <a href=https://en.wikipedia.org/wiki/LTAL>LTAL</a> for learning semantic representations, such as QA-SRL. We show that even an oracle policy that is allowed to pick examples that maximize performance on the test set (and constitutes an upper bound on the potential of LTAL), does not substantially improve performance compared to a random policy. We investigate factors that could explain this finding and show that a distinguishing characteristic of successful applications of LTAL is the interaction between <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> and the oracle policy selection process. In successful applications of LTAL, the examples selected by the oracle policy do not substantially depend on the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization procedure</a>, while in our setup the stochastic nature of <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a> in learning semantic meaning representations is limited.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1190 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1190/>Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing</a></strong><br><a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1190><div class="card-body p-3 small">Building a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> quickly in a new domain is a fundamental challenge for conversational interfaces, as current semantic parsers require expensive supervision and lack the ability to generalize to new domains. In this paper, we introduce a zero-shot approach to <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> that can parse utterances in unseen domains while only being trained on examples in other source domains. First, we map an utterance to an abstract, domain independent, logical form that represents the structure of the logical form, but contains slots instead of KB constants. Then, we replace slots with KB constants via lexical alignment scores and global inference. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> reaches an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>average accuracy</a> of 53.4 % on 7 domains in the OVERNIGHT dataset, substantially better than other zero-shot baselines, and performs as good as a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> trained on over 30 % of the target domain examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2000/>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2600/>Proceedings of the Workshop on Machine Reading for Question Answering</a></strong><br><a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/W18-26/ class=text-muted>Proceedings of the Workshop on Machine Reading for Question Answering</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1168.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1168.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285804766 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-1168/>Weakly Supervised Semantic Parsing with Abstract Examples</a></strong><br><a href=/people/o/omer-goldman/>Omer Goldman</a>
|
<a href=/people/v/veronica-latcinnik/>Veronica Latcinnik</a>
|
<a href=/people/e/ehud-nave/>Ehud Nave</a>
|
<a href=/people/a/amir-globerson/>Amir Globerson</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1168><div class="card-body p-3 small">Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> needs to be explored at training time to find a correct <a href=https://en.wikipedia.org/wiki/Computer_program>program</a>. Second, spurious programs that accidentally lead to a correct <a href=https://en.wikipedia.org/wiki/Denotation>denotation</a> add <a href=https://en.wikipedia.org/wiki/Noise>noise</a> to <a href=https://en.wikipedia.org/wiki/Training>training</a>. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> substantially improves performance, and reaches 82.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, a 14.7 % absolute accuracy improvement compared to the best reported <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> so far.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1003.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953110 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1003/>Neural Symbolic Machines : Learning Semantic Parsers on <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> with Weak Supervision<span class=acl-fixed-case>F</span>reebase with Weak Supervision</a></strong><br><a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/q/quoc-le/>Quoc Le</a>
|
<a href=/people/k/kenneth-forbus/>Kenneth D. Forbus</a>
|
<a href=/people/n/ni-lao/>Ni Lao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1003><div class="card-body p-3 small">Harnessing the statistical power of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to perform <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic reasoning</a> is difficult, when it requires executing efficient <a href=https://en.wikipedia.org/wiki/Discrete_mathematics>discrete operations</a> against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural programmer, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic computer</a>, i.e., a <a href=https://en.wikipedia.org/wiki/Lisp_(programming_language)>Lisp interpreter</a> that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the <a href=https://en.wikipedia.org/wiki/Stability_theory>stability</a> of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> or domain-specific knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S17-1020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S17-1020/>Evaluating <a href=https://en.wikipedia.org/wiki/Semantic_parsing>Semantic Parsing</a> against a Simple Web-based Question Answering Model</a></strong><br><a href=/people/a/alon-talmor/>Alon Talmor</a>
|
<a href=/people/m/mor-geva/>Mor Geva</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a><br><a href=/volumes/S17-1/ class=text-muted>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-1020><div class="card-body p-3 small">Semantic parsing shines at analyzing complex natural language that involves <a href=https://en.wikipedia.org/wiki/Composition_(language)>composition</a> and computation over multiple pieces of evidence. However, <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> contain many factoid questions that can be answered from a single <a href=https://en.wikipedia.org/wiki/Web_page>web document</a>. In this paper, we propose to evaluate semantic parsing-based question answering models by comparing them to a question answering baseline that queries the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and extracts the answer only from <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web snippets</a>, without access to the target knowledge-base. We investigate this approach on COMPLEXQUESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs well on complex questions involving <a href=https://en.wikipedia.org/wiki/Logical_conjunction>conjunctions</a>, but struggles on questions that involve relation composition and <a href=https://en.wikipedia.org/wiki/Superlative>superlatives</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jonathan+Berant" title="Search for 'Jonathan Berant' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/j/jonathan-herzig/ class=align-middle>Jonathan Herzig</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/matt-gardner/ class=align-middle>Matt Gardner</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/mor-geva/ class=align-middle>Mor Geva</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/ben-bogin/ class=align-middle>Ben Bogin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chen-liang/ class=align-middle>Chen Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/q/quoc-le/ class=align-middle>Quoc Le</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kenneth-forbus/ class=align-middle>Kenneth Forbus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/ni-lao/ class=align-middle>Ni Lao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guy-tevet/ class=align-middle>Guy Tevet</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maor-ivgi/ class=align-middle>Maor Ivgi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/uri-katz/ class=align-middle>Uri Katz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aviv-ben-arie/ class=align-middle>Aviv Ben-Arie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alon-talmor/ class=align-middle>Alon Talmor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/inbar-oren/ class=align-middle>Inbar Oren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nitish-gupta/ class=align-middle>Nitish Gupta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malvina-nissim/ class=align-middle>Malvina Nissim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alessandro-lenci/ class=align-middle>Alessandro Lenci</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eunsol-choi/ class=align-middle>Eunsol Choi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minjoon-seo/ class=align-middle>Minjoon Seo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/danqi-chen/ class=align-middle>Danqi Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robin-jia/ class=align-middle>Robin Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yotam-gil/ class=align-middle>Yotam Gil</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yoav-chai/ class=align-middle>Yoav Chai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/or-gorodissky/ class=align-middle>Or Gorodissky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dor-muhlgay/ class=align-middle>Dor Muhlgay</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ohad-rubin/ class=align-middle>Ohad Rubin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/omer-goldman/ class=align-middle>Omer Goldman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/veronica-latcinnik/ class=align-middle>Veronica Latcinnik</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ehud-nave/ class=align-middle>Ehud Nave</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amir-globerson/ class=align-middle>Amir Globerson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/omri-koshorek/ class=align-middle>Omri Koshorek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gabriel-stanovsky/ class=align-middle>Gabriel Stanovsky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichu-zhou/ class=align-middle>Yichu Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivek-srikumar/ class=align-middle>Vivek Srikumar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/spnlp/ class=align-middle>spnlp</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>