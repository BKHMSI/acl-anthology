<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jason Eisner - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jason</span> <span class=font-weight-bold>Eisner</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--405 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.405/>Limitations of <a href=https://en.wikipedia.org/wiki/Autoregressive_model>Autoregressive Models</a> and Their Alternatives</a></strong><br><a href=/people/c/chu-cheng-lin/>Chu-Cheng Lin</a>
|
<a href=/people/a/aaron-jaech/>Aaron Jaech</a>
|
<a href=/people/x/xin-li/>Xin Li</a>
|
<a href=/people/m/matthew-r-gormley/>Matthew R. Gormley</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--405><div class="card-body p-3 small">Standard autoregressive language models perform only <a href=https://en.wikipedia.org/wiki/Time_complexity>polynomial-time computation</a> to compute the probability of the next symbol. While this is attractive, it means they can not model <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a> whose next-symbol probability is hard to compute. Indeed, they can not even model them well enough to solve associated easy decision problems for which an engineer might want to consult a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. These limitations apply no matter how much computation and data are used to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, unless the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is given access to <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle parameters</a> that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.<i>hard</i> to compute. Indeed, they cannot even model them well enough to solve associated <i>easy</i> decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow <i>superpolynomially</i> in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--410 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Short Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.410.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code">
<i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.410.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.410" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.410/>Learning How to Ask : Querying LMs with Mixtures of Soft Prompts<span class=acl-fixed-case>LM</span>s with Mixtures of Soft Prompts</a></strong><br><a href=/people/g/guanghui-qin/>Guanghui Qin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--410><div class="card-body p-3 small">Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> retain factual knowledge from their training corpora that can be extracted by asking them to fill in the blank in a sentential prompt. However, where does this prompt come from? We explore the idea of learning <a href=https://en.wikipedia.org/wiki/Command-line_interface>prompts</a> by gradient descenteither fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of soft words, i.e., <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous vectors</a> that are not necessarily word type embeddings from the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Furthermore, for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we optimize a mixture of <a href=https://en.wikipedia.org/wiki/Command_(computing)>prompts</a>, learning which <a href=https://en.wikipedia.org/wiki/Command_(computing)>prompts</a> are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> was previously underestimated. Moreover, this knowledge is cheap to elicit : random initialization is nearly as good as informed initialization.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3900/>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a></strong><br><a href=/people/j/jason-eisner/>Jason Eisner</a>
|
<a href=/people/m/matthias-galle/>Matthias Gallé</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a>
|
<a href=/people/a/ariadna-quattoni/>Ariadna Quattoni</a>
|
<a href=/people/g/guillaume-rabusseau/>Guillaume Rabusseau</a><br><a href=/volumes/W19-39/ class=text-muted>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264026 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1203/>Contextualization of Morphological Inflection</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1203><div class="card-body p-3 small">Critical to <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> is the production of correctly inflected text. In this paper, we isolate the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> or surface realization, our task input does not provide gold tags that specify what morphological features to realize on each lemmatized word ; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> before predicting the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a>, and compare this to a system that directly predicts the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q18-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q18-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q18-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q18-1046/>Surface Statistics of an Unknown Language Indicate How to Parse It</a></strong><br><a href=/people/d/dingquan-wang/>Dingquan Wang</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/Q18-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 6</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q18-1046><div class="card-body p-3 small">We introduce a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for delexicalized dependency parsing in a <a href=https://en.wikipedia.org/wiki/Programming_language>new language</a>. We show that useful features of the target language can be extracted automatically from an unparsed corpus, which consists only of gold part-of-speech (POS) sequences. Providing these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to our neural parser enables <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to parse sequences like those in the corpus. Strikingly, our <a href=https://en.wikipedia.org/wiki/System>system</a> has no supervision in the target language. Rather, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is a multilingual system that is trained end-to-end on a variety of other languages, so <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> learns a <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a> that works well. We show experimentally across multiple languages : (1) Features computed from the unparsed corpus improve parsing accuracy. (2) Including thousands of <a href=https://en.wikipedia.org/wiki/Synthetic_language>synthetic languages</a> in the training yields further improvement. (3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work&#8217;s interpretable typological features that require parsed corpora or expert categorization of the language. Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 20.7 points over past grammar induction work that does not use training languages (Naseem et al., 2010).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276386708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1004/>A Deep Generative Model of Vowel Formant Typology</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1004><div class="card-body p-3 small">What makes some types of <a href=https://en.wikipedia.org/wiki/Language>languages</a> more probable than others? For instance, we know that almost all <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken languages</a> contain the <a href=https://en.wikipedia.org/wiki/Vowel>vowel phoneme</a> /i/ ; why should that be? The field of <a href=https://en.wikipedia.org/wiki/Linguistic_typology>linguistic typology</a> seeks to answer these questions and, thereby, divine the mechanisms that underlie <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. In our work, we tackle the problem of vowel system typology, i.e., we propose a <a href=https://en.wikipedia.org/wiki/Generative_model>generative probability model</a> of which vowels a language contains. In contrast to previous work, we work directly with the acoustic informationthe first two formant valuesrather than modeling discrete sets of symbols from the <a href=https://en.wikipedia.org/wiki/International_Phonetic_Alphabet>international phonetic alphabet</a>. We develop a novel generative probability model and report results on over 200 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-1085.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1085/>Neural Particle Smoothing for Sampling from Conditional Sequence Models</a></strong><br><a href=/people/c/chu-cheng-lin/>Chu-Cheng Lin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1085><div class="card-body p-3 small">We introduce neural particle smoothing, a <a href=https://en.wikipedia.org/wiki/Sequential_Monte_Carlo_method>sequential Monte Carlo method</a> for sampling annotations of an input string from a given <a href=https://en.wikipedia.org/wiki/Probability_model>probability model</a>. In contrast to conventional particle filtering algorithms, we train a proposal distribution that looks ahead to the end of the input string by means of a right-to-left LSTM. We demonstrate that this <a href=https://en.wikipedia.org/wiki/Innovation>innovation</a> can improve the quality of the sample. To motivate our formal choices, we explain how neural transduction models and our <a href=https://en.wikipedia.org/wiki/Sampler_(musical_instrument)>sampler</a> can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1095.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955407 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1095/>Bayesian Modeling of Lexical Resources for Low-Resource Settings<span class=acl-fixed-case>B</span>ayesian Modeling of Lexical Resources for Low-Resource Settings</a></strong><br><a href=/people/n/nicholas-andrews/>Nicholas Andrews</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1095><div class="card-body p-3 small">Lexical resources such as <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteers</a> are often used as auxiliary data for tasks such as part-of-speech induction and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a>. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> which generalize better. In this paper, we investigate a more robust approach : we stipulate that the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a>. The lexical resources provide training data for the <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings : part-of-speech induction and low-resource named-entity recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959176 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1109/>Probabilistic Typology : Deep Generative Models of Vowel Inventories</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1109><div class="card-body p-3 small">Linguistic typology studies the range of structures present in <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while mostbut not alllanguages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology : What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2028/>Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2028><div class="card-body p-3 small">The popular skip-gram model induces <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> by exploiting the signal from word-context coocurrence. We offer a new interpretation of <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> based on exponential family PCA-a form of matrix factorization to generalize the <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram model</a> to tensor factorization. In turn, this lets us train <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological information</a> (to share parameters across related words). We experiment on 40 languages and show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves upon <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jason+Eisner" title="Search for 'Jason Eisner' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/r/ryan-cotterell/ class=align-middle>Ryan Cotterell</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/b/benjamin-van-durme/ class=align-middle>Benjamin Van Durme</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/chu-cheng-lin/ class=align-middle>Chu-Cheng Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nicholas-andrews/ class=align-middle>Nicholas Andrews</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mark-dredze/ class=align-middle>Mark Dredze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/dingquan-wang/ class=align-middle>Dingquan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aaron-jaech/ class=align-middle>Aaron Jaech</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-li/ class=align-middle>Xin Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthew-r-gormley/ class=align-middle>Matthew R. Gormley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guanghui-qin/ class=align-middle>Guanghui Qin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matthias-galle/ class=align-middle>Matthias Gallé</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jeffrey-heinz/ class=align-middle>Jeffrey Heinz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ariadna-quattoni/ class=align-middle>Ariadna Quattoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guillaume-rabusseau/ class=align-middle>Guillaume Rabusseau</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ekaterina-vylomova/ class=align-middle>Ekaterina Vylomova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/trevor-cohn/ class=align-middle>Trevor Cohn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-baldwin/ class=align-middle>Timothy Baldwin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/adam-poliak/ class=align-middle>Adam Poliak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>