<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Joakim Nivre - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Joakim</span> <span class=font-weight-bold>Nivre</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.264/>Attention Can Reflect <a href=https://en.wikipedia.org/wiki/Syntactic_structure>Syntactic Structure</a> (If You Let It)</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/a/artur-kulmizev/>Artur Kulmizev</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--264><div class="card-body p-3 small">Since the popularization of the Transformer as a general-purpose feature encoder for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on English a language with <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>rigid word order</a> and a lack of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>inflectional morphology</a>. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.udw-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.udw-1.0/>Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</a></strong><br><a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a><br><a href=/volumes/2020.udw-1/ class=text-muted>Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6200/>Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing</a></strong><br><a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/b/bjorn-lindi/>Bjørn Lindi</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/j/jorg-tidemann/>Jörg Tidemann</a><br><a href=/volumes/W19-62/ class=text-muted>Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1159.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364704101 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1159/>Recursive Subtree Composition in LSTM-Based Dependency Parsing<span class=acl-fixed-case>LSTM</span>-Based Dependency Parsing</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1159><div class="card-body p-3 small">The need for tree structure modelling on top of sequence modelling is an open issue in neural dependency parsing. We investigate the impact of adding a <a href=https://en.wikipedia.org/wiki/Tree_layer>tree layer</a> on top of a <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a> by recursively composing subtree representations (composition) in a transition-based parser that uses features extracted by a BiLSTM. Composition seems superfluous with such a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, suggesting that BiLSTMs capture information about <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>subtrees</a>. We perform model ablations to tease out the conditions under which <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> helps. When ablating the backward LSTM, performance drops and <a href=https://en.wikipedia.org/wiki/Musical_composition>composition</a> does not recover much of the gap. When ablating the forward LSTM, performance drops less dramatically and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> recovers a substantial part of the gap, indicating that a forward LSTM and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> capture similar information. We take the backward LSTM to be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved <a href=https://en.wikipedia.org/wiki/Ahead-of-time_compilation>lookahead</a> of a backward LSTM is especially important for head-final languages.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1112/>An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/f/fabienne-cap/>Fabienne Cap</a>
|
<a href=/people/e/eva-pettersson/>Eva Pettersson</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1112><div class="card-body p-3 small">In this paper, we apply different NMT models to the problem of historical spelling normalization for five languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, <a href=https://en.wikipedia.org/wiki/Icelandic_language>Icelandic</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. The NMT models are at different levels, have different <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, and different neural network architectures. Our results show that NMT models are much better than SMT models in terms of character error rate. The vanilla RNNs are competitive to GRUs / LSTMs in historical spelling normalization. Transformer models perform better only when provided with more training data. We also find that subword-level models with a small subword vocabulary are better than character-level models. In addition, we propose a hybrid method which further improves the performance of historical spelling normalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6003/>Expletives in Universal Dependency Treebanks<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebanks</a></strong><br><a href=/people/g/gosse-bouma/>Gosse Bouma</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/d/dag-haug/>Dag Haug</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/p/per-erik-solberg/>Per Erik Solberg</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a><br><a href=/volumes/W18-60/ class=text-muted>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6003><div class="card-body p-3 small">Although treebanks annotated according to the guidelines of Universal Dependencies (UD) now exist for many languages, the goal of annotating the same phenomena in a cross-linguistically consistent fashion is not always met. In this paper, we investigate one phenomenon where we believe such consistency is lacking, namely expletive elements. Such elements occupy a position that is structurally associated with a <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>core argument</a> (or sometimes an oblique dependent), yet are non-referential and semantically void. Many UD treebanks identify at least some elements as expletive, but the range of phenomena differs between treebanks, even for closely related languages, and sometimes even for different treebanks for the same language. In this paper, we present criteria for identifying <a href=https://en.wikipedia.org/wiki/Profanity>expletives</a> that are applicable across languages and compatible with the goals of UD, give an overview of <a href=https://en.wikipedia.org/wiki/Profanity>expletives</a> as found in current UD treebanks, and present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6012/>Enhancing Universal Dependency Treebanks : A Case Study<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebanks: A Case Study</a></strong><br><a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/p/paola-marongiu/>Paola Marongiu</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/s/simonetta-montemagni/>Simonetta Montemagni</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a><br><a href=/volumes/W18-60/ class=text-muted>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6012><div class="card-body p-3 small">We evaluate two cross-lingual techniques for adding enhanced dependencies to existing <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> in Universal Dependencies. We apply a rule-based system developed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and a data-driven system trained on <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> to <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>. We find that both systems are accurate enough to bootstrap enhanced dependencies in existing UD treebanks. In the case of <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, results are even on par with those of a prototype language-specific system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6304/>An Analysis of Attention Mechanisms : The Case of <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a> in Neural Machine Translation</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/W18-63/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Research Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6304><div class="card-body p-3 small">Recent work has shown that the encoder-decoder attention mechanisms in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> are different from the <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> pay more attention to <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context tokens</a> when translating <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous words</a>. We explore the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distribution patterns</a> when translating ambiguous nouns. Counterintuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is not the main mechanism used by NMT models to incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for WSD. The experimental results suggest that NMT models learn to encode <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to align source and target tokens and the last few layers learn to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from the related but unaligned context tokens.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K18-2011/>82 Treebanks, 34 Models : Universal Dependency Parsing with Multi-Treebank Models<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing with Multi-Treebank Models</a></strong><br><a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a><br><a href=/volumes/K18-2/ class=text-muted>Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K18-2011><div class="card-body p-3 small">We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components : the first performs joint word and sentence segmentation ; the second predicts <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> ; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>, we trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with multiple treebanks for one language or closely related languages, greatly reducing the number of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, universal POS tagging, and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-2098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-2098 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-2098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-2098.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-2098" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-2098/>Parser Training with Heterogeneous Treebanks</a></strong><br><a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/aaron-smith/>Aaron Smith</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/P18-2/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-2098><div class="card-body p-3 small">How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We go on to propose a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on treebank embeddings. We perform experiments for several languages and show that in many cases <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and treebank embeddings lead to substantial improvements over single treebanks or <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a>, with average gains of 2.03.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1018.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1018/>Character-based Joint Segmentation and POS Tagging for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> using Bidirectional RNN-CRF<span class=acl-fixed-case>POS</span> Tagging for <span class=acl-fixed-case>C</span>hinese using Bidirectional <span class=acl-fixed-case>RNN</span>-<span class=acl-fixed-case>CRF</span></a></strong><br><a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1018><div class="card-body p-3 small">We present a character-based model for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and lower-than-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is accurate and robust across <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in different sizes, genres and <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a>. We obtain state-of-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2015/>Recall is the Proper Evaluation Metric for <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a></a></strong><br><a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2015><div class="card-body p-3 small">We extensively analyse the correlations and drawbacks of conventionally employed evaluation metrics for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. Unlike in standard <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> favours under-splitting systems and therefore can be misleading in <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. Overall, based on both theoretical and experimental analysis, we propose that precision should be excluded from the standard evaluation metrics and that the evaluation score obtained by using only <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> is sufficient and better correlated with the performance of word segmentation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-3001/>CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span> 2017 Shared Task: Multilingual Parsing from Raw Text to <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/d/daniel-zeman/>Daniel Zeman</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/m/milan-straka/>Milan Straka</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/j/juhani-luotolahti/>Juhani Luotolahti</a>
|
<a href=/people/s/sampo-pyysalo/>Sampo Pyysalo</a>
|
<a href=/people/s/slav-petrov/>Slav Petrov</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/e/elena-badmaeva/>Elena Badmaeva</a>
|
<a href=/people/m/memduh-gokirmak/>Memduh Gokirmak</a>
|
<a href=/people/a/anna-nedoluzhko/>Anna Nedoluzhko</a>
|
<a href=/people/s/silvie-cinkova/>Silvie Cinková</a>
|
<a href=/people/j/jan-hajic-jr/>Jan Hajič jr.</a>
|
<a href=/people/j/jaroslava-hlavacova/>Jaroslava Hlaváčová</a>
|
<a href=/people/v/vaclava-kettnerova/>Václava Kettnerová</a>
|
<a href=/people/z/zdenka-uresova/>Zdeňka Urešová</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/s/stina-ojala/>Stina Ojala</a>
|
<a href=/people/a/anna-missila/>Anna Missilä</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/d/dima-taji/>Dima Taji</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/herman-leung/>Herman Leung</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/m/manuela-sanguinetti/>Manuela Sanguinetti</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a>
|
<a href=/people/h/hiroshi-kanayama/>Hiroshi Kanayama</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a>
|
<a href=/people/k/kira-droganova/>Kira Droganova</a>
|
<a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/h/hans-uszkoreit/>Hans Uszkoreit</a>
|
<a href=/people/v/vivien-macketanz/>Vivien Macketanz</a>
|
<a href=/people/a/aljoscha-burchardt/>Aljoscha Burchardt</a>
|
<a href=/people/k/kim-harris/>Kim Harris</a>
|
<a href=/people/k/katrin-marheinecke/>Katrin Marheinecke</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/t/tolga-kayadelen/>Tolga Kayadelen</a>
|
<a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/a/ali-elkahky/>Ali Elkahky</a>
|
<a href=/people/z/zhuoran-yu/>Zhuoran Yu</a>
|
<a href=/people/e/emily-pitler/>Emily Pitler</a>
|
<a href=/people/s/saran-lertpradit/>Saran Lertpradit</a>
|
<a href=/people/m/michael-mandel/>Michael Mandl</a>
|
<a href=/people/j/jesse-kirchner/>Jesse Kirchner</a>
|
<a href=/people/h/hector-fernandez-alcalde/>Hector Fernandez Alcalde</a>
|
<a href=/people/j/jana-strnadova/>Jana Strnadová</a>
|
<a href=/people/e/esha-banerjee/>Esha Banerjee</a>
|
<a href=/people/r/ruli-manurung/>Ruli Manurung</a>
|
<a href=/people/a/antonio-stella/>Antonio Stella</a>
|
<a href=/people/a/atsuko-shimada/>Atsuko Shimada</a>
|
<a href=/people/s/sookyoung-kwak/>Sookyoung Kwak</a>
|
<a href=/people/g/gustavo-mendonca/>Gustavo Mendonça</a>
|
<a href=/people/t/tatiana-lando/>Tatiana Lando</a>
|
<a href=/people/r/rattima-nitisaroj/>Rattima Nitisaroj</a>
|
<a href=/people/j/josie-li/>Josie Li</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3001><div class="card-body p-3 small">The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their <a href=https://en.wikipedia.org/wiki/Machine_learning>learning systems</a> on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All <a href=https://en.wikipedia.org/wiki/Test_set>test sets</a> followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0400/>Proceedings of the <span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>D</span>a<span class=acl-fixed-case>L</span>i<span class=acl-fixed-case>D</span>a 2017 Workshop on Universal Dependencies (<span class=acl-fixed-case>UDW</span> 2017)</a></strong><br><a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a><br><a href=/volumes/W17-04/ class=text-muted>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-6314.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-6314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-6314/>Arc-Hybrid Non-Projective Dependency Parsing with a Static-Dynamic Oracle</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/W17-63/ class=text-muted>Proceedings of the 15th International Conference on Parsing Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6314><div class="card-body p-3 small">In this paper, we extend the arc-hybrid system for transition-based parsing with a swap transition that enables reordering of the words and construction of non-projective trees. Although this extension breaks the arc-decomposability of the transition system, we show how the existing dynamic oracle for this <a href=https://en.wikipedia.org/wiki/System>system</a> can be modified and combined with a static oracle only for the swap transition. Experiments on 5 languages show that the new <a href=https://en.wikipedia.org/wiki/System>system</a> gives competitive accuracy and is significantly better than a <a href=https://en.wikipedia.org/wiki/System>system</a> trained with a purely static oracle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6500/>Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017)</a></strong><br><a href=/people/s/simonetta-montemagni/>Simonetta Montemagni</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/W17-65/ class=text-muted>Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-5001/>Universal Dependencies<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/d/daniel-zeman/>Daniel Zeman</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a><br><a href=/volumes/E17-5/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-5001><div class="card-body p-3 small">Universal Dependencies (UD) is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages. This tutorial gives an introduction to the UD framework and resources, from basic design principles to annotation guidelines and existing <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. We also discuss tools for developing and exploiting UD treebanks and survey applications of UD in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Joakim+Nivre" title="Search for 'Joakim Nivre' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/miryam-de-lhoneux/ class=align-middle>Miryam de Lhoneux</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/s/sebastian-schuster/ class=align-middle>Sebastian Schuster</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/f/filip-ginter/ class=align-middle>Filip Ginter</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/marie-catherine-de-marneffe/ class=align-middle>Marie-Catherine de Marneffe</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yan-shao/ class=align-middle>Yan Shao</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/sara-stymne/ class=align-middle>Sara Stymne</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/gongbo-tang/ class=align-middle>Gongbo Tang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/christian-hardmeier/ class=align-middle>Christian Hardmeier</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/daniel-zeman/ class=align-middle>Daniel Zeman</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jan-hajic/ class=align-middle>Jan Hajic</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/francis-tyers/ class=align-middle>Francis Tyers</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jenna-kanerva/ class=align-middle>Jenna Kanerva</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/maria-simi/ class=align-middle>Maria Simi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/simonetta-montemagni/ class=align-middle>Simonetta Montemagni</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/aaron-smith/ class=align-middle>Aaron Smith</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/f/fabienne-cap/ class=align-middle>Fabienne Cap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eva-pettersson/ class=align-middle>Eva Pettersson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jorg-tiedemann/ class=align-middle>Jörg Tiedemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-popel/ class=align-middle>Martin Popel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/milan-straka/ class=align-middle>Milan Straka</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/juhani-luotolahti/ class=align-middle>Juhani Luotolahti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sampo-pyysalo/ class=align-middle>Sampo Pyysalo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/slav-petrov/ class=align-middle>Slav Petrov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martin-potthast/ class=align-middle>Martin Potthast</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elena-badmaeva/ class=align-middle>Elena Badmaeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/memduh-gokirmak/ class=align-middle>Memduh Gökırmak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-nedoluzhko/ class=align-middle>Anna Nedoluzhko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/silvie-cinkova/ class=align-middle>Silvie Cinková</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-hajic-jr/ class=align-middle>Jan Hajič jr.</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jaroslava-hlavacova/ class=align-middle>Jaroslava Hlaváčová</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vaclava-kettnerova/ class=align-middle>Václava Kettnerová</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zdenka-uresova/ class=align-middle>Zdenka Uresova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stina-ojala/ class=align-middle>Stina Ojala</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-missila/ class=align-middle>Anna Missilä</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christopher-d-manning/ class=align-middle>Christopher D. Manning</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/siva-reddy/ class=align-middle>Siva Reddy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dima-taji/ class=align-middle>Dima Taji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nizar-habash/ class=align-middle>Nizar Habash</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/herman-leung/ class=align-middle>Herman Leung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/manuela-sanguinetti/ class=align-middle>Manuela Sanguinetti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hiroshi-kanayama/ class=align-middle>Hiroshi Kanayama</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/valeria-de-paiva/ class=align-middle>Valeria de Paiva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kira-droganova/ class=align-middle>Kira Droganova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hector-martinez-alonso/ class=align-middle>Héctor Martínez Alonso</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cagri-coltekin/ class=align-middle>Çağrı Çöltekin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/umut-sulubacak/ class=align-middle>Umut Sulubacak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hans-uszkoreit/ class=align-middle>Hans Uszkoreit</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vivien-macketanz/ class=align-middle>Vivien Macketanz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aljoscha-burchardt/ class=align-middle>Aljoscha Burchardt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kim-harris/ class=align-middle>Kim Harris</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katrin-marheinecke/ class=align-middle>Katrin Marheinecke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/georg-rehm/ class=align-middle>Georg Rehm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tolga-kayadelen/ class=align-middle>Tolga Kayadelen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohammed-attia/ class=align-middle>Mohammed Attia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ali-elkahky/ class=align-middle>Ali Elkahky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhuoran-yu/ class=align-middle>Zhuoran Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-pitler/ class=align-middle>Emily Pitler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saran-lertpradit/ class=align-middle>Saran Lertpradit</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-mandel/ class=align-middle>Michael Mandel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jesse-kirchner/ class=align-middle>Jesse Kirchner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hector-fernandez-alcalde/ class=align-middle>Hector Fernandez Alcalde</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jana-strnadova/ class=align-middle>Jana Strnadová</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/esha-banerjee/ class=align-middle>Esha Banerjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruli-manurung/ class=align-middle>Ruli Manurung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonio-stella/ class=align-middle>Antonio Stella</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/atsuko-shimada/ class=align-middle>Atsuko Shimada</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sookyoung-kwak/ class=align-middle>Sookyoung Kwak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gustavo-mendonca/ class=align-middle>Gustavo Mendonca</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tatiana-lando/ class=align-middle>Tatiana Lando</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rattima-nitisaroj/ class=align-middle>Rattima Nitisaroj</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/josie-li/ class=align-middle>Josie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vinit-ravishankar/ class=align-middle>Vinit Ravishankar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artur-kulmizev/ class=align-middle>Artur Kulmizev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mostafa-abdou/ class=align-middle>Mostafa Abdou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gosse-bouma/ class=align-middle>Gosse Bouma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dag-haug/ class=align-middle>Dag Haug</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/per-erik-solberg/ class=align-middle>Per Erik Solberg</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lilja-ovrelid/ class=align-middle>Lilja Øvrelid</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paola-marongiu/ class=align-middle>Paola Marongiu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rico-sennrich/ class=align-middle>Rico Sennrich</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leon-derczynski/ class=align-middle>Leon Derczynski</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bjorn-lindi/ class=align-middle>Bjørn Lindi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stephan-oepen/ class=align-middle>Stephan Oepen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jorg-tidemann/ class=align-middle>Jörg Tidemann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miguel-ballesteros/ class=align-middle>Miguel Ballesteros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bernd-bohnet/ class=align-middle>Bernd Bohnet</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/udw/ class=align-middle>UDW</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>