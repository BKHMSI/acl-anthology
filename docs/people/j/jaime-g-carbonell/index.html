<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jaime G. Carbonell - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jaime G.</span> <span class=font-weight-bold>Carbonell</span></h2><p class="font-weight-light text-muted">CMU</p><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Jaime <span class=font-weight-normal>Carbonell</span></p><p class="font-weight-light text-muted"><span class=font-italic>Other people with similar names:</span>
<a href=/people/j/jaime-r-carbonell/>Jaime R. Carbonell</a>
(BBN; d. 1973)</p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.220" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.220/>StructSum : <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> via Structured Representations<span class=acl-fixed-case>S</span>truct<span class=acl-fixed-case>S</span>um: Summarization via Structured Representations</a></strong><br><a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/a/artidoro-pagnoni/>Artidoro Pagnoni</a>
|
<a href=/people/j/jay-yoon-lee/>Jay Yoon Lee</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--220><div class="card-body p-3 small">Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges : (i) layout bias : they overfit to the style of training corpora ; (ii) limited abstractiveness : they are optimized to copying <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> from the source rather than generating novel abstractive summaries ; (iii) lack of transparency : they are not interpretable. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN / DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a>, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939206 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.39/>Efficient Meta Lifelong-Learning with Limited Memory</a></strong><br><a href=/people/z/zirui-wang/>Zirui Wang</a>
|
<a href=/people/s/sanket-vaibhav-mehta/>Sanket Vaibhav Mehta</a>
|
<a href=/people/b/barnabas-poczos/>Barnabas Poczos</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--39><div class="card-body p-3 small">Current <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language processing models</a> work well on a single <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as <a href=https://en.wikipedia.org/wiki/Lifelong_learning>lifelong learning</a>. State-of-the-art lifelong language learning methods store past examples in <a href=https://en.wikipedia.org/wiki/Episodic_memory>episodic memory</a> and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments : (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1 % memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and <a href=https://en.wikipedia.org/wiki/Negative_transfer>negative transfer</a> at the same time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.tacl-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.8/>Improving Candidate Generation for Low-resource Cross-lingual Entity Linking</a></strong><br><a href=/people/s/shuyan-zhou/>Shuyan Zhou</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/j/john-wieting/>John Wieting</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a><br><a href=/volumes/2020.tacl-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 8</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--8><div class="card-body p-3 small">Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia pages</a>. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective : We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9 % in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also yields an average gain of 7.9 % in in-KB accuracy of end-to-end XEL.1</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4208/>CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology<span class=acl-fixed-case>CMU</span>-01 at the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task on Crosslinguality and Context in Morphology</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a><br><a href=/volumes/W19-42/ class=text-muted>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4208><div class="card-body p-3 small">This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatization</a> in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, <a href=https://en.wikipedia.org/wiki/Case_(disambiguation)>Case</a>, etc.) independently. However, most <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> are under-resourced, thus making it challenging to train <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a> for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1286.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1286/>Domain Adaptation of Neural Machine Translation by Lexicon Induction</a></strong><br><a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/m/mengzhou-xia/>Mengzhou Xia</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1286><div class="card-body p-3 small">It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1009/>Automatically Tagging Constructions of Causation and Their Slot-Fillers</a></strong><br><a href=/people/j/jesse-dunietz/>Jesse Dunietz</a>
|
<a href=/people/l/lori-levin/>Lori Levin</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1009><div class="card-body p-3 small">This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case. Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that <a href=https://en.wikipedia.org/wiki/Causality>causation</a> can take on. We therefore base our approach on the concept of <a href=https://en.wikipedia.org/wiki/Grammatical_construction>constructions</a> from the <a href=https://en.wikipedia.org/wiki/Paradigm>linguistic paradigm</a> known as Construction Grammar (CxG). In <a href=https://en.wikipedia.org/wiki/CxG>CxG</a>, a construction is a form / function pairing that can rely on arbitrary linguistic and semantic features. Rather than codifying all aspects of each construction&#8217;s form, as some attempts to employ <a href=https://en.wikipedia.org/wiki/CxG>CxG</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have done, we propose methods that offload that problem to <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. We describe two supervised approaches for tagging causal constructions and their arguments. Both approaches combine automatically induced pattern-matching rules with <a href=https://en.wikipedia.org/wiki/Statistical_classification>statistical classifiers</a> that learn the subtler parameters of the constructions. Our results show that these approaches are promising : they significantly outperform nave baselines for both construction recognition and cause and effect head matches.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jaime+G.+Carbonell" title="Search for 'Jaime G. Carbonell' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yulia-tsvetkov/ class=align-middle>Yulia Tsvetkov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/graham-neubig/ class=align-middle>Graham Neubig</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zirui-wang/ class=align-middle>Zirui Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sanket-vaibhav-mehta/ class=align-middle>Sanket Vaibhav Mehta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/barnabas-poczos/ class=align-middle>Barnabás Poczós</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/v/vidhisha-balachandran/ class=align-middle>Vidhisha Balachandran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artidoro-pagnoni/ class=align-middle>Artidoro Pagnoni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jay-yoon-lee/ class=align-middle>Jay Yoon Lee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dheeraj-rajagopal/ class=align-middle>Dheeraj Rajagopal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jesse-dunietz/ class=align-middle>Jesse Dunietz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lori-levin/ class=align-middle>Lori Levin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditi-chaudhary/ class=align-middle>Aditi Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elizabeth-salesky/ class=align-middle>Elizabeth Salesky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gayatri-bhat/ class=align-middle>Gayatri Bhat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-r-mortensen/ class=align-middle>David R. Mortensen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shuyan-zhou/ class=align-middle>Shuyan Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shruti-rijhwani/ class=align-middle>Shruti Rijhwani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-wieting/ class=align-middle>John Wieting</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/junjie-hu/ class=align-middle>Junjie Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mengzhou-xia/ class=align-middle>Mengzhou Xia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>