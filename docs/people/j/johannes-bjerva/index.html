<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Johannes Bjerva - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Johannes</span> <span class=font-weight-bold>Bjerva</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.38/>Does Typological Blinding Impede Cross-Lingual Sharing?</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--38><div class="card-body p-3 small">Bridging the performance gap between high- and low-resource languages has been the focus of much previous work. Typological features from <a href=https://en.wikipedia.org/wiki/Database>databases</a> such as the <a href=https://en.wikipedia.org/wiki/World_Atlas_of_Language_Structures>World Atlas of Language Structures (WALS)</a> are a prime candidate for this, as such data exists even for very low-resource languages. However, previous work has only found minor benefits from using <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typological information</a>. Our hypothesis is that a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained in a cross-lingual setting will pick up on typological cues from the input data, thus overshadowing the utility of explicitly using such <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. We verify this hypothesis by blinding a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to typological information, and investigate how cross-lingual sharing and performance is impacted. Our model is based on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from exploiting <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> somewhat improves performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.starsem-1.22.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.22/>Inducing Language-Agnostic Multilingual Representations</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/2021.starsem-1/ class=text-muted>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--22><div class="card-body p-3 small">Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this : (i) re-aligning the vector spaces of target languages (all together) to a pivot source language ; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product ; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approachesunlike <a href=https://en.wikipedia.org/wiki/Vector_normalization>vector normalization</a>, vector space re-alignment and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a> do not achieve consistent gains across encoders and languages. Due to the approaches&#8217; additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigtyp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigtyp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigtyp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939790 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigtyp-1.1/>SIGTYP 2020 Shared Task : Prediction of Typological Features<span class=acl-fixed-case>SIGTYP</span> 2020 Shared Task: Prediction of Typological Features</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/g/giuseppe-g-a-celano/>Giuseppe G. A. Celano</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/2020.sigtyp-1/ class=text-muted>Proceedings of the Second Workshop on Computational Research in Linguistic Typology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigtyp-1--1><div class="card-body p-3 small">Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world&#8217;s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>, and skewed, in that few <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> attracted 8 submissions from 5 teams, out of which the most successful <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> make use of such feature correlations. However, our <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6128 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6128/>Transductive Auxiliary Task Self-Training for Neural Multi-Task Models</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/D19-61/ class=text-muted>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6128><div class="card-body p-3 small">Multi-task learning and self-training are two common ways to improve a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a>&#8217;s performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training : training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56 % over the pure multi-task model for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>dependency relation tagging</a> and by up to 13.03 % for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>semantic tagging</a>.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1543 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1543.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1543.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1543/>Parameter sharing between dependency parsers for related languages</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1543><div class="card-body p-3 small">Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different <a href=https://en.wikipedia.org/wiki/Language_family>language family</a>. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1058/>KU-MTL at SemEval-2018 Task 1 : Multi-task Identification of Affect in Tweets<span class=acl-fixed-case>KU</span>-<span class=acl-fixed-case>MTL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Multi-task Identification of Affect in Tweets</a></strong><br><a href=/people/t/thomas-nyegaard-signori/>Thomas Nyegaard-Signori</a>
|
<a href=/people/c/casper-veistrup-helms/>Casper Veistrup Helms</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1058><div class="card-body p-3 small">We take a multi-task learning approach to the shared Task 1 at SemEval-2018. The general idea concerning the model structure is to use as little external data as possible in order to preserve the task relatedness and reduce <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>. We employ <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> with hard parameter sharing to exploit the relatedness between sub-tasks. As a base model, we use a standard <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> for both the classification and regression subtasks. Our system ranks 32nd out of 48 participants with a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson score</a> of 0.557 in the first subtask, and 20th out of 35 in the fifth subtask with an accuracy score of 0.464.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5025 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5025/>Neural Networks and Spelling Features for Native Language Identification</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/g/gintare-grigonyte/>Gintarė Grigonytė</a>
|
<a href=/people/r/robert-ostling/>Robert Östling</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a><br><a href=/volumes/W17-50/ class=text-muted>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5025><div class="card-body p-3 small">We present the RUG-SU team&#8217;s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> using word representations, a deep residual network using word and character features, and a system based on a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. Our best <a href=https://en.wikipedia.org/wiki/System>system</a> is an ensemble of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, reaching an F1 score of 0.8323. Although our <a href=https://en.wikipedia.org/wiki/System>system</a> is not the highest ranking one, we do outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5043 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5043/>The Power of Character N-grams in Native Language Identification</a></strong><br><a href=/people/a/artur-kulmizev/>Artur Kulmizev</a>
|
<a href=/people/b/bo-blankers/>Bo Blankers</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/m/martijn-wieling/>Martijn Wieling</a><br><a href=/volumes/W17-50/ class=text-muted>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5043><div class="card-body p-3 small">In this paper, we explore the performance of a linear SVM trained on language independent character features for the NLI Shared Task 2017. Our basic <a href=https://en.wikipedia.org/wiki/System>system</a> (GRONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a>. We compare this against several ensemble and meta-classifiers in order to examine how the <a href=https://en.wikipedia.org/wiki/Linear_system>linear system</a> fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2021/>ResSim at SemEval-2017 Task 1 : Multilingual Word Representations for Semantic Textual Similarity<span class=acl-fixed-case>R</span>es<span class=acl-fixed-case>S</span>im at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 1: Multilingual Word Representations for Semantic Textual Similarity</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/r/robert-ostling/>Robert Östling</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2021><div class="card-body p-3 small">Shared Task 1 at SemEval-2017 deals with assessing the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between sentences, either in the same or in different languages. In our system submission, we employ multilingual word representations, in which similar words in different languages are close to one another. Using such representations is advantageous, since the increasing amount of available parallel data allows for the application of such <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> to many of the languages in the world. Hence, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> can be inferred even for languages for which no <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a> exists. Our system is trained and evaluated on all language pairs included in the shared task (English, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and Turkish). Although development results are promising, our <a href=https://en.wikipedia.org/wiki/System>system</a> does not yield high performance on the shared task test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2039/>The Parallel Meaning Bank : Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations<span class=acl-fixed-case>P</span>arallel <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>B</span>ank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations</a></strong><br><a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/k/kilian-evang/>Kilian Evang</a>
|
<a href=/people/h/hessel-haagsma/>Hessel Haagsma</a>
|
<a href=/people/r/rik-van-noord/>Rik van Noord</a>
|
<a href=/people/p/pierre-ludmann/>Pierre Ludmann</a>
|
<a href=/people/d/duc-duy-nguyen/>Duc-Duy Nguyen</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2039><div class="card-body p-3 small">The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection : automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The <a href=https://en.wikipedia.org/wiki/Semantic_annotation>semantic annotation</a> consists of five main steps : (i) segmentation of the text in sentences and lexical items ; (ii) syntactic parsing with Combinatory Categorial Grammar ; (iii) universal semantic tagging ; (iv) symbolization ; and (v) compositional semantic analysis based on <a href=https://en.wikipedia.org/wiki/Discourse_representation_theory>Discourse Representation Theory</a>. These steps are performed using <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> trained in a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised manner</a>. The employed annotation models are all language-neutral. Our first results are promising.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Johannes+Bjerva" title="Search for 'Johannes Bjerva' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/i/isabelle-augenstein/ class=align-middle>Isabelle Augenstein</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/r/robert-ostling/ class=align-middle>Robert Östling</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/barbara-plank/ class=align-middle>Barbara Plank</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/gintare-grigonyte/ class=align-middle>Gintarė Grigonytė</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artur-kulmizev/ class=align-middle>Artur Kulmizev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/b/bo-blankers/ class=align-middle>Bo Blankers</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/malvina-nissim/ class=align-middle>Malvina Nissim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gertjan-van-noord/ class=align-middle>Gertjan van Noord</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martijn-wieling/ class=align-middle>Martijn Wieling</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miryam-de-lhoneux/ class=align-middle>Miryam de Lhoneux</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katharina-kann/ class=align-middle>Katharina Kann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-zhao/ class=align-middle>Wei Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steffen-eger/ class=align-middle>Steffen Eger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-nyegaard-signori/ class=align-middle>Thomas Nyegaard-Signori</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/casper-veistrup-helms/ class=align-middle>Casper Veistrup Helms</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lasha-abzianidze/ class=align-middle>Lasha Abzianidze</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kilian-evang/ class=align-middle>Kilian Evang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hessel-haagsma/ class=align-middle>Hessel Haagsma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rik-van-noord/ class=align-middle>Rik van Noord</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pierre-ludmann/ class=align-middle>Pierre Ludmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/duc-duy-nguyen/ class=align-middle>Duc-Duy Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/johan-bos/ class=align-middle>Johan Bos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elizabeth-salesky/ class=align-middle>Elizabeth Salesky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sabrina-j-mielke/ class=align-middle>Sabrina J. Mielke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aditi-chaudhary/ class=align-middle>Aditi Chaudhary</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/giuseppe-g-a-celano/ class=align-middle>Giuseppe G. A. Celano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/edoardo-maria-ponti/ class=align-middle>Edoardo Maria Ponti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ekaterina-vylomova/ class=align-middle>Ekaterina Vylomova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryan-cotterell/ class=align-middle>Ryan Cotterell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/starsem/ class=align-middle>*SEM</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/sigtyp/ class=align-middle>SIGTYP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>