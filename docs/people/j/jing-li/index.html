<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jing Li - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jing</span> <span class=font-weight-bold>Li</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.3/>Engage the Public : Poll Question Generation for Social Media Posts</a></strong><br><a href=/people/z/zexin-lu/>Zexin Lu</a>
|
<a href=/people/k/keyang-ding/>Keyang Ding</a>
|
<a href=/people/y/yuji-zhang/>Yuji Zhang</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--3><div class="card-body p-3 small">This paper presents a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to generate <a href=https://en.wikipedia.org/wiki/Opinion_poll>poll questions</a> for <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a>. It offers an easy way to hear the voice from the public and learn from their feelings to important <a href=https://en.wikipedia.org/wiki/Social_issue>social topics</a>. While most related work tackles <a href=https://en.wikipedia.org/wiki/Formal_language>formal languages</a> (e.g., exam papers), we generate <a href=https://en.wikipedia.org/wiki/Opinion_poll>poll questions</a> for short and colloquial social media messages exhibiting severe data sparsity. To deal with that, we propose to encode user comments and discover latent topics therein as contexts. They are then incorporated into a sequence-to-sequence (S2S) architecture for question generation and its extension with dual decoders to additionally yield poll choices (answers). For experiments, we collect a large-scale Chinese dataset from <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a> containing over 20 K polls. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the popular S2S models without exploiting topics from comments and the dual decoder design can further benefit the prediction of both questions and answers. Human evaluations further exhibit our superiority in yielding high-quality polls helpful to draw user engagements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--616 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.616" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.616/># HowYouTagTweets : Learning User Hashtagging Preferences via Personalized Topic Attention<span class=acl-fixed-case>H</span>ow<span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ag<span class=acl-fixed-case>T</span>weets: Learning User Hashtagging Preferences via Personalized Topic Attention</a></strong><br><a href=/people/y/yuji-zhang/>Yuji Zhang</a>
|
<a href=/people/y/yubo-zhang/>Yubo Zhang</a>
|
<a href=/people/c/chunpu-xu/>Chunpu Xu</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/z/ziyan-jiang/>Ziyan Jiang</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--616><div class="card-body p-3 small">Millions of <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> are created on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user&#8217;s hashtagging preferences via predicting how likely they will post with a <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a>. It is hypothesized that one&#8217;s interests in a <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> are related with what they said before (user history) and the existing posts present the <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a>. In this way, user interests learned from the past can be customized to match future <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, which is beyond the capability of existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.698.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--698 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.698 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.698" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.698/>MLEC-QA : A Chinese Multi-Choice Biomedical Question Answering Dataset<span class=acl-fixed-case>MLEC-QA</span>: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>C</span>hoice <span class=acl-fixed-case>B</span>iomedical <span class=acl-fixed-case>Q</span>uestion <span class=acl-fixed-case>A</span>nswering <span class=acl-fixed-case>D</span>ataset</a></strong><br><a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/s/shangping-zhong/>Shangping Zhong</a>
|
<a href=/people/k/kaizhi-chen/>Kaizhi Chen</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--698><div class="card-body p-3 small">Question Answering (QA) has been successfully applied in scenarios of <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interaction</a> such as <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> and <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a>. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields : <a href=https://en.wikipedia.org/wiki/Clinic>Clinic</a>, <a href=https://en.wikipedia.org/wiki/Oral_medicine>Stomatology</a>, <a href=https://en.wikipedia.org/wiki/Public_health>Public Health</a>, <a href=https://en.wikipedia.org/wiki/Traditional_Chinese_medicine>Traditional Chinese Medicine</a>, and <a href=https://en.wikipedia.org/wiki/Traditional_Chinese_medicine>Traditional Chinese Medicine</a> Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40 % to 55 % on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--406 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.406/>On the Transformer Growth for Progressive BERT Training<span class=acl-fixed-case>BERT</span> Training</a></strong><br><a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/h/hongkun-yu/>Hongkun Yu</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/c/chen-chen/>Chen Chen</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--406><div class="card-body p-3 small">As the excessive pre-training cost arouses the need to improve <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>, considerable efforts have been made to train BERT progressivelystart from an inferior but low-cost model and gradually increase the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors <a href=https://en.wikipedia.org/wiki/Scaling_(geometry)>compound scaling</a>. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up <a href=https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor>BERT pre-training</a> by 73.6 % and 82.2 % for the base and large models respectively while achieving comparable performances.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364687803 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1164" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1164/>Microblog Hashtag Generation via Encoding Conversation Contexts</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/i/irwin-king/>Irwin King</a>
|
<a href=/people/m/michael-r-lyu/>Michael R. Lyu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1164><div class="card-body p-3 small">Automatic hashtag annotation plays an important role in content understanding for <a href=https://en.wikipedia.org/wiki/Microblogging>microblog posts</a>. To date, progress made in this field has been restricted to phrase selection from limited candidates, or word-level hashtag discovery using <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>. Different from previous work considering <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> to be inseparable, our work is the first effort to annotate <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> with a novel sequence generation framework via viewing the <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> as a short sequence of words. Moreover, to address the data sparsity issue in processing short microblog posts, we propose to jointly model the target posts and the conversation contexts initiated by them with bidirectional attention. Extensive experimental results on two large-scale datasets, newly collected from <a href=https://en.wikipedia.org/wiki/Twitter>English Twitter</a> and <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Chinese Weibo</a>, show that our model significantly outperforms state-of-the-art models based on <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Further studies demonstrate our ability to effectively generate rare and even unseen hashtags, which is however not possible for most existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1240/>Topic-Aware Neural Keyphrase Generation for Social Media Language</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/h/hou-pong-chan/>Hou Pong Chan</a>
|
<a href=/people/i/irwin-king/>Irwin King</a>
|
<a href=/people/m/michael-r-lyu/>Michael R. Lyu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1240><div class="card-body p-3 small">A huge volume of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> is daily produced on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. To facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. While most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. Moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. Further discussions show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns meaningful topics, which interprets its superiority in social media keyphrase generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1270 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384738763 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1270" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1270/>Joint Effects of Context and User History for Predicting Online Conversation Re-entries</a></strong><br><a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1270><div class="card-body p-3 small">As the <a href=https://en.wikipedia.org/wiki/Online_and_offline>online world</a> continues its exponential growth, <a href=https://en.wikipedia.org/wiki/Interpersonal_communication>interpersonal communication</a> has come to play an increasingly central role in opinion formation and <a href=https://en.wikipedia.org/wiki/Social_change>change</a>. In order to help users better engage with each other online, we study a challenging problem of re-entry prediction foreseeing whether a user will come back to a conversation they once participated in. We hypothesize that both the context of the ongoing conversations and the users&#8217; previous chatting history will affect their continued interests in future engagement. Specifically, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting history jointly result in their re-entry behavior. We experiment with two <a href=https://en.wikipedia.org/wiki/Data_set>large-scale datasets</a> collected from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Results show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> with bi-attention achieves an F1 score of 61.1 on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter conversations</a>, outperforming the state-of-the-art methods from previous work.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1273 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1273" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1273/>A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check<span class=acl-fixed-case>C</span>hinese Spelling Check</a></strong><br><a href=/people/d/dingmin-wang/>Dingmin Wang</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/j/jialong-han/>Jialong Han</a>
|
<a href=/people/h/haisong-zhang/>Haisong Zhang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1273><div class="card-body p-3 small">Chinese spelling check (CSC) is a challenging yet meaningful task, which not only serves as a preprocessing in many natural language processing(NLP) applications, but also facilitates reading and understanding of running texts in peoples&#8217; daily lives. However, to utilize data-driven approaches for CSC, there is one major limitation that annotated corpora are not enough in applying <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> and building <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. In this paper, we propose a novel approach of constructing CSC corpus with automatically generated spelling errors, which are either visually or phonologically resembled characters, corresponding to the OCR- and ASR-based methods, respectively. Upon the constructed corpus, different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained and evaluated for CSC with respect to three standard test sets. Experimental results demonstrate the effectiveness of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, therefore confirm the validity of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4008 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4008/>A Joint Model of Conversational Discourse Latent Topics on Microblogs</a></strong><br><a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a><br><a href=/volumes/J18-4/ class=text-muted>Computational Linguistics, Volume 44, Issue 4 - December 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4008><div class="card-body p-3 small">Conventional <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> are ineffective for topic extraction from <a href=https://en.wikipedia.org/wiki/Microblogging>microblog messages</a>, because the data sparseness exhibited in short messages lacking structure and contexts results in poor message-level word co-occurrence patterns. To address this issue, we organize microblog messages as conversation trees based on their reposting and replying relations, and propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> that jointly learns word distributions to represent : (1) different roles of conversational discourse, and (2) various latent topics in reflecting content information. By explicitly distinguishing the probabilities of messages with varying discourse roles in containing topical words, our model is able to discover clusters of discourse words that are indicative of topical content. In an automatic evaluation on large-scale microblog corpora, our joint model yields topics with better <a href=https://en.wikipedia.org/wiki/Coherence_(statistics)>coherence scores</a> than competitive topic models from previous studies. Qualitative analysis on model outputs indicates that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> induces meaningful <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for both discourse and topics. We further present an empirical study on microblog summarization based on the outputs of our joint model. The results show that the jointly modeled discourse and topic representations can effectively indicate summary-worthy content in microblog conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://vimeo.com/276422518 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-1035/>Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse</a></strong><br><a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/n/nicholas-beauchamp/>Nicholas Beauchamp</a>
|
<a href=/people/s/sarah-shugars/>Sarah Shugars</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1035><div class="card-body p-3 small">Millions of conversations are generated every day on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. With limited attention, it is challenging for users to select which discussions they would like to participate in. Here we propose a new <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for microblog conversation recommendation. While much prior work has focused on post-level recommendation, we exploit both the conversational context, and user content and behavior preferences. We propose a <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical model</a> that jointly captures : (1) topics for representing user interests and conversation content, and (2) discourse modes for describing user replying behavior and conversation dynamics. Experimental results on two Twitter datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms methods that only model content without considering <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jing+Li" title="Search for 'Jing Li' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kam-fai-wong/ class=align-middle>Kam-Fai Wong</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yuji-zhang/ class=align-middle>Yuji Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/baolin-peng/ class=align-middle>Baolin Peng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yan-song/ class=align-middle>Yan Song</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yue-wang/ class=align-middle>Yue Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/i/irwin-king/ class=align-middle>Irwin King</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michael-r-lyu/ class=align-middle>Michael R. Lyu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/shuming-shi/ class=align-middle>Shuming Shi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xingshan-zeng/ class=align-middle>Xingshan Zeng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lu-wang/ class=align-middle>Lu Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zexin-lu/ class=align-middle>Zexin Lu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/keyang-ding/ class=align-middle>Keyang Ding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lemao-liu/ class=align-middle>Lemao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dingmin-wang/ class=align-middle>Dingmin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jialong-han/ class=align-middle>Jialong Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haisong-zhang/ class=align-middle>Haisong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yubo-zhang/ class=align-middle>Yubo Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chunpu-xu/ class=align-middle>Chunpu Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/ziyan-jiang/ class=align-middle>Ziyan Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shangping-zhong/ class=align-middle>Shangping Zhong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaizhi-chen/ class=align-middle>Kaizhi Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhongyu-wei/ class=align-middle>Zhongyu Wei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaotao-gu/ class=align-middle>Xiaotao Gu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liyuan-liu/ class=align-middle>Liyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongkun-yu/ class=align-middle>Hongkun Yu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chen-chen/ class=align-middle>Chen Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiawei-han/ class=align-middle>Jiawei Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nicholas-beauchamp/ class=align-middle>Nicholas Beauchamp</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sarah-shugars/ class=align-middle>Sarah Shugars</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hou-pong-chan/ class=align-middle>Hou Pong Chan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>