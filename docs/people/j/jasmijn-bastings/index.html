<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jasmijn Bastings - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jasmijn</span> <span class=font-weight-bold>Bastings</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable Mention for Best Short Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.eacl-main.156/>We Need To Talk About Random Splits</a></strong><br><a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/s/sebastian-ebert/>Sebastian Ebert</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--156><div class="card-body p-3 small">(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> to simulate real-world drift ; this is known as the covariate shift assumption. In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead ; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.blackboxnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.blackboxnlp-1.0/>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a><br><a href=/volumes/2021.blackboxnlp-1/ class=text-muted>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.15/>The Language Interpretability Tool : Extensible, Interactive Visualizations and Analysis for NLP Models<span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/i/ian-tenney/>Ian Tenney</a>
|
<a href=/people/j/james-wexler/>James Wexler</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/t/tolga-bolukbasi/>Tolga Bolukbasi</a>
|
<a href=/people/a/andy-coenen/>Andy Coenen</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/e/ellen-jiang/>Ellen Jiang</a>
|
<a href=/people/m/mahima-pushkarna/>Mahima Pushkarna</a>
|
<a href=/people/c/carey-radebaugh/>Carey Radebaugh</a>
|
<a href=/people/e/emily-reif/>Emily Reif</a>
|
<a href=/people/a/ann-yuan/>Ann Yuan</a><br><a href=/volumes/2020.emnlp-demos/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--15><div class="card-body p-3 small">We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior : Why did my <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> make this prediction? When does <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, <a href=https://en.wikipedia.org/wiki/Aggregate_data>aggregate analysis</a>, and <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual generation</a> into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, measuring <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in <a href=https://en.wikipedia.org/wiki/Coreference>coreference systems</a>, and exploring local behavior in text generation. LIT supports a wide range of modelsincluding <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, seq2seq, and structured predictionand is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939764 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.14/>The elephant in the interpretability room : Why use attention as explanation when we have <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>saliency methods</a>?</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a><br><a href=/volumes/2020.blackboxnlp-1/ class=text-muted>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--14><div class="card-body p-3 small">There is a recent surge of interest in using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> as explanation of model predictions, with mixed evidence on whether <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be used as such. While <a href=https://en.wikipedia.org/wiki/Attention>attention</a> conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2078.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2078/>Exploiting <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Graph Convolutional Networks</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2078><div class="card-body p-3 small">Semantic representations have long been argued as potentially useful for enforcing meaning preservation and improving generalization performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation methods</a>. In this work, we are the first to incorporate information about predicate-argument structure of source sentences (namely, semantic-role representations) into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We use Graph Convolutional Networks (GCNs) to inject a semantic bias into sentence encoders and achieve improvements in BLEU scores over the linguistic-agnostic and syntax-aware versions on the EnglishGerman language pair.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1209/>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/k/khalil-simaan/>Khalil Sima’an</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1209><div class="card-body p-3 small">We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We rely on graph-convolutional networks (GCNs), a recent class of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> developed for modeling <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured data</a>. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> as input and produce <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> as output, so they can easily be incorporated as layers into standard <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jasmijn+Bastings" title="Search for 'Jasmijn Bastings' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/katja-filippova/ class=align-middle>Katja Filippova</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ivan-titov/ class=align-middle>Ivan Titov</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/diego-marcheggiani/ class=align-middle>Diego Marcheggiani</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/i/ian-tenney/ class=align-middle>Ian Tenney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-wexler/ class=align-middle>James Wexler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/t/tolga-bolukbasi/ class=align-middle>Tolga Bolukbasi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andy-coenen/ class=align-middle>Andy Coenen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-gehrmann/ class=align-middle>Sebastian Gehrmann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/ellen-jiang/ class=align-middle>Ellen Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mahima-pushkarna/ class=align-middle>Mahima Pushkarna</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carey-radebaugh/ class=align-middle>Carey Radebaugh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-reif/ class=align-middle>Emily Reif</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ann-yuan/ class=align-middle>Ann Yuan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anders-sogaard/ class=align-middle>Anders Søgaard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-ebert/ class=align-middle>Sebastian Ebert</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wilker-aziz/ class=align-middle>Wilker Aziz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khalil-simaan/ class=align-middle>Khalil Sima’an</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yonatan-belinkov/ class=align-middle>Yonatan Belinkov</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanuel-dupoux/ class=align-middle>Emmanuel Dupoux</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mario-giulianelli/ class=align-middle>Mario Giulianelli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dieuwke-hupkes/ class=align-middle>Dieuwke Hupkes</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuval-pinter/ class=align-middle>Yuval Pinter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hassan-sajjad/ class=align-middle>Hassan Sajjad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/blackboxnlp/ class=align-middle>BlackboxNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>