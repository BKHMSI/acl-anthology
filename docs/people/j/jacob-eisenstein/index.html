<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jacob Eisenstein - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jacob</span> <span class=font-weight-bold>Eisenstein</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.184/>Learning to Recognize Dialect Features</a></strong><br><a href=/people/d/dorottya-demszky/>Dorottya Demszky</a>
|
<a href=/people/d/devyani-sharma/>Devyani Sharma</a>
|
<a href=/people/j/jonathan-h-clark/>Jonathan Clark</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--184><div class="card-body p-3 small">Building <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> that serve everyone requires accounting for <a href=https://en.wikipedia.org/wiki/Dialect>dialect differences</a>. But <a href=https://en.wikipedia.org/wiki/List_of_dialects_of_English>dialects</a> are not monolithic entities : rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of <a href=https://en.wikipedia.org/wiki/Dialect>dialect features</a> in speech and text, such as the deletion of the copula in He running. In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a>, large-scale annotated corpora for these <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> are unavailable, making it difficult to train recognizers. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on a small number of minimal pairs, building on how linguists typically define <a href=https://en.wikipedia.org/wiki/Dialect>dialect features</a>. Evaluation on a test set of 22 dialect features of <a href=https://en.wikipedia.org/wiki/Indian_English>Indian English</a> demonstrates that these models learn to recognize many <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of <a href=https://en.wikipedia.org/wiki/Dialect_continuum>dialect density</a> and as a dialect classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cinlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cinlp-1.0/>Proceedings of the First Workshop on Causal Inference and NLP</a></strong><br><a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/k/katherine-keith/>Katherine Keith</a>
|
<a href=/people/e/emaad-manzoor/>Emaad Manzoor</a>
|
<a href=/people/r/reid-pryzant/>Reid Pryzant</a>
|
<a href=/people/d/dhanya-sridhar/>Dhanya Sridhar</a>
|
<a href=/people/z/zach-wood-doughty/>Zach Wood-Doughty</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a>
|
<a href=/people/j/justin-grimmer/>Justin Grimmer</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/m/molly-roberts/>Molly Roberts</a>
|
<a href=/people/u/uri-shalit/>Uri Shalit</a>
|
<a href=/people/b/brandon-m-stewart/>Brandon Stewart</a>
|
<a href=/people/v/victor-veitch/>Victor Veitch</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a><br><a href=/volumes/2021.cinlp-1/ class=text-muted>Proceedings of the First Workshop on Causal Inference and NLP</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--529 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929269 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.529/>AdvAug : Robust Adversarial Augmentation for Neural Machine Translation<span class=acl-fixed-case>A</span>dv<span class=acl-fixed-case>A</span>ug: Robust Adversarial Augmentation for Neural Machine Translation</a></strong><br><a href=/people/y/yong-cheng/>Yong Cheng</a>
|
<a href=/people/l/lu-jiang/>Lu Jiang</a>
|
<a href=/people/w/wolfgang-macherey/>Wolfgang Macherey</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--529><div class="card-body p-3 small">In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5506/>Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation</a></strong><br><a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a><br><a href=/volumes/D19-55/ class=text-muted>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5506><div class="card-body p-3 small">Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> to these variations, without diminishing performance on clean text. We focus on <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance on natural typos, and show that robustness to such <a href=https://en.wikipedia.org/wiki/Noise>noise</a> can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4811" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4811/>Character Eyes : Seeing Language through Character-Level Taggers</a></strong><br><a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/m/marc-marone/>Marc Marone</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/W19-48/ class=text-muted>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4811><div class="card-body p-3 small">Character-level models have been used extensively in recent years in NLP tasks as both supplements and replacements for closed-vocabulary token-level word representations. In one popular architecture, character-level LSTMs are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (POS) tags. In this work, we examine the behavior of POS taggers across languages from the perspective of individual hidden units within the character LSTM. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-5003.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347475879 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-5003/>Measuring and Modeling Language Change</a></strong><br><a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/N19-5/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-5003><div class="card-body p-3 small">This tutorial is designed to help researchers answer the following sorts of questions :-Are people happier on the weekend?-What was 1861&#8217;s word of the year?-Are Democrats and Republicans more different than ever?-When did gay stop meaning happy?-Are <a href=https://en.wikipedia.org/wiki/Gender_role>gender stereotypes</a> getting weaker, stronger, or just different?-Who is a linguistic leader?-How can we get internet users to be more polite and objective? Such questions are fundamental to the <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a> and humanities, and scholars in these disciplines are increasingly turning to <a href=https://en.wikipedia.org/wiki/Computational_science>computational techniques</a> for answers. Meanwhile, the ACL community is increasingly engaged with data that varies across time, and with the social insights that can be offered by analyzing temporal patterns and trends. The purpose of this tutorial is to facilitate this convergence in two main ways : 1. By synthesizing recent <a href=https://en.wikipedia.org/wiki/Computational_science>computational techniques</a> for handling and modeling <a href=https://en.wikipedia.org/wiki/Temporal_database>temporal data</a>, such as dynamic word embeddings, the tutorial will provide a starting point for future <a href=https://en.wikipedia.org/wiki/Computational_science>computational research</a>. It will also identify useful tools for <a href=https://en.wikipedia.org/wiki/Social_science>social scientists</a> and <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities scholars</a>. The tutorial will provide an overview of techniques and <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from the quantitative social sciences and the <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>, which are not well-known in the <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics community</a>. These techniques include <a href=https://en.wikipedia.org/wiki/Vector_autoregressive_model>vector autoregressive models</a>, multiple comparisons corrections for <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>hypothesis testing</a>, and <a href=https://en.wikipedia.org/wiki/Causal_inference>causal inference</a>. Datasets include historical newspaper archives and corpora of contemporary political speech.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1467 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306120421 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1467/>Making fetch happen : The influence of social and linguistic context on nonstandard word growth and decline</a></strong><br><a href=/people/i/ian-stewart/>Ian Stewart</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1467><div class="card-body p-3 small">In an <a href=https://en.wikipedia.org/wiki/Online_community>online community</a>, new words come and go : today&#8217;s haha may be replaced by tomorrow&#8217;s lol. Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a <a href=https://en.wikipedia.org/wiki/Speech_community>speech community</a>. But unlike other types of <a href=https://en.wikipedia.org/wiki/Innovation>innovation</a>, <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> is shaped and constrained by the <a href=https://en.wikipedia.org/wiki/Grammar>grammatical system</a> in which it takes part. To investigate the role of social and structural factors in <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>, we undertake a large-scale analysis of the frequencies of <a href=https://en.wikipedia.org/wiki/Nonstandard_dialect>non-standard words</a> in <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Dissemination across many linguistic contexts is a predictor of success : words that appear in more linguistic contexts grow faster and survive longer. Furthermore, social dissemination plays a less important role in explaining word growth and decline than previously hypothesized.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4007/>Interactional Stancetaking in Online Forums</a></strong><br><a href=/people/s/scott-f-kiesling/>Scott F. Kiesling</a>
|
<a href=/people/u/umashanthi-pavalanathan/>Umashanthi Pavalanathan</a>
|
<a href=/people/j/jim-fitzpatrick/>Jim Fitzpatrick</a>
|
<a href=/people/x/xiaochuang-han/>Xiaochuang Han</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/J18-4/ class=text-muted>Computational Linguistics, Volume 44, Issue 4 - December 2018</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4007><div class="card-body p-3 small">Language is shaped by the relationships between the speaker / writer and the audience, the object of discussion, and the talk itself. In turn, <a href=https://en.wikipedia.org/wiki/Language>language</a> is used to reshape these relationships over the course of an interaction. Computational researchers have succeeded in operationalizing <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Formality>formality</a>, and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a>, but each of these constructs captures only some aspects of social and relational meaning. Theories of interactional stancetaking have been put forward as holistic accounts, but until now, these <a href=https://en.wikipedia.org/wiki/Theory>theories</a> have been applied only through detailed qualitative analysis of (portions of) a few individual conversations. In this article, we propose a new computational operationalization of interpersonal stancetaking. We begin with annotations of three linked stance dimensionsaffect, investment, and alignmenton 68 conversation threads from the online platform Reddit. Using these annotations, we investigate thread structure and linguistic properties of stancetaking in online conversations. We identify lexical features that characterize the extremes along each stancetaking dimension, and show that these stancetaking properties can be predicted with moderate accuracy from bag-of-words features, even with a relatively small labeled training set. These <a href=https://en.wikipedia.org/wiki/Quantitative_research>quantitative analyses</a> are supplemented by extensive <a href=https://en.wikipedia.org/wiki/Qualitative_research>qualitative analysis</a>, highlighting the compatibility of computational and qualitative methods in synthesizing evidence about the creation of interactional meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-1602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-1602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-1602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-1602/>Stylistic Variation in Social Media Part-of-Speech Tagging</a></strong><br><a href=/people/m/murali-raghu-babu-balusu/>Murali Raghu Babu Balusu</a>
|
<a href=/people/t/taha-merghani/>Taha Merghani</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/W18-16/ class=text-muted>Proceedings of the Second Workshop on Stylistic Variation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-1602><div class="card-body p-3 small">Social media features substantial stylistic variation, raising new challenges for syntactic analysis of online writing. However, this variation is often aligned with author attributes such as age, gender, and <a href=https://en.wikipedia.org/wiki/Geography>geography</a>, as well as more readily-available social network metadata. In this paper, we report new evidence on the link between language and social networks in the task of <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. We find that tagger error rates are correlated with <a href=https://en.wikipedia.org/wiki/Flow_network>network structure</a>, with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in some parts of the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a>, and lower <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> elsewhere. As a result, tagger accuracy depends on training from a balanced sample of the network, rather than training on texts from a narrow subcommunity. We also describe our attempts to add robustness to stylistic variation, by building a mixture-of-experts model in which each expert is associated with a region of the <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. While prior work found that similar approaches yield performance improvements in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, we were unable to obtain performance improvements in <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, despite strong evidence for the link between part-of-speech error rates and social network structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N18-1100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N18-1100/>Explainable Prediction of Medical Codes from Clinical Text</a></strong><br><a href=/people/j/james-mullenbach/>James Mullenbach</a>
|
<a href=/people/s/sarah-wiegreffe/>Sarah Wiegreffe</a>
|
<a href=/people/j/jon-duke/>Jon Duke</a>
|
<a href=/people/j/jimeng-sun/>Jimeng Sun</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/N18-1/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-1100><div class="card-body p-3 small">Clinical notes are <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text documents</a> that are created by clinicians for each patient encounter. They are typically accompanied by <a href=https://en.wikipedia.org/wiki/Medical_code>medical codes</a>, which describe the diagnosis and treatment. Annotating these <a href=https://en.wikipedia.org/wiki/Code>codes</a> is labor intensive and error prone ; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts <a href=https://en.wikipedia.org/wiki/Medical_classification>medical codes</a> from clinical text. Our method aggregates information across the document using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-5000/>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/y/yoav-artzi/>Yoav Artzi</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/P18-5/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952612 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1021/>Overcoming <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>Language Variation</a> in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> with Social Attention</a></strong><br><a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/Q17-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 5</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1021><div class="card-body p-3 small">Variation in <a href=https://en.wikipedia.org/wiki/Language>language</a> is ubiquitous, particularly in newer forms of <a href=https://en.wikipedia.org/wiki/Writing>writing</a> such as <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Fortunately, variation is not random ; it is often linked to social properties of the author. In this paper, we show how to exploit <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> to make <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> more robust to social language variation. The key idea is linguistic homophily : the tendency of socially linked individuals to use <a href=https://en.wikipedia.org/wiki/Language>language</a> in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author&#8217;s position in the <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. This has the effect of smoothing the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification function</a> across the <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the accuracies of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and on review data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=J17-3003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/J17-3003/>A Kernel Independence Test for Geographical Language Variation</a></strong><br><a href=/people/d/dong-nguyen/>Dong Nguyen</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/J17-3/ class=text-muted>Computational Linguistics, Volume 43, Issue 3 - September 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3003><div class="card-body p-3 small">Quantifying the degree of <a href=https://en.wikipedia.org/wiki/Spatial_dependence>spatial dependence</a> for <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>linguistic variables</a> is a key task for analyzing dialectal variation. However, existing <a href=https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets>approaches</a> have important drawbacks. First, they are based on parametric models of dependence, which limits their power in cases where the underlying parametric assumptions are violated. Second, they are not applicable to all types of linguistic data : Some approaches apply only to <a href=https://en.wikipedia.org/wiki/Frequency>frequencies</a>, others to boolean indicators of whether a linguistic variable is present. We present a new <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for measuring geographical language variation, which solves both of these problems. Our approach builds on Reproducing Kernel Hilbert Space (RKHS) representations for <a href=https://en.wikipedia.org/wiki/Nonparametric_statistics>nonparametric statistics</a>, and takes the form of a <a href=https://en.wikipedia.org/wiki/Test_statistic>test statistic</a> that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins. We compare this test with prior work using synthetic data as well as a diverse set of real data sets : a corpus of Dutch tweets, a Dutch syntactic atlas, and a data set of letters to the editor in North American newspapers. Our proposed <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>test</a> is shown to support robust inferences across a broad range of scenarios and types of data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1010.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234299 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1010/>Mimicking Word Embeddings using Subword RNNs<span class=acl-fixed-case>RNN</span>s</a></strong><br><a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/r/robert-guthrie/>Robert Guthrie</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1010><div class="card-body p-3 small">Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>function</a> from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus ; instead, <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low resource settings.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jacob+Eisenstein" title="Search for 'Jacob Eisenstein' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yuval-pinter/ class=align-middle>Yuval Pinter</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yong-cheng/ class=align-middle>Yong Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lu-jiang/ class=align-middle>Lu Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wolfgang-macherey/ class=align-middle>Wolfgang Macherey</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-yang/ class=align-middle>Yi Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/d/dong-nguyen/ class=align-middle>Dong Nguyen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ian-stewart/ class=align-middle>Ian Stewart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vladimir-karpukhin/ class=align-middle>Vladimir Karpukhin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/omer-levy/ class=align-middle>Omer Levy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marjan-ghazvininejad/ class=align-middle>Marjan Ghazvininejad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/scott-f-kiesling/ class=align-middle>Scott F. Kiesling</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/umashanthi-pavalanathan/ class=align-middle>Umashanthi Pavalanathan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jim-fitzpatrick/ class=align-middle>Jim Fitzpatrick</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaochuang-han/ class=align-middle>Xiaochuang Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-guthrie/ class=align-middle>Robert Guthrie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dorottya-demszky/ class=align-middle>Dorottya Demszky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/devyani-sharma/ class=align-middle>Devyani Sharma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jonathan-h-clark/ class=align-middle>Jonathan H. Clark</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vinodkumar-prabhakaran/ class=align-middle>Vinodkumar Prabhakaran</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/murali-raghu-babu-balusu/ class=align-middle>Murali Raghu Babu Balusu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/taha-merghani/ class=align-middle>Taha Merghani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marc-marone/ class=align-middle>Marc Marone</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/amir-feder/ class=align-middle>Amir Feder</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katherine-keith/ class=align-middle>Katherine Keith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emaad-manzoor/ class=align-middle>Emaad Manzoor</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/reid-pryzant/ class=align-middle>Reid Pryzant</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dhanya-sridhar/ class=align-middle>Dhanya Sridhar</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zach-wood-doughty/ class=align-middle>Zach Wood-Doughty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/justin-grimmer/ class=align-middle>Justin Grimmer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roi-reichart/ class=align-middle>Roi Reichart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/molly-roberts/ class=align-middle>Molly Roberts</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/u/uri-shalit/ class=align-middle>Uri Shalit</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/brandon-m-stewart/ class=align-middle>Brandon M. Stewart</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-veitch/ class=align-middle>Victor Veitch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diyi-yang/ class=align-middle>Diyi Yang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/james-mullenbach/ class=align-middle>James Mullenbach</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sarah-wiegreffe/ class=align-middle>Sarah Wiegreffe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jon-duke/ class=align-middle>Jon Duke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jimeng-sun/ class=align-middle>Jimeng Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yoav-artzi/ class=align-middle>Yoav Artzi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cinlp/ class=align-middle>CINLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>