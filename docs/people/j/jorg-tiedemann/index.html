<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jörg Tiedemann - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jörg</span> <span class=font-weight-bold>Tiedemann</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Jorg <span class=font-weight-normal>Tiedemann</span></p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.vardial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.vardial-1.0/>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a><br><a href=/volumes/2021.vardial-1/ class=text-muted>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nodalida-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nodalida-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nodalida-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nodalida-main.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nodalida-main.28/>NLI Data Sanity Check : Assessing the Effect of <a href=https://en.wikipedia.org/wiki/Data_corruption>Data Corruption</a> on Model Performance<span class=acl-fixed-case>NLI</span> Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/2021.nodalida-main/ class=text-muted>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nodalida-main--28><div class="card-body p-3 small">Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> constitutes a good testbed for evaluating the models&#8217; meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is likely to contain <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>statistical biases</a> and artefacts that guide <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Inversely, a large decrease in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>model accuracy</a> indicates that the original <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provides a proper challenge to the models&#8217; reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.0/>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></strong><br><a href=/people/p/paola-merlo/>Paola Merlo</a>
|
<a href=/people/j/jorg-tiedemann/>Jorg Tiedemann</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.8/>Creating an Aligned Russian Text Simplification Dataset from Language Learner Data<span class=acl-fixed-case>R</span>ussian Text Simplification Dataset from Language Learner Data</a></strong><br><a href=/people/a/anna-dmitrieva/>Anna Dmitrieva</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/2021.bsnlp-1/ class=text-muted>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--8><div class="card-body p-3 small">Parallel language corpora where regular texts are aligned with their simplified versions can be used in both <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistic studies</a>. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Simple English, but many other languages lack such <a href=https://en.wikipedia.org/wiki/Data>data</a>. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of <a href=https://en.wikipedia.org/wiki/Russian_language>Russian literature texts</a> adapted for learners of <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.29/>The Helsinki submission to the AmericasNLP shared task<span class=acl-fixed-case>H</span>elsinki submission to the <span class=acl-fixed-case>A</span>mericas<span class=acl-fixed-case>NLP</span> shared task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/2021.americasnlp-1/ class=text-muted>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--29><div class="card-body p-3 small">The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects : (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.575.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--575 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.575 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.575" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.575/>XED : A Multilingual Dataset for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> and Emotion Detection<span class=acl-fixed-case>XED</span>: A Multilingual Dataset for Sentiment Analysis and Emotion Detection</a></strong><br><a href=/people/e/emily-ohman/>Emily Öhman</a>
|
<a href=/people/m/marc-pamies/>Marc Pàmies</a>
|
<a href=/people/k/kaisla-kajava/>Kaisla Kajava</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--575><div class="card-body p-3 small">We introduce XED, a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. We use Plutchik&#8217;s core emotions to annotate the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with the addition of neutral to create a multilabel multiclass dataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to show that XED performs on par with other similar datasets and is therefore a useful tool for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.vardial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.vardial-1.0/>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a><br><a href=/volumes/2020.vardial-1/ class=text-muted>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.470.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--470 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.470 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.470/>The FISKM Project : Resources and Tools for Finnish-Swedish Machine Translation and Cross-Linguistic Research<span class=acl-fixed-case>FISKMÖ</span> Project: Resources and Tools for <span class=acl-fixed-case>F</span>innish-<span class=acl-fixed-case>S</span>wedish Machine Translation and Cross-Linguistic Research</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/t/tommi-nieminen/>Tommi Nieminen</a>
|
<a href=/people/m/mikko-aulamo/>Mikko Aulamo</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/a/akseli-leino/>Akseli Leino</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/n/niko-papula/>Niko Papula</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--470><div class="card-body p-3 small">This paper presents FISKM</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929617 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.10/>The University of Helsinki Submission to the IWSLT2020 Offline SpeechTranslation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submission to the <span class=acl-fixed-case>IWSLT</span>2020 Offline <span class=acl-fixed-case>S</span>peech<span class=acl-fixed-case>T</span>ranslation Task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/m/mikko-aulamo/>Mikko Aulamo</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/2020.iwslt-1/ class=text-muted>Proceedings of the 17th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--10><div class="card-body p-3 small">This paper describes the University of Helsinki Language Technology group&#8217;s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text. In line with this year&#8217;s task objective, we train both cascade and end-to-end systems for spoken language translation. We opt for an end-to-end multitasking architecture with shared internal representations and a cascade approach that follows a standard procedure consisting of ASR, correction, and MT stages. We also describe the experiments that served as a basis for the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a>. Our experiments reveal that multitasking training with shared internal representations is not only possible but allows for <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge-transfer</a> across modalities.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6506 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6506/>Analysing concatenation approaches to document-level NMT in two different domains<span class=acl-fixed-case>NMT</span> in two different domains</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a><br><a href=/volumes/D19-65/ class=text-muted>Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6506><div class="card-body p-3 small">In this paper, we investigate how different aspects of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>discourse context</a> affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore, we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context, arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are indeed affected by the manipulation of the test data, providing a different view on document-level translation quality than absolute sentence-level scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1400/>Proceedings of the Sixth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a><br><a href=/volumes/W19-14/ class=text-muted>Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2005.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2005/>Measuring Semantic Abstraction of Multilingual NMT with Paraphrase Recognition and Generation Tasks<span class=acl-fixed-case>NMT</span> with Paraphrase Recognition and Generation Tasks</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a><br><a href=/volumes/W19-20/ class=text-muted>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2005><div class="card-body p-3 small">In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> when applied to <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> of the source language. The intuition is that an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> produces better representations if a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is capable of recognizing synonymous sentences in the same language even though the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> is significantly reduced in each of the cases, indicating that meaning can be grounded in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. This is further supported by a study on <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> that we also include at the end of the paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4305/>Multilingual NMT with a Language-Independent Attention Bridge<span class=acl-fixed-case>NMT</span> with a Language-Independent Attention Bridge</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a><br><a href=/volumes/W19-43/ class=text-muted>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4305><div class="card-body p-3 small">In this paper, we propose an architecture for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> from each language for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and develops into a language-agnostic meaning representation that can efficiently be used for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We present a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5347 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5347/>The University of Helsinki Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/a/arvi-hurskainen/>Arvi Hurskainen</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W19-53/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5347><div class="card-body p-3 small">In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish-English</a>. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5441/>The University of Helsinki Submission to the WMT19 Parallel Corpus Filtering Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering Task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W19-54/ class=text-muted>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5441><div class="card-body p-3 small">This paper describes the University of Helsinki Language Technology group&#8217;s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of <a href=https://en.wikipedia.org/wiki/Filter_(software)>filters</a> to remove the &#8216;bad&#8217; quality sentences. Then, we produced scores for each sentence by weighting these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with a <a href=https://en.wikipedia.org/wiki/Statistical_model>classification model</a>. This methodology allowed us to build a simple and reliable <a href=https://en.wikipedia.org/wiki/System>system</a> that is easily adaptable to other language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6129 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-6129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-6129/>Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/a/antti-suni/>Antti Suni</a>
|
<a href=/people/h/hande-celikkanat/>Hande Celikkanat</a>
|
<a href=/people/s/sofoklis-kakouros/>Sofoklis Kakouros</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/martti-vainio/>Martti Vainio</a><br><a href=/volumes/W19-61/ class=text-muted>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6129><div class="card-body p-3 small">In this paper we introduce a new natural language processing dataset and <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for predicting prosodic prominence from written text. To our knowledge this will be the largest publicly available dataset with <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic labels</a>. We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature-based classifiers to <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network systems</a> for the prediction of discretized prosodic prominence. We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10 % of the training data. Finally we discuss the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> in light of the results and point to future research and plans for further improving both the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and methods of predicting prosodic prominence from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the code for the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> will be made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6146 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6146/>The OPUS Resource Repository : An Open Package for Creating Parallel Corpora and Machine Translation Services<span class=acl-fixed-case>OPUS</span> Resource Repository: An Open Package for Creating Parallel Corpora and Machine Translation Services</a></strong><br><a href=/people/m/mikko-aulamo/>Mikko Aulamo</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W19-61/ class=text-muted>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6146><div class="card-body p-3 small">This paper presents a flexible and powerful system for creating parallel corpora and for running neural machine translation services. Our package provides a scalable data repository backend that offers transparent data pre-processing pipelines and automatic alignment procedures that facilitate the compilation of extensive parallel data sets from a variety of sources. Moreover, we develop a <a href=https://en.wikipedia.org/wiki/Web_application>web-based interface</a> that constitutes an intuitive frontend for end-users of the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>. The whole <a href=https://en.wikipedia.org/wiki/System>system</a> can easily be distributed over virtual machines and implements a sophisticated permission system with secure connections and a flexible database for storing arbitrary metadata. Furthermore, we also provide an interface for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> that can run as a service on <a href=https://en.wikipedia.org/wiki/Virtual_machine>virtual machines</a>, which also incorporates a connection to the data repository software.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-3900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-3900/>Proceedings of the Fifth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects (<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2018)</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a><br><a href=/volumes/W18-39/ class=text-muted>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4510/>Normalizing Early English Letters to Present-day English Spelling<span class=acl-fixed-case>E</span>nglish Letters to Present-day <span class=acl-fixed-case>E</span>nglish Spelling</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/t/tanja-saily/>Tanja Säily</a>
|
<a href=/people/j/jack-rueter/>Jack Rueter</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/e/eetu-makela/>Eetu Mäkelä</a><br><a href=/volumes/W18-45/ class=text-muted>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4510><div class="card-body p-3 small">This paper presents multiple methods for normalizing the most deviant and infrequent historical spellings in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of <a href=https://en.wikipedia.org/wiki/Letter_(message)>personal correspondence</a> from the 15th to the 19th century. The methods include machine translation (neural and statistical), <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a> and rule-based FST. Different <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization methods</a> are compared and evaluated. All of the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> have their own strengths in <a href=https://en.wikipedia.org/wiki/Word_normalization>word normalization</a>. This calls for finding ways of combining the results from these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to leverage their individual strengths.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5431/>An Analysis of Encoder Representations in Transformer-Based Machine Translation</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W18-54/ class=text-muted>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5431><div class="card-body p-3 small">The attention mechanism is a successful technique in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, especially in tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The recently proposed <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> of the Transformer is based entirely on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> and achieves new state of the art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> while higher layers tend to encode more <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>.<i>Transformer</i> is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.13/>The MeMAD Submission to the IWSLT 2018 Speech Translation Task<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>MAD</span> Submission to the <span class=acl-fixed-case>IWSLT</span> 2018 Speech Translation Task</a></strong><br><a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/a/aku-rouhe/>Aku Rouhe</a>
|
<a href=/people/s/stig-arnegronroos/>Stig-ArneGrönroos</a>
|
<a href=/people/m/mikko-kurimo/>Mikko Kurimo</a><br><a href=/volumes/2018.iwslt-1/ class=text-muted>Proceedings of the 15th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--13><div class="card-body p-3 small">This paper describes the MeMAD project entry to the IWSLT Speech Translation Shared Task, addressing the translation of English audio into German text. Between the <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> and end-to-end model tracks, we participated only in the former, with three contrastive systems. We tried also the latter, but were not able to finish our end-to-end model in time. All of our systems start by transcribing the audio into text through an automatic speech recognition (ASR) model trained on the TED-LIUM English Speech Recognition Corpus (TED-LIUM). Afterwards, we feed the transcripts into English-German text-based neural machine translation (NMT) models. Our systems employ three different translation models trained on separate training sets compiled from the English-German part of the TED Speech Translation Corpus (TED-TRANS) and the OPENSUBTITLES2018 section of the OPUS collection. In this paper, we also describe the experiments leading up to our final <a href=https://en.wikipedia.org/wiki/Thermodynamic_system>systems</a>. Our experiments indicate that using OPENSUBTITLES2018 in training significantly improves <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance. We also experimented with various preand postprocessing routines for the NMT module, but we did not have much success with these. Our best-scoring system attains a BLEU score of 16.45 on the test set for this year&#8217;s task.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1018.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1018/>Character-based Joint Segmentation and POS Tagging for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> using Bidirectional RNN-CRF<span class=acl-fixed-case>POS</span> Tagging for <span class=acl-fixed-case>C</span>hinese using Bidirectional <span class=acl-fixed-case>RNN</span>-<span class=acl-fixed-case>CRF</span></a></strong><br><a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1018><div class="card-body p-3 small">We present a character-based model for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and lower-than-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is accurate and robust across <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in different sizes, genres and <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a>. We obtain state-of-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0200/>Proceedings of the 21st Nordic Conference on Computational Linguistics</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a><br><a href=/volumes/W17-02/ class=text-muted>Proceedings of the 21st Nordic Conference on Computational Linguistics</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1200/>Proceedings of the Fourth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects (<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial)</a></strong><br><a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/s/shervin-malmasi/>Shevin Malmasi</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a><br><a href=/volumes/W17-12/ class=text-muted>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1201/>Findings of the VarDial Evaluation Campaign 2017<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial Evaluation Campaign 2017</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/n/noemi-aepli/>Noëmi Aepli</a><br><a href=/volumes/W17-12/ class=text-muted>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1201><div class="card-body p-3 small">We present the results of the VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects, which we organized as part of the fourth edition of the VarDial workshop at EACL&#8217;2017. This year, we included four shared tasks : Discriminating between Similar Languages (DSL), Arabic Dialect Identification (ADI), German Dialect Identification (GDI), and Cross-lingual Dependency Parsing (CLP). A total of 19 teams submitted runs across the four <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, and 15 of them wrote system description papers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1216/>Cross-lingual dependency parsing for closely related languages-Helsinki’s submission to VarDial 2017<span class=acl-fixed-case>H</span>elsinki’s submission to <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2017</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W17-12/ class=text-muted>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1216><div class="card-body p-3 small">This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a> seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised models</a> trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> are rather modest. Norwegian works best with information coming from <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> whereas <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> contributes surprisingly little.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4800/>Proceedings of the Third Workshop on Discourse in Machine Translation</a></strong><br><a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a><br><a href=/volumes/W17-48/ class=text-muted>Proceedings of the Third Workshop on Discourse in Machine Translation</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4801.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4801/>Findings of the 2017 DiscoMT Shared Task on Cross-lingual Pronoun Prediction<span class=acl-fixed-case>D</span>isco<span class=acl-fixed-case>MT</span> Shared Task on Cross-lingual Pronoun Prediction</a></strong><br><a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/y/yannick-versley/>Yannick Versley</a><br><a href=/volumes/W17-48/ class=text-muted>Proceedings of the Third Workshop on Discourse in Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4801><div class="card-body p-3 small">We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a <a href=https://en.wikipedia.org/wiki/Pronoun>target-language pronoun</a> given a <a href=https://en.wikipedia.org/wiki/Pronoun>source-language pronoun</a> in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the target-language lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document. We offered four subtasks, each for a different language pair and translation direction : <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-French</a>, <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-German</a>, German-to-English, and <a href=https://en.wikipedia.org/wiki/Spanish_as_a_second_or_foreign_language>Spanish-to-English</a>. Five teams participated in the shared task, making submissions for all language pairs. The evaluation results show that most participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4811/>Neural Machine Translation with Extended Context</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a><br><a href=/volumes/W17-48/ class=text-muted>Proceedings of the Third Workshop on Discourse in Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4811><div class="card-body p-3 small">We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> at least in some selected cases.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=J%C3%B6rg+Tiedemann" title="Search for 'Jörg Tiedemann' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yves-scherrer/ class=align-middle>Yves Scherrer</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/p/preslav-nakov/ class=align-middle>Preslav Nakov</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/m/marcos-zampieri/ class=align-middle>Marcos Zampieri</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/n/nikola-ljubesic/ class=align-middle>Nikola Ljubešić</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/r/raul-vazquez/ class=align-middle>Raúl Vázquez</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/shervin-malmasi/ class=align-middle>Shervin Malmasi</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/ahmed-ali/ class=align-middle>Ahmed Ali</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/u/umut-sulubacak/ class=align-middle>Umut Sulubacak</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/a/aarne-talman/ class=align-middle>Aarne Talman</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/alessandro-raganato/ class=align-middle>Alessandro Raganato</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/mikko-aulamo/ class=align-middle>Mikko Aulamo</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/christian-hardmeier/ class=align-middle>Christian Hardmeier</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sharid-loaiciga/ class=align-middle>Sharid Loáiciga</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sami-virpioja/ class=align-middle>Sami Virpioja</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tommi-jauhiainen/ class=align-middle>Tommi Jauhiainen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marianna-apidianaki/ class=align-middle>Marianna Apidianaki</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stergios-chatzikyriakidis/ class=align-middle>Stergios Chatzikyriakidis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-shao/ class=align-middle>Yan Shao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joakim-nivre/ class=align-middle>Joakim Nivre</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paola-merlo/ class=align-middle>Paola Merlo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/reut-tsarfaty/ class=align-middle>Reut Tsarfaty</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nina-tahmasebi/ class=align-middle>Nina Tahmasebi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noemi-aepli/ class=align-middle>Noëmi Aepli</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bonnie-webber/ class=align-middle>Bonnie Webber</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrei-popescu-belis/ class=align-middle>Andrei Popescu-Belis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sara-stymne/ class=align-middle>Sara Stymne</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mauro-cettolo/ class=align-middle>Mauro Cettolo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yannick-versley/ class=align-middle>Yannick Versley</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-dmitrieva/ class=align-middle>Anna Dmitrieva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mika-hamalainen/ class=align-middle>Mika Hämäläinen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tanja-saily/ class=align-middle>Tanja Säily</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jack-rueter/ class=align-middle>Jack Rueter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eetu-makela/ class=align-middle>Eetu Mäkelä</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mathias-creutz/ class=align-middle>Mathias Creutz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/arvi-hurskainen/ class=align-middle>Arvi Hurskainen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antti-suni/ class=align-middle>Antti Suni</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hande-celikkanat/ class=align-middle>Hande Celikkanat</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sofoklis-kakouros/ class=align-middle>Sofoklis Kakouros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martti-vainio/ class=align-middle>Martti Vainio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emily-ohman/ class=align-middle>Emily Öhman</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marc-pamies/ class=align-middle>Marc Pàmies</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kaisla-kajava/ class=align-middle>Kaisla Kajava</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tommi-nieminen/ class=align-middle>Tommi Nieminen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jenna-kanerva/ class=align-middle>Jenna Kanerva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/akseli-leino/ class=align-middle>Akseli Leino</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/filip-ginter/ class=align-middle>Filip Ginter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/niko-papula/ class=align-middle>Niko Papula</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aku-rouhe/ class=align-middle>Aku Rouhe</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stig-arnegronroos/ class=align-middle>Stig-ArneGrönroos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mikko-kurimo/ class=align-middle>Mikko Kurimo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">17</span></li><li class=list-group-item><a href=/venues/vardial/ class=align-middle>VarDial</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/nodalida/ class=align-middle>NoDaLiDa</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/bsnlp/ class=align-middle>BSNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/americasnlp/ class=align-middle>AmericasNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>