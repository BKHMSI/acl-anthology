<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jieyu Zhao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jieyu</span> <span class=font-weight-bold>Zhao</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.305/>Double Perturbation : On the Robustness of Robustness and Counterfactual Bias Evaluation</a></strong><br><a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/h/huan-zhang/>Huan Zhang</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--305><div class="card-body p-3 small">Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these <a href=https://en.wikipedia.org/wiki/Evaluation>evaluations</a> robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a double perturbation framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models&#8217; robustness and counterfactual bias in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. (1) For <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed <a href=https://en.wikipedia.org/wiki/Cyberattack>attack</a> attains high success rates (96.0%-99.8 %) in finding vulnerable examples on both original and robustly trained <a href=https://en.wikipedia.org/wiki/Computer_simulation>CNNs</a> and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4posimpact-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4posimpact-1.0/>Proceedings of the 1st Workshop on NLP for Positive Impact</a></strong><br><a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/z/zhijing-jin/>Zhijing Jin</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a><br><a href=/volumes/2021.nlp4posimpact-1/ class=text-muted>Proceedings of the 1st Workshop on NLP for Positive Impact</a></span></p><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939216 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.155/>LOGAN : Local Group Bias Detection by Clustering<span class=acl-fixed-case>LOGAN</span>: Local Group Bias Detection by Clustering</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--155><div class="card-body p-3 small">Machine learning techniques have been widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. However, as revealed by many recent studies, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between <a href=https://en.wikipedia.org/wiki/Protected_group>protected groups</a> and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how <a href=https://en.wikipedia.org/wiki/Bias>biases</a> are embedded in a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In fact, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with similar aggregated performance between different groups on the entire data may behave differently on instances in a <a href=https://en.wikipedia.org/wiki/Region>local region</a>. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--260 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928955 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.260/>Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/s/saghar-hosseini/>Saghar Hosseini</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/a/ahmed-hassan-awadallah/>Ahmed Hassan Awadallah</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--260><div class="card-body p-3 small">Multilingual representations embed words from many languages into a single <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) model</a> trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> from the source to target languages. In this paper, we study <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in multilingual embeddings and how it affects <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We further provide recommendations for using the multilingual word representations for <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928917 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.264/>Mitigating Gender Bias Amplification in Distribution by Posterior Regularization</a></strong><br><a href=/people/s/shengyu-jia/>Shengyu Jia</a>
|
<a href=/people/t/tao-meng/>Tao Meng</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--264><div class="card-body p-3 small">Advanced machine learning techniques have boosted the performance of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models&#8217; top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--265 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.acl-main.265.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929244 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.265/>Towards Understanding Gender Bias in Relation Extraction</a></strong><br><a href=/people/a/andrew-gaut/>Andrew Gaut</a>
|
<a href=/people/t/tony-sun/>Tony Sun</a>
|
<a href=/people/s/shirlyn-tang/>Shirlyn Tang</a>
|
<a href=/people/y/yuxin-huang/>Yuxin Huang</a>
|
<a href=/people/j/jing-qian/>Jing Qian</a>
|
<a href=/people/m/mai-elsherief/>Mai ElSherief</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/d/diba-mirza/>Diba Mirza</a>
|
<a href=/people/e/elizabeth-belding/>Elizabeth Belding</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--265><div class="card-body p-3 small">Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, there have been no attempts in the literature to evaluate <a href=https://en.wikipedia.org/wiki/Bias>social biases</a> exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10 % human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as <a href=https://en.wikipedia.org/wiki/Birth_date>birthDate</a> or <a href=https://en.wikipedia.org/wiki/Place_of_birth>birthPlace</a>. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1531 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1531/>Examining Gender Bias in <a href=https://en.wikipedia.org/wiki/Language>Languages</a> with Grammatical Gender</a></strong><br><a href=/people/p/pei-zhou/>Pei Zhou</a>
|
<a href=/people/w/weijia-shi/>Weijia Shi</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/k/kuan-hao-huang/>Kuan-Hao Huang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1531><div class="card-body p-3 small">Recent studies have shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> exhibit <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such <a href=https://en.wikipedia.org/wiki/Bias>bias</a> only in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. These analyses can not be directly extended to <a href=https://en.wikipedia.org/wiki/Language>languages</a> that exhibit <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>morphological agreement</a> on gender, such as <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. In this paper, we propose new metrics for evaluating <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> of these languages and further demonstrate evidence of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in bilingual embeddings which align these languages with <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Finally, we extend an existing approach to mitigate gender bias in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> of these <a href=https://en.wikipedia.org/wiki/Language>languages</a> under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347396468 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1064/>Gender Bias in Contextualized Word Embeddings</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1064><div class="card-body p-3 small">In this paper, we quantify, analyze and mitigate <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> exhibited in ELMo&#8217;s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N18-2003.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-2003/>Gender Bias in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> : Evaluation and Debiasing Methods</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2003><div class="card-body p-3 small">In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for co-reference resolution focused on <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a>, WinoBias. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1323 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1323" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1323/>Men Also Like Shopping : Reducing Gender Bias Amplification using Corpus-level Constraints</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1323><div class="card-body p-3 small">Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study <a href=https://en.wikipedia.org/wiki/Data>data</a> and <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> associated with multilabel object classification and visual semantic role labeling. We find that (a) <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for these tasks contain significant <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> and (b) <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> further amplify existing bias. For example, the activity cooking is over 33 % more likely to involve females than males in a training set, and a trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> further amplifies the disparity to 68 % at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on <a href=https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field>Lagrangian relaxation</a> for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5 % and 40.5 % for multilabel classification and visual semantic role labeling, respectively</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jieyu+Zhao" title="Search for 'Jieyu Zhao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/k/kai-wei-chang/ class=align-middle>Kai-Wei Chang</a>
<span class="badge badge-secondary align-middle ml-2">9</span></li><li class=list-group-item><a href=/people/t/tianlu-wang/ class=align-middle>Tianlu Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/mark-yatskar/ class=align-middle>Mark Yatskar</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/v/vicente-ordonez/ class=align-middle>Vicente Ordonez</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/r/ryan-cotterell/ class=align-middle>Ryan Cotterell</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/subhabrata-mukherjee/ class=align-middle>Subhabrata Mukherjee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/saghar-hosseini/ class=align-middle>Saghar Hosseini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmed-hassan-awadallah/ class=align-middle>Ahmed Hassan Awadallah</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shengyu-jia/ class=align-middle>Shengyu Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tao-meng/ class=align-middle>Tao Meng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-gaut/ class=align-middle>Andrew Gaut</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tony-sun/ class=align-middle>Tony Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shirlyn-tang/ class=align-middle>Shirlyn Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuxin-huang/ class=align-middle>Yuxin Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jing-qian/ class=align-middle>Jing Qian</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mai-elsherief/ class=align-middle>Mai ElSherief</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/diba-mirza/ class=align-middle>Diba Mirza</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/elizabeth-belding/ class=align-middle>Elizabeth Belding</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/william-yang-wang/ class=align-middle>William Yang Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pei-zhou/ class=align-middle>Pei Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/weijia-shi/ class=align-middle>Weijia Shi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kuan-hao-huang/ class=align-middle>Kuan-Hao Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/muhao-chen/ class=align-middle>Muhao Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chong-zhang/ class=align-middle>Chong Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/huan-zhang/ class=align-middle>Huan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cho-jui-hsieh/ class=align-middle>Cho-Jui Hsieh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anjalie-field/ class=align-middle>Anjalie Field</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shrimai-prabhumoye/ class=align-middle>Shrimai Prabhumoye</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maarten-sap/ class=align-middle>Maarten Sap</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhijing-jin/ class=align-middle>Zhijing Jin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chris-brockett/ class=align-middle>Chris Brockett</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/nlp4posimpact/ class=align-middle>NLP4PosImpact</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>