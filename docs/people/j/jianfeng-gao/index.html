<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jianfeng Gao - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jianfeng</span> <span class=font-weight-bold>Gao</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--341 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.341/>RADDLE : An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems<span class=acl-fixed-case>RADDLE</span>: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems</a></strong><br><a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/z/zhu-zhang/>Zhu Zhang</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--341><div class="card-body p-3 small">For task-oriented dialog systems to be maximally useful, it must be able to process conversations in a way that is (1) generalizable with a small number of training examples for new task domains, and (2) robust to user input in various styles, modalities, or domains. In pursuit of these goals, we introduce the RADDLE benchmark, a collection of corpora and tools for evaluating the performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> across a diverse set of domains. By including tasks with limited training data, RADDLE is designed to favor and encourage <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with a strong <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a>. RADDLE also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variations</a>, <a href=https://en.wikipedia.org/wiki/Speech_error>speech errors</a>, unseen entities, and out-of-domain utterances. We evaluate recent state-of-the-art systems based on pre-training and fine-tuning, and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain. Adversarial training is also proposed to improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a> against <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy inputs</a>. Overall, existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are less than satisfactory in robustness evaluation, which suggests opportunities for future improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.381/>Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization</a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/p/paul-smolensky/>Paul Smolensky</a>
|
<a href=/people/p/paul-soulos/>Paul Soulos</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hamid-palangi/>Hamid Palangi</a>
|
<a href=/people/r/roland-fernandez/>Roland Fernandez</a>
|
<a href=/people/c/caitlin-smith/>Caitlin Smith</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--381><div class="card-body p-3 small">Abstractive summarization, the task of generating a concise summary of input documents, requires : (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs. (Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.378/>Optimus : Organizing Sentences via Pre-trained Modeling of a Latent Space</a></strong><br><a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiang-gao/>Xiang Gao</a>
|
<a href=/people/y/yuan-li/>Yuan Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--378><div class="card-body p-3 small">When trained effectively, the Variational Autoencoder (VAE) can be both a powerful <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> and an effective representation learning framework for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the <a href=https://en.wikipedia.org/wiki/Latent_vector>latent vectors</a>. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.463.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938933 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.463" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.463/>Understanding the Difficulty of Training Transformers</a></strong><br><a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--463><div class="card-body p-3 small">Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train <a href=https://en.wikipedia.org/wiki/Transformers_(TV_series)>Transformers</a> effectively). Our objective here is to understand _ _ what complicates Transformer training _ _ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> substantiallyfor each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies <a href=https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)>small parameter perturbations</a> (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage&#8217;s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928798 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-main.197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.197/>SMART : Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization<span class=acl-fixed-case>SMART</span>: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization</a></strong><br><a href=/people/h/haoming-jiang/>Haoming Jiang</a>
|
<a href=/people/p/pengcheng-he/>Pengcheng He</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--197><div class="card-body p-3 small">Transfer learning has fundamentally changed the landscape of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for pre-trained models to attain better <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> performance. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> contains two important ingredients : 1. Smoothness-inducing regularization, which effectively manages the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> ; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928611 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.16/>The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding<span class=acl-fixed-case>M</span>icrosoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding</a></strong><br><a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yu-wang/>Yu Wang</a>
|
<a href=/people/j/jianshu-ji/>Jianshu Ji</a>
|
<a href=/people/h/hao-cheng/>Hao Cheng</a>
|
<a href=/people/x/xueyun-zhu/>Xueyun Zhu</a>
|
<a href=/people/e/emmanuel-awa/>Emmanuel Awa</a>
|
<a href=/people/p/pengcheng-he/>Pengcheng He</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/h/hoifung-poon/>Hoifung Poon</a>
|
<a href=/people/g/guihong-cao/>Guihong Cao</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--16><div class="card-body p-3 small">We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon <a href=https://en.wikipedia.org/wiki/PyTorch>PyTorch</a> and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and <a href=https://en.wikipedia.org/wiki/Character_encoding>text encoders</a> (e.g., <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNNs</a>, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pre-trained models will be publicly available at https://github.com/namisan/mt-dnn.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928597 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.acl-demos.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.19/>ConvLab-2 : An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems<span class=acl-fixed-case>C</span>onv<span class=acl-fixed-case>L</span>ab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems</a></strong><br><a href=/people/q/qi-zhu/>Qi Zhu</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/y/yan-fang/>Yan Fang</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/r/ryuichi-takanobu/>Ryuichi Takanobu</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--19><div class="card-body p-3 small">We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab, ConvLab-2 inherits ConvLab&#8217;s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an <a href=https://en.wikipedia.org/wiki/Analysis>analysis tool</a> and an <a href=https://en.wikipedia.org/wiki/Interactive_computing>interactive tool</a> to assist researchers in diagnosing <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and <a href=https://en.wikipedia.org/wiki/Systems_engineering>system improvement</a>. The interactive tool provides an <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a> that allows developers to diagnose an assembled dialogue system by interacting with the <a href=https://en.wikipedia.org/wiki/System>system</a> and modifying the output of each system component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-demos.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-demos--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-demos.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38928605 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-demos.39/>Conversation Learner-A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems<span class=acl-fixed-case>C</span>onversation <span class=acl-fixed-case>L</span>earner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems</a></strong><br><a href=/people/s/swadheen-shukla/>Swadheen Shukla</a>
|
<a href=/people/l/lars-liden/>Lars Liden</a>
|
<a href=/people/s/shahin-shayandeh/>Shahin Shayandeh</a>
|
<a href=/people/e/eslam-kamal/>Eslam Kamal</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/m/matt-mazzola/>Matt Mazzola</a>
|
<a href=/people/t/thomas-park/>Thomas Park</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/2020.acl-demos/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-demos--39><div class="card-body p-3 small">Traditionally, industry solutions for building a task-oriented dialog system have relied on helping dialog authors define rule-based dialog managers, represented as dialog flows. While dialog flows are intuitively interpretable and good for simple scenarios, they fall short of performance in terms of the flexibility needed to handle complex dialogs. On the other hand, purely machine-learned models can handle complex dialogs, but they are considered to be black boxes and require large amounts of training data. In this demonstration, we showcase Conversation Learner, a machine teaching tool for building dialog managers. It combines the best of both approaches by enabling dialog authors to create a dialog flow using familiar tools, converting the dialog flow into a parametric model (e.g., neural networks), and allowing dialog authors to improve the <a href=https://en.wikipedia.org/wiki/Dialog_manager>dialog manager</a> (i.e., the parametric model) over time by leveraging user-system dialog logs as training data through a machine teaching interface.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1159.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1159/>Robust Navigation with Language Pretraining and Stochastic Sampling</a></strong><br><a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/q/qiaolin-xia/>Qiaolin Xia</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1159><div class="card-body p-3 small">Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-to-Room benchmark with 6 % absolute gain over the previous best result (47 %-53 %) on the Success Rate weighted by Path Length metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1021.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1021/>Cyclical Annealing Schedule : A Simple Approach to Mitigating KL Vanishing<span class=acl-fixed-case>KL</span> Vanishing</a></strong><br><a href=/people/h/hao-fu/>Hao Fu</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1021><div class="card-body p-3 small">Variational autoencoders (VAE) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. VAE objective consists of two terms, the KL regularization term and the reconstruction term, balanced by a weighting hyper-parameter. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a>. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, dialog response generation and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised text classification</a>.<tex-math>\\beta</tex-math>. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for <tex-math>\\beta</tex-math>, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of optimization. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing <tex-math>\\beta</tex-math> multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including language modeling, dialog response generation and semi-supervised text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1271 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364750438 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1271/>Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension</a></strong><br><a href=/people/y/yichong-xu/>Yichong Xu</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/j/jingjing-liu/>Jingjing Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1271><div class="card-body p-3 small">We propose a multi-task learning framework to learn a joint Machine Reading Comprehension (MRC) model that can be applied to a wide range of MRC tasks in different domains. Inspired by recent ideas of data selection in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, we develop a novel sample re-weighting scheme to assign sample-specific weights to the loss. Empirical study shows that our approach can be applied to many existing MRC models. Combined with contextual representations from pre-trained language models (such as ELMo), we achieve new state-of-the-art results on a set of MRC benchmark datasets. We release our code at.<url>https://github.com/xycforgithub/MultiTask-MRC</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1200.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1200/>Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models</a></strong><br><a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/l/liqun-chen/>Liqun Chen</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1200><div class="card-body p-3 small">Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a> with <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-VAEs</a>. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1648.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1648 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1648/>Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog</a></strong><br><a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a>
|
<a href=/people/a/ahmed-kholy/>Ahmed Kholy</a>
|
<a href=/people/l/linjie-li/>Linjie Li</a>
|
<a href=/people/j/jingjing-liu/>Jingjing Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1648><div class="card-body p-3 small">This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47 % NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305937184 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1253/>Subgoal Discovery for Hierarchical Dialogue Policy Learning</a></strong><br><a href=/people/d/da-tang/>Da Tang</a>
|
<a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/c/chong-wang/>Chong Wang</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/t/tony-jebara/>Tony Jebara</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1253><div class="card-body p-3 small">Developing <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a>. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these <a href=https://en.wikipedia.org/wiki/Goal>subgoals</a> to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined subgoals. Moreover, we show that the learned <a href=https://en.wikipedia.org/wiki/Goal>subgoals</a> are often human comprehensible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-1157.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P18-1157" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P18-1157/>Stochastic Answer Networks for Machine Reading Comprehension</a></strong><br><a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/P18-1/ class=text-muted>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-1157><div class="card-body p-3 small">We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1047.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1047/>Image-Grounded Conversations : Multimodal Context for Natural Question and Response Generation</a></strong><br><a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/l/lucy-vanderwende/>Lucy Vanderwende</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1047><div class="card-body p-3 small">The popularity of <a href=https://en.wikipedia.org/wiki/Image_sharing>image sharing</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and the engagement it creates between users reflect the important role that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a> plays in <a href=https://en.wikipedia.org/wiki/Conversation>everyday conversations</a>. We present a novel task, Image Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiple reference dataset of crowd-sourced, event-centric conversations on images. IGC falls on the continuum between <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a> and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialog research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1061/>Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models</a></strong><br><a href=/people/y/yi-luan/>Yi Luan</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1061><div class="card-body p-3 small">Building a persona-based conversation agent is challenging owing to the lack of large amounts of speaker-specific conversation data for model training. This paper addresses the problem by proposing a multi-task learning approach to training neural conversation models that leverages both conversation data across speakers and other types of <a href=https://en.wikipedia.org/wiki/Data>data</a> pertaining to the speaker and speaker roles to be modeled. Experiments show that our approach leads to significant improvements over baseline model quality, generating responses that capture more precisely speakers&#8217; traits and <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>speaking styles</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1074/>End-to-End Task-Completion Neural Dialogue Systems</a></strong><br><a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1074><div class="card-body p-3 small">One of the major drawbacks of modularized task-completion dialogue systems is that each <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> is trained individually, which presents several challenges. For example, downstream modules are affected by earlier <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, and the performance of the entire <a href=https://en.wikipedia.org/wiki/System>system</a> is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues. Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle <a href=https://en.wikipedia.org/wiki/Noise>noises</a> caused by other components of the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1096/>An Empirical Analysis of Multiple-Turn Reasoning Strategies in Reading Comprehension Tasks</a></strong><br><a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1096><div class="card-body p-3 small">Reading comprehension (RC) is a challenging task that requires synthesis of information across sentences and multiple turns of reasoning. Using a state-of-the-art RC model, we empirically investigate the performance of single-turn and multiple-turn reasoning on the SQuAD and MS MARCO datasets. The <a href=https://en.wikipedia.org/wiki/RC_model>RC model</a> is an end-to-end neural network with iterative attention, and uses <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to dynamically control the number of turns. We find that multiple-turn reasoning outperforms single-turn reasoning for all question and answer types ; further, we observe that enabling a flexible number of turns generally improves upon a fixed multiple-turn strategy. % across all question types, and is particularly beneficial to questions with lengthy, descriptive answers. We achieve results competitive to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-5003/>Open-Domain Neural Dialogue Systems</a></strong><br><a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/I17-5/ class=text-muted>Proceedings of the IJCNLP 2017, Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-5003><div class="card-body p-3 small">In the past decade, spoken dialogue systems have been the most prominent component in today&#8217;s personal assistants. A lot of devices have incorporated dialogue system modules, which allow users to speak naturally in order to finish tasks more efficiently. The traditional conversational systems have rather complex and/or modular pipelines. The advance of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a> has recently risen the applications of neural models to dialogue modeling. Nevertheless, applying deep learning technologies for building robust and scalable dialogue systems is still a challenging task and an open research area as it requires deeper understanding of the classic pipelines as well as detailed knowledge on the benchmark of the models of the prior work and the recent state-of-the-art work. Therefore, this tutorial is designed to focus on an overview of the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> development while describing most recent research for building task-oriented and chit-chat dialogue systems, and summarizing the challenges. We target the audience of students and practitioners who have some deep learning background, who want to get more familiar with conversational dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953832 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1045/>Towards <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>End-to-End Reinforcement Learning</a> of Dialogue Agents for Information Access</a></strong><br><a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/f/faisal-ahmad/>Faisal Ahmed</a>
|
<a href=/people/l/li-deng/>Li Deng</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1045><div class="card-body p-3 small">This paper proposes KB-InfoBot-a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the <a href=https://en.wikipedia.org/wiki/System>system</a> and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced soft posterior distribution over the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> that indicates which entities the user is interested in. Integrating the soft retrieval process with a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learner</a> leads to higher task success rate and <a href=https://en.wikipedia.org/wiki/Reward_system>reward</a> in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956745 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1070/>A Nested Attention Neural Hybrid Model for Grammatical Error Correction</a></strong><br><a href=/people/j/jianshu-ji/>Jianshu Ji</a>
|
<a href=/people/q/qinlong-wang/>Qinlong Wang</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/y/yongen-gong/>Yongen Gong</a>
|
<a href=/people/s/steven-truong/>Steven Truong</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1070><div class="card-body p-3 small">Grammatical error correction (GEC) systems strive to correct both global errors inword order and usage, and local errors inspelling and <a href=https://en.wikipedia.org/wiki/Inflection>inflection</a>. Further developing upon recent work on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, we propose a new hybrid neural model with nested attention layers for GEC.Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective incorrecting local errors that involve small edits in orthography.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2608/>Modeling Large-Scale Structured Relationships with <a href=https://en.wikipedia.org/wiki/Shared_memory>Shared Memory</a> for Knowledge Base Completion</a></strong><br><a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a><br><a href=/volumes/W17-26/ class=text-muted>Proceedings of the 2nd Workshop on Representation Learning for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2608><div class="card-body p-3 small">Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, learning multi-step relations directly on top of observed triplets could be costly. Hence, a manually designed procedure is often used when training the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform multi-step inference implicitly through a <a href=https://en.wikipedia.org/wiki/Controller_(computing)>controller</a> and <a href=https://en.wikipedia.org/wiki/Shared_memory>shared memory</a>. Without a human-designed inference procedure, IRNs use training data to learn to perform multi-step inference in an embedding neural space through the <a href=https://en.wikipedia.org/wiki/Shared_memory>shared memory</a> and controller. While the <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> does not explicitly operate on top of observed triplets, our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms all previous <a href=https://en.wikipedia.org/wiki/Statistical_inference>approaches</a> on the popular FB15k benchmark by more than 5.7 %.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jianfeng+Gao" title="Search for 'Jianfeng Gao' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/x/xiaodong-liu/ class=align-middle>Xiaodong Liu</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/x/xiujun-li/ class=align-middle>Xiujun Li</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/a/asli-celikyilmaz/ class=align-middle>Asli Celikyilmaz</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/b/baolin-peng/ class=align-middle>Baolin Peng</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/c/chunyuan-li/ class=align-middle>Chunyuan Li</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/y/yelong-shen/ class=align-middle>Yelong Shen</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/j/jinchao-li/ class=align-middle>Jinchao Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/y/yun-nung-chen/ class=align-middle>Yun-Nung Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lihong-li/ class=align-middle>Lihong Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/w/weizhu-chen/ class=align-middle>Weizhu Chen</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/chris-brockett/ class=align-middle>Chris Brockett</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/w/william-b-dolan/ class=align-middle>William B. Dolan</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/michel-galley/ class=align-middle>Michel Galley</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kevin-duh/ class=align-middle>Kevin Duh</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/y/yizhe-zhang/ class=align-middle>Yizhe Zhang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/p/pengcheng-he/ class=align-middle>Pengcheng He</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jianshu-ji/ class=align-middle>Jianshu Ji</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/l/lawrence-carin/ class=align-middle>Lawrence Carin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jingjing-liu/ class=align-middle>Jingjing Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhu-zhang/ class=align-middle>Zhu Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chenguang-zhu/ class=align-middle>Chenguang Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nasrin-mostafazadeh/ class=align-middle>Nasrin Mostafazadeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/georgios-spithourakis/ class=align-middle>Georgios Spithourakis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lucy-vanderwende/ class=align-middle>Lucy Vanderwende</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yi-luan/ class=align-middle>Yi Luan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-gao/ class=align-middle>Xiang Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuan-li/ class=align-middle>Yuan Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liyuan-liu/ class=align-middle>Liyuan Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiawei-han/ class=align-middle>Jiawei Han</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/haoming-jiang/ class=align-middle>Haoming Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tuo-zhao/ class=align-middle>Tuo Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-wang/ class=align-middle>Yu Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-cheng/ class=align-middle>Hao Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xueyun-zhu/ class=align-middle>Xueyun Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanuel-awa/ class=align-middle>Emmanuel Awa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hoifung-poon/ class=align-middle>Hoifung Poon</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/guihong-cao/ class=align-middle>Guihong Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qi-zhu/ class=align-middle>Qi Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zheng-zhang/ class=align-middle>Zheng Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yan-fang/ class=align-middle>Yan Fang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiang-li/ class=align-middle>Xiang Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ryuichi-takanobu/ class=align-middle>Ryuichi Takanobu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoyan-zhu/ class=align-middle>Xiaoyan Zhu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/minlie-huang/ class=align-middle>Minlie Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/swadheen-shukla/ class=align-middle>Swadheen Shukla</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lars-liden/ class=align-middle>Lars Liden</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shahin-shayandeh/ class=align-middle>Shahin Shayandeh</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eslam-kamal/ class=align-middle>Eslam Kamal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/matt-mazzola/ class=align-middle>Matt Mazzola</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-park/ class=align-middle>Thomas Park</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bhuwan-dhingra/ class=align-middle>Bhuwan Dhingra</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faisal-ahmad/ class=align-middle>Faisal Ahmad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/li-deng/ class=align-middle>Li Deng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qinlong-wang/ class=align-middle>Qinlong Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kristina-toutanova/ class=align-middle>Kristina Toutanova</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yongen-gong/ class=align-middle>Yongen Gong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steven-truong/ class=align-middle>Steven Truong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/po-sen-huang/ class=align-middle>Po-Sen Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/ming-wei-chang/ class=align-middle>Ming-Wei Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/da-tang/ class=align-middle>Da Tang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chong-wang/ class=align-middle>Chong Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tony-jebara/ class=align-middle>Tony Jebara</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/q/qiaolin-xia/ class=align-middle>Qiaolin Xia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yonatan-bisk/ class=align-middle>Yonatan Bisk</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-a-smith/ class=align-middle>Noah A. Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yejin-choi/ class=align-middle>Yejin Choi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichen-jiang/ class=align-middle>Yichen Jiang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-smolensky/ class=align-middle>Paul Smolensky</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-soulos/ class=align-middle>Paul Soulos</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sudha-rao/ class=align-middle>Sudha Rao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hamid-palangi/ class=align-middle>Hamid Palangi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/roland-fernandez/ class=align-middle>Roland Fernandez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/caitlin-smith/ class=align-middle>Caitlin Smith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/mohit-bansal/ class=align-middle>Mohit Bansal</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hao-fu/ class=align-middle>Hao Fu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yichong-xu/ class=align-middle>Yichong Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dinghan-shen/ class=align-middle>Dinghan Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/liqun-chen/ class=align-middle>Liqun Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xin-wang/ class=align-middle>Xin Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhe-gan/ class=align-middle>Zhe Gan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yu-cheng/ class=align-middle>Yu Cheng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ahmed-kholy/ class=align-middle>Ahmed Kholy</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/linjie-li/ class=align-middle>Linjie Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">10</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>