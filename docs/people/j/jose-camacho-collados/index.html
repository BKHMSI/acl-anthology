<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jose Camacho-Collados - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jose</span> <span class=font-weight-bold>Camacho-Collados</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.280" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.280/>BERT is to NLP what AlexNet is to CV : Can Pre-Trained Language Models Identify Analogies?<span class=acl-fixed-case>BERT</span> is to <span class=acl-fixed-case>NLP</span> what <span class=acl-fixed-case>A</span>lex<span class=acl-fixed-case>N</span>et is to <span class=acl-fixed-case>CV</span>: Can Pre-Trained Language Models Identify Analogies?</a></strong><br><a href=/people/a/asahi-ushio/>Asahi Ushio</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--280><div class="card-body p-3 small">Analogies play a central role in human commonsense reasoning. The ability to recognize <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> such as eye is to seeing what ear is to hearing, sometimes referred to as <a href=https://en.wikipedia.org/wiki/Analogy>analogical proportions</a>, shape how we structure knowledge and understand <a href=https://en.wikipedia.org/wiki/Language>language</a>. Surprisingly, however, the task of identifying such <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> has not yet received much attention in the <a href=https://en.wikipedia.org/wiki/Language_model>language model era</a>. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> obtained from educational settings, as well as more commonly used <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eacl-main.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eacl-main--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eacl-main.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eacl-main.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eacl-main.140/>WiC-TSV : An Evaluation Benchmark for Target Sense Verification of Words in Context<span class=acl-fixed-case>WiC-TSV</span>: <span class=acl-fixed-case>A</span>n Evaluation Benchmark for Target Sense Verification of Words in Context</a></strong><br><a href=/people/a/anna-breit/>Anna Breit</a>
|
<a href=/people/a/artem-revenko/>Artem Revenko</a>
|
<a href=/people/k/kiamehr-rezaee/>Kiamehr Rezaee</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/2021.eacl-main/ class=text-muted>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eacl-main--140><div class="card-body p-3 small">We present WiC-TSV, a new multi-domain evaluation benchmark for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> highly flexible for the evaluation of a diverse set of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We set baseline performance on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> using state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. Experimental results show that even though these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can perform decently on the task, there remains a gap between machine and human performance, especially in out-of-domain settings. WiC-TSV data is available at https://competitions.codalab.org/competitions/23683.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-2.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-2--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-2.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cl-2.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cl-2.14/>Analysis and Evaluation of <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Word Sense Disambiguation</a></strong><br><a href=/people/d/daniel-loureiro/>Daniel Loureiro</a>
|
<a href=/people/k/kiamehr-rezaee/>Kiamehr Rezaee</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/2021.cl-2/ class=text-muted>Computational Linguistics, Volume 47, Issue 2 - June 2021</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-2--14><div class="card-body p-3 small">Abstract Transformer-based language models have taken many fields in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to <a href=https://en.wikipedia.org/wiki/Ambiguity>lexical ambiguity</a>. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--712 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.712" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.712/>Distilling Relation Embeddings from Pretrained Language Models</a></strong><br><a href=/people/a/asahi-ushio/>Asahi Ushio</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--712><div class="card-body p-3 small">Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation embeddings</a>, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository : https://github.com/asahi417/relbert</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.148/>TweetEval : Unified Benchmark and Comparative Evaluation for Tweet Classification<span class=acl-fixed-case>T</span>weet<span class=acl-fixed-case>E</span>val: Unified Benchmark and Comparative Evaluation for Tweet Classification</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/l/leonardo-neves/>Leonardo Neves</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--148><div class="card-body p-3 small">The experimental landscape in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.481.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--481 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.481 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.481/>Go Simple and Pre-Train on Domain-Specific Corpora : On the Role of Training Data for Text Classification</a></strong><br><a href=/people/a/aleksandra-edwards/>Aleksandra Edwards</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/h/helene-de-ribaupierre/>Hélène De Ribaupierre</a>
|
<a href=/people/a/alun-preece/>Alun Preece</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--481><div class="card-body p-3 small">Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, i.e., <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> or <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. As for the comparison, BERT outperforms all baselines in standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with large training sets. However, in settings with small training datasets a simple method like <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--706 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.706/>A Short Survey on Sense-Annotated Corpora</a></strong><br><a href=/people/t/tommaso-pasini/>Tommaso Pasini</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--706><div class="card-body p-3 small">Large sense-annotated datasets are increasingly necessary for training deep supervised systems in <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>. However, gathering high-quality sense-annotated data for as many instances as possible is a laborious and expensive task. This has led to the proliferation of automatic and semi-automatic methods for overcoming the so-called knowledge-acquisition bottleneck. In this short survey we present an overview of sense-annotated corpora, annotated either manually- or (semi)automatically, that are currently available for different languages and featuring distinct <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resources</a> as inventory of senses, i.e. WordNet, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a>. Furthermore, we provide the reader with general statistics of each dataset and an analysis of their specific features.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S19-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S19-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-S19-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S19-2091/>UA at SemEval-2019 Task 5 : Setting A Strong Linear Baseline for Hate Speech Detection<span class=acl-fixed-case>UA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2019 Task 5: Setting A Strong Linear Baseline for Hate Speech Detection</a></strong><br><a href=/people/c/carlos-perello/>Carlos Perelló</a>
|
<a href=/people/d/david-tomas/>David Tomás</a>
|
<a href=/people/a/alberto-garcia-garcia/>Alberto Garcia-Garcia</a>
|
<a href=/people/j/jose-garcia-rodriguez/>Jose Garcia-Rodriguez</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/S19-2/ class=text-muted>Proceedings of the 13th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S19-2091><div class="card-body p-3 small">This paper describes the system developed at the University of Alicante (UA) for the SemEval 2019 Task 5 : Shared Task on Multilingual Detection of Hate. The purpose of this work is to build a strong baseline for hate speech detection, using a traditional machine learning approach with standard <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>textual features</a>, which could serve in a near future as a reference to compare with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning systems</a>. We participated in both task A (Hate Speech Detection against Immigrants and Women) and task B (Aggressive behavior and Target Classification). Despite its simplicity, our <a href=https://en.wikipedia.org/wiki/System>system</a> obtained a remarkable <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 72.5 (sixth highest) and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 73.6 (second highest) in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> (task A), outperforming more complex neural models from a total of 40 participant systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5800/>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a><br><a href=/volumes/W19-58/ class=text-muted>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1128/>WiC : the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations<span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>C</span>: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations</a></strong><br><a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/N19-1/ class=text-muted>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1128><div class="card-body p-3 small">By design, word embeddings are unable to model the <a href=https://en.wikipedia.org/wiki/Semantics>dynamic nature of words&#8217; semantics</a>, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few <a href=https://en.wikipedia.org/wiki/Benchmarking>evaluation benchmarks</a> exist that specifically focus on the dynamic semantics of words. In this paper we show that existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1003/>SemEval 2018 Task 2 : Multilingual Emoji Prediction<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2018 Task 2: Multilingual Emoji Prediction</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/f/francesco-ronzano/>Francesco Ronzano</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1003><div class="card-body p-3 small">This paper describes the results of the first Shared Task on Multilingual Emoji Prediction, organized as part of SemEval 2018. Given the text of a tweet, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> consists of predicting the most likely <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> to be used along such tweet. Two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a> were proposed, one for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and one for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and participants were allowed to submit a <a href=https://en.wikipedia.org/wiki/System>system</a> run to one or both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a>. In total, 49 teams participated to the <a href=https://en.wikipedia.org/wiki/English_language>English subtask</a> and 22 teams submitted a <a href=https://en.wikipedia.org/wiki/System>system</a> run to the <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish subtask</a>. Evaluation was carried out emoji-wise, and the final <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> was based on macro F-Score. Data and further information about this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can be found at.<url>https://competitions.codalab.org/competitions/17344</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-1115/>SemEval-2018 Task 9 : Hypernym Discovery<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 9: Hypernym Discovery</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/s/sergio-oramas/>Sergio Oramas</a>
|
<a href=/people/t/tommaso-pasini/>Tommaso Pasini</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a><br><a href=/volumes/S18-1/ class=text-muted>Proceedings of The 12th International Workshop on Semantic Evaluation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-1115><div class="card-body p-3 small">This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a>, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows : given an input term, retrieve (or discover) its suitable <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>subtasks</a>. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can be found at.<url>https://competitions.codalab.org/competitions/17119</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2011/>How Gender and Skin Tone Modifiers Affect Emoji Semantics in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a><span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a><br><a href=/volumes/S18-2/ class=text-muted>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2011><div class="card-body p-3 small">In this paper we analyze the use of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> with respect to <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and <a href=https://en.wikipedia.org/wiki/Human_skin_color>skin tone</a>. By gathering a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of over twenty two million tweets from United States some findings are clearly highlighted after performing a simple frequency-based analysis. Moreover, we carry out a <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a> on the usage of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and their modifiers (e.g. gender and skin tone) by embedding all <a href=https://en.wikipedia.org/wiki/Word>words</a>, <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and modifiers into the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>. Our analyses reveal that some <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> related to the <a href=https://en.wikipedia.org/wiki/Human_skin_color>skin color</a> and gender seem to be reflected on the use of these modifiers. For example, <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> representing <a href=https://en.wikipedia.org/wiki/List_of_gestures>hand gestures</a> are more widely utilized with lighter skin tones, and the usage across <a href=https://en.wikipedia.org/wiki/Human_skin_color>skin tones</a> differs significantly. At the same time, the vector corresponding to the male modifier tends to be semantically close to <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> related to business or technology, whereas their female counterparts appear closer to <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> about love or makeup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-6004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-6004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-6004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/279154260 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N18-6004/>The interplay between lexical resources and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a><br><a href=/volumes/N18-6/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-6004><div class="card-body p-3 small">Incorporating linguistic, world and common sense knowledge into AI / NLP systems is currently an important research area, with several open problems and challenges. At the same time, processing and storing this knowledge in <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resources</a> is not a straightforward task. We propose to address these complementary goals from two methodological perspectives : the use of NLP methods to help the process of constructing and enriching lexical resources and the use of lexical resources for improving NLP applications. This tutorial may be useful for two main types of audience : those working on language resources who are interested in becoming acquainted with automatic NLP techniques, with the end goal of speeding and/or easing up the process of resource curation ; and on the other hand, researchers in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> who would like to benefit from the knowledge of lexical resources to improve their systems and models.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K17-1012.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K17-1012/>Embedding Words and Senses Together via Joint Knowledge-Enhanced Training</a></strong><br><a href=/people/m/massimiliano-mancini/>Massimiliano Mancini</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/i/ignacio-iacobacci/>Ignacio Iacobacci</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/K17-1/ class=text-muted>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1012><div class="card-body p-3 small">Word embeddings are widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, mainly due to their success in capturing <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> from <a href=https://en.wikipedia.org/wiki/Mass_media>massive corpora</a>. However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector. We address this issue by proposing a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> which learns word and sense embeddings jointly. Our model exploits large corpora and knowledge from <a href=https://en.wikipedia.org/wiki/Semantic_network>semantic networks</a> in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to state-of-the-art word- and sense-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1170/>Towards a Seamless Integration of Word Senses into Downstream NLP Applications<span class=acl-fixed-case>NLP</span> Applications</a></strong><br><a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a><br><a href=/volumes/P17-1/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1170><div class="card-body p-3 small">Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP systems</a> has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2094/>EuroSense : Automatic Harvesting of Multilingual Sense Annotations from Parallel Text<span class=acl-fixed-case>E</span>uro<span class=acl-fixed-case>S</span>ense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text</a></strong><br><a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/P17-2/ class=text-muted>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2094><div class="card-body p-3 small">Parallel corpora are widely used in a variety of Natural Language Processing tasks, from <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> to cross-lingual Word Sense Disambiguation, where parallel sentences can be exploited to automatically generate high-quality sense annotations on a large scale. In this paper we present EuroSense, a multilingual sense-annotated resource based on the joint disambiguation of the Europarl parallel corpus, with almost 123 million sense annotations for over 155 thousand distinct concepts and entities from a language-independent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1900/>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a><br><a href=/volumes/W17-19/ class=text-muted>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-S17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S17-2002/>SemEval-2017 Task 2 : Multilingual and Cross-lingual Semantic Word Similarity<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/S17-2/ class=text-muted>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S17-2002><div class="card-body p-3 small">This paper introduces a new task on Multilingual and Cross-lingual SemanticThis paper introduces a new task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Persian_language>Farsi</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. High quality datasets were manually curated for the five languages with high <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreements</a> (consistently in the 0.9 ballpark). These were used for semi-automatic construction of ten cross-lingual datasets. 17 teams participated in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, submitting 24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>, in the form of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website :<url>http://alt.qcri.org/semeval2017/task2/</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1010/>Word Sense Disambiguation : A Unified Evaluation Framework and Empirical Comparison</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/E17-1/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1010><div class="card-body p-3 small">Word Sense Disambiguation is a long-standing task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, lying at the core of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>human language understanding</a>. However, the evaluation of <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> clearly outperform knowledge-based models. Among the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a>, a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2036/>BabelDomains : Large-Scale Domain Labeling of Lexical Resources<span class=acl-fixed-case>B</span>abel<span class=acl-fixed-case>D</span>omains: Large-Scale Domain Labeling of Lexical Resources</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2036><div class="card-body p-3 small">In this paper we present BabelDomains, a unified resource which provides lexical items with information about domains of knowledge. We propose an automatic method that uses knowledge from various lexical resources, exploiting both distributional and graph-based clues, to accurately propagate domain information. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> intrinsically on two lexical resources (WordNet and BabelNet), achieving a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> over 80 % in both cases. Finally, we show the potential of BabelDomains in a supervised learning setting, clustering training data by domain for hypernym discovery.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jose+Camacho-Collados" title="Search for 'Jose Camacho-Collados' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/m/mohammad-taher-pilehvar/ class=align-middle>Mohammad Taher Pilehvar</a>
<span class="badge badge-secondary align-middle ml-2">8</span></li><li class=list-group-item><a href=/people/r/roberto-navigli/ class=align-middle>Roberto Navigli</a>
<span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/people/l/luis-espinosa-anke/ class=align-middle>Luis Espinosa Anke</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/f/francesco-barbieri/ class=align-middle>Francesco Barbieri</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/a/asahi-ushio/ class=align-middle>Asahi Ushio</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/s/steven-schockaert/ class=align-middle>Steven Schockaert</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/n/nigel-collier/ class=align-middle>Nigel Collier</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/c/claudio-delli-bovi/ class=align-middle>Claudio Delli Bovi</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/alessandro-raganato/ class=align-middle>Alessandro Raganato</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kiamehr-rezaee/ class=align-middle>Kiamehr Rezaee</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/horacio-saggion/ class=align-middle>Horacio Saggion</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/t/tommaso-pasini/ class=align-middle>Tommaso Pasini</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/m/massimiliano-mancini/ class=align-middle>Massimiliano Mancini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ignacio-iacobacci/ class=align-middle>Ignacio Iacobacci</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anna-breit/ class=align-middle>Anna Breit</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/artem-revenko/ class=align-middle>Artem Revenko</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/daniel-loureiro/ class=align-middle>Daniel Loureiro</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/carlos-perello/ class=align-middle>Carlos Perelló</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/david-tomas/ class=align-middle>David Tomas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alberto-garcia-garcia/ class=align-middle>Alberto Garcia-Garcia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jose-garcia-rodriguez/ class=align-middle>Jose Garcia-Rodriguez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leonardo-neves/ class=align-middle>Leonardo Neves</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/francesco-ronzano/ class=align-middle>Francesco Ronzano</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/miguel-ballesteros/ class=align-middle>Miguel Ballesteros</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/valerio-basile/ class=align-middle>Valerio Basile</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/viviana-patti/ class=align-middle>Viviana Patti</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sergio-oramas/ class=align-middle>Sergio Oramas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/enrico-santus/ class=align-middle>Enrico Santus</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/vered-shwartz/ class=align-middle>Vered Shwartz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thierry-declerck/ class=align-middle>Thierry Declerck</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dagmar-gromann/ class=align-middle>Dagmar Gromann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/aleksandra-edwards/ class=align-middle>Aleksandra Edwards</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/helene-de-ribaupierre/ class=align-middle>Hélène De Ribaupierre</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alun-preece/ class=align-middle>Alun Preece</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/semeval/ class=align-middle>SemEval</a><span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>