<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Jing Liu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Jing</span> <span class=font-weight-bold>Liu</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.224/>RocketQAv2 : A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking<span class=acl-fixed-case>R</span>ocket<span class=acl-fixed-case>QA</span>v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking</a></strong><br><a href=/people/r/ruiyang-ren/>Ruiyang Ren</a>
|
<a href=/people/y/yingqi-qu/>Yingqi Qu</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/q/qiaoqiao-she/>QiaoQiao She</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--224><div class="card-body p-3 small">In various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedures</a> contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other&#8217;s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5828.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5828 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5828 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5828/>D-NET : A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension<span class=acl-fixed-case>D</span>-<span class=acl-fixed-case>NET</span>: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension</a></strong><br><a href=/people/h/hongyu-li/>Hongyu Li</a>
|
<a href=/people/x/xiyuan-zhang/>Xiyuan Zhang</a>
|
<a href=/people/y/yibing-liu/>Yibing Liu</a>
|
<a href=/people/y/yiming-zhang/>Yiming Zhang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/x/xiangyang-zhou/>Xiangyang Zhou</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a><br><a href=/volumes/D19-58/ class=text-muted>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5828><div class="card-body p-3 small">In this paper, we introduce a simple system Baidu submitted for MRQA (Machine Reading for Question Answering) 2019 Shared Task that focused on generalization of machine reading comprehension (MRC) models. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is built on a framework of pretraining and fine-tuning, namely D-NET. The techniques of pre-trained language models and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> are explored to improve the generalization of MRC models and we conduct experiments to examine the effectiveness of these strategies. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1226 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1226/>Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/k/kai-liu/>Kai Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/q/qiaoqiao-she/>Qiaoqiao She</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a><br><a href=/volumes/P19-1/ class=text-muted>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1226><div class="card-body p-3 small">Machine reading comprehension (MRC) is a crucial and challenging task in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Machine_learning>MRC</a>. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for <a href=https://en.wikipedia.org/wiki/Medical_record>MRC</a>. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>baselines</a> on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single <a href=https://en.wikipedia.org/wiki/Computer_model>model</a> on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019).</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1018/>Neural Math Word Problem Solver with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/d/danqing-huang/>Danqing Huang</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1018><div class="card-body p-3 small">Sequence-to-sequence model has been applied to solve <a href=https://en.wikipedia.org/wiki/Word_problem_(mathematics)>math word problems</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> takes math problem descriptions as input and generates equations as output. The advantage of sequence-to-sequence model requires no <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and can generate equations that do not exist in training data. However, our experimental analysis reveals that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> suffers from two shortcomings : (1) generate spurious numbers ; (2) generate numbers at wrong positions. In this paper, we propose incorporating copy and alignment mechanism to the sequence-to-sequence model (namely CASS) to address these shortcomings. To train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to directly optimize the solution accuracy. It overcomes the train-test discrepancy issue of <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation</a>, which uses the surrogate objective of maximizing equation likelihood during training while the evaluation metric is solution accuracy (non-differentiable) at test time. Furthermore, to explore the effectiveness of our neural model, we use our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> output as a <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> and incorporate it into the feature-based model. Experimental results show that (1) The copy and alignment mechanism is effective to address the two issues ; (2) <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement learning</a> leads to better performance than <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood</a> on this task ; (3) Our neural model is complementary to the feature-based model and their combination significantly outperforms the state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1427 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1427.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1427/>Answer-focused and Position-aware Neural Question Generation</a></strong><br><a href=/people/x/xingwu-sun/>Xingwu Sun</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/w/wei-he/>Wei He</a>
|
<a href=/people/y/yanjun-ma/>Yanjun Ma</a>
|
<a href=/people/s/shi-wang/>Shi Wang</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1427><div class="card-body p-3 small">In this paper, we focus on the problem of question generation (QG). Recent neural network-based approaches employ the sequence-to-sequence model which takes an answer and its context as input and generates a relevant question as output. However, we observe two major issues with these approaches : (1) The generated interrogative words (or question words) do not match the answer type. (2) The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer. To address these two issues, we propose an answer-focused and position-aware neural question generation model. (1) By answer-focused, we mean that we explicitly model question word generation by incorporating the answer embedding, which can help generate an interrogative word matching the answer type. (2) By position-aware, we mean that we model the relative distance between the context words and the answer. Hence the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be aware of the position of the context words when copying them to generate a question. We conduct extensive experiments to examine the effectiveness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2611/>Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task<span class=acl-fixed-case>ROUGE</span> and <span class=acl-fixed-case>BLEU</span> to Better Evaluate Machine Reading Comprehension Task</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/k/kai-liu/>Kai Liu</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a><br><a href=/volumes/W18-26/ class=text-muted>Proceedings of the Workshop on Machine Reading for Question Answering</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2611><div class="card-body p-3 small">Current evaluation metrics to <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, bias may appear when these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are used for specific question types, especially questions inquiring yes-no opinions and entity lists. In this paper, we make adaptations on the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to better correlate n-gram overlap with the <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> for answers to these two question types. Statistical analysis proves the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>. Our adaptations may provide positive guidance for the development of real-scene MRC systems.<tex-math>n</tex-math>-gram overlap with the human judgment for answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of real-scene MRC systems.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-2032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-2032 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-2032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-2032/>A Statistical Framework for Product Description Generation</a></strong><br><a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/y/yutai-hou/>Yutai Hou</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a><br><a href=/volumes/I17-2/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-2032><div class="card-body p-3 small">We present in this paper a statistical framework that generates accurate and fluent product description from product attributes. Specifically, after extracting templates and learning writing knowledge from attribute-description parallel data, we use the learned knowledge to decide what to say and how to say for product description generation. To evaluate accuracy and fluency for the generated descriptions, in addition to BLEU and Recall, we propose to measure what to say (in terms of attribute coverage) and to measure how to say (by attribute-specified generation) separately. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is effective.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Jing+Liu" title="Search for 'Jing Liu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yajuan-lyu/ class=align-middle>Yajuan Lyu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/h/hua-wu/ class=align-middle>Hua Wu (吴华)</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/c/chin-yew-lin/ class=align-middle>Chin-Yew Lin</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/q/qiaoqiao-she/ class=align-middle>Qiaoqiao She</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/haifeng-wang/ class=align-middle>Haifeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/q/quan-wang/ class=align-middle>Quan Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/a/an-yang/ class=align-middle>An Yang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kai-liu/ class=align-middle>Kai Liu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/s/sujian-li/ class=align-middle>Sujian Li</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/d/danqing-huang/ class=align-middle>Danqing Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-yin/ class=align-middle>Jian Yin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinpeng-wang/ class=align-middle>Jinpeng Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yutai-hou/ class=align-middle>Yutai Hou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yunbo-cao/ class=align-middle>Yunbo Cao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xingwu-sun/ class=align-middle>Xingwu Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-he/ class=align-middle>Wei He</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yanjun-ma/ class=align-middle>Yanjun Ma</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/shi-wang/ class=align-middle>Shi Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/ruiyang-ren/ class=align-middle>Ruiyang Ren</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yingqi-qu/ class=align-middle>Yingqi Qu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wayne-xin-zhao/ class=align-middle>Wayne Xin Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/ji-rong-wen/ class=align-middle>Ji-Rong Wen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hongyu-li/ class=align-middle>Hongyu Li</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiyuan-zhang/ class=align-middle>Xiyuan Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yibing-liu/ class=align-middle>Yibing Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yiming-zhang/ class=align-middle>Yiming Zhang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiangyang-zhou/ class=align-middle>Xiangyang Zhou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>