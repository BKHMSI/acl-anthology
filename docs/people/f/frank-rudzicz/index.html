<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Frank Rudzicz - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Frank</span> <span class=font-weight-bold>Rudzicz</span></h2><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.acl-long.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--acl-long--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.acl-long.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.acl-long.210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.acl-long.210/><span class=acl-fixed-case>K</span>en<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>SH</span>: Knowledge-enhanced End-to-end Biomedical Text Labelling</a></strong><br><a href=/people/x/xindi-wang/>Xindi Wang</a>
|
<a href=/people/r/robert-e-mercer/>Robert Mercer</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/2022.acl-long/ class=text-muted>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--acl-long--210><div class="card-body p-3 small">Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information. With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important. MeSH indexing is a challenging task for machine learning, as it needs to assign multiple labels to each article from an extremely large hierachically organized collection. To address this challenge, we propose KenMeSH, an end-to-end model that combines new text features and a dynamic knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--326 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-acl.326.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-acl.326" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.326/>On the data requirements of probing</a></strong><br><a href=/people/z/zining-zhu/>Zining Zhu</a>
|
<a href=/people/j/jixuan-wang/>Jixuan Wang</a>
|
<a href=/people/b/bai-li/>Bai Li</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--326><div class="card-body p-3 small">As large and powerful neural language models are developed researchers have been increasingly interested in developing <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnostic tools</a> to probe them There are many papers with conclusions of the form observation X$ is found in model Y$&#8217;&#8217; using their own datasets with varying sizes Larger probing datasets bring more reliability but are also expensive to collect There is yet to be a <a href=https://en.wikipedia.org/wiki/Quantitative_research>quantitative method</a> for estimating reasonable probing dataset sizes We tackle this omission in the context of comparing two probing configurations after we have collected a small dataset from a pilot study how many additional data samples are sufficient to distinguish two different configurations We present a novel method to estimate the required number of data samples in such experiments and across several case studies we verify that our estimations have sufficient statistical power Our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> helps to systematically construct probing datasets to diagnose neural NLP models<tex-math>X</tex-math> is found in model <tex-math>Y</tex-math>&#8221;, using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.325/>How is BERT surprised? Layerwise detection of linguistic anomalies<span class=acl-fixed-case>BERT</span> surprised? Layerwise detection of linguistic anomalies</a></strong><br><a href=/people/b/bai-li/>Bai Li</a>
|
<a href=/people/z/zining-zhu/>Zining Zhu</a>
|
<a href=/people/g/guillaume-thomas/>Guillaume Thomas</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--325><div class="card-body p-3 small">Transformer language models have shown remarkable ability in detecting when a word is anomalous in context, but likelihood scores offer no information about the cause of the anomaly. In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark. In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in upper layers. Next, we gather datasets of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic</a>, semantic, and commonsense anomalies from psycholinguistic studies ; we find that the best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic</a> than when it is semantic, while commonsense anomalies do not exhibit surprisal at any intermediate layer. These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cmcl-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.9/>TorontoCL at CMCL 2021 Shared Task : RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction<span class=acl-fixed-case>T</span>oronto<span class=acl-fixed-case>CL</span> at <span class=acl-fixed-case>CMCL</span> 2021 Shared Task: <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a with Multi-Stage Fine-Tuning for Eye-Tracking Prediction</a></strong><br><a href=/people/b/bai-li/>Bai Li</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/2021.cmcl-1/ class=text-muted>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--9><div class="card-body p-3 small">Eye movement data during reading is a useful source of information for understanding <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension processes</a>. In this paper, we describe our submission to the CMCL 2021 shared task on predicting human reading patterns. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> uses RoBERTa with a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression layer</a> to predict 5 eye-tracking features. We train the model in two stages : we first fine-tune on the Provo corpus (another eye-tracking dataset), then fine-tune on the task data. We compare different Transformer models and apply ensembling methods to improve the performance. Our final submission achieves a MAE score of 3.929, ranking 3rd place out of 13 teams that participated in this shared task.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.115.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939013 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.115/>Explainable Clinical Decision Support from Text</a></strong><br><a href=/people/j/jinyue-feng/>Jinyue Feng</a>
|
<a href=/people/c/chantal-shaib/>Chantal Shaib</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/2020.emnlp-main/ class=text-muted>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--115><div class="card-body p-3 small">Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> usability in a clinical decision support context. From domain-expert evaluations, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates informative rationales that have promising real-life applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939838 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.clinicalnlp-1.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.33/>Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/j/john-chen/>John Chen</a>
|
<a href=/people/i/ian-berlot-attwell/>Ian Berlot-Attwell</a>
|
<a href=/people/x/xindi-wang/>Xindi Wang</a>
|
<a href=/people/s/safwan-hossain/>Safwan Hossain</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/2020.clinicalnlp-1/ class=text-muted>Proceedings of the 3rd Clinical Natural Language Processing Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--33><div class="card-body p-3 small">Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured forms</a> such as free text. We propose a novel task of exploring fairness on a multimodal clinical dataset, adopting equalized odds for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm-equalized odds post processing-and compare it to a text-specific fairness algorithm : debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a> may simultaneously achieve a good balance of performance classical notions of <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a>. Our work opens the door for future work at the critical intersection of clinical NLP and <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a>.<i>fairness</i> on a multimodal clinical dataset, adopting <i>equalized odds</i> for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.208/>Identification of Primary and Collateral Tracks in Stuttered Speech</a></strong><br><a href=/people/r/rachid-riad/>Rachid Riad</a>
|
<a href=/people/a/anne-catherine-bachoud-levi/>Anne-Catherine Bachoud-Lévi</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--208><div class="card-body p-3 small">Disfluent speech has been previously addressed from two main perspectives : the clinical perspective focusing on <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnostic</a>, and the Natural Language Processing (NLP) perspective aiming at modeling these events and detect them for downstream tasks. In addition, previous works often used different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> depending on whether the input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are text or speech, making it difficult to compare the different contributions. Here, we introduce a new evaluation framework for disfluency detection inspired by the clinical and NLP perspective together with the theory of performance from (Clark, 1996) which distinguishes between primary and collateral tracks. We introduce a novel forced-aligned disfluency dataset from a corpus of semi-directed interviews, and present baseline results directly comparing the performance of text-based features (word and span information) and speech-based (acoustic-prosodic information). Finally, we introduce new audio features inspired by the word-based span features. We show experimentally that using these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> outperformed the baselines for speech-based predictions on the present <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6209 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6209/>Extracting relevant information from physician-patient dialogues for automated clinical note taking</a></strong><br><a href=/people/s/serena-jeblee/>Serena Jeblee</a>
|
<a href=/people/f/faiza-khan-khattak/>Faiza Khan Khattak</a>
|
<a href=/people/n/noah-crampton/>Noah Crampton</a>
|
<a href=/people/m/muhammad-mamdani/>Muhammad Mamdani</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/D19-62/ class=text-muted>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6209><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for automatically extracting pertinent medical information from dialogues between clinicians and patients. The <a href=https://en.wikipedia.org/wiki/System>system</a> parses each dialogue and extracts <a href=https://en.wikipedia.org/wiki/Legal_person>entities</a> such as medications and symptoms, using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> to predict which entities are relevant. We also classify the primary diagnosis for each conversation. In addition, we extract <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topic information</a> and identify relevant utterances. This serves as a baseline for a system that extracts information from dialogues and automatically generates a patient note, which can be reviewed and edited by the clinician.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3107/>Detecting Anxiety through Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/j/judy-hanwen-shen/>Judy Hanwen Shen</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/W17-31/ class=text-muted>Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology — From Linguistic Signal to Clinical Reality</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3107><div class="card-body p-3 small">Previous investigations into detecting mental illnesses through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have predominately focused on detecting depression through Twitter corpora. In this paper, we study <a href=https://en.wikipedia.org/wiki/Anxiety_disorder>anxiety disorders</a> through personal narratives collected through the popular social media website, <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. We build a substantial data set of typical and anxiety-related posts, and we apply N-gram language modeling, vector embeddings, topic analysis, and emotional norms to generate features that accurately classify posts related to binary levels of anxiety. We achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 91 % with vector-space word embeddings, and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 98 % when combined with lexicon-based features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2004/>Identifying and Avoiding Confusion in Dialogue with People with Alzheimer’s Disease<span class=acl-fixed-case>A</span>lzheimer’s Disease</a></strong><br><a href=/people/h/hamidreza-chinaei/>Hamidreza Chinaei</a>
|
<a href=/people/l/leila-chan-currie/>Leila Chan Currie</a>
|
<a href=/people/a/andrew-danks/>Andrew Danks</a>
|
<a href=/people/h/hubert-lin/>Hubert Lin</a>
|
<a href=/people/t/tejas-mehta/>Tejas Mehta</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a><br><a href=/volumes/J17-2/ class=text-muted>Computational Linguistics, Volume 43, Issue 2 - June 2017</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2004><div class="card-body p-3 small">Alzheimer&#8217;s disease (AD) is an increasingly prevalent <a href=https://en.wikipedia.org/wiki/Cognitive_disorder>cognitive disorder</a> in which <a href=https://en.wikipedia.org/wiki/Memory>memory</a>, <a href=https://en.wikipedia.org/wiki/Language>language</a>, and <a href=https://en.wikipedia.org/wiki/Executive_functions>executive function</a> deteriorate, usually in that order. There is a growing need to support individuals with AD and other forms of <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a> in their daily lives, and our goal is to do so through speech-based interaction. Given that 33 % of conversations with people with middle-stage AD involve a breakdown in communication, it is vital that automated dialogue systems be able to identify those breakdowns and, if possible, avoid them. In this article, we discuss several linguistic features that are verbal indicators of <a href=https://en.wikipedia.org/wiki/Confusion>confusion</a> in AD (including vocabulary richness, parse tree structures, and acoustic cues) and apply several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> to identify dialogue-relevant confusion from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> with up to 82 % accuracy. We also learn dialogue strategies to avoid <a href=https://en.wikipedia.org/wiki/Confusion>confusion</a> in the first place, which is accomplished using a <a href=https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process>partially observable Markov decision process</a> and which obtains accuracies (up to 96.1 %) that are significantly higher than several baselines. This work represents a major step towards automated dialogue systems for individuals with dementia.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Frank+Rudzicz" title="Search for 'Frank Rudzicz' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/b/bai-li/ class=align-middle>Bai Li</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/z/zining-zhu/ class=align-middle>Zining Zhu</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/x/xindi-wang/ class=align-middle>Xindi Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/g/guillaume-thomas/ class=align-middle>Guillaume Thomas</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-xu/ class=align-middle>Yang Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/j/jinyue-feng/ class=align-middle>Jinyue Feng</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/chantal-shaib/ class=align-middle>Chantal Shaib</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/judy-hanwen-shen/ class=align-middle>Judy Hanwen Shen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/robert-e-mercer/ class=align-middle>Robert E. Mercer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hamidreza-chinaei/ class=align-middle>Hamidreza Chinaei</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/leila-chan-currie/ class=align-middle>Leila Chan Currie</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrew-danks/ class=align-middle>Andrew Danks</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hubert-lin/ class=align-middle>Hubert Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tejas-mehta/ class=align-middle>Tejas Mehta</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/serena-jeblee/ class=align-middle>Serena Jeblee</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/faiza-khan-khattak/ class=align-middle>Faiza Khan Khattak</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/noah-crampton/ class=align-middle>Noah Crampton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/muhammad-mamdani/ class=align-middle>Muhammad Mamdani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jixuan-wang/ class=align-middle>Jixuan Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-chen/ class=align-middle>John Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/ian-berlot-attwell/ class=align-middle>Ian Berlot-Attwell</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/safwan-hossain/ class=align-middle>Safwan Hossain</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rachid-riad/ class=align-middle>Rachid Riad</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/anne-catherine-bachoud-levi/ class=align-middle>Anne-Catherine Bachoud-Lévi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/emmanuel-dupoux/ class=align-middle>Emmanuel Dupoux</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cl/ class=align-middle>CL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/clinicalnlp/ class=align-middle>ClinicalNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/cmcl/ class=align-middle>CMCL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>