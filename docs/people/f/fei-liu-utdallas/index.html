<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Fei Liu - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Fei</span> <span class=font-weight-bold>Liu</span></h2><p class="font-weight-light text-muted">UT Dallas, Bosch, CMU, University of Central Florida</p><p class="font-weight-light text-muted"><span class=font-italic>Other people with similar names:</span>
<a href=/people/f/fei-liu/>Fei Liu</a>
(May refer to several people),
<a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
(University of Melbourne)</p><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.110/>A New Approach to Overgenerating and Scoring Abstractive Summaries</a></strong><br><a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/b/bingqing-wang/>Bingqing Wang</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/2021.naacl-main/ class=text-muted>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--110><div class="card-body p-3 small">We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users&#8217; needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both <a href=https://en.wikipedia.org/wiki/Stage_(theatre)>stages</a> can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> can achieve state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.adaptnlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--adaptnlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.adaptnlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.adaptnlp-1.25/>Semantic Parsing of Brief and Multi-Intent Natural Language Utterances</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/c/charles-newton/>Charles Newton</a>
|
<a href=/people/v/victor-hung/>Victor Hung</a>
|
<a href=/people/b/beth-atkinson/>Beth Atkinson</a>
|
<a href=/people/j/john-killilea/>John Killilea</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/2021.adaptnlp-1/ class=text-muted>Proceedings of the Second Workshop on Domain Adaptation for NLP</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--adaptnlp-1--25><div class="card-body p-3 small">Many military communication domains involve rapidly conveying situation awareness with few words. Converting natural language utterances to logical forms in these domains is challenging, as these utterances are brief and contain multiple intents. In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms. Our findings suggest a new projection and reduction method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective. We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.0/>Proceedings of the Third Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a><br><a href=/volumes/2021.newsum-1/ class=text-muted>Proceedings of the Third Workshop on New Frontiers in Summarization</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.newsum-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--newsum-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.newsum-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.newsum-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.newsum-1.13/>Modeling Endorsement for Multi-Document Abstractive Summarization</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/b/bingqing-wang/>Bingqing Wang</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/2021.newsum-1/ class=text-muted>Proceedings of the Third Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--newsum-1--13><div class="card-body p-3 small">A crucial difference between single- and multi-document summarization is how <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salient content</a> manifests itself in the document(s). While such <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a> may appear at the beginning of a single document, essential information is frequently reiterated in a set of documents related to a particular topic, resulting in an endorsement effect that increases information salience. In this paper, we model the cross-document endorsement effect and its utilization in multiple document summarization. Our method generates a synopsis from each document, which serves as an endorser to identify salient content from other documents. Strongly endorsed text segments are used to enrich a neural encoder-decoder model to consolidate them into an abstractive summary. The method has a great potential to learn from fewer examples to identify salient content, which alleviates the need for costly retraining when the set of documents is dynamically adjusted. Through extensive experiments on benchmark multi-document summarization datasets, we demonstrate the effectiveness of our proposed method over strong published baselines. Finally, we shed light on future research directions and discuss broader challenges of this task using a case study.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.coling-main.499.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--coling-main--499 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.coling-main.499 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.coling-main.499" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.coling-main.499/>How Domain Terminology Affects Meeting Summarization Performance</a></strong><br><a href=/people/j/jia-jin-koay/>Jia Jin Koay</a>
|
<a href=/people/a/alexander-roustai/>Alexander Roustai</a>
|
<a href=/people/x/xiaojin-dai/>Xiaojin Dai</a>
|
<a href=/people/d/dillon-burns/>Dillon Burns</a>
|
<a href=/people/a/alec-kerrigan/>Alec Kerrigan</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/2020.coling-main/ class=text-muted>Proceedings of the 28th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--coling-main--499><div class="card-body p-3 small">Meetings are essential to modern organizations. Numerous meetings are held and recorded daily, more than can ever be comprehended. A meeting summarization system that identifies salient utterances from the transcripts to automatically generate meeting minutes can help. It empowers users to rapidly search and sift through large meeting collections. To date, the impact of domain terminology on the performance of meeting summarization remains understudied, despite that meetings are rich with <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. In this paper, we create gold-standard annotations for domain terminology on a sizable meeting corpus ; they are known as <a href=https://en.wikipedia.org/wiki/Jargon>jargon terms</a>. We then analyze the performance of a meeting summarization system with and without <a href=https://en.wikipedia.org/wiki/Jargon>jargon terms</a>. Our findings reveal that domain terminology can have a substantial impact on <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> performance. We publicly release all domain terminology to advance research in meeting summarization.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1053.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1053/>MoverScore : Text Generation Evaluating with <a href=https://en.wikipedia.org/wiki/Contextualization>Contextualized Embeddings</a> and <a href=https://en.wikipedia.org/wiki/Earth_mover_distance>Earth Mover Distance</a><span class=acl-fixed-case>M</span>over<span class=acl-fixed-case>S</span>core: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/c/christian-m-meyer/>Christian M. Meyer</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1053><div class="card-body p-3 small">A robust evaluation metric has a profound impact on the development of text generation systems. A desirable <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> compares system output against references based on their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Closed_captioning>image captioning</a>, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> combining contextualized representations with a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>distance measure</a> perform the best. Such <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> also demonstrate strong generalization capability across tasks. For ease-of-use we make our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> available as <a href=https://en.wikipedia.org/wiki/Web_service>web service</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5400/>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/D19-54/ class=text-muted>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5408 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5408/>Towards Annotating and Creating Summary Highlights at Sub-sentence Level</a></strong><br><a href=/people/k/kristjan-arumae/>Kristjan Arumae</a>
|
<a href=/people/p/parminder-bhatia/>Parminder Bhatia</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/D19-54/ class=text-muted>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5408><div class="card-body p-3 small">Highlighting is a powerful tool to pick out important content and emphasize. Creating summary highlights at the sub-sentence level is particularly desirable, because sub-sentences are more concise than whole sentences. They are also better suited than individual words and phrases that can potentially lead to disfluent, fragmented summaries. In this paper we seek to generate summary highlights by annotating summary-worthy sub-sentences and teaching <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> to do the same. We frame the task as jointly selecting important sentences and identifying a single most informative textual unit from each sentence. This <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a> dramatically reduces the <a href=https://en.wikipedia.org/wiki/Complexity>task complexity</a> involved in sentence compression. Our study provides new benchmarks and baselines for generating highlights at the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sub-sentence level</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5413 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5413/>Analyzing Sentence Fusion in Abstractive Summarization</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/j/john-muchovej/>John Muchovej</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/w/walter-chang/>Walter Chang</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/D19-54/ class=text-muted>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5413><div class="card-body p-3 small">While recent work in abstractive summarization has resulted in higher scores in automatic metrics, there is little understanding on how these systems combine information taken from multiple document sentences. In this paper, we analyze the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion. We ask assessors to judge the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>, and method of fusion for summary sentences. Our analysis reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/C18-1101/>Abstract Meaning Representation for Multi-Document Summarization<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation for Multi-Document Summarization</a></strong><br><a href=/people/k/kexin-liao/>Kexin Liao</a>
|
<a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1101><div class="card-body p-3 small">Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic theory</a>, as a form of content representation. Our approach condenses source documents to a set of <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>summary graphs</a> following the <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>AMR formalism</a>. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/C18-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-C18-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-C18-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=C18-1146" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/C18-1146/>Structure-Infused Copy Mechanisms for Abstractive Summarization</a></strong><br><a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/l/lin-zhao/>Lin Zhao</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/C18-1/ class=text-muted>Proceedings of the 27th International Conference on Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-C18-1146><div class="card-body p-3 small">Seq2seq learning has produced promising results on <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the <a href=https://en.wikipedia.org/wiki/System>system</a>, and our proposed approach compares favorably to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1387.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1387 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1387 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1387/>Automatic Detection of Vague Words and Sentences in Privacy Policies</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1387><div class="card-body p-3 small">Website privacy policies represent the single most important source of information for users to gauge how their personal data are collected, used and shared by companies. However, <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a> are often vague and people struggle to understand the content. Their opaqueness poses a significant challenge to both users and policy regulators. In this paper, we seek to identify vague content in <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a>. We construct the first <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of human-annotated vague words and sentences and present empirical studies on automatic vagueness detection. In particular, we investigate context-aware and context-agnostic models for predicting vague words, and explore auxiliary-classifier generative adversarial networks for characterizing sentence vagueness. Our experimental results demonstrate the effectiveness of proposed approaches. Finally, we provide suggestions for resolving <a href=https://en.wikipedia.org/wiki/Vagueness>vagueness</a> and improving the usability of <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1446 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1446" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1446/>Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1446><div class="card-body p-3 small">Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. In contrast, parallel data for <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P18-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-P18-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P18-3015.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P18-3015/>Reinforced Extractive Summarization with Question-Focused Rewards</a></strong><br><a href=/people/k/kristjan-arumae/>Kristjan Arumae</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/P18-3/ class=text-muted>Proceedings of ACL 2018, Student Research Workshop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P18-3015><div class="card-body p-3 small">We investigate a new training paradigm for extractive summarization. Traditionally, human abstracts are used to derive goldstandard labels for extraction units. However, the labels are often inaccurate, because <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>human abstracts</a> and source documents can not be easily aligned at the word level. In this paper we convert <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>human abstracts</a> to a set of Cloze-style comprehension questions. System summaries are encouraged to preserve salient source content useful for answering questions and share common words with the <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a>. We use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to explore the space of possible extractive summaries and introduce a question-focused reward function to promote concise, fluent, and informative summaries. Our experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is effective. It surpasses state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> on the standard summarization dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P18-4000/>Proceedings of <span class=acl-fixed-case>ACL</span> 2018, System Demonstrations</a></strong><br><a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a><br><a href=/volumes/P18-4/ class=text-muted>Proceedings of ACL 2018, System Demonstrations</a></span></p><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1056" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1056/>Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields</a></strong><br><a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a><br><a href=/volumes/I17-1/ class=text-muted>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1056><div class="card-body p-3 small">Despite successful applications across a broad range of NLP tasks, conditional random fields (CRFs), in particular the linear-chain variant, are only able to model local features. While this has important benefits in terms of inference tractability, it limits the ability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to capture long-range dependencies between items. Attempts to extend CRFs to capture long-range dependencies have largely come at the cost of <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> and <a href=https://en.wikipedia.org/wiki/Approximate_inference>approximate inference</a>. In this work, we propose an extension to CRFs by integrating <a href=https://en.wikipedia.org/wiki/External_memory>external memory</a>, taking inspiration from memory networks, thereby allowing CRFs to incorporate information far beyond neighbouring steps. Experiments across two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> show substantial improvements over strong CRF and LSTM baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4500/>Proceedings of the Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a><br><a href=/volumes/W17-45/ class=text-muted>Proceedings of the Workshop on New Frontiers in Summarization</a></span></p></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Fei+Liu" title="Search for 'Fei Liu' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/l/logan-lebanoff/ class=align-middle>Logan Lebanoff</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/k/kaiqiang-song/ class=align-middle>Kaiqiang Song</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/lu-wang/ class=align-middle>Lu Wang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jackie-chi-kit-cheung/ class=align-middle>Jackie Chi Kit Cheung</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/g/giuseppe-carenini/ class=align-middle>Giuseppe Carenini</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/k/kristjan-arumae/ class=align-middle>Kristjan Arumae</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/b/bingqing-wang/ class=align-middle>Bingqing Wang</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhe-feng/ class=align-middle>Zhe Feng</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/k/kexin-liao/ class=align-middle>Kexin Liao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lin-zhao/ class=align-middle>Lin Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/timothy-baldwin/ class=align-middle>Timothy Baldwin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/trevor-cohn/ class=align-middle>Trevor Cohn</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-zhao/ class=align-middle>Wei Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maxime-peyrard/ class=align-middle>Maxime Peyrard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yang-gao/ class=align-middle>Yang Gao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christian-m-meyer/ class=align-middle>Christian M. Meyer</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/steffen-eger/ class=align-middle>Steffen Eger</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/parminder-bhatia/ class=align-middle>Parminder Bhatia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-muchovej/ class=align-middle>John Muchovej</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/franck-dernoncourt/ class=align-middle>Franck Dernoncourt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/doo-soon-kim/ class=align-middle>Doo Soon Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/seokhwan-kim/ class=align-middle>Seokhwan Kim</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/walter-chang/ class=align-middle>Walter Chang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jia-jin-koay/ class=align-middle>Jia Jin Koay</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alexander-roustai/ class=align-middle>Alexander Roustai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaojin-dai/ class=align-middle>Xiaojin Dai</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dillon-burns/ class=align-middle>Dillon Burns</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alec-kerrigan/ class=align-middle>Alec Kerrigan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/charles-newton/ class=align-middle>Charles Newton</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/v/victor-hung/ class=align-middle>Victor Hung</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/beth-atkinson/ class=align-middle>Beth Atkinson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/john-killilea/ class=align-middle>John Killilea</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yue-dong/ class=align-middle>Yue Dong</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thamar-solorio/ class=align-middle>Thamar Solorio</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/venues/coling/ class=align-middle>COLING</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/newsum/ class=align-middle>newsum</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ijcnlp/ class=align-middle>IJCNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/adaptnlp/ class=align-middle>AdaptNLP</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>