<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>François Yvon - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>François</span> <span class=font-weight-bold>Yvon</span></h2><p class="font-weight-light text-muted"><span class=font-italic>Also published as:</span>
Francois <span class=font-weight-normal>Yvon</span></p><hr><div class=row><div class=col-lg-9><h4>2022</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-acl.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-acl--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-acl.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-acl.108/>Graph Neural Networks for Multiparallel Word Alignment</a></strong><br><a href=/people/a/ayyoob-imani/>Ayyoob Imani</a>
|
<a href=/people/l/lutfi-kerem-senel/>Lütfi Kerem Senel</a>
|
<a href=/people/m/masoud-jalili-sabet/>Masoud Jalili Sabet</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a><br><a href=/volumes/2022.findings-acl/ class=text-muted>Findings of the Association for Computational Linguistics: ACL 2022</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-acl--108><div class="card-body p-3 small">After a period of decrease interest in word alignments is increasing again for their usefulness in domains such as typological research cross lingual annotation projection and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> Generally alignment algorithms only use <a href=https://en.wikipedia.org/wiki/Bitext>bitext</a> and do not make use of the fact that many parallel corpora are multiparallel Here we compute high quality <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> between multiple language pairs by considering all language pairs together First we create a multiparallel word alignment graph joining all bilingual word alignment pairs in one graph Next we use graph neural networks GNNs to exploit the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> Our GNN approach i utilizes information about the meaning position and language of the input words ii incorporates information from multiple parallel sentences iii adds and removes edges from the initial alignments and iv yields a prediction model that can generalize beyond the training sentences We show that community detection algorithms can provide valuable information for multiparallel word alignment Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms previous work on three word alignment datasets and on a downstream task</div></div><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.671.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--671 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.671 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.671" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.671/>One Source, Two Targets : Challenges and Rewards of Dual Decoding<span class=acl-fixed-case>C</span>hallenges and Rewards of Dual Decoding</a></strong><br><a href=/people/j/jitao-xu/>Jitao Xu</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--671><div class="card-body p-3 small">Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement : to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.tacl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--tacl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.tacl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.tacl-1.2/>Revisiting Multi-Domain Machine Translation</a></strong><br><a href=/people/m/minh-quang-pham/>MinhQuang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Maria Crego</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/2021.tacl-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 9</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--tacl-1--2><div class="card-body p-3 small">When building <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940631 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.147/>SimAlign : High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>A</span>lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings</a></strong><br><a href=/people/m/masoud-jalili-sabet/>Masoud Jalili Sabet</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--147><div class="card-body p-3 small">Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and <a href=https://en.wikipedia.org/wiki/Data_quality>quality</a> decreases as less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available. We propose word alignment methods that require no <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. The key idea is to leverage multilingual word embeddings both static and contextualized for <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>. Our multilingual embeddings are created from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only without relying on any parallel data or <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a>. We find that alignments created from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners even with abundant parallel data ; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939618 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.86/>LIMSI @ WMT 2020<span class=acl-fixed-case>LIMSI</span> @ <span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/j/jose-carlos-rosales-nunez/>José Carlos Rosales Núñez</a>
|
<a href=/people/m/minh-quang-pham/>Minh Quang Pham</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/2020.wmt-1/ class=text-muted>Proceedings of the Fifth Conference on Machine Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--86><div class="card-body p-3 small">This paper describes LIMSI&#8217;s submissions to the translation shared tasks at WMT&#8217;20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/French_language>French</a>, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/German_language>German</a> ; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions : zero-shot and few-shot domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lrec-1.407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lrec-1--407 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lrec-1.407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lrec-1.407/>The European Language Technology Landscape in 2020 : Language-Centric and Human-Centric AI for Cross-Cultural Communication in Multilingual Europe<span class=acl-fixed-case>E</span>uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric <span class=acl-fixed-case>AI</span> for Cross-Cultural Communication in Multilingual <span class=acl-fixed-case>E</span>urope</a></strong><br><a href=/people/g/georg-rehm/>Georg Rehm</a>
|
<a href=/people/k/katrin-marheinecke/>Katrin Marheinecke</a>
|
<a href=/people/s/stefanie-hegele/>Stefanie Hegele</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/k/khalid-choukri/>Khalid Choukri</a>
|
<a href=/people/a/andrejs-vasiljevs/>Andrejs Vasiļjevs</a>
|
<a href=/people/g/gerhard-backfried/>Gerhard Backfried</a>
|
<a href=/people/c/christoph-prinz/>Christoph Prinz</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>José Manuel Gómez-Pérez</a>
|
<a href=/people/l/luc-meertens/>Luc Meertens</a>
|
<a href=/people/p/paul-lukowicz/>Paul Lukowicz</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/a/andrea-losch/>Andrea Lösch</a>
|
<a href=/people/p/philipp-slusallek/>Philipp Slusallek</a>
|
<a href=/people/m/morten-irgens/>Morten Irgens</a>
|
<a href=/people/p/patrick-gatellier/>Patrick Gatellier</a>
|
<a href=/people/j/joachim-kohler/>Joachim Köhler</a>
|
<a href=/people/l/laure-le-bars/>Laure Le Bars</a>
|
<a href=/people/d/dimitra-anastasiou/>Dimitra Anastasiou</a>
|
<a href=/people/a/albina-auksoriute/>Albina Auksoriūtė</a>
|
<a href=/people/n/nuria-bel/>Núria Bel</a>
|
<a href=/people/a/antonio-branco/>António Branco</a>
|
<a href=/people/g/gerhard-budin/>Gerhard Budin</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/k/koenraad-de-smedt/>Koenraad De Smedt</a>
|
<a href=/people/r/radovan-garabik/>Radovan Garabík</a>
|
<a href=/people/m/maria-gavriilidou/>Maria Gavriilidou</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/s/svetla-koeva/>Svetla Koeva</a>
|
<a href=/people/s/simon-krek/>Simon Krek</a>
|
<a href=/people/c/cvetana-krstev/>Cvetana Krstev</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a>
|
<a href=/people/j/jan-odijk/>Jan Odijk</a>
|
<a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/e/eirikur-rognvaldsson/>Eiríkur Rögnvaldsson</a>
|
<a href=/people/m/michael-rosner/>Mike Rosner</a>
|
<a href=/people/b/bolette-sandford-pedersen/>Bolette Pedersen</a>
|
<a href=/people/i/inguna-skadina/>Inguna Skadiņa</a>
|
<a href=/people/m/marko-tadic/>Marko Tadić</a>
|
<a href=/people/d/dan-tufis/>Dan Tufiș</a>
|
<a href=/people/t/tamas-varadi/>Tamás Váradi</a>
|
<a href=/people/k/kadri-vider/>Kadri Vider</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/2020.lrec-1/ class=text-muted>Proceedings of the 12th Language Resources and Evaluation Conference</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lrec-1--407><div class="card-body p-3 small">Multilingualism is a cultural cornerstone of Europe and firmly anchored in the <a href=https://en.wikipedia.org/wiki/Treaties_of_the_European_Union>European treaties</a> including <a href=https://en.wikipedia.org/wiki/Linguistic_rights>full language equality</a>. However, <a href=https://en.wikipedia.org/wiki/Language_barrier>language barriers</a> impacting business, <a href=https://en.wikipedia.org/wiki/Cross-cultural_communication>cross-lingual and cross-cultural communication</a> are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of <a href=https://en.wikipedia.org/wiki/Software_development_process>approaches</a> and technologies tailored to Europe&#8217;s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a> including many opportunities, synergies but also misconceptions has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.0/>Proceedings of the 17th International Conference on Spoken Language Translation</a></strong><br><a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/d/dekai-wu/>Dekai Wu</a>
|
<a href=/people/j/joseph-mariani/>Joseph Mariani</a>
|
<a href=/people/f/francois-yvon/>Francois Yvon</a><br><a href=/volumes/2020.iwslt-1/ class=text-muted>Proceedings of the 17th International Conference on Spoken Language Translation</a></span></p><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.iwslt-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--iwslt-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.iwslt-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.iwslt-1.26/>Generic and Specialized Word Embeddings for Multi-Domain Machine Translation</a></strong><br><a href=/people/m/minh-quang-pham/>MinhQuang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a><br><a href=/volumes/2019.iwslt-1/ class=text-muted>Proceedings of the 16th International Conference on Spoken Language Translation</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--iwslt-1--26><div class="card-body p-3 small">Supervised machine translation works well when the train and test data are sampled from the same distribution. When this is not the case, adaptation techniques help ensure that the knowledge learned from out-of-domain texts generalises to in-domain sentences. We study here a related setting, multi-domain adaptation, where the number of domains is potentially large and adapting separately to each domain would waste training resources. Our proposal transposes to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> the feature expansion technique of (Daum III, 2007): it isolates domain-agnostic from domain-specific lexical representations, while sharing the most of the network across domains. Our experiments use two architectures and two language pairs : they show that our approach, while simple and computationally inexpensive, outperforms several strong baselines and delivers a multi-domain system that successfully translates texts from diverse sources.</div></div><h4>2018</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1328 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1328" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1328/>Fixing Translation Divergences in Parallel Corpora for Neural MT<span class=acl-fixed-case>MT</span></a></strong><br><a href=/people/m/minh-quang-pham/>MinhQuang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/D18-1/ class=text-muted>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1328><div class="card-body p-3 small">Corpus-based approaches to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> for detecting translation divergences in parallel sentences. We rely on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a> are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered / corrected corpora actually improves MT performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5804/>Adaptor Grammars for the Linguist : <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a> Experiments for Very Low-Resource Languages<span class=acl-fixed-case>A</span>daptor <span class=acl-fixed-case>G</span>rammars for the Linguist: Word Segmentation Experiments for Very Low-Resource Languages</a></strong><br><a href=/people/p/pierre-godard/>Pierre Godard</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/m/martine-adda-decker/>Martine Adda-Decker</a>
|
<a href=/people/g/gilles-adda/>Gilles Adda</a>
|
<a href=/people/h/helene-bonneau-maynard/>Hélène Maynard</a>
|
<a href=/people/a/annie-rialland/>Annie Rialland</a><br><a href=/volumes/W18-58/ class=text-muted>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5804><div class="card-body p-3 small">Computational Language Documentation attempts to make the most recent research in speech and language technologies available to linguists working on <a href=https://en.wikipedia.org/wiki/Language_preservation>language preservation</a> and documentation. In this paper, we pursue two main goals along these lines. The first is to improve upon a strong baseline for the unsupervised word discovery task on two very low-resource Bantu languages, taking advantage of the expertise of linguists on these particular languages. The second consists in exploring the Adaptor Grammar framework as a decision and prediction tool for linguists studying a new language. We experiment 162 grammar configurations for each language and show that using Adaptor Grammars for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> enables us to test hypotheses about a language. Specializing a generic grammar with language specific knowledge leads to great improvements for the word discovery task, ultimately achieving a leap of about 30 % token F-score from the results of a strong baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6315/>Using Monolingual Data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : a Systematic Study</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/W18-63/ class=text-muted>Proceedings of the Third Conference on Machine Translation: Research Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6315><div class="card-body p-3 small">Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation-a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2064/>Automatically Selecting the Best Dependency Annotation Design with Dynamic Oracles</a></strong><br><a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/o/ophelie-lacroix/>Ophélie Lacroix</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2064><div class="card-body p-3 small">This work introduces a new strategy to compare the numerous conventions that have been proposed over the years for expressing dependency structures and discover the one for which a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> will achieve the highest <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance. Instead of associating each sentence in the training set with a single gold reference we propose to consider a set of references encoding alternative syntactic representations. Training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> with a dynamic oracle will then automatically select among all alternatives the reference that will be predicted with the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Experiments on the UD corpora show the validity of this approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N18-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N18-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-N18-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N18-2066/>Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective Trees</a></strong><br><a href=/people/l/lauriane-aufrant/>Lauriane Aufrant</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/N18-2/ class=text-muted>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N18-2066><div class="card-body p-3 small">Because the most common transition systems are projective, training a transition-based dependency parser often implies to either ignore or rewrite the non-projective training examples, which has an adverse impact on accuracy. In this work, we propose a simple modification of dynamic oracles, which enables the use of non-projective data when training projective parsers. Evaluation on 73 treebanks shows that our method achieves significant gains (+2 to +7 UAS for the most non-projective languages) and consistently outperforms traditional projectivization and pseudo-projectivization approaches.</div></div><h4>2017</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-3017/>LIMSI@CoNLL’17 : UD Shared Task<span class=acl-fixed-case>LIMSI</span>@<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span>’17: <span class=acl-fixed-case>UD</span> Shared Task</a></strong><br><a href=/people/l/lauriane-aufrant/>Lauriane Aufrant</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/K17-3/ class=text-muted>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-3017><div class="card-body p-3 small">This paper describes LIMSI&#8217;s submission to the CoNLL 2017 UD Shared Task, which is focused on small treebanks, and how to improve low-resourced parsing only by ad hoc combination of multiple views and resources. We present our approach for low-resourced parsing, together with a detailed analysis of the results for each test treebank. We also report extensive analysis experiments on <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> for the PUD treebanks, and on annotation consistency among UD treebanks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1044/>Learning the Structure of Variable-Order CRFs : a finite-state perspective<span class=acl-fixed-case>CRF</span>s: a finite-state perspective</a></strong><br><a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/D17-1/ class=text-muted>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1044><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies. Such situations are not rare and arise when dealing with <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a> or joint labelling tasks. We extend here recent proposals to consider variable order CRFs. Using an effective finite-state representation of variable-length dependencies, we propose new ways to perform <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> at large scale and report experimental results where we outperform strong baselines on a tagging task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2051 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2051/>Do n’t Stop Me Now ! Using Global Dynamic Oracles to Correct Training Biases of Transition-Based Dependency Parsers</a></strong><br><a href=/people/l/lauriane-aufrant/>Lauriane Aufrant</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a><br><a href=/volumes/E17-2/ class=text-muted>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2051><div class="card-body p-3 small">This paper formalizes a sound extension of dynamic oracles to global training, in the frame of transition-based dependency parsers. By dispensing with the pre-computation of references, this extension widens the training strategies that can be entertained for such parsers ; we show this by revisiting two standard training procedures, early-update and max-violation, to correct some of their search space sampling biases. Experimentally, on the SPMRL treebanks, this improvement increases the similarity between the train and test distributions and yields performance improvements up to 0.7 UAS, without any <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computation overhead</a>.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Fran%C3%A7ois+Yvon" title="Search for 'François Yvon' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/g/guillaume-wisniewski/ class=align-middle>Guillaume Wisniewski</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/m/minh-quang-pham/ class=align-middle>Minh Quang Pham</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/l/lauriane-aufrant/ class=align-middle>Lauriane Aufrant</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/josep-m-crego/ class=align-middle>Josep M. Crego</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/j/jean-senellart/ class=align-middle>Jean Senellart</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/m/masoud-jalili-sabet/ class=align-middle>Masoud Jalili Sabet</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/h/hinrich-schutze/ class=align-middle>Hinrich Schütze</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/j/jitao-xu/ class=align-middle>Jitao Xu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/ayyoob-imani/ class=align-middle>Ayyoob Imani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lutfi-kerem-senel/ class=align-middle>Lütfi Kerem Senel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/thomas-lavergne/ class=align-middle>Thomas Lavergne</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-dufter/ class=align-middle>Philipp Dufter</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pierre-godard/ class=align-middle>Pierre Godard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laurent-besacier/ class=align-middle>Laurent Besacier</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/martine-adda-decker/ class=align-middle>Martine Adda-Decker</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gilles-adda/ class=align-middle>Gilles Adda</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/helene-bonneau-maynard/ class=align-middle>Hélène Bonneau-Maynard</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/annie-rialland/ class=align-middle>Annie Rialland</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/franck-burlot/ class=align-middle>Franck Burlot</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/o/ophelie-lacroix/ class=align-middle>Ophélie Lacroix</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sadaf-abdul-rauf/ class=align-middle>Sadaf Abdul Rauf</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jose-carlos-rosales-nunez/ class=align-middle>José Carlos Rosales Núñez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/georg-rehm/ class=align-middle>Georg Rehm</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/katrin-marheinecke/ class=align-middle>Katrin Marheinecke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stefanie-hegele/ class=align-middle>Stefanie Hegele</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/stelios-piperidis/ class=align-middle>Stelios Piperidis</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kalina-bontcheva/ class=align-middle>Kalina Bontcheva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-hajic/ class=align-middle>Jan Hajic</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/khalid-choukri/ class=align-middle>Khalid Choukri</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrejs-vasiljevs/ class=align-middle>Andrejs Vasiļjevs</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gerhard-backfried/ class=align-middle>Gerhard Backfried</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/christoph-prinz/ class=align-middle>Christoph Prinz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jose-manuel-gomez-perez/ class=align-middle>José Manuel Gómez-Pérez</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/luc-meertens/ class=align-middle>Luc Meertens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/paul-lukowicz/ class=align-middle>Paul Lukowicz</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/josef-van-genabith/ class=align-middle>Josef van Genabith</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andrea-losch/ class=align-middle>Andrea Lösch</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/philipp-slusallek/ class=align-middle>Philipp Slusallek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/morten-irgens/ class=align-middle>Morten Irgens</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/patrick-gatellier/ class=align-middle>Patrick Gatellier</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joachim-kohler/ class=align-middle>Joachim Köhler</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/laure-le-bars/ class=align-middle>Laure Le Bars</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dimitra-anastasiou/ class=align-middle>Dimitra Anastasiou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/albina-auksoriute/ class=align-middle>Albina Auksoriūtė</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/n/nuria-bel/ class=align-middle>Núria Bel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/antonio-branco/ class=align-middle>António Branco</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/g/gerhard-budin/ class=align-middle>Gerhard Budin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/walter-daelemans/ class=align-middle>Walter Daelemans</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/koenraad-de-smedt/ class=align-middle>Koenraad De Smedt</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/radovan-garabik/ class=align-middle>Radovan Garabík</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maria-gavriilidou/ class=align-middle>Maria Gavriilidou</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dagmar-gromann/ class=align-middle>Dagmar Gromann</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/svetla-koeva/ class=align-middle>Svetla Koeva</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/simon-krek/ class=align-middle>Simon Krek</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/cvetana-krstev/ class=align-middle>Cvetana Krstev</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/krister-linden/ class=align-middle>Krister Lindén</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bernardo-magnini/ class=align-middle>Bernardo Magnini</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-odijk/ class=align-middle>Jan Odijk</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/maciej-ogrodniczuk/ class=align-middle>Maciej Ogrodniczuk</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/e/eirikur-rognvaldsson/ class=align-middle>Eirikur Rögnvaldsson</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/michael-rosner/ class=align-middle>Michael Rosner</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bolette-sandford-pedersen/ class=align-middle>Bolette Sandford Pedersen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/i/inguna-skadina/ class=align-middle>Inguna Skadiņa</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marko-tadic/ class=align-middle>Marko Tadić</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dan-tufis/ class=align-middle>Dan Tufiş</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tamas-varadi/ class=align-middle>Tamás Váradi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kadri-vider/ class=align-middle>Kadri Vider</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/andy-way/ class=align-middle>Andy Way</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/m/marcello-federico/ class=align-middle>Marcello Federico</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/a/alex-waibel/ class=align-middle>Alex Waibel</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/k/kevin-knight/ class=align-middle>Kevin Knight</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/satoshi-nakamura/ class=align-middle>Satoshi Nakamura</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/hermann-ney/ class=align-middle>Hermann Ney</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jan-niehues/ class=align-middle>Jan Niehues</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/sebastian-stuker/ class=align-middle>Sebastian Stüker</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/d/dekai-wu/ class=align-middle>Dekai Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/joseph-mariani/ class=align-middle>Joseph Mariani</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/ws/ class=align-middle>WS</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/naacl/ class=align-middle>NAACL</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/venues/iwslt/ class=align-middle>IWSLT</a><span class="badge badge-secondary align-middle ml-2">2</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-venues aria-expanded=false aria-controls=more-venues>show all...</li><div class="collapse border-top" id=more-venues><li class=list-group-item><a href=/venues/conll/ class=align-middle>CoNLL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/wmt/ class=align-middle>WMT</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/lrec/ class=align-middle>LREC</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/eacl/ class=align-middle>EACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>