<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Fei Huang - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><span class=font-weight-normal>Fei</span> <span class=font-weight-bold>Huang</span></h2><hr><div class=row><div class=col-lg-9><h4>2021</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.46/>Structural Knowledge Distillation : Tractably Distilling Information for Structured Predictor</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/z/zhaohui-yan/>Zhaohui Yan</a>
|
<a href=/people/z/zixia-jia/>Zixia Jia</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--46><div class="card-body p-3 small">Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). The <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> of knowledge distillation is typically the <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a> between the teacher and the student&#8217;s output distributions. However, for structured prediction problems, the output space is exponential in size ; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a>, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios : 1) the teacher and student share the same factorization form of the output structure scoring function ; 2) the student factorization produces more fine-grained substructures than the teacher factorization ; 3) the teacher factorization produces more fine-grained substructures than the student factorization ; 4) the factorization forms from the teacher and the student are incompatible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.142" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.142/>Improving <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> by External Context Retrieving and Cooperative Learning</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--142><div class="card-body p-3 small">Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a>, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by <a href=https://en.wikipedia.org/wiki/Cooperative_learning>Cooperative Learning</a>, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.206/>Automated Concatenation of Embeddings for <a href=https://en.wikipedia.org/wiki/Structured_prediction>Structured Prediction</a></a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--206><div class="card-body p-3 small">Pretrained contextualized embeddings are powerful <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. However, the selection of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and updates the belief based on a <a href=https://en.wikipedia.org/wiki/Reward_system>reward</a>. We follow strategies in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.207/>Multi-View Cross-Lingual Structured Prediction with Minimum Supervision</a></strong><br><a href=/people/z/zechuan-hu/>Zechuan Hu</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--207><div class="card-body p-3 small">In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can dynamically adjust the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence level</a> of each source model and improve the performance of both <a href=https://en.wikipedia.org/wiki/View_model>views</a> during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.237/>A Semantic-based Method for Unsupervised Commonsense Question Answering</a></strong><br><a href=/people/y/yilin-niu/>Yilin Niu</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/j/jiaming-liang/>Jiaming Liang</a>
|
<a href=/people/w/wenkai-chen/>Wenkai Chen</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--237><div class="card-body p-3 small">Unsupervised commonsense question answering is appealing since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> can be easily affected by irrelevant factors, such as <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequencies</a>, <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structures</a>, etc. These distracting factors may not only mislead the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> first generates a set of plausible answers with <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between each plausible answer and each choice. We devise a simple, yet sound <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a> for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on four <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark datasets</a>, and our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves the best results in <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised settings</a>. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.308/>VECO : Variable and Flexible Cross-lingual Pre-training for <a href=https://en.wikipedia.org/wiki/Language_understanding>Language Understanding</a> and Generation<span class=acl-fixed-case>VECO</span>: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation</a></strong><br><a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/j/jiahao-liu/>Jiahao Liu</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/b/bin-bi/>Bin Bi</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/luo-si/>Luo Si</a><br><a href=/volumes/2021.acl-long/ class=text-muted>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--308><div class="card-body p-3 small">Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.emnlp-main.185" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.185/>DialogueCSE : Dialogue-based Contrastive Learning of Sentence Embeddings<span class=acl-fixed-case>D</span>ialogue<span class=acl-fixed-case>CSE</span>: Dialogue-based Contrastive Learning of Sentence Embeddings</a></strong><br><a href=/people/c/che-liu/>Che Liu</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/j/jinghua-liu/>Jinghua Liu</a>
|
<a href=/people/j/jian-sun/>Jian Sun</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/luo-si/>Luo Si</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--185><div class="card-body p-3 small">Learning sentence embeddings from <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets : the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in terms of MAP and Spearman&#8217;s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.emnlp-main.339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--emnlp-main--339 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.emnlp-main.339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.emnlp-main.339.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.emnlp-main.339/>A Unified Encoding of Structures in Transition Systems</a></strong><br><a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/x/xiaoling-wang/>Xiaoling Wang</a><br><a href=/volumes/2021.emnlp-main/ class=text-muted>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--emnlp-main--339><div class="card-body p-3 small">Transition systems usually contain various <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic structures</a> (e.g., <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>stacks</a>, buffers). An ideal transition-based model should encode these <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structures</a> completely and efficiently. Previous works relying on <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a> or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a <a href=https://en.wikipedia.org/wiki/Transition_system>transition system</a>. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding <a href=https://en.wikipedia.org/wiki/Transition_state>transition states</a> with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods.</div></div><h4>2020</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.acl-main.713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--acl-main--713 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.acl-main.713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929257 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.acl-main.713/>A Joint Neural Model for <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> with Global Features</a></strong><br><a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a><br><a href=/volumes/2020.acl-main/ class=text-muted>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--acl-main--713><div class="card-body p-3 small">Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> from an input sentence. OneIE performs end-to-end IE in four stages : (1) Encoding a given sentence as contextualized word representations ; (2) Identifying entity mentions and event triggers as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> ; (3) Computing label scores for all <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and their pairwise links using local classifiers ; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.356/>More <a href=https://en.wikipedia.org/wiki/Embedding>Embeddings</a>, Better Sequence Labelers?</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a><br><a href=/volumes/2020.findings-emnlp/ class=text-muted>Findings of the Association for Computational Linguistics: EMNLP 2020</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--356><div class="card-body p-3 small">Recent work proposes a family of contextual embeddings that significantly improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> in various settings. In this paper, we conduct extensive experiments on 3 tasks over 18 datasets and 8 languages to study the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labeling with various embedding concatenations and make three observations : (1) concatenating more embedding variants leads to better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in rich-resource and cross-domain settings and some conditions of low-resource settings ; (2) concatenating contextual sub-word embeddings with contextual character embeddings hurts the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in extremely low-resource settings ; (3) based on the conclusion of (1), concatenating additional similar contextual embeddings can not lead to further improvements. We hope these conclusions can help people build stronger sequence labelers in various settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.tacl-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--tacl-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.tacl-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.tacl-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.tacl-1.7/>A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation</a></strong><br><a href=/people/j/jian-guan/>Jian Guan</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/z/zhihao-zhao/>Zhihao Zhao</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a><br><a href=/volumes/2020.tacl-1/ class=text-muted>Transactions of the Association for Computational Linguistics, Volume 8</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--tacl-1--7><div class="card-body p-3 small">Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>, understanding the <a href=https://en.wikipedia.org/wiki/Causality>causal relationships</a>, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, which combines a discriminative objective to distinguish true and fake stories during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.</div></div><h4>2019</h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1436 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1436/>ARAML : A Stable Adversarial Training Framework for Text Generation<span class=acl-fixed-case>ARAML</span>: A Stable Adversarial Training Framework for Text Generation</a></strong><br><a href=/people/p/pei-ke/>Pei Ke</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a><br><a href=/volumes/D19-1/ class=text-muted>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1436><div class="card-body p-3 small">Most of the existing generative adversarial networks (GAN) for text generation suffer from the instability of reinforcement learning training algorithms such as policy gradient, leading to unstable performance. To tackle this problem, we propose a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> called Adversarial Reward Augmented Maximum Likelihood (ARAML). During adversarial training, the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> assigns rewards to samples which are acquired from a <a href=https://en.wikipedia.org/wiki/Stationary_distribution>stationary distribution</a> near the data rather than the generator&#8217;s distribution. The generator is optimized with <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation</a> augmented by the discriminator&#8217;s rewards instead of policy gradient. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can outperform state-of-the-art text GANs with a more stable training process.</div></div></div><div class=col-lg-3><a class="btn btn-lg btn-secondary btn-block mb-2" href="https://www.semanticscholar.org/search?q=Fei+Huang" title="Search for 'Fei Huang' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class=pl-sm-2>Search</span></a><div class=row><div class="col-12 col-md-6 col-lg-12"><div class=card><h5 class=card-header>Co-authors</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/people/y/yong-jiang/ class=align-middle>Yong Jiang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/t/tao-wang/ class=align-middle>Tao Wang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/z/zhongqiang-huang/ class=align-middle>Zhongqiang Huang</a>
<span class="badge badge-secondary align-middle ml-2">6</span></li><li class=list-group-item><a href=/people/n/nguyen-bach/ class=align-middle>Nguyen Bach</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class=list-group-item><a href=/people/k/kewei-tu/ class=align-middle>Kewei Tu</a>
<span class="badge badge-secondary align-middle ml-2">5</span></li><li class="list-group-item list-group-toggle-btn py-1" data-toggle=collapse data-target=#more-coauthors aria-expanded=false aria-controls=more-coauthors>show all...</li><div class="collapse border-top" id=more-coauthors><li class=list-group-item><a href=/people/x/xinyu-wang/ class=align-middle>Xinyu Wang</a>
<span class="badge badge-secondary align-middle ml-2">4</span></li><li class=list-group-item><a href=/people/x/xiaoyan-zhu/ class=align-middle>Xiaoyan Zhu</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/m/minlie-huang/ class=align-middle>Minlie Huang</a>
<span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/people/l/luo-si/ class=align-middle>Luo Si</a>
<span class="badge badge-secondary align-middle ml-2">2</span></li><li class=list-group-item><a href=/people/z/zhaohui-yan/ class=align-middle>Zhaohui Yan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zixia-jia/ class=align-middle>Zixia Jia</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zechuan-hu/ class=align-middle>Zechuan Hu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yilin-niu/ class=align-middle>Yilin Niu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiaming-liang/ class=align-middle>Jiaming Liang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wenkai-chen/ class=align-middle>Wenkai Chen</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/f/fuli-luo/ class=align-middle>Fuli Luo</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/w/wei-wang/ class=align-middle>Wei Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jiahao-liu/ class=align-middle>Jiahao Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yijia-liu/ class=align-middle>Yijia Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/b/bin-bi/ class=align-middle>Bin Bi</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/s/songfang-huang/ class=align-middle>Songfang Huang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/ying-lin/ class=align-middle>Ying Lin</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/h/heng-ji/ class=align-middle>Heng Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/l/lingfei-wu/ class=align-middle>Lingfei Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/c/che-liu/ class=align-middle>Che Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/r/rui-wang/ class=align-middle>Rui Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jinghua-liu/ class=align-middle>Jinghua Liu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-sun/ class=align-middle>Jian Sun</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/t/tao-ji/ class=align-middle>Tao Ji</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/y/yuanbin-wu/ class=align-middle>Yuanbin Wu</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/x/xiaoling-wang/ class=align-middle>Xiaoling Wang</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/p/pei-ke/ class=align-middle>Pei Ke</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/j/jian-guan/ class=align-middle>Jian Guan</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/people/z/zhihao-zhao/ class=align-middle>Zhihao Zhao</a>
<span class="badge badge-secondary align-middle ml-2">1</span></li></div></ul></div></div><div class="col-12 col-md-6 col-lg-12"><div class="card my-2 my-md-0 my-lg-2"><h5 class=card-header>Venues</h5><ul class="list-group list-group-flush list-group-compact"><li class=list-group-item><a href=/venues/acl/ class=align-middle>ACL</a><span class="badge badge-secondary align-middle ml-2">7</span></li><li class=list-group-item><a href=/venues/emnlp/ class=align-middle>EMNLP</a><span class="badge badge-secondary align-middle ml-2">3</span></li><li class=list-group-item><a href=/venues/findings/ class=align-middle>Findings</a><span class="badge badge-secondary align-middle ml-2">1</span></li><li class=list-group-item><a href=/venues/tacl/ class=align-middle>TACL</a><span class="badge badge-secondary align-middle ml-2">1</span></li></ul></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>