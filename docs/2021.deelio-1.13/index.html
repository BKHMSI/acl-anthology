<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Attention vs non-attention for a Shapley-based explanation method - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Attention vs non-attention for a Shapley-based explanation method" name=citation_title><meta content="Tom Kersten" name=citation_author><meta content="Hugh Mee Wong" name=citation_author><meta content="Jaap Jumelet" name=citation_author><meta content="Dieuwke Hupkes" name=citation_author><meta content="Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures" name=citation_conference_title><meta content="2021/6" name=citation_publication_date><meta content="https://aclanthology.org/2021.deelio-1.13.pdf" name=citation_pdf_url><meta content="129" name=citation_firstpage><meta content="139" name=citation_lastpage><meta content="10.18653/v1/2021.deelio-1.13" name=citation_doi><meta property="og:title" content="Attention vs non-attention for a Shapley-based explanation method"><meta property="og:image" content="https://aclanthology.org/thumb/2021.deelio-1.13.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2021.deelio-1.13"><meta property="og:description" content="Tom Kersten, Hugh Mee Wong, Jaap Jumelet, Dieuwke Hupkes. Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 2021."><link rel=canonical href=https://aclanthology.org/2021.deelio-1.13></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2021.deelio-1.13.pdf>Attention vs non-attention for a Shapley-based explanation method</a>
<a id=af_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Name</a>
<a id=am_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>ተቃውሞ ሳይታወቅ ለShapley-based ትርጉም</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>الانتباه مقابل عدم الانتباه لطريقة الشرح المبنية على Shapley</a>
<a id=az_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Shapley tabanlı a çıqlama yöntemi üçün dikkat edilməz</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Внимание срещу липса на внимание за метод на обяснение базиран на Shapley</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>শ্যাপলি ভিত্তিক ব্যাখ্যা পদ্ধতির জন্য মনোযোগ বিরোধী</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>ཆ་འཕྲིན་ལྷན་པ་དང་ལྷན་མེད་པའི་ཕྱོགས་སྟོན་ལམ་ལ་བཞག་ནི།</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Pažnja protiv ne-pažnje za metodu objašnjavanja na Shapley-u</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Attention vs non-attention for a Shapley-based explanation method</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Pozornost versus nepozornost pro metodu vysvětlení založenou na Shapley</a>
<a id=da_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Opmærksomhed vs. ikke-opmærksomhed for en Shapley-baseret forklaringsmetode</a>
<a id=de_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Aufmerksamkeit vs. Nicht-Aufmerksamkeit für eine Shapley-basierte Erklärungsmethode</a>
<a id=el_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Προσοχή έναντι μη προσοχής για μια μέθοδο επεξήγησης βασισμένη στο Shapley</a>
<a id=es_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Atención frente a falta de atención para un método de explicación basado en Shapley</a>
<a id=et_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Tähelepanu vs tähelepanuväärtus Shapley-põhise seletusmeetodi puhul</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>توجه علیه توجه غیر توجه برای روش توضیح بر اساس شیپلی</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Huomio vs. huomiotta jättäminen Shapley-pohjaisessa selitysmenetelmässä</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Attention ou non-attention pour une méthode d'explication basée sur Shapley</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Aird vs neamhaird ar mhodh mínithe Shapley-bhunaithe</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>AttAttachment</a>
<a id=he_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>תשומת לב נגד לא תשומת לב לשיטת הסבר מבוססת על שיפלי</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>एक Shapley-आधारित स्पष्टीकरण विधि के लिए ध्यान बनाम गैर-ध्यान</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Pažnja protiv ne-pažnje za metodu objašnjenja na osnovu Shapley a</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Figyelem kontra figyelem nem figyelem Shapley-alapú magyarázat esetén</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Ուշադրություն հակառակ ոչ ուշադրությունը Շեփլին հիմնված բացատրության մեթոդի համար</a>
<a id=id_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Perhatian vs nonperhatian untuk metode penjelasan berdasarkan Shapley</a>
<a id=is_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Attenzione vs non attenzione per un metodo di spiegazione basato su Shapley</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Shapleyベースの説明方法に対する注意と非注意</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Attribute</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>შაპლის განახსნავის მეთოდისთვის დაახსნა</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Name</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Shapley 해석 방법에 근거한 주의와 부주의</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Atkreipimas į Shapley pagrįstą paaiškinimo metodą</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Внимание против невнимание за метод на објаснување базиран на Шапли</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>ശാപ്ലി അടിസ്ഥാനമായി വിശദീകരിക്കുന്ന രീതിയില്‍ ശ്രദ്ധിക്കുക</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Шапли-д суурилсан тайлбарлалтын арга болон анхаарал биш,</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Perhatian vs tidak-perhatian untuk kaedah penjelasan berdasarkan Shapley</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Attenzjoni kontra nuqqas ta’ attenzjoni għal metodu ta’ spjegazzjoni bbażat fuq Shapley</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Aandacht vs niet-aandacht voor een op Shapley gebaseerde uitleg methode</a>
<a id=no_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Name</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Uwaga vs brak uwagi na metodę wyjaśnienia opartą na Shapley</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Atenção versus não atenção para um método de explicação baseado em Shapley</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Atenție vs non-atenție pentru o metodă de explicație bazată pe Shapley</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Внимание или невнимание к методу объяснения на основе Шапли</a>
<a id=si_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Name</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Pozornost v primerjavi z nepozornostjo za metodo razlage, ki temelji na Shapleyju</a>
<a id=so_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Attention vs non-attention for a Shapley-based explanation method</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Vëmendje kundër jo-vëmendjes për një metodë shpjegimi bazuar në Shapley</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Pažnja protiv ne-pažnje za metodu objašnjenja na Shapley-u</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Uppmärksamhet vs icke-uppmärksamhet för en Shapley-baserad förklaringsmetod</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Attention vs non-attention for a Shapley-based explanation method</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>நிழலில் அடிப்படையிலான விளக்கம் முறையில் கவனம் எதிர்பார்ப்பு</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Shapley tabanly düşündirim yöntemi üçin üns berilmeýän</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>شیپلی کی بنیادی توضیح طریقہ کے لئے اظہار غیر توجه</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>Chú ý chống không chú ý đến phương pháp giải thích hình mẫu</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2021.deelio-1.13.pdf>盖沙普利之说,注意与非注意也</a></h2><p class=lead><a href=/people/t/tom-kersten/>Tom Kersten</a>,
<a href=/people/h/hugh-mee-wong/>Hugh Mee Wong</a>,
<a href=/people/j/jaap-jumelet/>Jaap Jumelet</a>,
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods that are often proposed and tested in the domain of <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> are appropriate to address the explainability challenges in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operations</a> necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, considering a number of different syntactic structures in two different languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>. Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. In particular, using <a href=https://en.wikipedia.org/wiki/Compact_disc>CD</a>, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Die veld van verduidelik AI het onlangs 'n eksplosie gesien in die aantal uitduidelingsmetodes vir hoë nie- lineêre diep neuralnetwerke. Die uitbreiding waarmee sodanige metodes - wat dikwels voorgestel word en toegestel word in die domein van rekenaar sien - is geskik om die verduidelikheidsverdigheidsverdigheidsverdighede in NLP te adres is nog relativief ongeverklar. In hierdie werk, ons beoorsaak Konteksual Deskompositie (CD) - ' n Shapley-gebaseerde invoer funksie toewysing metode wat gewys is om goed te werk vir herhaalde NLP modele - en ons toets die uitbreiding waarin dit is nuttig vir modele wat aandag operasies bevat. Na hierdie einde, ons uitbrei CD om die operasies wat nodig is vir aanmerksgebaseerde modele te oordek. Ons vergelyk dan hoe lank afstand onderwerp-verb verhoudings deur modele met en sonder aandag verwerk word, terwyl ons 'n aantal verskillende sintaktiske strukture in twee verskillende tale: Engels en Nederlandse. Ons eksperimente bevestig dat Cd suksesvol kan aangepas word vir aandag-gebaseerde modele ook, en verskaf 'n alternatiewe Shapley-gebaseerde toewysing metode vir moderne neuralnetwerke. In besonderhede, gebruik ons CD, wys ons dat die Engelse en Nederlandse modele soortgelyke verwerking gedrag bevestig, maar dat onder die hoof daar bestaande verskille tussen ons aandag en nie-aandag modele is.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የአ.আই. መሬት በቅርብ ጊዜ በጥልቅ ጥልቅ የደዌብ መረብ ላይ ያልተረፈ የጥልቅ የጥልቅ የጥልቅ ጥረት ጥያቄ የሚደረገውን የውጤት አግኝቷል፡፡ በኮምፒውተር ራእይ ውስጥ እንደዚህ ዓይነት ልማድ - ብዙ ጊዜም በተመሳሳይና በሚፈትኑት የኮምፒዩተር ራእይ ውስጥ - የNLP የግልጽን የግልጽ ጥቃት ገና በተለየ ፍላጎት ያልታወቀ ነው፡፡ በዚህ ስራ ውስጥ የውይይት አካባቢ አካባቢ (CD) - በShapley-based የinput attribution method - ለቀጥተኛ NLP ሞዴላዎች መልካም ለመሥራት የተገለጸ ነው - እና በቁጥጥር ለሞዴላዎች የሚጠቅመውን ቁጥጥር እንሞክራለን፡፡ ለዚህ መጨረሻ፣ ለጥያቄ ምሳሌዎች የሚያስፈልገውን ስርዓቶች ለማክበር ሲዲን እንጨርጋለን፡፡ ከዚያም በኋላ የሩቅ ጉዳዩ-የቃላት ግንኙነት እንዴት ያህል እንደተቃውሞ እና ያለ ማስታወቂያ እንደሆነ እናሳስመስላለን፤ በተለያዩ ቋንቋዎች፣ እንግሊዘኛ እና ዶሎክ፡፡ ፈተናዎቻችን ሲዲዲ በጥቅምት የተመሳሳይ እና በጥቅምት የተመሳሳይ ጥያቄ ላይ የተመሳሳይ የናውሬል መረብ ላይ የተመሳሳይ የጥያቄ ድርጅት እንዲሰጥ እንዲችል ያረጋግጣሉ፡፡ በተለይም CD በመጠቀም እንግሊዘኛ እና ድል ሞላት መሰላቸውን የሥርዓት ሥርዓት ማሳየት እናደርጋለን፤ ነገር ግን ከሀገራው በታች በጥያቄያችን እና በማይጠያየቅ ሞዴላዎች መካከል የሚተያየው ልዩነት አሉ፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>شهد مجال الذكاء الاصطناعي القابل للتفسير مؤخرًا انفجارًا في عدد طرق التفسير للشبكات العصبية العميقة غير الخطية للغاية. إلى أي مدى تكون هذه الأساليب - التي غالبًا ما يتم اقتراحها واختبارها في مجال رؤية الكمبيوتر - مناسبة للتعامل مع تحديات قابلية التفسير في البرمجة اللغوية العصبية (NLP) لم يتم استكشافها بعد نسبيًا. في هذا العمل ، نعتبر التحليل السياقي (CD) - طريقة إسناد ميزات الإدخال المستندة إلى Shapley والتي ثبت أنها تعمل بشكل جيد لنماذج البرمجة اللغوية العصبية المتكررة - ونختبر مدى فائدتها للنماذج التي تحتوي على عمليات الانتباه. تحقيقا لهذه الغاية ، نقوم بتمديد القرص المضغوط لتغطية العمليات اللازمة للنماذج القائمة على الانتباه. ثم نقارن بعد ذلك مدى المسافة الطويلة التي تتم بها معالجة علاقات الفاعل والفعل بواسطة النماذج مع الانتباه أو بدون الانتباه ، مع الأخذ في الاعتبار عددًا من الهياكل النحوية المختلفة في لغتين مختلفتين: الإنجليزية والهولندية. تؤكد تجاربنا أنه يمكن تطبيق القرص المضغوط بنجاح على النماذج القائمة على الانتباه أيضًا ، مما يوفر طريقة إسناد بديلة قائمة على Shapley للشبكات العصبية الحديثة. على وجه الخصوص ، باستخدام القرص المضغوط ، أظهرنا أن النموذجين الإنجليزي والهولندي يظهران سلوك معالجة متشابهًا ، ولكن هناك اختلافات ثابتة بين نماذج الانتباه وعدم الانتباه لدينا.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Açıqlanabilir AI sahəsi çox yaxın zamanda çox çətin nöral ağları üçün açıqlama metodlarının sayısında bir patlama gördü. NLP'deki açıq-aydınlıq çətinliklərini çəkmək üçün bu metodlar - çox dəfə təbliğ edilən və sınaqlanan kompjuter görünüşünün domeinində təşkil edilən və təşkil edilən münasibdir. Bu işdə, biz "Contextual Decomposition" (CD) – Shapley-ə dayanan giriş fəaliyyəti təmizləmə metodlarını düşünürük. NLP modelləri üçün daha yaxşı çalışmaq üçün göstərilmişdir. Bütün bunlara görə, biz CD'yi dikkati modellərə görə ehtiyacı olan işləri örtüb genişləyirik. Sonra, müxtəlif dillərdə bir neçə müxtəlif sintaktik quruları ilə modellər ilə işlədiləcək və təsirsiz işlədiləcək müxtəlif məsələlər ilə qarşılaşdırırıq: İngilizce və Holanda. Bizim təcrübələrimiz CD'nin dikkati daxilində olan modellərə də müvəffəqiyyətlə uygulanabileceğini təsdiqləyir, modern nöral ağları üçün Shapley tabanlı başqa bir təcrübə metodu təklif edir. Özellikle, CD vasitəsilə, İngilizci və Holandi modellərin bənzər işləmə davranışlarını göstərdiyini göstərdik, amma qeyd altında dikkatimiz və dikkatimiz olmayan modellərin arasında müxtəlif fərqlər vardır.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>В областта на обяснимия изкуствен интелект наскоро се наблюдава експлозия в броя на обяснителните методи за високо нелинейни дълбоки невронни мрежи. Все още е сравнително неизследвана степента, до която такива методи, които често се предлагат и тестват в областта на компютърното зрение, са подходящи за справяне с обясняемите предизвикателства в НЛП. В тази работа разглеждаме метода за приписване на входни функции, който е доказано, че работи добре при повтарящи се модели на НЛП, и тестваме до каква степен е полезен за модели, които съдържат операции по внимание. За тази цел разширяваме компактдиска, така че да обхване операциите, необходими за моделите, базирани на вниманието. След това сравняваме как отношенията субект-глагол на дълги разстояния се обработват от модели с и без внимание, като се вземат предвид редица различни синтактични структури на два различни езика: английски и холандски. Нашите експерименти потвърждават, че компактдискът може успешно да бъде приложен и за модели, базирани на вниманието, като предоставя алтернативен метод за присвояване на Shapley базиран на съвременни невронни мрежи. По-специално, използвайки компактдиска, показваме, че английските и холандските модели демонстрират подобно поведение на обработка, но че под капака има последователни разлики между нашите модели на внимание и не внимание.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>সম্প্রতি ব্যাখ্যাত AI-এর ক্ষেত্রে ব্যাখ্যা করা হয়েছে যে ব্যাখ্যা ব্যাখ্যা করা হয়েছে তার সংখ্যা ব্যাখ্যার মাধ্যমে ব কোন ধরনের পদ্ধতি- যা প্রায়শ কম্পিউটার ভিশনের ডোমেইনে প্রস্তাব করা এবং পরীক্ষা করা হয়- এনএলপির ব্যাখ্যাত চ্যালেঞ্জের ব্যাখ্যা করার জন্য এই কাজে আমরা বিভিন্ন বিষয়বস্তু (সিডি) - একটি শ্যাপলি ভিত্তিক ইনপুট বৈশিষ্ট্যাবলিক বৈশিষ্ট্যের মাধ্যমে পুনরায় এনএলপি মডেলের জন্য ভালো কাজ করার জন্য প্রদর্শন করা হয়েছ To this end, we extend CD to cover the operations necessary for attention-based models. তারপর আমরা দুই ভাষায় বিভিন্ন ভিন্ন ভিন্ন ভাষায় বিভিন্ন সিন্ট্যাক্টিক কাঠামো বিবেচনা করি, ইংরেজি এবং ডাচ। আমাদের পরীক্ষা নিশ্চিত করেছে যে সিডি সফলভাবে মনোযোগ মোডেলের জন্য প্রয়োগ করা যাবে এবং আধুনিক নিউরেল নেটওয়ার্কের জন্য একটি বিকল্প শ্যা In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>གནད་དོན་འགྲེལ་བཤད་རུང་བའི་AI་གི་སྒོ་ཕྱེ་བ་དེ་ཉེ་ཆར་ཡོད་པ་དེ་ཉིད་ཆས་རང་ཉིད་ཀྱི་དྲ་རྒྱ་ལ་ཉིད་ལ་སྐྱེས་ཉེན NLP ནང་གི་འགྲེལ་བཤད་ཀྱི་གདོང་ལེན་ཚད་དཔག་ཡོད་པ་དང་རྩིས་འཁོར་ཐོག་གི་མཐོང་ཐོག་ཏུ་བརྟག་ཞིབ་བྱེད་པའི་གནས་སྟངས In this work, we consider Contextual Decomposition (CD) - a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models - and we test the extent to which it is useful for models that contain attention operations. མཇུག་མམ་དེ་ལ། འོད་ཀྱིས་འོད་སྡེར་སྒྲུབ་འདོད་བྱས་པའི་བྱ་སྤྱོད་ཚོར་མཁན་མཐུན་ཡོད་པ We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. ང་ཚོའི་བརྟག་ཞིབ་ཀྱིས་འོད་སྡེར་སྟེང་གི་གནད་དོན་བཟོ་བྱས་ན་མཐར་འཁྱོར་སྐྱོད་བྱས་ཡོད་པ་དང་མཐུན་སྒྲིག་གི་ཐབས་ལམ་ལ་ཉ ང་ཚོས་རང་ཉིད་ཀྱི་འོད་སྡེར་སྤྱོད་སྤྱད་པར་ཨིན་རིས་དང་རྒྱ་ནག་གི་མིག་དཔེ་གཟུགས་རིས་ཀྱིས་ལས་སྦྱོར་བྱ་སྟངས་དང་མཐུན་རྐྱེན་པའི་</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Polje objašnjavajućih AI nedavno je vidio eksploziju u broju metoda objašnjavanja za visoko neolinearne duboke neuralne mreže. Koliko su takve metode - koje su često predložene i testirane u domenu kompjuterske vizije - odgovarajuće za rješavanje izazova objašnjivosti u NLP-u, još je relativno neobjašnjivo. U ovom poslu razmišljamo o kontekstualnoj dekompoziciji (CD) - metodi privlačenja ulaznih funkcija na Shapley-u koja je pokazala kako dobro funkcioniše za rekonstruirane modele NLP-a - i testiramo mjeru u kojoj je korisno za modele koji sadrže operacije pažnje. Za taj cilj, proširimo CD da pokrijemo operacije potrebne za modele na osnovu pažnje. Onda uspoređujemo koliko duge veze s tema-verbom obrađuju modeli sa i bez pažnje, s obzirom na broj različitih sintaktičkih struktura na dva različita jezika: engleski i holandski. Naši eksperimenti potvrđuju da se CD može uspješno primjenjivati i za modele na osnovu pažnje, pružajući alternativnu metodu privlačenja na osnovu Shapley-a za moderne neuralne mreže. Posebno, koristeći CD, pokazujemo da engleski i holandski modeli pokazuju slično ponašanje procesa, ali da pod kapuljom postoje konsistentne razlike između naših pažnje i modela neopažnje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods - that are often proposed and tested in the domain of computer vision - are appropriate to address the explainability challenges in NLP is yet relatively unexplored. En aquesta feina, considerem la descomposició contextual (CD) - un mètode d'atribució de característiques d'entrada basat en Shapley que ha demostrat que funciona bé per a models recurrents de NLP - i compruem el punt en què és útil per models que contén operacions d'atenció. Per això estendem CD per cobrir les operacions necessàries per a models basats en l'atenció. Llavors comparem com de llarga distància les relacions subjecte-verb es processen per models amb i sense atenció, considerant una sèrie d'estructures sinàctiques diferents en dues llengües diferents: anglès i holandes. Els nostres experiments confirmen que el CD també es pot aplicar amb èxit a models basats en l'atenció, proporcionant un mètode alternativ d'atribució basat en Shapley per a xarxes neurals modernes. En particular, utilitzant CD, demostram que els models anglès i holandeses demostren comportaments de processament semblants, però que sota el capítol hi ha diferències consistents entre la nostra atenció i els models sense atenció.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>V oblasti vysvětlitelné umělé inteligence v poslední době došlo k explozi počtu metod vysvětlení vysoce nelineárních hlubokých neuronových sítí. Rozsah, do jaké jsou tyto metody, které jsou často navrženy a testovány v oblasti počítačového vidění, vhodné pro řešení výzev vysvětlitelnosti v NLP, je dosud relativně neprozkoumán. V této práci uvažujeme o metodě atribuce vstupních vlastností založené na Shapley, která funguje dobře pro recidivující NLP modely a testujeme, do jaké míry je užitečná pro modely obsahující operace pozornosti. Za tímto účelem rozšíříme CD o operace potřebné pro modely založené na pozornosti. Následně porovnáme, jak jsou vztahy subjektu-slovesa na dlouhou vzdálenost zpracovávány modely s pozorností a bez pozornosti, s ohledem na řadu různých syntaktických struktur ve dvou různých jazycích: angličtině a nizozemštině. Naše experimenty potvrzují, že CD lze úspěšně aplikovat i na modely založené na pozornosti, což poskytuje alternativní Shapleyovou atribuční metodu pro moderní neuronové sítě. Zejména pomocí CD ukazujeme, že anglický a nizozemský model vykazují podobné chování zpracování, ale že pod kapotou existují konzistentní rozdíly mezi našimi modely pozornosti a modely bez pozornosti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Området for forklarelig AI har for nylig oplevet en eksplosion i antallet af forklaringsmetoder for meget ikke-lineære dybe neurale netværk. I hvilket omfang sådanne metoder - som ofte foreslås og afprøves inden for computersyn - er egnede til at imødegå forklaringsudfordringerne i NLP, er stadig relativt uudforsket. I dette arbejde betragter vi Contextual Decomposition (CD) - en Shapley-baseret input feature attribution metode, der har vist sig at fungere godt for tilbagevendende NLP modeller - og vi tester i hvilket omfang det er nyttigt for modeller, der indeholder opmærksomhedsoperationer. Til dette formål udvider vi CD til at dække de operationer, der er nødvendige for opmærksomhedsbaserede modeller. Vi sammenligner derefter, hvor langdistance subjekt-verbe relationer behandles af modeller med og uden opmærksomhed, idet vi tager hensyn til en række forskellige syntaktiske strukturer på to forskellige sprog: engelsk og hollandsk. Vores eksperimenter bekræfter, at CD med succes kan anvendes til opmærksomhedsbaserede modeller også, hvilket giver en alternativ Shapley-baseret attribution metode til moderne neurale netværk. Især ved hjælp af CD viser vi, at de engelske og hollandske modeller viser lignende bearbejdningsadfærd, men at der under emhætten er konsekvente forskelle mellem vores opmærksomheds- og ikke-opmærksomhedsmodeller.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Der Bereich der erklärbaren KI hat in jüngster Zeit eine Explosion in der Anzahl der Erklärungsmethoden für hochgradig nichtlineare tiefe neuronale Netze erlebt. Inwieweit solche Methoden, die häufig im Bereich des Computer Vision vorgeschlagen und getestet werden, geeignet sind, um die erklärbaren Herausforderungen im NLP anzugehen, ist noch relativ unerforscht. In dieser Arbeit betrachten wir Contextual Decomposition (CD) eine Shapley-basierte Eingabefeature-Attributionsmethode, die nachweislich gut für wiederkehrende NLP-Modelle funktioniert, und wir testen, inwieweit sie für Modelle nützlich ist, die Aufmerksamkeitsoperationen enthalten. Zu diesem Zweck erweitern wir CD um die für aufmerksamkeitsbasierte Modelle notwendigen Operationen. Anschließend vergleichen wir, wie lange Distanz Subjekt-Verb-Beziehungen von Modellen mit und ohne Aufmerksamkeit verarbeitet werden, wobei wir eine Reihe verschiedener syntaktischer Strukturen in zwei verschiedenen Sprachen berücksichtigen: Englisch und Niederländisch. Unsere Experimente bestätigen, dass CD auch für aufmerksamkeitsbasierte Modelle erfolgreich angewendet werden kann und eine alternative Shapley-basierte Attributionsmethode für moderne neuronale Netze darstellt. Insbesondere mit CD zeigen wir, dass das englische und das niederländische Modell ein ähnliches Verarbeitungsverhalten aufweisen, aber dass unter der Haube konsistente Unterschiede zwischen unseren Aufmerksamkeits- und Nicht-Aufmerksamkeitsmodellen bestehen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ο τομέας της εξηγητής τεχνητής νοημοσύνης έχει πρόσφατα δει μια έκρηξη στον αριθμό των μεθόδων επεξήγησης για εξαιρετικά μη γραμμικά βαθιά νευρωνικά δίκτυα. Ο βαθμός στον οποίο τέτοιες μέθοδοι, οι οποίες συχνά προτείνονται και δοκιμάζονται στον τομέα της υπολογιστικής όρασης, είναι κατάλληλες για την αντιμετώπιση των προκλήσεων εξηγησιμότητας στο ΝΛΠ, είναι ακόμη σχετικά ανεξερεύνητος. Σε αυτή την εργασία, εξετάζουμε τη μέθοδο απόδοσης χαρακτηριστικών εισαγωγής που έχει αποδειχθεί ότι λειτουργεί καλά για επαναλαμβανόμενα μοντέλα και εξετάζουμε τον βαθμό στον οποίο είναι χρήσιμο για μοντέλα που περιέχουν λειτουργίες προσοχής. Για το σκοπό αυτό, επεκτείνουμε το CD για να καλύψει τις απαραίτητες λειτουργίες για μοντέλα με βάση την προσοχή. Στη συνέχεια, συγκρίνουμε τον τρόπο επεξεργασίας των μεγάλων αποστάσεων σχέσεων υποκειμένου-ρήματος από μοντέλα με και χωρίς προσοχή, λαμβάνοντας υπόψη μια σειρά διαφορετικών συντακτικών δομών σε δύο διαφορετικές γλώσσες: Αγγλικά και Ολλανδικά. Τα πειράματά μας επιβεβαιώνουν ότι το CD μπορεί να εφαρμοστεί με επιτυχία και σε μοντέλα βασισμένα στην προσοχή, παρέχοντας μια εναλλακτική μέθοδο αποδόσεως βασισμένη στο Shapley για τα σύγχρονα νευρωνικά δίκτυα. Ειδικότερα, με τη χρήση του δείχνουμε ότι το αγγλικό και το ολλανδικό μοντέλο παρουσιάζουν παρόμοια συμπεριφορά επεξεργασίας, αλλά κάτω από την κουκούλα υπάρχουν συνεπείς διαφορές μεταξύ των μοντέλων προσοχής και μη προσοχής.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>El campo de la IA explicable ha visto recientemente una explosión en el número de métodos de explicación para redes neuronales profundas altamente no lineales. Todavía no se ha explorado hasta qué punto tales métodos, que a menudo se proponen y prueban en el dominio de la visión artificial, son apropiados para abordar los desafíos de la explicabilidad en la PNL. En este trabajo, consideramos la descomposición contextual (CD), un método de atribución de entidades de entrada basado en Shapley que ha demostrado funcionar bien para modelos de PNL recurrentes, y probamos hasta qué punto es útil para modelos que contienen operaciones de atención. Con este fin, ampliamos el CD para cubrir las operaciones necesarias para los modelos basados en la atención. Luego comparamos cómo las relaciones sujeto-verbo a larga distancia son procesadas por modelos con y sin atención, considerando una serie de estructuras sintácticas diferentes en dos idiomas diferentes: inglés y holandés. Nuestros experimentos confirman que la EC también se puede aplicar con éxito para modelos basados en la atención, proporcionando un método de atribución alternativo basado en Shapley para las redes neuronales modernas. En particular, al usar CD, mostramos que los modelos inglés y holandés demuestran un comportamiento de procesamiento similar, pero que bajo el capó hay diferencias consistentes entre nuestros modelos de atención y falta de atención.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Selgitatava tehisintellekti valdkonnas on hiljuti nähtud plahvatust väga mittelineaarsete sügavate närvivõrkude seletusmeetodite arvus. Kui palju sellised meetodid, mida sageli pakutakse välja ja testitakse arvutinägemise valdkonnas, sobivad uue õppekava seletatavusprobleemide lahendamiseks, on veel suhteliselt uurimata. Käesolevas töös käsitleme kontekstuaalset dekompositsiooni (CD) - Shapley-põhist sisendifunktsioonide omistamise meetodit, mis on näidanud toimivat korduvate NLP mudelite puhul - ja testime, mil määral on see kasulik tähelepanu operatsioone sisaldavate mudelite puhul. Selleks laiendame CD-d tähelepanupõhiste mudelite jaoks vajalikele toimingutele. Seejärel võrdleme, kuidas teema-verbi suhteid käsitletakse tähelepanuta ja tähelepanuta mudelites, arvestades erinevaid süntaktilisi struktuure kahes erinevas keeles: inglise ja hollandi keeles. Meie eksperimendid kinnitavad, et CD-d saab edukalt rakendada ka tähelepanupõhistele mudelitele, pakkudes alternatiivset Shapley-põhist atributsioonimeetodit kaasaegsetele närvivõrkudele. CD kasutades näitame eelkõige, et Inglise ja Hollandi mudelid näitavad sarnast töötlemiskäitumist, kuid kapoti all esineb järjepidevaid erinevusi meie tähelepanu- ja tähelepanuta jätmise mudelite vahel.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>زمینه های AI قابل توضیح اخیراً در تعداد روش توضیح برای شبکه های عصبی عمیق غیر خطی یک انفجار دیده است. مقداری که این روش‌ها - که اغلب پیشنهاد می‌شوند و در حوزه دید کامپیوتر آزمایش می‌شوند - مناسب است برای حل چالش‌های توضیح‌پذیری در NLP هنوز نسبتا بی‌توضیح است. در این کار، ما نظر می‌گیریم که یک روش ویژه‌های ورودی بنیاد شاپلی برای مدل‌های NLP بازگشت خوب کار می‌کند، و ما به اندازه‌ای که برای مدل‌های توجه دارند مفید است، آزمایش می‌کنیم. برای این قسمت، ما سی دی را گسترش می‌دهیم تا عملیات نیاز برای مدل‌های بنیاد توجه را پوشش دهیم. سپس ما مقایسه می‌کنیم که چقدر رابطه‌های موضوع و کلمه‌های فاصله با مدل‌ها و بدون توجه، با توجه به تعداد ساختارهای متفاوتی در دو زبان متفاوت: انگلیسی و هلندی پرداخته می‌شوند. آزمایشات ما تایید می‌کند که سی‌دی می‌تواند موفق به موفقیت برای مدل‌های بنیاد توجه استفاده می‌شود، و به عنوان یک روش تهیه‌کننده‌ای بر اساس شیپلی برای شبکه‌های عصبی مدرن استفاده می‌کند. مخصوصا، از استفاده از سی دی، نشان می دهیم که مدل انگلیسی و هلندی رفتار پردازش مشابه را نشان می دهند، ولی در زیر محل تفاوت مشابه بین مدل توجه و توجه غیر توجه ما وجود دارد.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Selittävän tekoälyn kentällä on viime aikoina ollut räjähdysmäinen määrä selittämismenetelmiä erittäin epälineaarisille syvähermoverkoille. Sitä, missä määrin tällaiset menetelmät, joita usein ehdotetaan ja testataan tietokonenäön alalla, soveltuvat vastaamaan selittettävyyshaasteisiin NLP:ssä, on vielä suhteellisen tutkimatonta. Tässä työssä tarkastellaan kontekstual decompositionia (CD) - Shapley-pohjaista syöttöominaisuuden määritysmenetelmää, jonka on osoitettu toimivan hyvin toistuvissa NLP-malleissa - ja testaamme, missä määrin se on hyödyllinen huomiotoimintoja sisältävissä malleissa. Tätä varten laajennamme CD:n kattamaan huomiopohjaisten mallien edellyttämät toiminnot. Tämän jälkeen vertaamme kuinka pitkän matkan subjekti-verbi-suhteita käsitellään malleissa huomiotta ja huomiotta ottaen huomioon useita erilaisia syntaktisia rakenteita kahdella eri kielellä: englanti ja hollanti. Kokeet vahvistavat, että CD:tä voidaan soveltaa menestyksekkäästi myös huomiopohjaisiin malleihin, tarjoten vaihtoehtoisen Shapley-pohjaisen määritysmenetelmän nykyaikaisille hermoverkoille. Erityisesti CD:n avulla osoitamme, että englanninkieliset ja hollantilaiset mallit osoittavat samanlaista prosessointikäyttäytymistä, mutta että konepellin alla on johdonmukaisia eroja huomio- ja huomiokyvyttömyysmallien välillä.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Le domaine de l'IA explicable a récemment connu une explosion du nombre de méthodes d'explication pour les réseaux de neurones profonds hautement non linéaires. La mesure dans laquelle de telles méthodes — qui sont souvent proposées et testées dans le domaine de la vision par ordinateur — sont appropriées pour relever les défis d'explicabilité de la PNL est encore relativement inexplorée. Dans ce travail, nous examinons la décomposition contextuelle (CD) — une méthode d'attribution d'entités en entrée basée sur Shapley qui fonctionne bien pour les modèles de PNL récurrents — et nous testons dans quelle mesure elle est utile pour les modèles qui contiennent des opérations d'attention. À cette fin, nous étendons le CD pour couvrir les opérations nécessaires aux modèles axés sur l'attention. Nous comparons ensuite la façon dont les relations sujet-verbe à distance sont traitées par des modèles avec et sans attention, en tenant compte d'un certain nombre de structures syntaxiques différentes dans deux langues différentes : l'anglais et le néerlandais. Nos expériences confirment que la CD peut également être appliquée avec succès aux modèles basés sur l'attention, fournissant une méthode d'attribution alternative basée sur Shapley pour les réseaux neuronaux modernes. En particulier, à l'aide de CD, nous montrons que les modèles anglais et néerlandais présentent un comportement de traitement similaire, mais que sous le capot, il existe des différences constantes entre nos modèles d'attention et de non-attention.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tháinig méadú le déanaí i réimse an AI inmhínithe ar líon na modhanna mínithe do líonraí néaracha doimhne neamhlíneacha. Níl iniúchadh déanta fós ar a mhéid a bhíonn modhanna den sórt sin - a mholtar agus a thástáiltear go minic i réimse fhís an ríomhaire - oiriúnach chun aghaidh a thabhairt ar na dúshláin inmhínithe in NLP. San obair seo, breithnímid ar Dhianscaoileadh Comhthéacsúil (CD) – modh sanntar gné ionchuir atá bunaithe ar Shapley a léiríodh go n-oibríonn sé go maith do mhúnlaí athfhillteacha NLP – agus déanaimid tástáil ar a mhéid atá sé úsáideach do mhúnlaí ina bhfuil oibríochtaí aird. Chuige sin, leathnaímid CD chun na hoibríochtaí atá riachtanach do mhúnlaí aird-bhunaithe a chlúdach. Déanaimid comparáid ansin maidir le cé chomh fada agus a phróiseálann samhlacha le haird agus gan aird ar ghaolmhaireachtaí ábhar-briathar, ag smaoineamh ar roinnt struchtúir chomhréire difriúla in dhá theanga dhifriúla: Béarla agus Ollainnis. Deimhníonn ár dturgnaimh gur féidir CD a chur i bhfeidhm go rathúil ar mhúnlaí aird-bhunaithe freisin, ag soláthar modh leithdháilte eile atá bunaithe ar Shapley do líonraí neural nua-aimseartha. Go háirithe, ag baint úsáide as CD, léirímid go léiríonn samhlacha Béarla agus Ollainnis iompar próiseála cosúil leis, ach go bhfuil difríochtaí comhsheasmhacha idir ár múnlaí aird agus neamhaird faoin gcochall.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bayyar AI da ake fassarawa a yanzu ya gane wata firgita cikin ƙidãyar shiryoyin ayuka na bayyana wa zanayen masu tsari na masu tsarin taruwar neural na sarki. @ action: button A cikin wannan aikin, munã bincike da tsarin Dekkomposition (CDs) - wata shirin cikin shirin ayuka na Shafi wanda aka nuna shi don ya aikata aiki mai kyau wa misãlai na NLP wanda aka sake koma-bayan - kuma muna jarraba gwargwadon da yake ya amfani da shi ga misãlai wanda ke ƙunsa da aikin muhimmanci. Ga wannan, Munã faɗa cd dõmin mu rufe aikin da za'a buƙata zuwa misali masu bincike. Sa'an nan kuma, muna samfani da gwargwadon zumunta masu nau'i da mazaɓa masu nau'i da misãlai, kuma bã da saurãre, kuma masu bincike da wasu bakwai masu taratiki cikin lugha biyu dabam-dabam: Ingiriya da Dukkan. Kayan jarrabõnmu sun gaskata cewa za'a saka wa CDs a sami da misãlai masu bincike, da kuma a sami wata hanyor cire-na-Shappy-da-yanzu zuwa zanen neural na yanzu. Kayyai, da amfani da CDs, muna nuna misãlai Ingiriya da Dutsu sun nuna aikin aiki mai kama da shi, kuma amma, a ƙarƙashin bangonmu akwai diffaniki mai daidai a tsakanin aikin mu da misãlai masu bincike.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>השדה של AI הסביר ראה לאחרונה פיצוץ במספר שיטות הסבר לרשתות עצביות עמוקות מאוד לא לינריות. המידה שבה שיטות כאלה - שלעתים קרובות מציעות ומבחנות בתחום חזון המחשב - מתאימות להתמודד עם אתגרי ההסבירות ב-NLP עדיין לא נחקרת יחסית. בעבודה הזו, אנו שוקלים את התפרצות הקונקסטית (CD) - שיטת שימוש של תכונות הכניסה מבוססת על Shapley שהוכיח לעבוד היטב עבור דוגמנים NLP חוזרים - ואנחנו בודקים את המידה עד כמה זה שימושי עבור דוגמנים שמכילים פעולות תשומת לב. למטרה זו, אנו ממשיכים את הדיסק כדי לכסות את המבצעים הנדרשים לדוגמנים מבוססים על תשומת לב. ואז נשווה כמה מערכות יחסים מרחק ארוך נושא-אלברים מתעסקות על ידי דוגמנים עם ולא תשומת לב, בהתחשב במספר מבנים סינטקטיים שונים בשתי שפות שונות: אנגלית ולנדית. הניסויים שלנו מאשרים שדיסק ניתן להשתמש בהצלחה גם במודלים מבוססים על תשומת לב, ומספקים שיטה אלטרנטיבית של שיפלי מבוססת שיפוט לרשתות עצביות מודרניות. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>समझाने योग्य एआई के क्षेत्र ने हाल ही में अत्यधिक गैर-रैखिक गहरे तंत्रिका नेटवर्क के लिए स्पष्टीकरण विधियों की संख्या में एक विस्फोट देखा है। किस हद तक इस तरह के तरीके - जो अक्सर प्रस्तावित और कंप्यूटर दृष्टि के डोमेन में परीक्षण किए जाते हैं - एनएलपी में व्याख्यात्मकता चुनौतियों को संबोधित करने के लिए उपयुक्त हैं, अभी तक अपेक्षाकृत अनपेक्षित है। इस काम में, हम प्रासंगिक अपघटन (सीडी) पर विचार करते हैं - एक शैप्ले-आधारित इनपुट फीचर एट्रिब्यूशन विधि जिसे आवर्तक एनएलपी मॉडल के लिए अच्छी तरह से काम करने के लिए दिखाया गया है - और हम उस हद तक परीक्षण करते हैं जिसमें यह उन मॉडलों के लिए उपयोगी है जिनमें ध्यान संचालन शामिल हैं। इस अंत तक, हम ध्यान-आधारित मॉडल के लिए आवश्यक संचालन को कवर करने के लिए सीडी का विस्तार करते हैं। फिर हम तुलना करते हैं कि दो अलग-अलग भाषाओं में कई अलग-अलग वाक्यात्मक संरचनाओं पर विचार करते हुए, ध्यान के साथ और बिना मॉडल द्वारा लंबी दूरी के विषय-क्रिया संबंधों को कैसे संसाधित किया जाता है: अंग्रेजी और डच। हमारे प्रयोगों की पुष्टि है कि सीडी सफलतापूर्वक ध्यान आधारित मॉडल के लिए लागू किया जा सकता है के रूप में अच्छी तरह से, आधुनिक तंत्रिका नेटवर्क के लिए एक वैकल्पिक Shapley-आधारित एट्रिब्यूशन विधि प्रदान करते हैं. विशेष रूप से, सीडी का उपयोग करके, हम दिखाते हैं कि अंग्रेजी और डच मॉडल समान प्रसंस्करण व्यवहार का प्रदर्शन करते हैं, लेकिन हुड के तहत हमारे ध्यान और गैर-ध्यान मॉडल के बीच लगातार अंतर हैं।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Polje objašnjavajućih AI nedavno je vidjelo eksploziju u broju metoda objašnjavanja za visoko neolinearne duboke neuralne mreže. Koliko su takve metode - koje su često predložene i testirane u domenu računalnog vizije - odgovarajuće za rješavanje izazova objašnjivosti u NLP-u, još je relativno neobjašnjivo. U ovom poslu razmišljamo o kontekstualnoj dekompoziciji (CD) - metodi privlačenja ulaznih karakteristika na osnovi Shapley a koja je pokazala kako dobro funkcionira za rekonstruirane modele NLP-a - i testiramo mjeru u kojoj je korisno za modele koji sadrže operacije pažnje. Za taj cilj, proširimo CD da pokrijemo operacije potrebne za modele na temelju pažnje. Onda uspoređujemo koliko duge veze s tema-verbom obrađuju modeli s i bez pažnje, s obzirom na broj različitih sintaktičkih struktura na dva različita jezika: engleski i holandski. Naši eksperimenti potvrđuju da se CD može uspješno primjenjivati i za modele na temelju pažnje, pružajući alternativnu metodu privlačenja na temelju Shapley-a za moderne neuralne mreže. Posebno, koristeći CD, pokazujemo da engleski i nizozemski modeli pokazuju slično ponašanje procesa, ali da pod kapuljom postoje konsistentne razlike između naših pažnje i modela neopažnje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A megmagyarázható AI területe a közelmúltban robbanást tapasztalt a rendkívül nemlineáris mélyneurális hálózatok magyarázatainak számában. Viszonylag feltáratlan, hogy a számítógépes látás területén gyakran javasolt és tesztelt ilyen módszerek milyen mértékben alkalmasak az NLP megmagyarázhatósági kihívásainak kezelésére. Ebben a munkában a Contextual Decomposition (CD) egy Shapley-alapú bemeneti funkció attribúciós módszert veszünk figyelembe, amely jól működik a visszatérő NLP modelleknél, és teszteljük, milyen mértékben hasznos a figyelemműveleteket tartalmazó modelleknél. Ennek érdekében a CD-t kiterjesztjük a figyelem alapú modellekhez szükséges műveletekre. Ezután összehasonlítjuk, hogy a modellek milyen távolsági alany-ige kapcsolatokat dolgoznak fel figyelemmel és anélkül, figyelembe véve számos különböző szintaktikus struktúrát két különböző nyelven: angol és holland. Kísérleteink megerősítik, hogy a CD sikeresen alkalmazható figyelem-alapú modellekhez is, alternatív Shapley-alapú attribúciós módszert biztosítva a modern neurális hálózatok számára. Különösen CD használatával mutatjuk meg, hogy az angol és holland modellek hasonló feldolgozási viselkedést mutatnak, de hogy a motorháztető alatt következetes különbségek vannak figyelmünk és figyelmünk nélküli modelleink között.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. Այսպիսի մեթոդները, որոնք հաճախ առաջարկում են և փորձարկում են համակարգչային տեսողության ոլորտում, համապատասխանում են ՆԼՊ-ի բացատրելիության խնդիրներին լուծելու համար, դեռևս համեմատաբար չեն ուսումնասիրել: Այս աշխատանքի ընթացքում մենք համարում ենք Կոնտեքստալ Դեկոմպոզիցիայի (CD) մեթոդ, որը հիմնված է Շեփլեյի ներմուծի հատկանիշների հատկանիշների հատկանիշների մեթոդ է, որը ցույց է տալիս, որ լավ է աշխատում կրկնվող ՆԼՊ մոդելների համար, և մենք ստուգում ենք To this end, we extend CD to cover the operations necessary for attention-based models. Այնուհետև մենք համեմատում ենք, թե որքան երկար հեռավորության թեմա-բայ հարաբերությունները վերլուծում են մոդելներով ուշադրության հետ և առանց, հաշվի առնելով տարբեր սինտակտիկ կառուցվածքներ երկու տարբեր լեզուներում՝ անգլերեն և հոլանդ Մեր փորձարկումները հաստատում են, որ CD-ը կարող է հաջողությամբ կիրառվել նաև ուշադրության հիմնված մոդելների համար, տրամադրելով այլընտրանքային ShaPLY-ի հիմնված հատկագրման մեթոդ ժամանակակից նեյրոնական ցանցերի համար: Հատկապես, օգտագործելով CD-ը, մենք ցույց ենք տալիս, որ անգլերենի և հոլանդացի մոդելները ցույց են տալիս նման վերամշակման վարքագիծ, սակայն գլխարկի տակ կա համեմատական տարբերություններ մեր ուշադրության և ոչ ուշադրության մոդելների միջ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Lapangan AI yang dapat dijelaskan baru-baru ini melihat ledakan dalam jumlah metode penjelasan untuk jaringan saraf dalam yang sangat tidak linear. The extent to which such methods - that are often proposed and tested in the domain of computer vision - are appropriate to address the explainability challenges in NLP is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) - a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models - and we test the extent to which it is useful for models that contain attention operations. Untuk tujuan ini, kami memperluas CD untuk menutupi operasi yang diperlukan untuk model berdasarkan perhatian. Lalu kita membandingkan bagaimana jarak jauh hubungan subjek-verb diproses oleh model dengan dan tanpa perhatian, mempertimbangkan sejumlah struktur sintaksi yang berbeda dalam dua bahasa berbeda: Inggris dan Belanda. Eksperimen kami mengkonfirmasi bahwa CD dapat berhasil diterapkan untuk model berdasarkan perhatian juga, menyediakan metode atribut alternatif berdasarkan Shapley untuk jaringan saraf modern. Terutama, menggunakan CD, kami menunjukkan bahwa model Inggris dan Belanda menunjukkan perilaku proses yang sama, tetapi di bawah kapus ada perbedaan konsisten antara perhatian kita dan model yang tidak perhatian.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il campo dell'IA spiegabile ha recentemente visto un'esplosione nel numero di metodi di spiegazione per reti neurali profonde altamente non lineari. La misura in cui tali metodi - spesso proposti e testati nel campo della visione informatica - siano appropriati per affrontare le sfide di spiegabilità della PNL è ancora relativamente inesplorata. In questo lavoro consideriamo Contextual Decomposition (CD) - un metodo di attribuzione delle funzionalità di input basato su Shapley che ha dimostrato di funzionare bene per i modelli NLP ricorrenti - e testiamo fino a che punto è utile per i modelli che contengono operazioni di attenzione. A tal fine, estendiamo il CD per coprire le operazioni necessarie per modelli basati sull'attenzione. Confrontiamo quindi le relazioni a distanza soggetto-verbo elaborate da modelli con e senza attenzione, considerando una serie di diverse strutture sintattiche in due lingue diverse: inglese e olandese. I nostri esperimenti confermano che il CD può essere applicato con successo anche per modelli basati sull'attenzione, fornendo un metodo alternativo di attribuzione basato su Shapley per le moderne reti neurali. In particolare, utilizzando il CD, mostriamo che i modelli inglesi e olandesi dimostrano un comportamento di lavorazione simile, ma che sotto il cofano ci sono differenze consistenti tra i nostri modelli di attenzione e di non attenzione.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>説明可能なAIの分野では、最近、非常に非線形な深層ニューラルネットワークの説明方法の数が爆発的に増加しています。 コンピュータビジョンの領域でしばしば提案され、テストされるこのような方法が、NLPにおける説明可能性の課題に対処するのにどの程度適切であるかは、まだ比較的未解明である。 この研究では、Shapleyベースの入力機能帰属法であるContextual Decomposition (CD)を検討し、再発NLPモデルにうまく機能することが示されており、注意演算を含むモデルにとってどの程度有用かを検証します。 そのために、注目モデルに必要な操作をカバーするようにCDを拡張します。 次に、英語とオランダ語の2つの異なる言語のさまざまな構文構造を考慮して、注意を払っているかどうかにかかわらず、モデルによってどのくらいの距離の主語と動詞の関係が処理されるかを比較します。 私たちの実験は、CDが注意に基づくモデルにもうまく適用できることを確認し、現代のニューラルネットワークに代わるShapleyベースの帰属方法を提供します。 特に、CDを使用すると、英語とオランダ語のモデルが同様の処理挙動を示していることがわかりますが、フードの下では、私たちの注目モデルと非注目モデルの間に一貫した違いがあることがわかります。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>deep Kowe mesthi apik karo hal-hal sing sampeyan karo hal-hal bisa nguasai sak ujian ing domain komputer In this job, we count contextual Deompposition Nambah iki, kita ngubah cd kanggo ngilangno operasi layanan kanggo model sing isingan atens Awak dhéwé ngerasakno piye pangan langgar sampeyan Subject-verb resmi sing nyeasakno karo model karo ngono kuwi kesempatan, nggunakake sistem sing sampeyan sampeyan sampeyan karo langgar sampeyan: Inggris karo Pak holandh. Awak dhéwé éntuk dhéwé ngerasahi Cdromek iso nguasai nggawe modèlan sing wis nguasai nggawe barang nggawe tarjamahan, supoyo akeh sistem anyar sumungot na shapely Genjer-Genjer, iso nggambar cd, kita ngomong nik model ingkang karo Pak holes kuwi bisa diuntingi podho operasi sing mengko, nangut nguasai karo nguasai kapan karo perusahaan langgar sampeyan ingkang sampeyan ingkang sampeyan ingkang sampeyan ingkang sampeyan ingkang sampeyan pakan.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>აღწერილი AI-ის პანელი აღმოჩნდა ექსპლეციას, რამდენიმე განახსნავის მეტოვების რაოდენობაში, ძალიან ბოლონური ნეიროლური ქსელებისთვის. რომელიც ეს მეტი - რომელიც ძალიან მოწყვეტილია და ტესტირებულია კომპიუტერის ხედის დიომინში - უფრო საჭიროა NLP-ში განახსნა განახსნა განსაზღვრებელობის განსაზღვრება - არაფერად ამ სამუშაოში, ჩვენ ვფიქრობთ კონტექსტური განკომპოზაცია (CD) - შაფლის დაბათი შეტყობინებული შეტყობინებების მეტი, რომელიც გამოჩვენებულია, რომ მუშაობს რეკონტური NLP მოდელებისთვის - და ჩვენ ვცადოთ ამ მიზეზით, ჩვენ CD-ს გავაზრუნეთ, რომ აღმოჩენოთ მოდელებისთვის მონაცემებისთვის მონაცემები. შემდეგ ჩვენ განსხვავებული სინტექტიკური სტრუქტურების რამდენიმე განსხვავებული ენები: ანგლისური და დონდელისური. ჩვენი ექსპერიმენტები დარწმუნდება, რომ CD შეიძლება მსოფლიოდ დააყენებული მოდელებისთვის დააყენება, რომლებიც აღმოჩენა ალტენტრუმენტური შაპლის დაბათი ატრი განსაკუთრებით, CD გამოყენებული, ჩვენ ჩვენ აჩვენებთ, რომ ანგლისური და დონდენური მოდელები გამოყენებენ განსხვავებული პროცესის ქცევა, მაგრამ ჩვენი მოდელების განსხვავება არსებობს ჩ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Түсініктіретін AI өрісі жаңа-ақ түсіндіретін түсіндіру әдістерін сызық емес түсіндіретін невралдық желілердің саны көрді. Компьютердің көрінісінің доменінде тексерілген әдістер NLP- деген түсініктердің мәселелерін шешу үшін әлі салыстырмалы түсініктемесіз. Бұл жұмыс ішінде Contextual Decomposition (CD) - Shapley- негіздеген келтіру мүмкіндіктерінің қайталанатын NLP үлгілерінде жақсы жұмыс істеу әдісін қалаймыз. Біз қайталанатын NLP үлгілерінде қайталанатын үлгілер үшін пайдалы шект Бұл үшін, біз CD-ді қажетті үлгілер үшін қолдану үшін қолдануға арналған әрекеттерді кеңейту үшін кеңейту. Содан кейін, қанша ұзындық тақырыпты верб қатынастары үлгілерімен, қанша ұзындық құрылымдарды салыстырып, екі түрлі тілде бірнеше синтактикалық құрылымдарды: ағылшын және Нидерландша Біздің тәжірибелеріміз CD-нің қазіргі невралдық желілердің альтернативті үлгілеріне сәтті қолдануға болады. Ағылшын және Нидерландық моделдері ұқсас процесстердің қасиеттерін көрсетеді, бірақ көпшілігінің астында біздің тәртіпсіздіктеріміз мен тәртіпсіздік үлгілеріміздің арасындағы қасиеттері</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>최근 인공지능을 해석할 수 있는 분야에서 고도의 비선형 심층신경망의 해석 방법이 급증하고 있다.이러한 방법은 컴퓨터 시각 분야에서 자주 제기되고 테스트되는데 그것이 어느 정도에 자연 언어 처리에서의 해석 가능한 도전을 해결하기에 적합한지 아직 분명하지 않다.이 작업에서 우리는 상하문 분해(CD)-Shapley 기반의 입력 특징 귀인 방법을 고려하여 반복적으로 나타나는 NLP 모델에 효과가 있음을 증명하고 주의 조작을 포함하는 모델에 대한 유용도를 테스트했다.이렇게 하려면 CD를 주의력 기반 모델에 필요한 작업으로 확장합니다.그리고 영어와 네덜란드어 두 가지 서로 다른 언어 중의 많은 서로 다른 문법 구조를 고려하여 우리는 주의와 무주의 모델이 원거리 주술 관계를 처리하는 정도를 비교했다.우리의 실험은 CD도 주의 기반 모델에 성공적으로 응용되고 현대 신경 네트워크에 또 다른 Shapley 기반 귀인 방법을 제공할 수 있음을 증명했다.특히 CD를 사용해 영어와 네덜란드어 모델이 비슷한 가공 행위를 보였다는 것을 증명했지만 우리의 주의 모델과 비주의 모델 사이에 일치된 차이가 있었다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. Šie metodai, kurie dažnai siūlomi ir bandomi kompiuterinės vizijos srityje, yra tinkami NLP paaiškinimo problemoms spręsti, vis dar nėra išnagrinėti. Šiame darbe manome, kad kontekstinė dekompozicija (CD) – Shapley pagrindu pagrįstas įėjimo savybių priskyrimo metodas, įrodytas gerai veikiantis pakartotiniams NLP modeliams – ir bandome, kiek tai naudinga modeliams, kuriuose yra dėmesio operacijų. Šiuo tikslu išplečiame CD, kad apimtų veiksmus, reikalingus dėmesiu pagrįstiems modeliams. Tuomet palyginame, kaip ilgo atstumo objekto ir žodžio santykius tvarko modeliai su dėmesiu ir be jo, atsižvelgiant į įvairias sintaktines struktūras dviem skirtingomis kalbomis: anglų ir olandų kalbomis. Mūsų eksperimentai patvirtina, kad CD galima sėkmingai taikyti ir dėmesiu pagrįstiems modeliams, suteikiant alternatyvų Shapley pagrįstą priskyrimo metodą šiuolaikiniams nerviniams tinklams. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Пологот на објаснливата ВИ неодамна виде експлозија во бројот на објаснувачки методи за високо нелинеарни длабоки неурални мрежи. До каде ваквите методи - кои честопати се предложени и тестирани во доменот на компјутерската визија - се соодветни за решавање на предизвиците за објаснливост во НЛП се сé уште релативно неиспитани. Во оваа работа, ја сметаме Контекстуалната декомпозиција (ЦД) - метод на припишување на внатрешни функции базиран на Shapley кој се покажа дека функционира добро за рецидентни NLP модели - и го тестираме степенот во кој е корисен за моделите кои содржат операции на внимание. За ова, го прошируваме ЦД за да ги покриеме операциите потребни за моделите базирани на внимание. Потоа споредуваме колку долга оддалеченост врските со субјектите се процесирани од модели со и без внимание, земајќи во предвид број различни синтактички структури на два различни јазици: англиски и холандски. Нашите експерименти потврдуваат дека ЦД може успешно да се примени и за модели базирани на внимание, обезбедувајќи алтернативен метод на припишување базиран на Шепли за модерните нервни мрежи. Посебно, користејќи ЦД, покажуваме дека англиските и холандските модели демонстрираат слично однесување на процесорот, но дека под капутата постојат константни разлики помеѓу нашето внимание и моделите без внимание.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. കമ്പ്യൂട്ടര്‍ ദര്‍ശനത്തിന്റെ ഡോമെയിനില്‍ പ്രായശ്ചിത്തമായി പരീക്ഷിക്കപ്പെടുകയും ചെയ്യുന്ന ഇങ്ങനെ രീതികള്‍- എംഎല്‍പിയിലെ വിശദീകരണവ ഈ പ്രവര്‍ത്തനത്തില്‍, നമ്മള്‍ കോണ്‍ട്ടെക്സ്റ്റെക്സ്റ്റല്‍ ഡികോമ്പോസിഷന്‍ (സിഡി) - ഒരു ഷാപ്ലി അടിസ്ഥാനമായിട്ടുള്ള ഇന്‍പുട്ടിന്റെ വിശേഷപ്രദര്‍ശിപ്പിക്കുന് ഈ അവസാനത്തിനു വേണ്ടി നമ്മള്‍ സിഡിയിലേക്ക് വികസിപ്പിക്കുന്നു. ശ്രദ്ധിക്കുന്ന മോഡലുകള്‍ക്ക് പിന്നീട് നമ്മള്‍ എത്ര ദൂരം വിഷയങ്ങളുടെ ബന്ധങ്ങള്‍ പ്രവര്‍ത്തിപ്പിക്കുന്നുവെന്ന് തുല്യമാക്കുന്നു. മോഡലുകള്‍ ശ്രദ്ധിക്കാതെയാണ് നമ്മുടെ പരീക്ഷണങ്ങള്‍ സിഡിയില്‍ വിജയകരമായി ശ്രദ്ധിക്കുന്ന മോഡലുകള്‍ക്കും വേണ്ടി പ്രയോഗിക്കാന്‍ സാധിക്കുന്നു എന്ന് ഉറപ്പ് വരുത് പ്രത്യേകിച്ച്, സിഡിയില്‍ ഉപയോഗിച്ച്, ഇംഗ്ലീഷും ഡച്ചും മോഡലുകളും ഒരേ പ്രക്രിയ നടപടിക്രമങ്ങള്‍ കാണിക്കുന്നു, പക്ഷെ ഹൂട്ടിന്‍റെ കീഴി</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Тайлбарлах боломжтой AI-ын талбар саяхан мэдрэлийн гүн гүнзгий мэдрэлийн сүлжээнд маш олон тодорхойлолтын арга замыг харсан. Компьютерийн үзэл дүрэм дээр шийдвэрлэгдэж, шалгаж үзэх ийм арга зам нь NLP-д тайлбарлах боломжгүй сорилтуудыг зохиохын тулд хэрэгтэй. Энэ ажил дээр бид "Contextual Decomposition" (CD) - Шапли-д суурилсан орнуудын харилцааны арга загвар нь дахин дахин ажиллаж байгаа NLP загваруудын хувьд сайн ажиллах боломжтой болсон юм. Мөн бид анхаарлын ажиллагааны загваруудын хувьд хэрхэн хэрэг Энэ төгсгөлд бид CD-г анхаарлын үндсэн загваруудын тулд хэрэгтэй үйл ажиллагааг дэлгэрүүлнэ. Дараа нь бид хоёр өөр хэл дээр олон синтактик бүтээгдэхүүнийг харьцуулж, анхаарлын загвараар хэр урт зай холбоотой вэ гэдгийг харьцуулдаг. Бидний туршилтууд CD-г анхаарлын үндсэн загваруудын тулд амжилттай ашиглаж чадна гэдгийг батладаг. Шапли-н үндсэн өөр арга загваруудыг орчин үеийн мэдрэлийн сүлжээнд хангах арга загвар өгдөг Ялангуяа, CD-г ашиглан, Англи болон Далландын загварууд төстэй процесс үйл ажиллагааг харуулж байна. Гэхдээ бидний анхаарал болон анхаарлын анхаарлын биш загваруудын хооронд төстэй ялгаа байдаг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Medan AI yang boleh dijelaskan baru-baru ini telah melihat letupan dalam bilangan kaedah penjelasan untuk rangkaian saraf dalam yang tidak linear tinggi. Sejauh mana kaedah-kaedah seperti itu - yang sering diusulkan dan diuji dalam domain penglihatan komputer - sesuai untuk mengatasi cabaran penjelasan dalam NLP masih relatif tidak dikenalpasti. In this work, we consider Contextual Decomposition (CD) - a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models - and we test the extent to which it is useful for models that contain attention operations. Untuk tujuan ini, kami memperluas CD untuk menutupi operasi yang diperlukan untuk model berdasarkan perhatian. Kemudian kita membandingkan bagaimana jarak jauh hubungan subjek-verb diproses oleh model dengan dan tanpa perhatian, mempertimbangkan sejumlah struktur sintaktik berbeza dalam dua bahasa berbeza: Bahasa Inggeris dan Belanda. Eksperimen kami mengesahkan bahawa CD boleh berjaya dilaksanakan untuk model berdasarkan perhatian juga, menyediakan kaedah atribut berdasarkan Shapley alternatif untuk rangkaian saraf modern. Terutama, menggunakan CD, kita menunjukkan bahawa model Inggeris dan Belanda menunjukkan perilaku pemprosesan yang sama, tetapi di bawah kapus terdapat perbezaan konsisten antara perhatian kita dan model yang tidak perhatian.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il-qasam tal-AI spjegabbli dan l-aħħar wera splużjoni fin-numru ta’ metodi ta’ spjegazzjoni għal netwerks newrali profondi mhux lineari ħafna. Il-punt sa fejn metodi bħal dawn - li spiss jiġu proposti u ttestjati fil-qasam tal-viżjoni tal-kompjuter - huma xierqa biex jindirizzaw l-isfidi ta’ spjegabbiltà fil-NLP għadu relattivament mhux esplorat. In this work, we consider Contextual Decomposition (CD) - a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models - and we test the extent to which it is useful for models that contain attention operations. Għal dan il-għan, aħna jestendu CD biex ikopri l-operazzjonijiet meħtieġa għal mudelli bbażati fuq l-attenzjoni. Imbagħad nipparagunaw kemm ir-relazzjonijiet bejn is-suġġett u l-verb fuq distanza twila jiġu pproċessati minn mudelli b’attenzjoni u mingħajr attenzjoni, filwaqt li nikkunsidraw għadd ta’ strutturi sintattiċi differenti f’żewġ lingwi differenti: l-Ingliż u l-Olandiż. L-esperimenti tagħna jikkonfermaw li CD jista’ jiġi applikat b’suċċess ukoll għal mudelli bbażati fuq l-attenzjoni, billi jipprovdi metodu alternattiv ta’ attribuzzjoni bbażat fuq Shapley għal netwerks newrali moderni. B’mod partikolari, bl-użu tas-CD, nuru li l-mudelli Ingliżi u Olandiżi juru mġiba ta’ pproċessar simili, iżda li taħt il-kappa hemm differenzi konsistenti bejn l-attenzjoni tagħna u mudelli mhux ta’ attenzjoni.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Het gebied van verklarbare AI heeft onlangs een explosie gezien in het aantal verklaringsmethoden voor zeer niet-lineaire diepe neurale netwerken. In hoeverre dergelijke methoden, die vaak worden voorgesteld en getest op het gebied van computervisie, geschikt zijn om de verklarbaarheidsuitdagingen in NLP aan te pakken, is nog relatief onontdekt. In dit werk beschouwen we Contextual Decomposition (CD) als een Shapley-gebaseerde input feature attributie methode waarvan is aangetoond dat het goed werkt voor terugkerende NLP modellen en we testen in hoeverre het nuttig is voor modellen die aandachtsoperaties bevatten. Daartoe breiden we CD uit tot de bewerkingen die nodig zijn voor aandachtsmodellen. Vervolgens vergelijken we hoe lange afstand subject-werkwoord relaties verwerkt worden door modellen met en zonder aandacht, rekening houdend met een aantal verschillende syntactische structuren in twee verschillende talen: Engels en Nederlands. Onze experimenten bevestigen dat CD ook succesvol kan worden toegepast op attentie-based modellen, wat een alternatieve Shapley-gebaseerde attributiemethode biedt voor moderne neurale netwerken. Met name met behulp van CD laten we zien dat de Engelse en Nederlandse modellen hetzelfde verwerkingsgedrag vertonen, maar dat er onder de motorkap consistente verschillen zijn tussen onze aandacht- en niet-aandacht modellen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Feltet for forklarbare AI har nyleg sett eit eksplosjon i antall forklaringsmetodar for stor ikkje-lineær dyp neuralnettverk. Kor mykje slike metodar - som ofte vert foreslått og testa i domenet for datavising - er tilgjengeleg for å handtera utfordringane for forklaringar i NLP er enno relativt uventa. I denne arbeida ser vi på kontekst- dekomposisjon (CD) - ein funksjonsattribusjonsmetode for inndata basert på Shapley som er vist å fungera bra for gjentaande NLP- modeller - og vi testar kor mykje det er nyttig for modeller som inneheld oppmerksomhetar. I denne slutten utvidar vi CD for å dekka operasjonane som treng for oppmerksbaserte modeller. Vi sammenliknar så kor lang avstandsverktøy av temaverbverb-relasjonar vert handsama av modeller med og utan oppmerksomhet, ved å sjå på mange ulike syntaktiske strukturar i to ulike språk: engelsk og nederlandsk. Eksperimentane våre stadfestar at CD kan vellykkeleg brukast til oppmerksbaserte modeller også, og tilbyr ein alternativ metode for Shapley-basert attribusjon for moderne neuralnettverk. I særskilt bruk av CD viser vi at engelske og nederlandske modelane demonstrerer liknande handlingsmodular, men at under høyden er det konsistent forskjeller mellom våre oppmerksområde og ikkje-oppmerksområde.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>W dziedzinie wytłumaczalnej sztucznej inteligencji doszło ostatnio do eksplozji liczby metod wyjaśniania wysoce nieliniowych głębokich sieci neuronowych. W jakim stopniu takie metody, które są często proponowane i testowane w dziedzinie wizji komputerowej, są właściwe do sprostania wyzwaniom związanym z wytłumaczalnością w NLP, jest jeszcze stosunkowo niezbadane. W niniejszej pracy rozważamy metodę atrybucji cech wejściowych opartą na Shapley, która okazała się dobrze działać dla modeli powtarzających NLP oraz testujemy, w jakim stopniu jest ona przydatna dla modeli zawierających operacje uwagi. W tym celu rozszerzamy CD o operacje niezbędne dla modeli uwagi. Następnie porównujemy, jak długodystansowe relacje podmiot-czasownik są przetwarzane przez modele z uwagą i bez uwagi, biorąc pod uwagę szereg różnych struktur składni w dwóch różnych językach: angielskim i holenderskim. Nasze eksperymenty potwierdzają, że CD można z powodzeniem zastosować również do modeli opartych na uwadze, zapewniając alternatywną metodę atrybucji opartą na Shapley'u dla nowoczesnych sieci neuronowych. W szczególności przy użyciu CD pokazujemy, że model angielski i holenderski wykazuje podobne zachowanie przetwarzania, ale pod maską istnieją konsekwentne różnice między naszymi modelami uwagi a modelami nieuwagi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>O campo da IA explicável viu recentemente uma explosão no número de métodos de explicação para redes neurais profundas altamente não lineares. Até que ponto esses métodos – que são frequentemente propostos e testados no domínio da visão computacional – são apropriados para enfrentar os desafios de explicabilidade da PNL ainda é relativamente inexplorado. Neste trabalho, consideramos a Decomposição Contextual (CD) – um método de atribuição de recursos de entrada baseado em Shapley que demonstrou funcionar bem para modelos recorrentes de PNL – e testamos até que ponto é útil para modelos que contêm operações de atenção. Para este fim, estendemos o CD para cobrir as operações necessárias para modelos baseados em atenção. Em seguida, comparamos como as relações sujeito-verbo de longa distância são processadas por modelos com e sem atenção, considerando várias estruturas sintáticas diferentes em duas línguas diferentes: inglês e holandês. Nossos experimentos confirmam que o CD também pode ser aplicado com sucesso para modelos baseados em atenção, fornecendo um método alternativo de atribuição baseado em Shapley para redes neurais modernas. Em particular, usando CD, mostramos que os modelos inglês e holandês demonstram comportamento de processamento semelhante, mas que sob o capô existem diferenças consistentes entre nossos modelos de atenção e não atenção.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Câmpul AI explicabil a văzut recent o explozie în numărul de metode de explicare pentru rețelele neuronale profunde extrem de non-liniare. Măsura în care astfel de metode - care sunt adesea propuse și testate în domeniul viziunii informatice - sunt adecvate pentru a aborda provocările de explicabilitate din PNL este încă relativ neexplorată. În această lucrare, considerăm Contextual Decomposition (CD) - o metodă de atribuire a caracteristicilor de intrare bazată pe Shapley care s-a dovedit a funcționa bine pentru modelele recurente NLP - și testăm măsura în care este utilă pentru modelele care conțin operațiuni de atenție. În acest scop, extindem CD-ul pentru a acoperi operațiunile necesare modelelor bazate pe atenție. Comparăm apoi cât de lungă distanță sunt procesate relațiile subiect-verb de modele cu și fără atenție, luând în considerare o serie de structuri sintactice diferite în două limbi diferite: engleză și olandeză. Experimentele noastre confirmă că CD-ul poate fi aplicat cu succes și pentru modelele bazate pe atenție, oferind o metodă alternativă de atribuire bazată pe Shapley pentru rețelele neurale moderne. În special, folosind CD, arătăm că modelele engleze și olandeze demonstrează un comportament similar de procesare, dar că sub capotă există diferențe consistente între modelele noastre de atenție și cele de non-atenție.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>В области объяснимого ИИ недавно произошел взрыв в количестве методов объяснения для высоко нелинейных глубоких нейронных сетей. Степень, в которой такие методы – которые часто предлагаются и тестируются в области компьютерного зрения – подходят для решения проблем объяснимости в NLP, еще относительно не изучена. В этой работе мы рассматриваем контекстное разложение (CD) – метод атрибуции входных признаков на основе Shapley, который, как было показано, хорошо работает для повторяющихся моделей NLP, и мы проверяем, насколько он полезен для моделей, которые содержат операции внимания. С этой целью мы расширяем КР, с тем чтобы охватить операции, необходимые для моделей, основанных на внимании. Затем мы сравниваем, как отношения между субъектом и глаголом на большом расстоянии обрабатываются моделями с вниманием и без внимания, учитывая ряд различных синтаксических структур на двух разных языках: английском и голландском. Наши эксперименты подтверждают, что CD можно успешно применять и для моделей, основанных на внимании, предоставляя альтернативный метод атрибуции на основе Шепли для современных нейронных сетей. В частности, используя CD, мы показываем, что английская и голландская модели демонстрируют сходное поведение обработки, но под капотом есть последовательные различия между нашими моделями внимания и невнимательности.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>විස්තර කරන්න පුළුවන් AI ක්ෂේත්රයේ අවසානයෙන් ප්‍රවේශනයක් දැක්කා ගොඩක් ගොඩක් ගොඩක් න්‍යුරෝල් ජාලයේ න පරිගණක දර්ශනයේ පරීක්ෂණය සහ පරීක්ෂණය කරලා තියෙන අවස්ථාව - NLP වලින් පැහැදිලි අවස්ථාවක් තියෙන්න සමහර විශ්වාස කරන්න මේ වැඩේ අපි හිතන්නේ ප්‍රතිස්ථිත විස්තරය (CD) - ශාප්ලි විස්තරයෙන් ඇතුළු ඇතුළු විශේෂ විශේෂ විධානය සඳහා හොඳ වැඩ කරන්න පෙන්වන්න පුළුවන මේ අවසානයෙන්, අපි CD විස්තර කරන්න අවශ්‍ය වැඩ කරන්නේ අවධානය අධාරිත මොඩල් වලට අවශ්‍ය වැඩ කරන්න. අපි ඊට පස්සේ කොච්චර දුරක් විදියට ප්‍රශ්න විදියට සම්බන්ධතා වෙනුවෙන් මොඩේල් එක්ක හා අවධානයක් නැතුව, වෙනස් භාෂාවල අපේ පරීක්ෂණය සාධාරණය කරන්න පුළුවන් කියලා සිඩි එක සමහරවිට අවධානය අධාරිත විද්‍යාපයක් සඳහා අවධානය කරන්න පුළුවන විශේෂයෙන්, CD භාවිතා කරන්න, අපි පෙන්වන්නේ ඉංග්‍රීසි සහ ඩච්ච් මොඩේල්ස් වල වගේ ප්‍රතික්‍රියාව පෙන්වන්න පුළුවන් වෙන</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Na področju pojasnljive umetne inteligence je pred kratkim prišlo do eksplozije števila metod pojasnjevanja zelo nelinearnih globokih nevronskih omrežij. Obseg, v katerem so takšne metode, ki se pogosto predlagajo in preizkušajo na področju računalniškega vida, primerne za obravnavanje izzivov pojasnljivosti pri NLP, je še razmeroma neraziskan. V tem delu obravnavamo kontekstualno razgradnjo (CD) - metodo dodeljevanja vhodnih funkcij, ki temelji na Shapleyju, za katero je bilo dokazano, da dobro deluje pri ponavljajočih se modelih NLP - in testiramo, v kolikšni meri je uporabna pri modelih, ki vsebujejo operacije pozornosti. V ta namen CD razširimo na operacije, potrebne za modele, ki temeljijo na pozornosti. Nato primerjamo, kako razmerja med subjekti in glagoli na dolgi razdalji obdelujejo modeli s pozornostjo in brez nje, pri čemer upoštevamo številne različne sintaktične strukture v dveh različnih jezikih: angleščini in nizozemščini. Naši eksperimenti potrjujejo, da je CD mogoče uspešno uporabiti tudi za modele, ki temeljijo na pozornosti, in zagotavljajo alternativno metodo pripisovanja na osnovi Shapleyja za sodobna nevronska omrežja. Zlasti z uporabo CD-ja pokažemo, da angleški in nizozemski modeli kažeta podobno obdelavo, vendar so pod pokrovom dosledne razlike med modeli pozornosti in nepozornosti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>beerta AI ee la caddeeyo ugu dhowaan wuxuu arkay baabuur oo ku yaal hababka kala duduwan ee shabakado hoos u dheer oo neurada ah. The extent to which such methods - that are often proposed and tested in the domain of computer vision - are appropriate to address the explainability challenges in NLP is yet relatively unexplored. Markaas waxan ka fiirsanaynaa daboolka hoose-hoose (CD) - qaab ah oo hoos-hoos u saaran, oo loo muujiyey inuu si wanaagsan u shaqeeyo tusaalayaasha NLP ee soo socda - waxaana imtixaamaynaa darajada ay u faa’iido u leedahay modellada ku haysta waxqabadka digtoonaanta. Taas darteed waxaynu ku fidinnaa CD si aan u daboolno shuqullada loo baahan yahay tusaalaha daryeelka caafimaadka. Markaas waxaynu isbarbardhignaa inta badan xiriirka la xiriira warqada ah waxaa lagu baaraandegayaa tusaale ahaan islamarkaasna aan la tahayn, waxaana ka fiirsanaynaa dhismo kala duduwan oo ku qoran laba luqadood: Ingiriis iyo Holand. Imtixaankayadu waxay xaqiijineysaa in CD lagu codsan karo samooyin aad u taxadaraysan karto, sidoo kale, in lagu siiyo qaab kale oo Shapley ku saleyn karo shabakado nooca ah. Si gaar ah, isticmaalka CD, waxaynu muujinnaa in noocyada Ingiriiska iyo Dutch ay muujiyaan dabeecada isku mid ah, laakiin daboolka hoosteeda waxaa ku jira kala duwanaansho kala duwan oo u dhexeeya fiirsashada iyo modelalka aan la jeedin.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>fusha e AI të shpjeguar ka parë kohët e fundit një shpërthim në numrin e metodave shpjeguese për rrjetet nervore të thella shumë jo-lineare. Mënyra në të cilën metodat e tilla - që shpesh propozohen dhe testohen në fushën e vizionit kompjuterik - janë të përshtatshme për të trajtuar sfidat e shpjegueshmërisë në NLP është ende relativisht e pazgjidhur. Në këtë punë, ne konsiderojmë Dekompozimin Konteksual (CD) - një metodë atribucioni i elementeve të hyrjes bazuar në Shapley që është treguar se funksionon mirë për modelet e përsëritur NLP - dhe ne testojmë shkallën në të cilën është e dobishme për modelet që përmbajnë operacionet e vëmendjes. Për këtë qëllim, ne shtrijmë CD për të mbuluar operacionet e nevojshme për modelet bazuar në vëmendje. Ne pastaj krahasojmë se sa distancë të gjatë lidhjet subjekt-verb procesohen nga modele me dhe pa vëmendje, duke konsideruar një numër strukturash sintaktike të ndryshme në dy gjuhë të ndryshme: anglisht dhe hollandez. Eksperimentet tona konfirmojnë se CD mund të aplikohet me sukses edhe për modele bazuar në vëmendje, duke ofruar një metodë alternative të atribucionit bazuar në Shapley për rrjetet moderne neurale. Në veçanti, duke përdorur CD, ne tregojmë se modelet angleze dhe hollandeze demonstrojnë sjellje të ngjashme procesimi, por nën kapuçin ka dallime të konsistenta midis vëmendjes sonë dhe modeleve jo të vëmendjes.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Polje objašnjavajućih AI nedavno je vidio eksploziju u broju metoda objašnjenja za visoko neolinearne duboke neuralne mreže. Koliko su takve metode - koje su često predložene i testovane u domenu kompjuterske vizije - odgovaraju za rješavanje izazova objašnjivosti u NLP-u, još je relativno neobjašnjivo. U ovom poslu razmišljamo o kontekstualnoj dekompoziciji (CD) - metodi privlačenja ulaznih karakteristika na Shapley-u koji je pokazan da dobro funkcioniše za rekonstruirane modele NLP-a - i testiramo koliku je koristan za modele koji sadrže operacije pažnje. Za taj cilj, proširimo CD da pokrijemo operacije potrebne za modele na pažnji. Onda uspoređujemo koliko duge veze sa tema-verbom obrađuju modeli sa i bez pažnje, s obzirom na broj različitih sintaktičkih struktura na dva različita jezika: engleski i holandski. Naši eksperimenti potvrđuju da se CD može uspešno primjenjivati i za modele na osnovu pažnje, pružajući alternativnu metodu privlačenja na osnovu Shapley-a za moderne neuralne mreže. Posebno, koristeći CD, pokazujemo da engleski i holandski modeli pokazuju slièno ponašanje procesa, ali da pod kapuljom postoje konsistentne razlike između naših pažnje i modela neopažnje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Inom området förklaringsbar AI har nyligen sett en explosion i antalet förklaringsmetoder för mycket icke-linjära djupa neurala nätverk. I vilken utsträckning sådana metoder - som ofta föreslås och testas inom området datorseende - är lämpliga för att ta itu med förklaringsutmaningarna i NLP är fortfarande relativt outforskade. I detta arbete betraktar vi Contextual Decomposition (CD) - en Shapley-baserad metod för tilldelning av indatfunktion som har visat sig fungera bra för återkommande NLP-modeller - och vi testar i vilken utsträckning det är användbart för modeller som innehåller uppmärksamhetsoperationer. För detta ändamål utökar vi CD till att omfatta den verksamhet som krävs för uppmärksamhetsbaserade modeller. Vi jämför sedan hur långa distansrelationer mellan subjekt och verb behandlas av modeller med och utan uppmärksamhet, med hänsyn till ett antal olika syntaktiska strukturer på två olika språk: engelska och nederländska. Våra experiment bekräftar att CD framgångsrikt kan användas även för uppmärksamhetsbaserade modeller, vilket ger en alternativ Shapley-baserad attributionsmetod för moderna neurala nätverk. Särskilt med hjälp av CD visar vi att de engelska och nederländska modellerna uppvisar liknande bearbetningsbeteende, men att det under huven finns konsekventa skillnader mellan våra uppmärksamhets- och icke-uppmärksamhetsmodeller.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ardhi ya AI inayoelezea hivi karibuni imeshuhudia mlipuko katika idadi ya mbinu za maelezo kwa mitandao ya kisasa isiyo na msingi. The extent to which such methods - that are often proposed and tested in the domain of computer vision - are appropriate to address the explainability challenges in NLP is yet relatively unexplored. Katika kazi hii, tunachukulia hatua inayotumiwa na mfumo wa kudhalilisha madaraka (CD) - mwelekeo wa kituo kinachotumiwa na msingi unaoonyeshwa kufanya kazi vizuri kwa ajili ya mifano ya NLP yanayoendelea - na tunajaribu kiwango ambacho kinafaa kwa mifano inayohusu shughuli za ufuatiliaji. Mpaka mwishoni huu, tunaongeza CD kwa ajili ya kuweka shughuli zinazohitajika kwa mifano yenye mwangalizi. We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. Majaribio yetu yanathibitisha kwamba CD inaweza kutumiwa kwa mafanikio ya mifano yenye msimamo wa ufuatiliaji na pia, kutoa njia mbadala ya kujitolea kwa mitandao ya kisasa ya kisasa. hasa, kwa kutumia CD, tunaonyesha kuwa modeli za Kiingereza na Dutch zinaonyesha tabia za namna hiyo, lakini chini ya pande hilo kuna tofauti tofauti tofauti tofauti kati ya hisia zetu na modeli zisizo za kusikiliza.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>தற்போது விளக்கமுடியாத AI புலம் மிகவும் கோடு இல்லாத நெறிய பிணையங்களின் எண்ணிக்கையில் ஒரு வெடிப்பை பார்த்துள்ளது. கணினி காட்சியின் களத்தில் பெரும்பாலாக பரிந்துரைக்கப்பட்டு சோதிக்கப்படும் இவ்வாறு முறைகளின் அளவு - NLP விளக்கமுடியாத சவால்களை முட இந்த வேலையில், நாம் உள்ளடக்க Decomposition (சிடி) - வடிவமைப்பு அடிப்படையில் உள்ளீடு குணங்கள் கூடுதல் முறைமையை கருதுகிறோம். மீண்டும் நிகழ்வு NLP மாதிரிகளுக்கு நன்றாக வேல இந்த முடிவிற்கு, நாம் சிடி விரிவாக்கி கவனத்தை அடிப்படையான மாதிரிகளுக்கு தேவையான செயல்களை மறை பிறகு நாம் எவ்வளவு நீண்ட தூரத்தை பொருள் சார்ந்த உறவுகளை மாதிரிகளால் செயல்படுத்தப்படுகிறது மற்றும் கவனத்தில்லாமல் ஒப்பிடுகிறோம். பல Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern neural networks. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aňlanabilýän AI sahypasy ýakynda ýokary çyzgyly döwletler üçin düşündirilmeýän netral netrallaryň sanynda patlamasyny gördü. NLP'deki düşündirilebilirlik kynçylyklary çözmek üçin (köplenç teklip edilen we maslahat edilen) netijesi kompýuter görünüşünde nähili döwletlere golaýdyr. Bu işde, Kontekst Beýik gabdalyk (CD) diýip pikir edýäris - Shapley tabanly gabdaly gaýd etmek üçin bir NLP modelleri üçin gowy işleýär we üns beren modelleriň üçin ullanyşyny barýarys. Bu üçin, biz CD-i üns daýanýan nusgalar üçin gerekli işleri örtmek üçin genişletip uzadyrys. Soňra tema-verb ilişkileri örän uzak nusga bilen örän üns berilýän nusga bilen we üns berilmeýän nusga bilen, iki dürli dilde birnäçe dürli syntaktik strukturlary düşünýäris: Iňlisçe we Holandiýa. Biziň deneylerimiz CD üns tabanly nusgalara hem başarıyla üýtgedilip biler diýip pikir edýärler. Şol günümizdeki nusgalar üçin Shapley tabanly takyklama yöntemi üýtgedip biler. Aýratyn CD ullanýarys, iňlisçe we holandiýa nusgalaryň meňzeş işleýän davranışyny görkezýäris, ýöne bu nusgalaryň altynda dikkatimiz we üns etmeýän nusgalarymyzyň arasyndaky düýbürlikleri bar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>مفصل صاف صاف صاف صاف صاف صاف صاف صاف صاف طریقوں کی تعداد میں ایک انفجار دیکھا ہے جو بالکل غیر لینی نیورل نیورل نیٹورک کے لئے ہے. جس طرح یہ طریقے - جو اکثر کمپیوٹر دیدئون کے ڈمین میں آزمائش کی جاتی ہیں اور آزمائش کی جاتی ہیں - NLP میں واضح طریقے کی چالوں کے بارے میں بہت مناسب ہیں، اب بھی نسبتا غیر واضح ہے. اس کام میں، ہم نے Contextual Decomposition (CD) کو سمجھ لیا ہے - ایک شاپلی بنیاد رکھی اینپ فوجیٹ کا اضافہ طریقہ جو دکھائی گئی ہے کہ دوبارہ NLP موڈل کے لئے اچھا کام کرے - اور ہم اس طرح امتحان کرتے ہیں جس طرح یہ موڈل کے لئے مفید ہے جو توجه کی عملیات حاوی رکھتے ہیں اس کے لئے ہم سی ڈی کو ڈھیل دیتے ہیں کہ توجه کی بنیادی موڈل کے لئے ضرورت کی عملیات کو پورا کریں۔ اس کے بعد ہم مثال بیان کرتے ہیں کہ کس طرح دور کی دور کی معاملات کے معاملات میں موڈل کے ذریعے اور بغیر توجه کے مطابق، دو مختلف زبانوں میں مختلف سینٹکتیک ساختاروں کی تعداد کی توجه کرتی ہے: انگلیسی اور ڈرچ. ہماری آزمائش کی تصدیق کرتی ہے کہ سی دی موڈل پر بھی موفقیت کے ساتھ کاروبار کر سکتی ہے، جو مدرنی نیورل نیٹورک کے لئے ایک الٹ شیپلی بنیاد آزمائش طریقہ دے رہی ہے. مخصوصا، سی دی استعمال کرتے ہیں، ہم نشان دیتے ہیں کہ انگلیسی اور ڈچ ڈیچ موڈلز ایسی طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Yaqinda ko'pchilik AI maydoni juda qiyin qo'l tarmoq tarmoqlarining tarkibi sonlarda eksplosini ko'rdi. Ушбу усуллар - компьютер кўриниши доменидаги маълумотлар таклиф қилинаётган ва тафтиш - NLP тўғрисида аниқ маслаҳатларни талаб қилиш мумкин. In this work, we consider Contextual Decomposition (CD) – a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models – and we test the extent to which it is useful for models that contain attention operations. Bu hozir uchun biz murakkab qilish modellari uchun kerak amallarni kodlash uchun CD ni uzatimiz. Keyin biz bir necha necha uzun masofadagi masofadagi munosabatlar modellar bilan ishlab chiqarishimizni kamaytamiz, ikkita xil tillarda ko'p bir necha sintaktik tuzuvlarini tasavvur qilamiz: Ingliz va Dutch. Bizning imtiyozlarimizni tasdiqlash imkoniyatlarimizga, CD muvaffaqiyatli foydalanishi mumkin va yangi neyrol tarmoqlari uchun boshqa shaklga hisoblash usulini qoʻllash mumkin. Ko'rsatilgan, CD yordamida biz Ingliz va Dutch modellari bir xil jarayonlarning xuddi ko'rsatadi, lekin odamning ichida bizning taqdimiz va o'xshash modellarimiz orasidagi tofautimiz bor.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Đồng vực của AI có triển vọng gần đây đã chứng kiến một vụ nổ trong số lượng các phương pháp giải thích cho các mạng thần kinh sâu không tuyến tính cao. Độ sâu mà các phương pháp như vậy Độ cao 8111, thường được đề xuất và thử nghiệm trong lĩnh vực ảo thuật máy tính, bây giờ vẫn chưa được kiểm tra. Trong công việc này, chúng ta xem xét sự phân phối kép (CD) 2;8111; một phương pháp phân biệt tính năng nhập dựa trên hình thể được cho thấy hoạt động tốt cho các mô hình lập chuỗi NLP 8111; và chúng ta thử độ nó có ích cho các mô hình chứa các thao tác tập trung. Vì vậy, chúng tôi mở rộng CD để bao gồm các thao tác cần thiết cho các mô hình tập trung. Sau đó chúng ta so sánh cách các mối quan hệ đối tượng dài hạn được xử lý bởi các mô hình không cần chú ý, xem xét một số cấu trúc cú pháp khác nhau trong hai ngôn ngữ khác nhau: Anh và Hòa Lan. Những thí nghiệm của chúng tôi xác nhận rằng CD cũng có thể được áp dụng với các mô hình dựa trên sự chú ý, cung cấp phương pháp phân bổ khác cho mạng thần kinh tân. Đặc biệt, dùng đĩa CD, chúng tôi cho thấy các mô hình Anh và Hòa Lan có hành vi xử lý tương tự, nhưng dưới lớp mũ có sự khác biệt nhất quán giữa các mô hình chú ý và không chú ý.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>说AI近见高非线性深神经网络之说,其数爆炸式长。 此法-常于计算机视领中试-于多大程度上宜解NLP中可解释性挑战,未得其对。 于此之中,吾思上下文分解(CD)——一本于Shapley之输归因,已验于循环NLP效矣——吾试其有用于意也。 是以广其 CD,涵盖其形势。 然后较之以意力,与无意之形相离主语 - 动词关系多长,思两语之多异句法:英语与荷兰语。 吾实验证之,CD亦可以成功于注意,为今世神经网络供一代之Shapley归因法。 用CD者,英国与荷兰相似也;引擎盖者,用意与非意相似也。</span></div></div><dl><dt>Anthology ID:</dt><dd>2021.deelio-1.13</dd><dt>Volume:</dt><dd><a href=/volumes/2021.deelio-1/>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a></dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/deelio/>DeeLIO</a>
| <a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>129–139</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.deelio-1.13>https://aclanthology.org/2021.deelio-1.13</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2021.deelio-1.13 title="To the current version of the paper by DOI">10.18653/v1/2021.deelio-1.13</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">kersten-etal-2021-attention</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Tom Kersten, Hugh Mee Wong, Jaap Jumelet, and Dieuwke Hupkes. 2021. <a href=https://aclanthology.org/2021.deelio-1.13>Attention vs non-attention for a Shapley-based explanation method</a>. In <i>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</i>, pages 129–139, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2021.deelio-1.13>Attention vs non-attention for a Shapley-based explanation method</a> (Kersten et al., DeeLIO 2021)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.deelio-1.13.pdf>https://aclanthology.org/2021.deelio-1.13.pdf</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.deelio-1.13.pdf title="Open PDF of 'Attention vs non-attention for a Shapley-based explanation method'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Attention+vs+non-attention+for+a+Shapley-based+explanation+method" title="Search for 'Attention vs non-attention for a Shapley-based explanation method' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Attention vs non-attention for a Shapley-based explanation method'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Attention vs non-attention for a Shapley-based explanation method](https://aclanthology.org/2021.deelio-1.13) (Kersten et al., DeeLIO 2021)</p><ul class=mt-2><li><a href=https://aclanthology.org/2021.deelio-1.13>Attention vs non-attention for a Shapley-based explanation method</a> (Kersten et al., DeeLIO 2021)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Tom Kersten, Hugh Mee Wong, Jaap Jumelet, and Dieuwke Hupkes. 2021. <a href=https://aclanthology.org/2021.deelio-1.13>Attention vs non-attention for a Shapley-based explanation method</a>. In <i>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</i>, pages 129–139, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>