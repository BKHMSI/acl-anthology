<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021 - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021" name=citation_title><meta content="Hwichan Kim" name=citation_author><meta content="Mamoru Komachi" name=citation_author><meta content="Proceedings of the 8th Workshop on Asian Translation (WAT2021)" name=citation_conference_title><meta content="2021/8" name=citation_publication_date><meta content="https://aclanthology.org/2021.wat-1.13.pdf" name=citation_pdf_url><meta content="133" name=citation_firstpage><meta content="137" name=citation_lastpage><meta content="10.18653/v1/2021.wat-1.13" name=citation_doi><meta property="og:title" content="TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021"><meta property="og:image" content="https://aclanthology.org/thumb/2021.wat-1.13.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2021.wat-1.13"><meta property="og:description" content="Hwichan Kim, Mamoru Komachi. Proceedings of the 8th Workshop on Asian Translation (WAT2021). 2021."><link rel=canonical href=https://aclanthology.org/2021.wat-1.13></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System with Japanese BART for the Patent task of WAT 2021<span class=acl-fixed-case>TMU</span> <span class=acl-fixed-case>NMT</span> System with <span class=acl-fixed-case>J</span>apanese <span class=acl-fixed-case>BART</span> for the Patent task of <span class=acl-fixed-case>WAT</span> 2021</a>
<a id=af_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT Stelsel met Japaanse BART vir die Patent taak van WAT 2021</a>
<a id=am_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>ከጃፓንኛ BART ጋር TMU NMT ስርዓት ለWAT 2021 ለPatent ስራ</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>نظام TMU NMT مع BART الياباني لمهمة براءة اختراع WAT 2021</a>
<a id=az_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System with Japanese BART for the Patent task of WAT 2021</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>ТМУ НМТ Система с японски БАРТ за патентната задача на ВАТ 2021</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>ওয়াট ২০১২ সালের প্যাটেন্ট কাজের জন্য জাপানী বার্টের সাথে TMU NMT সিস্টেম</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT མ་ལག་གི་རྒྱ་ནག་གི་BART སྤྱད་ནས་ WAT 2021 ཡི་patent task of the WAT 2021</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT sustav sa japanskim BART za patentni zadatak WAT 2021.</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System with Japanese BART for the Patent task of WAT 2021</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT systém s japonským BART pro patentový úkol WAT 2021</a>
<a id=da_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System med japansk BART til patentopgaven af WAT 2021</a>
<a id=de_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System mit japanischem BART für die Patentaufgabe von WAT 2021</a>
<a id=el_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Σύστημα TMU NMT με ιαπωνικό BART για το έργο ευρεσιτεχνίας του WAT 2021</a>
<a id=es_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Sistema TMU NMT con BART japonés para la tarea de patentes de WAT 2021</a>
<a id=et_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT süsteem Jaapani BARTiga WAT 2021 patendiülesandeks</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>سیستم TMU NMT با BART ژاپنی برای کار patent از WAT 2021</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT -järjestelmä japanilaisen BART:n kanssa WAT 2021 -patenttitehtävään</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Système TMU NMT avec BART japonais pour la tâche de brevet du WAT 2021</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Córas TMU NMT le BART na Seapáine le haghaidh tasc Paitinne WAT 2021</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>KCharselect unicode block name</a>
<a id=he_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>מערכת TMU NMT עם BART יפני למשימת הפטנטים של WAT 2021</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>WAT 2021 के पेटेंट कार्य के लिए जापानी BART के साथ TMU NMT सिस्टम</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT sustav sa japanskim BART za patentni zadatak WAT 2021.</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT rendszer japán BART-vel a WAT 2021 szabadalmi feladatához</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>ԹՄՄ ՆՄԹ համակարգը ճապոնական Բարթ-ով, որն օգտագործում է պայթենտի աշխատանքի համար</a>
<a id=id_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Sistem TMU NMT dengan BART Jepang untuk tugas Paten dari WAT 2021</a>
<a id=is_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System con BART giapponese per il compito di brevetto di WAT 2021</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>WAT 2021の特許業務のための日本のBARTを備えたTMU NMTシステム</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>MU NMT System karo BaRT japongan kanggo nggawe patent kanggo WAT 2020 1</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT სისტემა იაპონური BART-ის პოტენტის დავალებისთვის WAT 2021</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>WAT 2021 патенттік тапсырманың жапон BART жүйесі TMU NMT жүйесі</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT 시스템은 일본 BART와 협력하여 WAT 2021 특허 작업 수행</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT sistema su Japonijos BART, skirta 2021 m. WAT patentų uždaviniui atlikti</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT систем со јапонски BART за задачата на патентот на WAT 2021</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>ജാപ്പനീസ് ബാര്‍ട്ടിനുള്ള ടിഎംഎംടി സിസ്റ്റം വാട്ട് 2021-ന്റെ പാതിന്റ് ജോലി</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System with Japanese BART for the Patent task of WAT 2021</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Sistem NMT TMU dengan BART Jepun untuk tugas Paten WAT 2021</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Sistema TMU NMT b’BART Ġappuniż għall-kompitu tal-Privattivi tal-WAT 2021</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT Systeem met Japanse BART voor de Octrooitaak van WAT 2021</a>
<a id=no_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT- systemet med japansk BART for patentoppgåva av WAT 2021</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>System TMU NMT z japońskim BART do zadania patentowego WAT 2021</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Sistema TMU NMT com BART japonês para a tarefa de patente do WAT 2021</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Sistemul TMU NMT cu BART japonez pentru sarcina de brevetare a WAT 2021</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Система TMU NMT с японским BART для патентной задачи WAT 2021</a>
<a id=si_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT පද්ධතිය ජාපානි BART එක්ක WAT 2021 ගේ පැටෙන්ට් වැඩේ වෙනුවෙන්.</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System z japonskim BART za patentno nalogo WAT 2021</a>
<a id=so_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System with Japanese BART for the Patent task of WAT 2021</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System with Japanese BART for the Patent task of WAT 2021</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT sistem sa japanskim BART za patentni zadatak WAT 2021.</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT System med japansk BART för patentuppgiften för WAT 2021</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Mfumo wa TMU NMT na BART wa Japani kwa ajili ya kazi ya Patients ya WAT 2021</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>ஜாப்பானிய BART உடன் TMU NMT அமைப்பு</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Japonça BART bilen TMU NMT sistemi WAT 2021'iň Patent işi üçin</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>WAT 2021 کے پیٹینٹ کام کے لئے جاپانی BART کے ساتھ TMU NMT سیسٹم</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>Giao thông TMU NMT với BART Nhật Bản cho công việc sáng chế của WAT 2021</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2021.wat-1.13.pdf>TMU NMT系统与日本BART同成WAT 2021专利</a></h2><p class=lead><a href=/people/h/hwichan-kim/>Hwichan Kim</a>,
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using <a href=https://en.wikipedia.org/wiki/BART>English BART</a>. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In hierdie papier, ons introduseer ons TMU Neurale Masjien Vertaling (NMT) stelsel voorgestel vir die Patent-taak (Koreaanse Japanse en Engelse Japanse) van die 8de Werkshop op Asies Vertaling (Nakazawa et al., 2021). Onlangs het verskeie studie voorgestel vooraf-opgelei koder-dekoder modele gebruik van monolinglike data. Een van die voorafgevorderde modele, BART (Lewis et al., 2020), was vertoon om vertaling presisiteit te verbeter deur fyn-tuning met twee-tale data. Maar hulle het slegs Romaniese eksperimenteer! Engels vertaling gebruik Engels BART. In hierdie papier, ons ondersoek die effektiviteit van japanse BART deur Japan Patent Office Corpus 2.0 te gebruik. Ons eksperimente wys dat Japanse BART ook vertaling presies kan verbeter in Koreaanse Japanse en Engelse vertalings.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>በዚህ ገጽ፣ በእስያ ትርጉም ላይ ስምንተኛው workshop (ናkazawa et al., 2021) ለPatent ሥራ (የቆሬያዊ ጃፓንኛ እና እንግሊዘኛ) የTMU ኔural machine ትርጉም (NMT) ስርዓታችንን እናሳውቃለን፡፡ በቅርብ ጊዜ ብዙዎች ተማርከቶች በሞሎልቋል ዳታ የተጠቃሚ የencoder-decoder models በመጠቀም ይገልጻሉ፡፡ የቀድሞው ተማሪ ሞዴል BART (Lewis et al., 2020) በሁለት ቋንቋዎች ዳታ በመጠቀም ትርጉም እርግጠኛ ማድረግ ተገልጦአል፡፡ ነገር ግን ሮማኒያን ብቻ ፈተናቸው፡፡ እንግሊዘኛ ትርጉም BART በተጠቃሚ በዚህ ገጽ የጃፓን ፓንቲ ኮርፓስ 2.0 በመጠቀም የጃፓን BART ጥያቄን መርምረናል፡፡ ፈተናዎቻችን የጃፓን BART እና በቆሬያዊ ጃፓንኛ እና ኢንጂፓን ትርጓሜዎች ሁለቱም ትርጓሜዎችን ማሻሻል እንደሚችል ያሳያል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>في هذه الورقة ، نقدم نظام TMU Neural Machine Translation (NMT) المقدم لمهمة براءات الاختراع (الكورية اليابانية والإنجليزية اليابانية) في ورشة العمل الثامنة حول الترجمة الآسيوية (ناكازاوا وآخرون ، 2021). في الآونة الأخيرة ، اقترحت العديد من الدراسات نماذج مدربة مسبقًا لوحدات فك التشفير باستخدام بيانات أحادية اللغة. تم عرض أحد النماذج المدربة مسبقًا ، BART (Lewis et al. ، 2020) ، لتحسين دقة الترجمة من خلال الضبط الدقيق للبيانات ثنائية اللغة. ومع ذلك ، فقد جربوا الترجمة الرومانية فقط الإنجليزية باستخدام BART الإنجليزية. في هذه الورقة ، قمنا بفحص فعالية BART اليابانية باستخدام مجموعة مكتب براءات الاختراع اليابانية 2.0. تشير تجاربنا إلى أن تقنية BART اليابانية يمكنها أيضًا تحسين دقة الترجمة في الترجمات اليابانية الكورية واليابانية والإنجليزية.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu kańüńĪzda TMU Neural Machine Translation (NMT) sistemimizi Aziya √áeviri (Nakazawa et al., 2021) 8. Workshop on 8. Workshop for the Patent Task (Korean Japanese and English Japanese) il…ô t…ôblińü edirik. Son zamanlarda, bir ne√ß…ô t…ôhsil …ôvv…ôlc…ô t…ôhsil edilmiŇü koder-dekoder modell…ôri monodil veril…ônl…ôrd…ôn istifad…ô edir. √Ėn t…ôhsil edilmiŇü modell…ôrd…ôn biri BART (Lewis et al., 2020), iki dil m…ôlumatlarńĪ il…ô t…ôhsil edilm…ôsi il…ô t…ôhsil edilm…ôsi √ľ√ß√ľn t…ôhsil edilmiŇüdir. Ancaq onlar yalnńĪz Rumun t…ôcr√ľb…ôl…ôrini t…ôcr√ľb…ô etdil…ôr! ńįngiliz…ô BART vasit…ôsil…ô ńįngiliz…ô terc√ľm…ôsi. Bu kańüńĪzda Japon Patent Office Corpus 2.0 vasit…ôsil…ô Japon BART'un etkinlińüini incidirik. Bizim t…ôcr√ľb…ôl…ôrimiz Japonca BART h…ôm√ßin in Kore Japonca v…ô ńįngilizce √ß…ôtinlikl…ôrd…ô d…ô d…ôyiŇüiklik ed…ô bil…ôr.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>В настоящата статия представяме нашата система за невронен машинен превод (НМТ), подадена за патентна задача (корейски японски и английски японски) на 8-ма работилница по азиатски превод (Наказава и др., 2021). Наскоро няколко проучвания предлагат предварително обучени модели кодер-декодер, използващи едноезични данни. Доказано е, че един от предварително обучените модели подобрява точността на превода чрез фина настройка с двуезични данни. Но те експериментираха само румънски! Английски превод с помощта на английски BART. В настоящата статия изследваме ефективността на японския БАРТ, използвайки Японското патентно ведомство Корпус 2.0. Нашите експерименти показват, че японският може да подобри точността на превода както в корейския, така и в английския японски превод.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>এই কাগজটিতে আমরা আমাদের টিএমউ নিউরাল মেশিন অনুবাদ (এনএমটি) সিস্টেমের পরিচয় প্রদান করেছি প্যাটেন্ট কাজের (কোরিয়ান জাপানী এবং ইংরেজী জাপানীয়) যা এশিয়ার সম্প্রতি বেশ কিছু গবেষণা প্রশিক্ষণের পূর্বে প্রশিক্ষিত এনকোডার-ডেকোডার মডেল প্রস্তাব করা হয়েছে মোনোলিভা প্রাক্তন প্রশিক্ষিত মডেলের মধ্যে একটি বার্ট (লেউস এন্ট এল ২০২০), দুই ভাষার তথ্যের মাধ্যমে অনুবাদের সঠিকভাবে উন্নত করা হয়েছে। তবে, তারা শুধুমাত্র রোমানীয় পরীক্ষা করেছে! ইংরেজি অনুবাদ বিআরটি ব্যবহার করে। এই পত্রিকায় আমরা জাপানের প্যাটেন্ট অফিস ব্যবহার করে জাপানী বার্টের কার্যকরী পরীক্ষা করি। আমাদের পরীক্ষা নির্দেশ করছে যে জাপানী বার্টি কোরিয়ান জাপানী এবং ইংরেজি ভাষায় অনুবাদের সঠিকভাবে উন্নত করতে পারে।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ང་ཚོའི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོའི་TMU Neural Machine Translation (NMT)རིགས་འདིས་ཨ་རེ་ཤི་ཡིག་སྣོད་ཀྱི་ལས་འགུལ་ལ་བཤད་པ་སོགས། འཕྲལ་ཁམས་ཀྱི་ལྟ་བ་དག་གི་སྔོན་སྒྲིག་འཛུགས་པའི་སྔོན་སྒྲིག་འཛུགས་པའི་ཨིན་ཀོ་ཌིར་སྤྱོད་པའི་མ་དཔེ་གཞུང་མང་པོ་ཞི སྔོན་གྲངས་འཛིན་པའི་མིག་གཟུགས་རིས་གཅིག་ནི་BART (Lewis et al., 2020)ནང་དུ་སྔོན་གྲངས་སྒྲིག ཡིན་ནའང་། ཁོང་ཚོས་རོ་མ་ཡིན་ལས་བརྟག་ཞིབ་བྱེད་པ་རེད། English translation using English BART. ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་དུ་ཉེ་ཧོང་གི་ཡུལ་སྤྱོད་པའི་ཉེ་ཧོང་གི་BART་གི་ལྕགས་འབྱུང་བ་ཞིག ང་ཚོའི་བརྟག་ཞིག་གིས་ཉེ་ཧོང་གི་རྨང་གཞིའི་ནང་ནས་སྐད་ཡིག་ཆ་ཉེ་ཧོང་དང་ཨིན་ཇིའི་སྐད་ཡིག</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>U ovom papiru predstavljamo naš sistem neurološkog prevoda (NMT) TMU koji je predan za zadatak patenta (korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno, nekoliko ispitivanja predložilo je predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazuje se da će poboljšati preciznost prevođenja putem fine-tuning sa dvojezičkim podacima. Međutim, oni su eksperimentirali samo rumunski! Engleski prevod koristeći engleski BART. U ovom papiru pregledavamo učinkovitost japanskog BART korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART također može poboljšati preciznost prevoda na korejskim japanskim i engleskim prevodima.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>En aquest article introduïm el nostre sistema de traducció de màquines neuronals (NMT) submetit a la tasca de brevets (japonès coreans i anglès) de la 8ª taller sobre traducció asiàtica (Nakazawa et al., 2021). Recentment, molts estudis van proposar models de codificador pré-entrenats que utilitzaven dades monolingües. Un dels models pré-entrenats, BART (Lewis et al., 2020), va demostrar que millora la precisió de la traducció ajustant-se a les dades bilingües. No obstant això, només experimentaven rumà! traducció anglesa amb BART anglès. En aquest paper examinem l'eficacia del BART japonès utilitzant el Japan Patent Office Corpus 2.0. Els nostres experiments indican que el BART japonès també pot millorar la precisió de la traducció tant en coreans japonès com en anglès.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>V tomto článku představujeme náš systém TMU Neural Machine Translation (NMT) předložený k patentovému úkolu (korejská japonština a anglická japonština) osmého workshopu o asijském překladu (Nakazawa et al., 2021). V poslední době několik studií navrhlo předem trénované modely kodéru-dekodéru s využitím jednojjazyčných dat. Bylo ukázáno, že jeden z předškolených modelů BART (Lewis et al., 2020) zlepšuje přesnost překladu díky jemnému ladění s dvojjazyčnými daty. Nicméně experimentovali pouze rumunsky! Anglický překlad pomocí angličtiny BART. V tomto článku zkoumáme efektivitu japonského BART s využitím japonského patentového úřadu Corpus 2.0. Naše experimenty ukazují, že japonština BART může také zlepšit přesnost překladu v korejském japonštině a anglickém japonštině.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I denne artikel introducerer vi vores TMU Neural Machine Translation (NMT) system indsendt til patentopgaven (koreansk japansk og engelsk japansk) på 8. workshop om asiatisk oversættelse (Nakazawa et al., 2021). For nylig foreslog flere undersøgelser præ-trænede encoder-dekoder modeller ved hjælp af ensprogede data. En af de forududdannede modeller, BART (Lewis et al., 2020), viste sig at forbedre oversættelsesnøjagtigheden ved finjustering med tosprogede data. Men de eksperimenterede kun rumænsk! Engelsk oversættelse ved hjælp af engelsk BART. I denne artikel undersøger vi effektiviteten af japansk BART ved hjælp af Japan Patent Office Corpus 2.0. Vores eksperimenter viser, at japansk BART også kan forbedre oversættelsesnøjagtigheden i både koreansk japansk og engelsk japansk oversættelse.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In diesem Beitrag stellen wir unser TMU Neural Machine Translation (NMT) System vor, das für die Patentaufgabe (Koreanisch Japanisch und Englisch Japanisch) des achten Workshops zur asiatischen Übersetzung (Nakazawa et al., 2021) eingereicht wurde. Kürzlich schlugen mehrere Studien vortrainierte Encoder-Decoder-Modelle vor, die monolinguale Daten verwenden. Eines der vortrainierten Modelle, BART (Lewis et al., 2020), verbesserte die Übersetzungsgenauigkeit durch Feinabstimmung mit zweisprachigen Daten. Allerdings experimentierten sie nur rumänisch! Englische Übersetzung mit Hilfe von BART. In diesem Beitrag untersuchen wir die Wirksamkeit des japanischen BART unter Verwendung des japanischen Patentamts Corpus 2.0. Unsere Experimente zeigen, dass Japanisch BART auch die Übersetzungsgenauigkeit sowohl in Koreanisch-Japanisch- als auch in Englisch-Japanisch-Übersetzungen verbessern kann.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Στην παρούσα εργασία, παρουσιάζουμε το σύστημα Νευρικής Μηχανικής Μετάφρασης (NMT) που υποβλήθηκε για το έργο Διπλώματος ευρεσιτεχνίας (Κορεατικά Ιαπωνικά και Αγγλικά Ιαπωνικά) του 8ου Εργαστηρίου Ασιατικής Μετάφρασης (Νακαζάβα κ.α., 2021). Πρόσφατα, αρκετές μελέτες πρότειναν προ-εκπαιδευμένα μοντέλα κωδικοποιητών-αποκωδικοποιητών χρησιμοποιώντας μονογλωσσικά δεδομένα. Ένα από τα προ-εκπαιδευμένα μοντέλα, το BART (Lewis et al., 2020), αποδείχθηκε ότι βελτιώνει την ακρίβεια της μετάφρασης μέσω της τελειοποίησης με δίγλωσσα δεδομένα. Ωστόσο, πειραματίστηκαν μόνο ρουμανικά! Αγγλική μετάφραση χρησιμοποιώντας την αγγλική BART. Σε αυτή την εργασία, εξετάζουμε την αποτελεσματικότητα του ιαπωνικού BART χρησιμοποιώντας το Ιαπωνικό Γραφείο Διπλωμάτων Ευρεσιτεχνίας Corpus 2.0. Τα πειράματά μας δείχνουν ότι τα ιαπωνικά μπορούν επίσης να βελτιώσουν την ακρίβεια της μετάφρασης τόσο σε κορεατικές ιαπωνικές όσο και σε αγγλικές ιαπωνικές μεταφράσεις.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>En este artículo, presentamos nuestro sistema de traducción automática neuronal (NMT) TMU presentado para la tarea de patentes (coreano, japonés e inglés japonés) del octavo taller de traducción asiática (Nakazawa et al., 2021). Recientemente, varios estudios propusieron modelos de codificador-decodificador previamente entrenados que utilizan datos monolingües. Se demostró que uno de los modelos previamente entrenados, BART (Lewis et al., 2020), mejora la precisión de la traducción mediante el ajuste fino con datos bilingües. Sin embargo, ¡solo experimentaron rumano! Traducción al inglés con BART en inglés. En este artículo, examinamos la eficacia del BART japonés utilizando el Corpus 2.0 de la Oficina de Patentes de Japón. Nuestros experimentos indican que el BART japonés también puede mejorar la precisión de la traducción en las traducciones de coreano, japonés e inglés japonés.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Käesolevas töös tutvustame TMU neuroalse masintõlke (NMT) süsteemi, mis on esitatud 8. Aasia tõlke seminari patendiülesandeks (Korea jaapani ja inglise jaapani keeles) (Nakazawa et al., 2021). Hiljuti pakuti mitmes uuringus välja eelnevalt koolitatud kodeerija-dekooderi mudelid, mis kasutasid ühekeelseid andmeid. Üks eelkoolitud mudelitest, BART (Lewis et al., 2020), tõendas tõlke täpsust kahekeelsete andmetega peenhäälestuse kaudu. Kuid nad katsetasid ainult rumeenia keelt! Inglise tõlge inglise keeles BART. Käesolevas töös uurime Jaapani BART efektiivsust, kasutades Jaapani Patendiameti Corpus 2.0. Meie eksperimendid näitavad, et jaapani BART võib parandada ka tõlke täpsust nii korea jaapani kui ka inglise jaapani tõlketes.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>در این کاغذ، سیستم ترجمه ماشین عصبی TMU (NMT) ما را معرفی می‌کنیم که برای کار patent (ژاپنی ژاپنی و ژاپنی ژاپنی کوریه) هشتم کارشناسی در ترجمه آسیا (Nakazawa et al., 2021) فرستاده شده است. اخیرا، چند مطالعه پیش از آموزش مدل‌های رمزبندی پیش آموزش داده شده با استفاده از داده‌های یک زبان. یکی از مدلهای پیش آموزش داده شده، BART (Lewis et al., 2020) نشان داده شد که دقیق ترجمه را با اطلاعات دو زبان بهتر کند. ولی اونا فقط رومانی آزمایش کردند! ترجمه انگلیسی با استفاده از BART انگلیسی. در این کاغذ، ما موثیت BART ژاپنی را با استفاده از اداره پتانس ژاپن Corpus 2.0 تحقیق می کنیم. آزمایشات ما نشان می دهند که BART ژاپنی هم می تواند دقیق ترجمه را در ترجمه ژاپنی ژاپنی و ژاپنی ژاپنی بهتر کند.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tässä artikkelissa esittelemme TMU Neural Machine Translation (NMT) -järjestelmämme, joka on toimitettu 8. Aasian kääntämisen työpajan patenttitehtävään (Nakazawa et al., 2021). Viime aikoina useissa tutkimuksissa on ehdotettu esikoulutettuja kooderi-dekooderimalleja, joissa käytetään monikielistä dataa. Yhden esikoulutetun mallin, BART (Lewis et al., 2020), osoitettiin parantavan käännöksen tarkkuutta hienosäätämällä kaksikielisiä tietoja. Mutta he kokeilivat vain romaniaa! Englanninkielinen käännös käyttäen englantia BART. Tässä artikkelissa tarkastellaan japanilaisen BART:n tehokkuutta Japanin patenttiviraston Corpus 2.0:n avulla. Kokemuksemme osoittavat, että japanin BART voi myös parantaa käännösten tarkkuutta sekä korean japanin että englannin japanin käännöksissä.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dans cet article, nous présentons notre système de traduction automatique neuronale TMU (NMT) soumis pour la tâche de brevet (coréen japonais et anglais japonais) du 8e atelier sur la traduction asiatique (Nakazawa et al., 2021). Récemment, plusieurs études ont proposé des modèles encodeur-décodeur pré-entraînés utilisant des données monolingues. Il a été démontré que l'un des modèles pré-entraînés, BART (Lewis et al., 2020), améliorait la précision de la traduction grâce à un réglage précis avec des données bilingues. Cependant, ils n'ont expérimenté que le roumain ! Traduction en anglais en utilisant le BART anglais. Dans cet article, nous examinons l'efficacité du BART japonais en utilisant le Corpus 2.0 de l'Office des brevets du Japon. Nos expériences indiquent que le BART japonais peut également améliorer la précision des traductions en coréen japonais et en anglais japonais.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sa pháipéar seo, tugaimid isteach ár gcóras TMU Neural Machine Translation (NMT) a cuireadh isteach don tasc Paitinne (Seapáinis na Cóiré agus Béarla na Seapáine) den 8ú Ceardlann ar Aistriúchán na hÁise (Nakazawa et al., 2021). Le déanaí, mhol roinnt staidéar samhlacha réamh-oilte ionchódóra-díchódóra ag baint úsáide as sonraí aonteangacha. Léiríodh go bhfuil ceann de na samhlacha réamhoilte, BART (Lewis et al., 2020), chun cruinneas an aistriúcháin a fheabhsú trí mhionchoigeartú a dhéanamh ar shonraí dátheangacha. Mar sin féin, rinne siad tástáil ar aistriúchán Béarla Rómáinis amháin ag baint úsáide as Béarla BART. Sa pháipéar seo, scrúdaímid éifeachtacht BART na Seapáine ag baint úsáide as Japan Paitinn Office Corpus 2.0. Léiríonn ár dturgnaimh gur féidir le BART na Seapáine feabhas a chur ar chruinneas aistriúcháin in aistriúcháin Cóiréis Seapánacha agus Béarla araon.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A cikin wannan takardan, Munã introduce na tsarin TMU Tarjima na Neural Mashine (NMT) wanda aka yi wajen aikin Nagon Farawa (Koriya Japanese da Ingiriya) na 8. workworkload on Asian Translate (Nakazawa et al., 2021). A yanzu, wasu fitina masu amfani da data na monoli'ura da aka yi amfani da shiryoyin kode-kode-kode. Babu wani daga shiryoyin ayuka da aka yi amfani da shi gaba-tuned, BArT (Lewi et al., 2020), aka nuna shi to improve translation uranci through fin-tuning with data bilin languages. To, a cikin fitinar ba su zamo ba fãce runtsũma. @ item Spelling dictionary Ga wannan takardan, Munã jarraba aikin BAT na Jamapi da Corbas 2.0. Kayan jarrabayanmu, yana madaidaita cẽwa BERT za'a iya samar da fassarar taƙaita a cikin fassarar Yahũdiya da Ingiriya.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>בעיתון הזה, אנחנו מציגים את מערכת המכונה העצבית TMU (NMT) שלנו שנשלחה למשימת הפטנטים (יפנית וקוריאנית) של Workshop 8 על התרגום אסיאטי (Nakazawa et al., 2021). לאחרונה, מספר מחקרים הציעו מודלים קודם-קודם מאומנים מראש בשימוש בנתונים מונולשונים. אחד מהדוגמנים המאמנים מראש, BART (Lewis et al., 2020), הוכח לשפר את מדויקת התרגום דרך התאים עם נתונים שתיים שפתיים. עם זאת, הם ניסו רק רומני! English translation using English BART. בעיתון הזה, אנחנו בודקים את היעילות של BART היפני באמצעות משרד פטנטים יפני קורפוס 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>इस पेपर में, हम एशियाई अनुवाद पर 8 वीं कार्यशाला के पेटेंट कार्य (कोरियाई जापानी और अंग्रेजी जापानी) के लिए प्रस्तुत हमारे टीएमयू न्यूरल मशीन ट्रांसलेशन (एनएमटी) सिस्टम को पेश करते हैं (नाकाज़ावा एट अल। हाल ही में, कई अध्ययनों ने मोनोलिंगुअल डेटा का उपयोग करके पूर्व-प्रशिक्षित एन्कोडर-डिकोडर मॉडल का प्रस्ताव दिया। पूर्व-प्रशिक्षित मॉडलों में से एक, BART (लुईस एट अल, 2020), द्विभाषी डेटा के साथ ठीक-ट्यूनिंग के माध्यम से अनुवाद सटीकता में सुधार करने के लिए दिखाया गया था। हालांकि, उन्होंने केवल रोमानियाई प्रयोग किया! अंग्रेजी BART का उपयोग करके अंग्रेजी अनुवाद। इस पेपर में, हम जापान पेटेंट ऑफिस कॉर्पस 2.0 का उपयोग करके जापानी BART की प्रभावशीलता की जांच करते हैं। हमारे प्रयोगों से संकेत मिलता है कि जापानी BART भी कोरियाई जापानी और अंग्रेजी जापानी अनुवाद दोनों में अनुवाद सटीकता में सुधार कर सकता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>U ovom papiru predstavljamo naš sistem za neurološki prevod (NMT) TMU koji je podignut za zadatak patenta (korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno je nekoliko ispitivanja predložilo predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazalo je kako bi poboljšao preciznost prevođenja putem ispravnog korištenja s dvojezičkim podacima. Međutim, oni su eksperimentirali samo rumunski! English translation using English BART. U ovom papiru pregledamo učinkovitost japanskog BART korištenja Japanskog patentnog ureda korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART također može poboljšati preciznost prevoda na korejskim japanskim i engleskim japanskim prevodima.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ebben a tanulmányban bemutatjuk a TMU Neural Machine Translation (NMT) rendszerünket, amelyet az Ázsiai Fordításról szóló 8. Workshop on Asian Translation (Nakazawa et al., 2021) szabadalmi feladatára benyújtottak (koreai japán és angol japán). A közelmúltban több tanulmány előre képzett útmérő-dekódoló modelleket javasolt egynyelvű adatok felhasználásával. Az egyik előre képzett modell, a BART (Lewis et al., 2020) kimutatták, hogy a kétnyelvű adatok finomhangolásával javítja a fordítási pontosságot. De csak románul kísérleteztek! Angol fordítás angol BART használatával. Ebben a tanulmányban a japán BART hatékonyságát vizsgáljuk a Japan Patent Office Corpus 2.0 alkalmazással. Kísérleteink azt mutatják, hogy a japán BART a koreai japán és angol japán fordítások pontosságát is javíthatja.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Այս թղթի մեջ մենք ներկայացնում ենք մեր ԹՄԱ-ի նյարդային մեքենայի թարգմանման (ՆՄԹ) համակարգը, որը ներկայացվել է Ասիական թարգմանման 8-րդ աշխատասենյակի համար (Նակազավա և այլն., 2021 թ.): Վերջերս, մի քանի ուսումնասիրություններ առաջարկեցին նախապատրաստված կոդեր-կոդեր մոդելներ, որոնք օգտագործում են միալեզու տվյալներ: Պարզվեց, որ նախապատրաստված մոդելներից մեկը, Բարթը (Leouis et al., 2020), բարելավում է թարգմանման ճշգրտությունը երկլեզու տվյալների հետ կապված կերպով: Այնուամենայնիվ, նրանք փորձեցին միայն ռոմաներեն: Անգլերեն թարգմանություն օգտագործելով անգլերեն Բարտ: Այս թղթի մեջ մենք ուսումնասիրում ենք Ճապոնական Բարտի արդյունավետությունը՝ օգտագործելով Ճապոնիայի փաստաբանական գրասենյակ Կորպուս 2.0: Մեր փորձարկումները ցույց են տալիս, որ ճապոնական Բարթը կարող է նաև բարելավել թարգմանման ճշգրտությունը կորեացի ճապոնացի և անգլերենի թարգմանություններում:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Baru-baru ini, beberapa studi mengusulkan model koder-dekoder yang dilatih-dilatih menggunakan data monobahasa. Salah satu model prapelatih, BART (Lewis et al., 2020), ditunjukkan untuk meningkatkan akurasi terjemahan melalui fine-tuning dengan data dua bahasa. Namun, mereka hanya eksperimen Rumania! Terjemahan Inggris menggunakan BART Inggris. Dalam kertas ini, kami memeriksa efektivitas dari BART Jepang menggunakan Korpus Paten Jepang 2.0. Eksperimen kami menunjukkan bahwa BART Jepang juga dapat meningkatkan akurasi terjemahan dalam terjemahan Jepang Korea dan bahasa Inggris Jepang.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In questo articolo presentiamo il nostro sistema di traduzione automatica neurale TMU (NMT) presentato per il compito di brevetto (coreano giapponese e inglese giapponese) dell'8° Workshop sulla traduzione asiatica (Nakazawa et al., 2021). Recentemente, diversi studi hanno proposto modelli di encoder-decoder pre-addestrati utilizzando dati monolingue. Uno dei modelli pre-addestrati, BART (Lewis et al., 2020), ha dimostrato di migliorare l'accuratezza della traduzione tramite la messa a punto con i dati bilingui. Tuttavia, hanno sperimentato solo rumeno! Traduzione inglese usando l'inglese BART. In questo articolo esaminiamo l'efficacia del BART giapponese utilizzando Japan Patent Office Corpus 2.0. I nostri esperimenti indicano che il giapponese BART può anche migliorare l'accuratezza della traduzione sia in giapponese coreano che in giapponese inglese.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>本稿では、第8回アジア翻訳ワークショップ（ Nakazawa et al., 2021 ）の特許業務（韓国語、日本語、英語、日本語）のために提出されたTMU神経機械翻訳（ NMT ）システムについて紹介する。最近、いくつかの研究では、モノリンガルデータを使用した事前に訓練されたエンコーダデコーダモデルが提案されています。事前に訓練されたモデルの一つであるBART （ Lewis et al., 2020 ）は、バイリンガルデータを用いた微調整を介して翻訳精度を向上させることが示された。しかし、彼らはルーマニア語のみを実験した！英語BARTを用いた英訳。本稿では、日本特許庁コーパス2.0を用いた日本語BARTの有効性について検討する。私たちの実験では、日本語のBARTは韓国語の日本語訳と英語の日本語訳の両方で翻訳の精度を向上させることもできることが示されています。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nang mapun iki, kéné gunakake sistem tanggal (NMT) sing nyimpen kanggo nggawe patent kanggo nganggo dolanan sing wis pitik (japongan karo japongan ingkang Korea) ning wis asai Workspace nang pitik-terjamahan asita (nakutawa et al, 2020). plug-in-action metadata Nanging, dheweke entuk-ingkang rumane mungkin ! Terjamahan Inggris nang nggambar LPRT. Nang pemilih iki, awak dhéwé isih bakal nggawe barang japon barang nggambar barang patent Ofis 2.0 nggambar barang japon. Awak dhéwé éntuk ngerti barang, barang Hapon barang bisa nggawe turuné sak pangan anyar tentang kanggo barang Hapon karo Perancis Inggris.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ამ დოკუნში ჩვენ ჩვენი TMU ნეიროლური მაქსინური განაცვლის (NMT) სისტემა, რომელიც აზიანეთის განაცვლის 8-ი სამუშაო სამუშაო სამუშაო (კორეული იაპონეთი და იაპონეთი წაპონეთ მიმდინარე, რამდენიმე კვლევები მოძლევა მონოლენგური მონაცემების გამოყენებით წინასწარმოვიდგინეთ კოდერების რეკოდერების მოდელები. ერთი მოდელეში, BART (Lewis et al., 2020) გამოჩვენებულია, რომ უფრო უფრო მეტად გადაწყვეტა წესიერება ორიენგური მონაცემებით. მაგრამ, ისინი ექსპერიმენტირებენ მხოლოდ პრომინული! English translation using English BART. ჩვენ ამ დოკუნში წაპონეთის BART-ის ეფექტიურობას გამოყენებთ წაპონეთის პეტენტის კოპუს 2.0. ჩვენი ექსპერიმენტები აჩვენებს, რომ იაპონური BART შეუძლია ასევე უფრო უფრო უფრო მეტადება თავისწორეული იაპონური და ანგლიური წონური</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Бұл қағазда, Азия аудармасының 8- ші жұмысының патенттік тапсырмасына (Корея жапон және жапон тілінде) жүйеңізді таңдап береміз. Жуырда бірнеше зерттеулерді бірнеше тілдік деректерді қолдану үшін алдын- оқылған кодер- декодер үлгілерін қолданылады. БаRT (Lewis et al., 2020) бағдарламасының бірінші оқылған үлгілері, екі тілі деректерді түзету арқылы аудармалардың дұрыстығын жақсарту үшін көрсетілді. Бірақ олар тек Румынша тәжірибеледі! Ағылшын BART қолданып ағылшын тілінің аудармасы. Бұл қағазда, Япония патенттер офисы Корпус 2.0 қолданатын Япония BART әрекетін тексереміз. Біздің тәжірибелеріміз жапон BART және Корея жапон және ағылшын жапон аудармаларында аудармалардың дұрыстығын жасай алады деп белгіледі.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>본고에서 제8회 아시아번역세미나(Nakazawa 등, 2021년)의 특허 임무(아사히와 영일)를 위해 제출한 TMU 신경기계번역(NMT) 시스템을 소개한다.최근 일부 연구에서는 단어 데이터를 사용하는 예비 트레이닝 인코더인 디코더 모델을 제시했다.BART(Lewis et al., 2020)는 사전 훈련을 거친 모델로 이중 언어 데이터를 미세하게 조정하여 번역 정밀도를 높일 수 있다.그러나 그들은 루마니아어만 시험했다!영어 BART를 사용하여 영어로 번역합니다.본고에서 우리는 일본 특허국 어료고 2.0을 이용하여 일본 BART의 유효성을 검증한다.우리의 실험은 일본어 BART도 한일과 영일 번역의 정확성을 높일 수 있음을 나타냈다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Šiame dokumente pristatome mūsų TMU neurologinių mašinų vertimo (NMT) sistemą, pateiktą 8-ojo seminaro „Azijos vertimas“ (Nakazawa ir kt., 2021 m.). Neseniai keliuose tyrimuose siūlomi iš anksto parengti kodavimo kodavimo modeliai, naudojantys vienkalbius duomenis. Įrodyta, kad vienas iš iš iš anksto apmokytų modelių, BART (Lewis et al., 2020), gerina vertimo tikslumą tiksliau derinant su dvikalbiais duomenimis. Tačiau jie eksperimentavo tik rumunų! English translation using English BART. Šiame dokumente nagrinėjame Japonijos BART veiksmingumą naudojant Japonijos patentų biurą Corpus 2.0. Mūsų eksperimentai rodo, kad Japonijos BART taip pat gali pagerinti vertimo tikslumą Korėjos japonų ir anglų kalbomis.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Неодамна, неколку студии предложија предобучени модели за декодирање на кодери користејќи монојазични податоци. Еден од предобучените модели, БАРТ (Lewis и други, 2020), покажа дека ја подобрува преведувачката точност преку финетизирање со двојјазични податоци. Сепак, тие експериментираа само романски! Англиски превод користејќи англиски BART. Во овој весник ја испитуваме ефикасноста на Јапонската Барт користејќи Јапонска патентска канцеларија Корпус 2.0. Нашите експерименти покажуваат дека јапонскиот Барт, исто така, може да ја подобри преведувачката точност на корејски јапонски и англиски јапонски преведувања.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ഈ പത്രത്തില്‍ ഞങ്ങള്‍ നമ്മുടെ ടിഎം യു ന്യൂറല്‍ മെഷീന്‍ പരിഭാഷ (NMT) സിസ്റ്റമിനെ പരിചയപ്പെടുത്തുന്നു. ഏഷ്യയിലെ പരിഭാഷണത്തിന്റെ എട്ടാം വര്‍ക്കാര്‍ക്ക് വേണ അടുത്തുതന്നെ, പല പഠനങ്ങളും മുമ്പ് പരിശീലിക്കപ്പെട്ട കോഡെര്‍ ഡെക്കോഡെര്‍ മോഡലുകള്‍ പരിശോധിച്ചു മുമ്പ് പരിശീലിക്കപ്പെട്ട മോഡലുകളില്‍ ഒരാള്‍ ബാര്‍ട്ടി (ലെവിസ് et al., 2020), രണ്ടു ഭാഷ വിവരങ്ങള്‍ മുഖേന പരിഭാഷപ്രകാരം മെച്ചപ്പെടു എന്നാലും, അവര്‍ റൊമാനിയന്‍ മാത്രം പരീക്ഷിച്ചു! ഇംഗ്ലീഷ് BART ഉപയോഗിച്ച് ഇംഗ്ലീഷ് പരിഭാഷ ഈ പത്രത്തില്‍, ജപ്പാന്‍ പാപ്റ്റന്റ് ഓഫീസ് 2.0 ഉപയോഗിച്ച് ജപ്പാന്‍ ഭാര്‍ട്ടിന്റെ പ്രഭാവം പരിശോധിക്കുന് Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Энэ цаасан дээр бид TMU мэдрэлийн машин хөгжүүлэх (NMT) системийг Азийн хөгжүүлэх (Nakazawa et al., 2021) 8-р ажлын Патент даалгаврын ажлын төлөө хийсэн. Саяхан олон судалгаанууд нэг хэлний өгөгдлийг ашиглаж сургалтын өмнө сургалтын кодлогч-декодлогч загварыг санал болгосон. БАРТ (Lewis et al., 2020) дасгал сургалтын нэг загвар нь хоёр хэлний өгөгдлийг сайжруулахын аргаар хөгжүүлсэн орнуудын тодорхойлолтыг сайжруулах боломжтой болсон. Гэхдээ тэд зөвхөн Румын зөвхөн туршилт хийсэн. Англи хэлний БАРТ ашиглаж Англи хэлний орчуулалт. Энэ цаасанд бид Японы Патент Оффис Корпус 2.0 ашиглан Япон Бартын үр дүнг шалгаж байна. Бидний туршилтууд Япон БАРТ нь Корейн Япон болон Англи Япон хэлбэрээр орчуулах зөв байдлыг илүү сайжруулж чадна.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dalam kertas ini, kami memperkenalkan sistem TMU Neural Machine Translation (NMT) kami dihantar untuk tugas Patent (Korea Jepun dan Inggeris Jepun) dari Workshop ke-8 tentang Translation Asia (Nakazawa et al., 2021). Baru-baru ini, beberapa kajian mencadangkan model pengekod-dekoder terlatih-terlatih menggunakan data monobahasa. Salah satu model pra-dilatih, BART (Lewis et al., 2020), dipaparkan untuk meningkatkan ketepatan terjemahan melalui penyesuaian dengan data dua bahasa. However, they experimented only Romanian! Terjemahan Bahasa Inggeris menggunakan BART Bahasa Inggeris. Dalam kertas ini, kami memeriksa kegunaan BART Jepun menggunakan Pejabat Paten Jepun Corpus 2.0. Eksperimen kami menunjukkan bahawa BART Jepun juga boleh meningkatkan ketepatan terjemahan dalam kedua-dua terjemahan Jepun Korea dan bahasa Inggeris Jepun.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>F’dan id-dokument, aħna nintroduċu s-sistema tagħna tat-Traduzzjoni tal-Magni Newrali tat-TMU (NMT) ippreżentata għall-kompitu tal-Privattivi (Ġappuniż Korean u Ġappuniż Ingliż) tat-8 Workshop dwar it-Traduzzjoni Asjatika (Nakazawa et al., 2021). Dan l-aħħar, diversi studji pproponu mudelli ta’ kodifikatur-dekoder imħarrġa minn qabel bl-użu ta’ dejta monolingwi. Wieħed mill-mudelli mħarrġa minn qabel, BART (Lewis et al., 2020), intwera li jtejjeb il-pre ċiżjoni tat-traduzzjoni permezz ta’ aġġustament fin b’dejta bilingwi. Madankollu, esperimentaw biss ir-Rumen! Traduzzjoni bl-Ingliż bl-użu tal-BART bl-Ingliż. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In dit artikel introduceren we ons TMU Neural Machine Translation (NMT) systeem dat is ingediend voor de Patenttaak (Koreaans Japans en Engels Japans) van 8e Workshop on Asian Translation (Nakazawa et al., 2021). Onlangs hebben verschillende studies voorgetrainde encoder-decoder modellen voorgesteld met eentalige gegevens. Een van de voorgetrainde modellen, BART (Lewis et al., 2020), bleek de vertaalnauwkeurigheid te verbeteren door finetuning met tweetalige gegevens. Ze experimenteerden echter alleen met Roemeens! Engelse vertaling met behulp van het Engels BART. In dit artikel onderzoeken we de effectiviteit van Japanse BART met behulp van Japan Patent Office Corpus 2.0. Uit onze experimenten blijkt dat Japans BART ook vertaalnauwkeurigheid kan verbeteren in zowel Koreaans Japans als Engels Japans vertalingen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I denne papiret introduserer vi vårt TMU Neural Machine Translation (NMT) system som er sendt til patentoppgåva (Koreansk og engelsk japansk) av 8. arbeidsområdet på Asian Translation (Nakazawa et al., 2021). Nyleg har fleire studier foreslått først trengte koderingsmodeller med monospråk- data. Ein av dei først trengte modelane, BART (Lewis et al., 2020), vert vist til å forbedra omsetjingskokretasjonen ved å finne opp med bilinguelt data. Men dei eksperimenterte berre romnisk! Engelsk omsetjing med engelsk BART. I denne papiret undersøker vi effektiviteten av japansk BART med Japan Patent Office Corpus 2.0. Våre eksperimenter tyder på at Japansk BART kan også forbedra omsetjingsakratitet i både Koreanske og engelske japanske omsetjingar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>W niniejszym artykule przedstawiamy nasz system neuronowego tłumaczenia maszynowego TMU (NMT) zgłoszony do zadania patentowego (koreański japoński i angielski japoński) VIII Warsztatu Tłumaczenia Azjatyckiego (Nakazawa et al., 2021). Ostatnio kilka badań zaproponowało wstępnie przeszkolone modele kodera-dekodera z wykorzystaniem danych jednojęzycznych. Wykazano, że jeden z wstępnie przeszkolonych modeli, BART (Lewis et al., 2020), poprawia dokładność tłumaczenia poprzez dostosowanie danych dwujęzycznych. Jednak eksperymentowali tylko po rumuńsku! Tłumaczenie angielskie przy użyciu angielskiego BART. W niniejszym artykule badamy skuteczność japońskiego BART przy użyciu Japońskiego Urzędu Patentowego Corpus 2.0. Nasze eksperymenty wskazują, że japoński BART może również poprawić dokładność tłumaczenia zarówno w koreańskim, jak i angielskim tłumaczeniu japońskim.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Neste artigo, apresentamos nosso sistema TMU Neural Machine Translation (NMT) enviado para a tarefa de Patentes (Japonês Coreano e Japonês Inglês) do 8º Workshop sobre Tradução Asiática (Nakazawa et al., 2021). Recentemente, vários estudos propuseram modelos codificadores-decodificadores pré-treinados usando dados monolíngues. Um dos modelos pré-treinados, o BART (Lewis et al., 2020), demonstrou melhorar a precisão da tradução por meio do ajuste fino com dados bilíngues. No entanto, eles experimentaram apenas a tradução romeno!inglês usando o inglês BART. Neste artigo, examinamos a eficácia do BART japonês usando o Japan Patent Office Corpus 2.0. Nossos experimentos indicam que o BART japonês também pode melhorar a precisão das traduções em coreano japonês e inglês japonês.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>În această lucrare, prezentăm sistemul nostru TMU Neural Machine Translation (NMT) depus pentru sarcina de brevetare (coreeană japoneză și engleză japoneză) al celui de-al 8-lea atelier de traducere asiatică (Nakazawa et al., 2021). Recent, mai multe studii au propus modele pre-instruite de encoder-decoder utilizând date monolingve. Unul dintre modelele pre-instruite, BART (Lewis et al., 2020), a demonstrat că îmbunătățește acuratețea traducerii prin ajustarea fină cu date bilingve. Cu toate acestea, au experimentat doar români! Traducere în engleză folosind limba engleză BART. În această lucrare, examinăm eficacitatea BART japoneză folosind Japan Patent Office Corpus 2.0. Experimentele noastre indică faptul că BART japoneză poate îmbunătăți precizia traducerii atât în traducerile japoneze coreene, cât și în engleză japoneză.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>В этой статье мы представляем нашу систему нейронного машинного перевода (НМП) TMU, представленную для патентной задачи (корейский японский и английский японский) 8-го семинара по азиатскому переводу (Nakazawa et al., 2021). Недавно в нескольких исследованиях были предложены предварительно обученные модели кодировщик-декодер с использованием одноязычных данных. Было показано, что одна из предварительно обученных моделей, BART (Lewis et al., 2020), повышает точность перевода за счет точной настройки с двуязычными данными. Однако они экспериментировали только на румынском языке!Английский перевод с использованием английского языка BART. В этой статье мы исследуем эффективность японского языка BART с использованием японского патентного ведомства Corpus 2.0. Наши эксперименты показывают, что японский БАРТ также может улучшить точность перевода как в корейском японском, так и в английском японском переводе.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>මේ පැත්තට, අපි අපේ TMU න්‍යුරල් මැෂින් අවවාදය (NMT) පද්ධතිය ප්‍රදේශය (කෝරියාන් ජාපානි සහ ඉංග්‍රීසි ජාපානි වලින්) 8ම වැඩසටහන් අ අවසානයෙන්, විශේෂ අධ්‍යානය ප්‍රීක්ෂණා කරලා තියෙන්නේ ප්‍රීක්ෂණා කරපු කෝඩෝර් ඩිකොඩර් ම ප්‍රධානය කරපු මොඩේල් එකක්, BART (Luis et al., 2020යි), පෙන්වන්න පුළුවන් විදිහට පරිවර්තන ක්‍රියාත්මක විශේෂය කරන්න පුළුවන ඒත් ඔවුන් රෝමානියාන් විතරයි පරීක්ෂා කරලා! ඉංග්‍රීසි BART භාවිතානය කරන්න ඉංග්‍රීසි භාවිතානය. මේ පත්තරේ අපි ජාපාන් පැටෙන්ට් කාර්පුස් 2.0 භාවිතා කරන්න ජාපාන් බාර්ට් එකේ ප්‍රශ්ණතාවක් පරීක් අපේ පරීක්ෂණය පෙන්වන්නේ ජාපාන් බාර්ට් වලින් කෝරියාන් ජාපානි වලින් ඉංග්‍රීසි ජාපානි වලි</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>V tem prispevku predstavljamo sistem TMU nevral strojnega prevajanja (NMT), ki je bil predložen za nalogo patenta (korejska japonščina in angleščina japonščina) 8. delavnice o azijskem prevajanju (Nakazawa et al., 2021). V zadnjem času je več študij predlagalo vnaprej usposobljene modele kodirnikov-dekoderjev z uporabo enojezičnih podatkov. Eden od vnaprej usposobljenih modelov, BART (Lewis et al., 2020), je izboljšal natančnost prevajanja prek finega nastavitve z dvojezičnimi podatki. Vendar so eksperimentirali samo romunsko! Angleški prevod z uporabo angleškega BART. V tem prispevku preučujemo učinkovitost japonskega BART z uporabo japonskega patentnega urada Corpus 2.0. Naši eksperimenti kažejo, da lahko japonski BART izboljša natančnost prevodov v korejski japonski in angleški japonski prevodi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Qoraalkan waxaan ku soo bandhignaa nidaamka tarjumaadda ee TMU Neural Machine (NMT) ee loo soo dhiibay shaqada bukaanka (Koreaniya Japanese iyo Ingiriis) oo ah 8aad Workshop on Translation Aasiya (Nakazawa et al., 2021). Muddii u dhowaad, waxbarasho badan ayaa lagu soo jeeday modelal koordiyuhu uu ku isticmaalayo macluumaad luuqad ah. Mid ka mid ah modellada horay loo tababaray, BART (Lewis et al., 2020), waxaa looga muujiyey inuu kordhiso saxda turjumista via fine-tuning with labada luuqadood. Si kastaba ha ahaatee, waxay jirrabeen Romanian oo keliya! Turjumista Ingiriis ee isticmaalaya Ingiriis BART. Warqadan waxaynu baaritaan waxyaabaha ay japaniya BART ku leeyihiin isticmaalka xafiiska bukaanka Japan Korpus 2.0. Imtixaanadayada waxay muujinayaan in Jabanees BART sidoo kale uu kordhin karo saxda turjumaadda ee Koreaniya iyo turjumaadda Ingiriis ee Jabanees.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Në këtë letër, ne prezantojmë sistemin tonë TMU Neural Machine Translation (NMT) të paraqitur për detyrën e patentave (Korean Japanese and English Japanese) të Workshop të 8-të mbi Translation Asian (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. Një nga modelet e paratrajnuar, BART (Lewis et al., 2020), u tregua se përmirëson saktësinë e përkthimit nëpërmjet rregullimit me të dhënat dygjuhëse. Megjithatë, ata eksperimentuan vetëm rumun! Përkthimi anglez duke përdorur anglisht BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Eksperimentet tona tregojnë se BART japonez mund gjithashtu të përmirësojë saktësinë e përkthimit në përkthimet japoneze dhe angleze.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>U ovom papiru predstavljamo naš sistem Neuralnog prevoda (NMT) TMU koji je predan za patentni zadatak (Korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno, nekoliko studija predložilo je predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazuje se da je poboljšao preciznost prevoda putem fino-tuniranja sa dvojezičkim podacima. Međutim, eksperimentirali su samo rumunski! Engleski prevod koristeći engleski BART. U ovom papiru pregledamo učinkovitost japanskog BART korištenja Japanske patentne kancelarije korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART takođe može poboljšati preciznost prevoda na korejskim japanskim i engleskim japanskim prevodima.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I denna uppsats presenterar vi vårt TMU Neural Machine Translation (NMT) system som lämnats in för patentuppgiften (koreansk japansk och engelsk japansk) i 8:e Workshop on Asian Translation (Nakazawa et al., 2021). Nyligen har flera studier föreslagit färdigutbildade encoder-avkodarmodeller med enspråkiga data. En av de förberedda modellerna, BART (Lewis et al., 2020), visade sig förbättra översättningens noggrannhet genom finjustering med tvåspråkiga data. Men de experimenterade bara rumänska! Engelska översättning med engelska BART. I denna uppsats undersöker vi effektiviteten av japansk BART med hjälp av Japan Patent Office Corpus 2.0. Våra experiment visar att japanska BART också kan förbättra översättningens noggrannhet i både koreanska japanska och engelska japanska översättningar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katika karatasi hii, tunautambulisha mfumo wetu wa Tafsiri ya Mashine ya Kifaransa (NMT) uliotolewa kwa ajili ya kazi ya Wazapani (Kijapani na Kiingereza) wa warsha ya 8 kuhusu Tafsiri ya Asia (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. Moja ya mifano ya zamani ya mafunzo, BART (Lewis et al., 2020), ilionyesha kuongeza uhakika wa tafsiri kwa kutumia taarifa za lugha mbili. Hata hivyo, walijaribu WaRomania pekee! Tafsiri ya Kiingereza kwa kutumia Kiingereza BART. Katika karatasi hii, tunachunguza ufanisi wa BART wa Japani kwa kutumia Ofisi ya Wagonjwa wa Japan 2.0. Majaribio yetu yanaonyesha kuwa BART ya Japani inaweza pia kuboresha ukweli wa tafsiri katika tafsiri za Kijapani na Kiingereza.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>இந்த காக்கியத்தில், நாம் எங்கள் டிஎம்யு நெருக்கர் இயந்திரம் மொழிபெயர்ப்பு (NMT) அமைப்பை குறிப்பிடுகிறோம் எட்டாவது ஆசிய மொழிபெயர்ப்பின் மொழிபெயர்ப்ப சமீபத்தில், பல ஆராய்ச்சிகள் மோனோலிங்கல் தரவை பயன்படுத்தி முன் பயிற்சி குறியீட்டு மாதிரிகளை பரிந்துரைக்கப முன்பயிற்சிக்கப்பட்ட மாதிரிகளில் ஒன்று, BART (லீவி et al., 2020), இரு மொழிகள் தரவுடன் மொழிபெயர்ப்பு சரியான தெளிவை மேம்படுத்துவதற்கு ஆனாலும், அவர்கள் ரோமானியன் மட்டும் சோதனைப்படுத்தினார்கள்! English translation using English BART. இந்த காகிதத்தில், நாம் ஜப்பான் பாதுகாப்பு அலுவலகம் 2.0 பயன்படுத்தி ஜாப்பான் பார்ட் விளைவுகளை பரிசோதி எங்கள் சோதனைகள் குறிப்பிடுகிறது ஜப்பானிய பார்ட் மொழிபெயர்ப்பு சரியை மேலும் மொழிபெயர்ப்பு தெளிவாக்க முடி</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu kagyzda, biz TMU NMT-iň näral Maşynyň terjimelerini (NMT) sistemamyzy Aziýa terjimelerinde 8-nji Iýpet bellenilýär (Nakazawa et al., 2021). Soňky wagtlar, birnäçe öňki bilim öňki arkalanmış kodeýan nusgalary monodil maglumaty ullanýar. BART (Lewis et al., 2020) öňündeki bilim sistemasynda terjime edilen hatlaryň dogrylygyny ýüzeltmek üçin görkezildi. Ýöne olar diňe rumunça synanyşdylar! Iňlisçe BART ulanan Iňlisçe terjime edildi. Bu kagyzda Japon Patent Ofis Korpus 2.0 ulanarak Japon BART'yň etkinliýetini barlap bardyk. Biziň deneylerimiz Japon BART-yň hem Koreýan we Iňlis dilinde terjime edilmegiň dogrylygyny gowurap biljekdigini aýdýar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>اس کاغذ میں ہم نے اپنی TMU نیورال ماشین ترجمہ (NMT) سیستم کو آسیا ترجمہ کے 8م کارشاپ کے لئے پیش کیا ہے۔ اچھا، بہت سی تحقیقات پیش آموزش کی پیش آموزش دی گئی ایک زبان دکھانے کے مطابق ایک کوڈر-ڈیکوڈر موڈل پیش کیے گئے ہیں. پہلے آموزش کی مدل میں سے ایک BART (Lewis et al., 2020) کو دکھایا گیا تھا کہ دو زبان اولاد کے ذریعہ مطابق ترجمہ کی دقیقیقیت کو بہتر کرنے کے لئے۔ لیکن وہ صرف رومانی آزمائش کرتے ہیں۔ انگلیسی BART کے مطابق انگلیسی ترجمہ. اس کاغذ میں، ہم جاپانی پٹینٹ ऑفیس کورپوس 2.0 کے مطابق جاپانی برٹ کی فعالیت کی تحقیق کرتے ہیں. ہماری آزمائش نشان دیتی ہے کہ جاپانی BART بھی کوریا ژاپنی اور انگلیسی ژاپنی ترجمہ میں ترجمہ دقیق بھی بہتر کر سکتا ہے.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu hujjatda biz Asiy tarjima daftarining 8- toʻplami (Nakazawa et al, 2021) uchun Patent vazifasi (Koriya Yaponiya va Ingliz Yaponcha) uchun tarjima qiladigan TMU Neural Mashine tarjima (NMT) tizimini ko'rsamiz. Yaqinda ko'pchilik o'rganishlar monolingual maʼlumot yordamida bir necha ta'minlovchi kodekoder modellarini talab qiladi. Birinchi taʼminlovchi modellardan biri BART (Lewis et al., 2020), ikkita tillar yordamida tarjima tayyorligini oshirish mumkin. Lekin, улар фақат Румий синовчи бўлган. Inglizcha tarjima qilindiName In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Bizning imtiyozlarimiz, Yaponcha BART xitoycha Japoniya va Ingliz Japoniya tarjimalarining ikkita tarjimalarini oshirish imkoniyatini oshirish mumkin.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Trong tờ giấy này, chúng tôi xin giới thiệu hệ thống dịch máy thần kinh TMU (NMB) được giao cho công việc sáng chế (Nhật Bản Hàn Quốc và Anh Quốc) của 8th Workshop in Asian Translation (Nakazawa et al., 2021). Gần đây, nhiều nghiên cứu đề xuất mô hình mã hóa đã được đào tạo. Một trong những mô hình được huấn luyện trước, BART (Lewis et al., 2020) đã được cho thấy cải thiện độ chính xác của dịch qua việc tinh chỉnh hai thứ bằng độ chính xác. Tuy nhiên, họ chỉ thử nghiệm Romani! Dịch bằng tiếng Anh BART. Trong tờ giấy này, chúng tôi kiểm tra hiệu quả của Nhật BART sử dụng Nhật bản sáng chế Corpus 2.0. Những thí nghiệm của chúng tôi cho thấy rằng bên Nhật BART cũng có thể cải thiện độ chính xác dịch trong cả tiếng Nhật Triều Tiên và Anh.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>本文引为第8届亚洲译研讨会专利(韩语日语与英语日语)交TMU神经机器翻译(NMT)系统(Nakazawa等,2021)。 近者,几项讲求用单语数预训练编码器 - 解码器模形。 其一先训者 BART(Lewis 等,2020 年)证可因双语数以重译准确性。 但试之罗马尼亚语! 用英语 BART 英语翻译。 本文,究日本专利局用日本专利局语料库2.0日本BART有效性。 吾实验之明,日语BART可以崇韩语日语英语日语翻译之准确性。</span></div></div><dl><dt>Anthology ID:</dt><dd>2021.wat-1.13</dd><dt>Volume:</dt><dd><a href=/volumes/2021.wat-1/>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/ijcnlp/>IJCNLP</a>
| <a href=/venues/wat/>WAT</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>133–137</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.wat-1.13>https://aclanthology.org/2021.wat-1.13</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2021.wat-1.13 title="To the current version of the paper by DOI">10.18653/v1/2021.wat-1.13</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">kim-komachi-2021-tmu</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Hwichan Kim and Mamoru Komachi. 2021. <a href=https://aclanthology.org/2021.wat-1.13>TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021</a>. In <i>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</i>, pages 133–137, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2021.wat-1.13>TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021</a> (Kim & Komachi, WAT 2021)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.wat-1.13.pdf>https://aclanthology.org/2021.wat-1.13.pdf</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.wat-1.13.pdf title="Open PDF of 'TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=TMU+NMT+System+with+Japanese+BART+for+the+Patent+task+of+WAT+2021TMU+NMT+System+with+Japanese+BART+for+the+Patent+task+of+WAT+2021" title="Search for 'TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021](https://aclanthology.org/2021.wat-1.13) (Kim & Komachi, WAT 2021)</p><ul class=mt-2><li><a href=https://aclanthology.org/2021.wat-1.13>TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021</a> (Kim & Komachi, WAT 2021)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Hwichan Kim and Mamoru Komachi. 2021. <a href=https://aclanthology.org/2021.wat-1.13>TMU NMT System with Japanese BART for the Patent task of WAT 2021TMU NMT System with Japanese BART for the Patent task of WAT 2021</a>. In <i>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</i>, pages 133–137, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>