<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Differentially Private Language Models Benefit from Public Pre-training - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Differentially Private Language Models Benefit from Public Pre-training" name=citation_title><meta content="Gavin Kerrigan" name=citation_author><meta content="Dylan Slack" name=citation_author><meta content="Jens Tuyls" name=citation_author><meta content="Proceedings of the Second Workshop on Privacy in NLP" name=citation_conference_title><meta content="2020/11" name=citation_publication_date><meta content="https://aclanthology.org/2020.privatenlp-1.5.pdf" name=citation_pdf_url><meta content="39" name=citation_firstpage><meta content="45" name=citation_lastpage><meta content="10.18653/v1/2020.privatenlp-1.5" name=citation_doi><meta property="og:title" content="Differentially Private Language Models Benefit from Public Pre-training"><meta property="og:image" content="https://aclanthology.org/thumb/2020.privatenlp-1.5.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.privatenlp-1.5"><meta property="og:description" content="Gavin Kerrigan, Dylan Slack, Jens Tuyls. Proceedings of the Second Workshop on Privacy in NLP. 2020."><link rel=canonical href=https://aclanthology.org/2020.privatenlp-1.5></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Differentially Private Language Models Benefit from Public Pre-training</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Verskillende Private Taal Modelle Benefinieerd van Publieke Voorvoering</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>የግል ቋንቋ ሞዴል</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>تستفيد النماذج اللغوية الخاصة التفاضلية من التدريب المسبق العام</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Q칲dr톛tli 칐n-t톛hsilind톛n faydalan캼r</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Диференциално частните езикови модели се възползват от публичното предобучение</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>পাবলিক প্রশিক্ষণের পূর্ব থেকে প্রাইভেট ভাষা মোডেল</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>སྤྱི་ཚོགས་གྱི་སྔོན་ལྟར་གྱི་སྒེར་གྱི་སྐད་ཆ་དབྱིབས་རིགས་ཀྱི་རྣམ་པ</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Različiti privatni jezički modeli koristi od javne predobuke</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Models de llenguatge diferencialment privat beneficien de la pré-capacitació pública</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Odlišně soukromé jazykové modely těží z veřejného předškolení</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Differentielt private sprogmodeller drager fordel af offentlig foruddannelse</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Unterschiedlich private Sprachmodelle profitieren von öffentlichen Vortrainings</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Διαφορετικά ιδιωτικά γλωσσικά μοντέλα επωφελούνται από τη δημόσια προεκπαίδευση</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Los modelos lingüísticos diferencialmente privados se benefician de la capacitación previa pública</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Erinevalt erakeele mudelid saavad kasu avalikust eelkoolitusest</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>مدل های مختلف زبان خصوصی استفاده از پیش آموزش عمومی</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Eri yksityiset kielimallit hyötyvät julkisesta esikoulutuksesta</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Les modèles linguistiques privés bénéficient différemment de la pré-formation publique</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Baineann Múnlaí Teanga Príobháideacha Difreálacha tairbhe as Réamhoiliúint Phoiblí</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Phonon:: MMF:: EffectFactory</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>מודלים שפות פרטיים שונים, תועלת מהאימון הקדמי הציבורי</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>विभेदक रूप से निजी भाषा मॉडल सार्वजनिक पूर्व प्रशिक्षण से लाभ</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Različiti privatni jezički modeli koristi od javnog predobuka</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>A különböző magánnyelvi modellek előnyei a nyilvános előképzésből</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Համաշխարհային նախապատրաստման առավելությունը</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Berbeda-beda Model Bahasa Pribadi Benefitnya dari Pre-pelatihan Publik</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Modelli linguistici differentemente privati Beneficiare della pre-formazione pubblica</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>公共の事前トレーニングの恩恵を受ける差別的私立言語モデル</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>politenessoffpolite"), and when there is a change ("assertive</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>განსხვავებული პირადი ენის მოდელების გამოსახულება სახელსაწყისი წინატრიქციისგან</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Жалпы алдындағы жеке тіл үлгілерінің пайдалануы</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>서로 다른 개인 언어 모델이 공공 예비 교육에서 이익을 얻다</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Įvairių privačių kalbų modelių nauda iš viešojo parengiamojo mokymo</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Друференцијално приватни јазички модели користат од јавното преобука</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>വ്യത്യസ്ത്രീയ ഭാഷ മോഡലുകളില്‍ നിന്നും പൊതു പരിശീലനത്തില്‍ നിന്നും</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Хувь хэл загваруудыг олон нийтийн өмнөх сургалтын ашиглах</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Mudah Model Bahasa Pribadi Dari Latihan Awam</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Benefiċċju ta’ mudelli ta’ lingwa privata b’mod differenzjat minn taħriġ pubbliku minn qabel</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Verschillende private taalmodellen profiteren van openbare pre-training</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Forskjellig privat språk- modeller nyttar frå offentleg føreøving</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Różnicowo prywatne modele językowe korzystają z publicznego przedszkolenia</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Modelos de linguagem diferencialmente privados se beneficiam do pré-treinamento público</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Modelele lingvistice diferenţial private beneficiază de pregătirea publică</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Различные модели частного языка получают выгоду от государственного предварительного обучения</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>වෙනස් පුද්ගලික භාෂා මොඩේල් ප්‍රශ්නයක් ප්‍රශ්නයක් සාමාන්‍ය ප්‍රධාන ප්‍රශ්නයක්</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Različno zasebni jezikovni modeli imajo koristi od javnega predusposabljanja</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Kaalmada waxbarashada dadweynaha</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Modelet e gjuhës private në mënyrë të ndryshme përfitojnë nga parastërvitja publike</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Razlièno privatne jezičke modele koristi od javne predobuke</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Olika privata språkmodeller Dra nytta av offentlig fortbildning</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Kitofauti cha Modeli za Lugha Binafsi Utetezi kutoka mafunzo ya Umma</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>பொது முன் பயிற்சியிலிருந்து வேறு தனிப்பட்ட மொழி மாதிரிகளின் உதவி</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Aýratyn gizli diller</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>مختلف مختلف خصوصی زبان موڈل جن کی پیشتربینی سے فائدہ اٹھائی جاتی ہے</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>KCharselect unicode block name</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.privatenlp-1.5.pdf>差异化私语模形受益于公共预培训</a></h2><p class=lead><a href=/people/g/gavin-kerrigan/>Gavin Kerrigan</a>,
<a href=/people/d/dylan-slack/>Dylan Slack</a>,
<a href=/people/j/jens-tuyls/>Jens Tuyls</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Language modeling is a keystone task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. When training a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> on <a href=https://en.wikipedia.org/wiki/Information_sensitivity>sensitive information</a>, differential privacy (DP) allows us to quantify the degree to which our <a href=https://en.wikipedia.org/wiki/Personal_data>private data</a> is protected. However, training algorithms which enforce <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> often lead to degradation in model quality. We study the feasibility of learning a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> possible.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Taal modellering is 'n sleutelbestel taak in natuurlike taal verwerking. Wanneer 'n taal model op sensitiewe inligting onderwerp, laat ons verskillende privateit (DP) toe om die grad waarin ons privaat data beskerm word om te kvantifiseer. Maar onderwerp algoritme wat verskillende privaatheid vervolg het, dikwels lei na degradasie in model kwaliteit. Ons studeer die feilikheid van die leer van 'n taal model wat simultaanlik hoë-kwaliteit en privateit bewaar word deur 'n publieke basis model op 'n privaat korpus te stel. Ons vind dat DP fyn-tuning die prestasie van taal modele in die privaat domein verhoog, maak die opvoering van sodanige modele moontlik.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ቋንቋ ምረጡ የፊደል ቋንቋ ማቀናጃ ነው፡፡ የቋንቋ ምሳሌ በተለየ መረጃ ላይ ባስተማርን ጊዜ የልዩ የግል ግለፅ (DP) የግል መረጃችን የሚጠበቀበትን ክፍተት ማረጋገጥን ይፈቅዳል፡፡ ነገር ግን የግዛት ብሔራዊነት የሚያስፈልገውን አሌጎርቲም ብዙ ጊዜ በሞዴል ጥሩ ላይ የሚያሳፍር ነው፡፡ የቋንቋን ምሳሌ ለመማር ስልጣን እናስተምራለን፡፡ የDP ደብዳቤ የቋንቋ ምሳሌዎችን የግል ድምፅ ማድረግ እናገኘዋለን፤ እንደዚህ ዓይነቶች ማስተምር የሚችል ነው፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>نمذجة اللغة مهمة أساسية في معالجة اللغة الطبيعية. عند تدريب نموذج لغوي على المعلومات الحساسة ، تتيح لنا الخصوصية التفاضلية (DP) تحديد درجة حماية بياناتنا الخاصة. ومع ذلك ، غالبًا ما تؤدي خوارزميات التدريب التي تفرض الخصوصية التفاضلية إلى تدهور جودة النموذج. ندرس جدوى تعلم نموذج لغة يتميز بجودة عالية ويحافظ على الخصوصية في نفس الوقت من خلال ضبط نموذج قاعدة عامة على مجموعة خاصة. نجد أن الضبط الدقيق لـ DP يعزز أداء نماذج اللغة في المجال الخاص ، مما يجعل تدريب مثل هذه النماذج ممكنًا.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dil modelləri təbii dil işləməsində anahtar taş işləridir. Sensitif məlumatlar haqqında dil modelini təhsil etdikdə, fərqli xüsusiyyət (DP) bizə xüsusi məlumatlarımızın qorunub saxlanıldığı dərəcəsini qədər təhsil edir. Ancaq, fərqli təhsil təhsil etmək üçün müxtəlif təhsil algorizmi modellərin keyfiyyətində degradasyona yol açar. Biz bir dil modelini öyrənmək mümkünlüyünü təhsil edirik ki, eyni zamanda çox yüksək kaliteli və gizli təhlükəsizlik təhlükəsizlik edir, özgür korpus üzərində halkı baz modelini düzenləyirik. Bu modellərin təhsil edilməsini mümkün edərək DP-nin düzgün təhsil modellərinin performansını özlərinə kömək edir.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Езиковото моделиране е ключова задача в обработката на естествения език. Когато обучаваме езиков модел за чувствителна информация, диференциалната поверителност (ДП) ни позволява да определим количествено степента, до която нашите лични данни са защитени. Обаче алгоритмите за обучение, които налагат диференциалната поверителност, често водят до влошаване на качеството на модела. Изследваме осъществимостта на изучаване на езиков модел, който едновременно е висококачествен и запазва неприкосновеността на личния живот чрез настройване на модел на публична база върху частен корпус. Установяваме, че фината настройка на ДП подобрява ефективността на езиковите модели в частната сфера, което прави обучението на такива модели възможно.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ভাষা মডেলিং হচ্ছে প্রাকৃতিক ভাষা প্রক্রিয়ায় একটি কী পাথর কাজ। যখন সংবেদনশীল তথ্য সম্পর্কে ভাষার মডেল প্রশিক্ষণ করা হয়, তখন বৈচিত্র্যকভাবে গোপনীয়তা (ডিপিপি) আমাদের সুযোগ দেয় যে ডিগ্রি যে ডি However, training algorithms which enforce differential privacy often lead to degradation in model quality. আমরা একটি ভাষার মডেল শিখার ব্যাপারে গবেষণা করছি যা একই সাথে উচ্চমান এবং গোপনীয়তা রক্ষা করে একটি ব্যক্তিগত কোর্পাসে একটি পাবলিক ভেস মডেল প্রদান আমরা খুঁজে পাচ্ছি যে ডিপি ভাষার মডেল বাড়িয়ে দিচ্ছে, ব্যক্তিগত ডোমেইনে ভাষার মডেলের প্রতিষ্ঠান বাড়িয়ে দিয়েছে, য</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>སྐད་རིགས་མ་དབྱིབས་བཟོ་བྱས་ནི་རང་བཞིན་པའི་སྐད་རིགས་ཀྱི་ལས་འགུལ་ལྡན་དག་ཆེ་བ་ཞིག་རེད། སྐད་རིགས་ཀྱི་མ་དབྱིབས་དཔྱད་དབྱིབས་ཆེན་དང་སྤྱིར་བཏང་བའི་གསལ་བཤད་ཀྱི་ཆ་འཕྲིན་(DP)དེ་གིས་ང་ཚོའི་སྒེར་གྱི་ཆ་འཕྲིན ཡིན་ནའང་། སྒེར་དབང་གི་སྒེར་ཚུལ་སྔོན་སྒྲིག་གི་སྒྲིག་སྟངས་རྣམས་མེད་དུ་བསམ་མཐུན་རྐྱེན་སྐྱིད་དང་། ང་ཚོས་སྐད་ཡིག་གི་མ་དབྱིབས་དཔྱད་པར་ལྟ་བུ་མཐུན་ནུས་མེད་པར་མཐུན་རིམ་དང་རང་དབང་སྒེར་གྱི་རིགས ང་ཚོས་DP་ལྟ་བུའི་མཐུན་བཟོ་བྱས་ན་སྒེར་གྱི་dominio་ནང་གི་སྐད་ཡིག་ཆའི་མིག་དཔེ་དབུས་ཡོད་ཚད་ལྟར་ཡར་སྒྲིག</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modeliranje jezika je ključni zadatak u procesu prirodnog jezika. Kada obučavamo jezički model o osjetljivim informacijama, diferencijalna privatnost (DP) nam omogućava da kvantificiramo stupnju zaštite naših privatnih podataka. Međutim, algoritmi obuke koji primjenjuju diferencijalnu privatnost često vode do degradacije u kvaliteti modela. Proučavamo mogućnost učenja jezičkog model a koji je istovremeno visoka kvaliteta i privatnost očuvajući, uspostavljajući javni model baze na privatnom korpusu. Nalazimo da je DP-ova fino-tuning povećala učinkovitost jezičkih modela u privatnom domenu, čineći mogućnost obuke takvih modela.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La modelació de llengües és una tasca clau en el processament natural de llengües. Quan entrenem un model de llenguatge en informació sensible, la privacitat diferencial (DP) ens permet quantificar el grau en què es protegeixen les nostres dades privades. Però els algoritmes de formació que enforten la privacitat diferencial sovint provoquen degradació en la qualitat del model. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. Trobem que la perfeccionació del DP augmenta el rendiment dels models de llenguatge en el domini privat, fent possible l'entrenament d'aquests models.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Jazykové modelování je klíčovým úkolem ve zpracování přirozeného jazyka. Při školení jazykového modelu citlivých informací nám diferenciální ochrana osobních údajů (DP) umožňuje kvantifikovat stupeň ochrany našich soukromých údajů. Nicméně tréninkové algoritmy, které vynucují diferenciální soukromí, často vedou ke zhoršení kvality modelu. Studujeme proveditelnost učení se jazykového modelu, který je současně vysoce kvalitní a zachovává soukromí laděním veřejného základního modelu na soukromý korpus. Zjišťujeme, že DP jemné ladění zvyšuje výkon jazykových modelů v soukromé doméně, což umožňuje trénink těchto modelů.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sprogmodellering er en nøgleopgave i naturlig sprogbehandling. Når vi træner en sprogmodel om følsomme oplysninger, giver differentieret privatliv (DP) os mulighed for at kvantificere, i hvilket omfang vores private data er beskyttet. Men træningsalgoritmer, der håndhæver differentieret privatliv, fører ofte til forringelse af modelkvaliteten. Vi undersøger muligheden for at lære en sprogmodel, der samtidig er af høj kvalitet og beskyttelse af privatlivets fred ved at justere en offentlig grundmodel på et privat korpus. Vi finder ud af, at DP finjustering øger ydeevnen af sprogmodeller i det private domæne, hvilket gør uddannelse af sådanne modeller mulig.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sprachmodellierung ist eine Schlüsselaufgabe in der Verarbeitung natürlicher Sprache. Beim Training eines Sprachmodells zu sensiblen Informationen ermöglicht Differential Privacy (DP) es uns, den Grad zu quantifizieren, in dem unsere privaten Daten geschützt sind. Trainingsalgorithmen, die differentielle Privatsphäre erzwingen, führen jedoch häufig zu einer Verschlechterung der Modellqualität. Wir untersuchen die Machbarkeit, ein Sprachmodell zu lernen, das gleichzeitig qualitativ hochwertig und privat ist, indem wir ein öffentliches Basismodell auf einem privaten Korpus abstimmen. Wir stellen fest, dass DP-Feinabstimmung die Leistung von Sprachmodellen im privaten Bereich steigert und das Training solcher Modelle ermöglicht.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Η γλωσσική μοντελοποίηση είναι μια βασική εργασία στην επεξεργασία φυσικής γλώσσας. Όταν εκπαιδεύουμε ένα γλωσσικό μοντέλο για ευαίσθητες πληροφορίες, η διαφορική ιδιωτικότητα (DP) μας επιτρέπει να ποσοτικοποιήσουμε τον βαθμό στον οποίο προστατεύονται τα προσωπικά μας δεδομένα. Ωστόσο, οι αλγόριθμοι κατάρτισης που επιβάλλουν τη διαφορετική ιδιωτικότητα συχνά οδηγούν σε υποβάθμιση της ποιότητας των μοντέλων. Μελετάμε τη σκοπιμότητα της εκμάθησης ενός γλωσσικού μοντέλου που είναι ταυτόχρονα υψηλής ποιότητας και διαφυλάσσεται της ιδιωτικής ζωής, συντονίζοντας ένα μοντέλο δημόσιας βάσης σε ένα ιδιωτικό σώμα. Διαπιστώνουμε ότι ο συντονισμός DP ενισχύει την απόδοση των γλωσσικών μοντέλων στον ιδιωτικό τομέα, καθιστώντας δυνατή την εκπαίδευση τέτοιων μοντέλων.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>El modelado del lenguaje es una tarea clave en el procesamiento del lenguaje natural. Al entrenar un modelo lingüístico sobre información confidencial, la privacidad diferencial (DP) nos permite cuantificar el grado de protección de nuestros datos privados. Sin embargo, los algoritmos de entrenamiento que aplican la privacidad diferencial a menudo conducen a la degradación de la calidad del modelo. Estudiamos la viabilidad de aprender un modelo lingüístico que sea simultáneamente de alta calidad y preserve la privacidad ajustando un modelo de base pública en un corpus privado. Observamos que el ajuste fino del PD aumenta el rendimiento de los modelos lingüísticos en el dominio privado, haciendo posible la formación de dichos modelos.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Keele modelleerimine on loodusliku keele töötlemise keskne ülesanne. Tundliku teabe keelemudeli koolitamisel võimaldab diferentsiaalne privaatsus (DP) meil kvantifitseerida, mil määral meie isikuandmeid kaitstakse. Erinevat privaatsust rakendavad koolitusalgoritmid põhjustavad sageli mudeli kvaliteedi halvenemist. Uurime samaaegselt kvaliteetse ja privaatsuse säilitava keelemudeli õppimise teostatavust, häälestades avaliku baasi mudeli erakorpusele. Leiame, et DP peenhäälestus suurendab keelemudelite jõudlust eravaldkonnas, muutes selliste mudelite koolitamise võimalikuks.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>مدل زبان یک کار سنگ کلید در پردازش زبان طبیعی است. وقتی یک مدل زبانی در مورد اطلاعات حساسی آموزش می‌کند، خصوصی مختلف (DP) اجازه می‌دهد که درجه‌ای که داده‌های خصوصی ما در آن محافظت می‌شود مقدار تعداد کنیم. ولی الگوریتم‌های آموزش که محرمانیت متفاوتی را اجرا می‌کنند، اغلب به نابودی در کیفیت مدل می‌رسد. ما قابلیت یادگیری مدل زبانی را مطالعه می کنیم که همزمان کیفیت بالا و خصوصی محافظت می کند، با تنظیم یک مدل پایگاه عمومی روی یک جسد خصوصی. ما پیدا می‌کنیم که دانشگاه نیک تنظیم نمونه‌های زبان را در دامنه خصوصی بیشتر می‌دهد، و آموزش این مدل‌ها را امکان می‌دهد.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kielimuodostus on keskeinen tehtävä luonnollisen kielen käsittelyssä. Kun koulutamme kielimallia arkaluonteisista tiedoista, differential privacy (DP) antaa meille mahdollisuuden kvantifioida, missä määrin yksityisiä tietojamme suojataan. Erilaista yksityisyyttä vahvistavat koulutusalgoritmit johtavat kuitenkin usein mallin laadun heikkenemiseen. Tutkimme mahdollisuutta oppia samanaikaisesti laadukasta ja yksityisyyttä säilyttävää kielimallia virittämällä julkisen perusmallin yksityiseen korpuseen. Havaitsemme, että DP hienosäätö parantaa kielimallien suorituskykyä yksityisellä alalla, mikä mahdollistaa tällaisten mallien koulutuksen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La modélisation du langage est une tâche clé dans le traitement du langage naturel. Lors de la formation d'un modèle linguistique sur des informations sensibles, la confidentialité différentielle (DP) nous permet de quantifier le degré de protection de nos données privées. Cependant, les algorithmes d'apprentissage qui appliquent la confidentialité différentielle entraînent souvent une dégradation de la qualité du modèle. Nous étudions la possibilité d'apprendre un modèle linguistique qui soit à la fois de haute qualité et de préservation de la vie privée en ajustant un modèle de base publique sur un corpus privé. Nous constatons que le réglage fin du Programme du diplôme améliore les performances des modèles linguistiques dans le domaine privé, ce qui rend possible la formation de tels modèles.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Príomhthasc i bpróiseáil nádúrtha teanga is ea samhaltú teanga. Nuair a chuirtear oiliúint ar mhúnla teanga ar fhaisnéis íogair, ligeann príobháideacht dhifreálach (DP) dúinn a mhéid a chosnaítear ár sonraí príobháideacha a chainníochtú. Mar sin féin, is minic go dtiocfaidh díghrádú ar cháilíocht na samhla as halgartaim oiliúna a fhorfheidhmíonn príobháideacht dhifreálach. Déanaimid staidéar ar an bhféidearthacht atá ann múnla teanga a fhoghlaim atá ar ardchaighdeán agus ag caomhnú príobháideachta ag an am céanna trí mhúnla bunáit phoiblí a oiriúnú ar chorpas príobháideach. Feictear dúinn go gcuireann mionchoigeartú DP le feidhmíocht na múnlaí teanga sa réimse príobháideach, rud a fhágann gur féidir na samhlacha sin a oiliúint.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Masana'antar harshe na zama wani aikin maɓalli cikin shirin aiki na fassarar harshe. Idan an kõre wani misali na harshe a kan information mai fasahawa, yana yarda mu ƙayyade tsarin daraja da za'a tsare data masu kanana da shi. A lokacin da, algoritori waɗanda ke lazimtar da farafinka masu ɓarna, ko da yawa, yana ƙara wa taƙaitarwa a cikin sifar misali. Tuna karanta abincin da za'a sanar da wani misalin harshe wanda ke sami sami da tsari mai nauyi da farat ɗaya da za'a tsare wani misalin jama'a a kan wani matabbaci na faransa. Tuna gane cewa DD na ƙara cikakken misãlai na harshe a cikin wuri farat ɗaya, kuma ya sami tsarin misãlai masu yiwuwa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>דוגמניות שפות היא משימה אבן מפתח בעבודת שפות טבעית. כשאימונים מודל שפה על מידע רגיש, פרטיות דיפרנציאלית (DP) מאפשרת לנו לקוונטיב את התמונה שבה הנתונים הפרטיים שלנו מוגנים. עם זאת, אלגוריתמים אימונים שמכריחים פרטיות דיפרנציאלית לעתים קרובות מובילים לשפלה באיכות מודל. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>भाषा मॉडलिंग प्राकृतिक भाषा प्रसंस्करण में एक कीस्टोन कार्य है। संवेदनशील जानकारी पर एक भाषा मॉडल को प्रशिक्षित करते समय, विभेदक गोपनीयता (डीपी) हमें उस डिग्री को मापने की अनुमति देती है जिस पर हमारे निजी डेटा की रक्षा की जाती है। हालांकि, प्रशिक्षण एल्गोरिदम जो विभेदक गोपनीयता को लागू करते हैं, अक्सर मॉडल की गुणवत्ता में गिरावट का कारण बनते हैं। हम एक भाषा मॉडल सीखने की व्यवहार्यता का अध्ययन करते हैं जो एक साथ एक निजी कॉर्पस पर एक सार्वजनिक आधार मॉडल को ट्यून करके उच्च गुणवत्ता और गोपनीयता संरक्षण है। हम पाते हैं कि डीपी ठीक ट्यूनिंग निजी डोमेन में भाषा मॉडल के प्रदर्शन को बढ़ाती है, जिससे ऐसे मॉडलों का प्रशिक्षण संभव हो जाता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modeliranje jezika je ključni zadatak u procesu prirodnog jezika. Kada obučavamo jezički model o osjetljivim informacijama, diferencijalna privatnost (DP) nam omogućava kvantificirati stupnju zaštite naših privatnih podataka. Međutim, algoritmi obuke koji primjenjuju diferencijalnu privatnost često vode do degradacije u kvaliteti modela. Proučavamo mogućnost učenja jezičkog model a koja je istovremeno visoka kvaliteta i privatnost očuvajući, uspostavljajući javni model baze na privatnom korpusu. Pronašli smo da se DP-ova finalna prilagodba povećava učinkovitost jezičkih modela u privatnom domenu, čineći mogućnost obuke takvih modela.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A nyelvmodellezés kulcsfontosságú feladat a természetes nyelv feldolgozásában. Az érzékeny információkra vonatkozó nyelvi modellek képzésekor a differenciális adatvédelem (DP) lehetővé teszi számunkra, hogy számszerűsítsük a személyes adataink védelmének mértékét. Ugyanakkor a differenciális adatvédelmet érvényesítő képzési algoritmusok gyakran a modellminőség romlásához vezetnek. Tanulmányozzuk egy olyan nyelvi modell elsajátításának megvalósíthatóságát, amely egyidejűleg magas színvonalú és a magánélet megőrzését biztosítja egy nyilvános alapmodell magánkorpuszra történő hangolásával. Úgy találjuk, hogy a DP finomhangolás növeli a nyelvi modellek teljesítményét a magánterületen, lehetővé téve az ilyen modellek képzését.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Լեզու մոդելը գլխավոր խնդիր է բնական լեզուների վերամշակում: Երբ լեզվի մոդելը սովորեցնում է զգայուն ինֆորմացիայի վերաբերյալ, տարբերակային գաղտնիությունը (ԴՊ) թույլ է տալիս մեզ չափել այն քանակը, որի չափով են պաշտպանվում մեր անձնական տվյալ Այնուամենայնիվ, ալգորիթմները, որոնք պաշտպանում են տարբերակային գաղտնիքը, հաճախ հանգեցնում են մոդելի որակի դեգրադացիայի: Մենք ուսումնասիրում ենք լեզվի մոդելի սովորելու հնարավորությունը, որը միևնույն ժամանակ բարձր որակի և գաղտնիքի պահպանության միջոցով կազմակերպում է հասարակական հիմքի մոդելը սեփական կորպոսի վրա: Մենք հայտնաբերեցինք, որ ԴՊ-ի բարելավումը բարձրացնում է լեզվի մոդելների արտադրողությունը մասնավոր ոլորտում, դարձնելով հնարավոր այդ մոդելների ուսումնասիրությունը:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modeling bahasa adalah tugas batu kunci dalam proses bahasa alami. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. Namun, algoritma latihan yang memaksa privasi diferensial sering menyebabkan degradasi dalam kualitas model. Kami mempelajari feasibilitas belajar model bahasa yang secara bersamaan kualitas tinggi dan memelihara privasi dengan mengatur model dasar publik pada korpus pribadi. Kami menemukan bahwa penyesuaian DP meningkatkan prestasi model bahasa di domain pribadi, membuat pelatihan model tersebut mungkin.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La modellazione linguistica è un compito fondamentale nell'elaborazione del linguaggio naturale. Quando si addestra un modello linguistico sulle informazioni sensibili, la privacy differenziale (DP) ci consente di quantificare il grado di protezione dei nostri dati privati. Tuttavia, gli algoritmi di formazione che impongono la privacy differenziale spesso portano a un degrado della qualità del modello. Studiamo la fattibilità dell'apprendimento di un modello linguistico che sia allo stesso tempo di alta qualità e tutela della privacy sintonizzando un modello di base pubblica su un corpus privato. Troviamo che la messa a punto DP migliora le prestazioni dei modelli linguistici nel settore privato, rendendo possibile la formazione di tali modelli.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>言語モデリングは、自然言語処理における重要なタスクです。機密情報に関する言語モデルをトレーニングする場合、差分プライバシー（ DP ）により、プライベートデータがどの程度保護されているかを定量化できます。しかし、差分プライバシーを強制するトレーニングアルゴリズムは、多くの場合、モデルの品質の低下につながります。私たちは、プライベートコーパス上のパブリックベースモデルをチューニングすることによって、同時に高品質でプライバシーを保つ言語モデルを学習することの実現可能性を研究しています。DPの微調整は、プライベートドメインにおける言語モデルのパフォーマンスを向上させ、そのようなモデルのトレーニングを可能にすることがわかります。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tambah Ora kudu nggawe sistem sistem sing dibutuhke informasi susahe, nggambar perusahaan (PP) iso diangkat dhéwé kuwi tindang nggawe datain pribadi sing ngejaraké awak dhéwé. politenessoffpolite"), and when there is a change ("assertive Awak dhéwé ngerasah perbudhakan kanggo ngerasah model luwih dumadhi sak ngono nggawe barang-sistem sing ngewehi kuwi nggawe modèl kuwi nggawe barang pengguna nêmêr, kuwi nggawe gerakan akeh dumadhi sak kudu Awak dhéwé éntuk depan ngono nggawe model sing luwih nggawe barang pengguna kuwi, nik nggawe barang pengguna kuwi tindakan.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ენის მოდელირება არის სახელსაწყოთა ენის პროცესის კლავისტური დავალება. როცა ენის მოდელის შემწავლობა სიგრძნობითი ინფორმაციის შესახებ, განსხვავებული პრივიაცია (DP) მოგვიძლია ჩვენ კონტაქტირება სიგრძნობა, რომელიც ჩვენი პრივი მაგრამ განსწავლება ალგორიტემი, რომელიც განსხვავებული პრივისატების გამოყენებას ზოგიერთად მოდეგრადიაციას მოდეგრადის კაalitეში. ჩვენ ვისწავლით ენის მოდელის შესაძლებლობა, რომელიც ერთადერთად უფრო დიდი კაalitეტი და პრივიაციის შესაძლებლობა, რომელიც საშუალო ბაზის მოდელის შესაძლებლობად პრივიაციული კორ ჩვენ ვიფიქრობთ, რომ DP-ს სწორად კონფიგურაცია იქნება ენის მოდელების გამოსახულებას პირადი დემონიში, რომლებიც ასეთი მოდელების შესაძლებლობა შექმნა.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Тіл моделі - табиғи тілді өңдеу үшін перне тапсырмасы. Тілді мәлімет үлгісін оқыту кезінде, әртүрлі жеке мәліметтеріміздің қай деңгейіне қорғалған градусын анықтауға мүмкіндік береді. Бірақ айырмашылық жұмыс істеу алгоритмдері үлгі сапасында деградациялауға көбірек болады. Біз тіл үлгісін оқыту мүмкіндігін зерттейміз. Бірақ жеке корпус үшін көпшілік негізінің үлгісін баптау арқылы көпшілік үлгісін және жеке сақтау үлгісін са Біз ДП жақсы баптаулары жеке доменде тіл үлгілерінің әдістерін көтереді. Бұл үлгілерді оқытуға мүмкін болады.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>언어 모델링은 자연 언어 처리의 중요한 임무이다.민감한 정보의 언어 모델을 교육할 때, 차별화된 프라이버시 (DP) 는 우리의 개인 데이터가 보호되는 정도를 계량화할 수 있도록 한다.그러나 프라이버시를 차별화하는 훈련 알고리즘을 실시하면 모델의 질이 떨어지는 경우가 많다.우리는 사유 자료 라이브러리에서 공공 기초 모델을 조정함으로써 고품질과 프라이버시 보호를 동시에 가진 언어 모델의 타당성을 연구했다.우리는 DP 마이크로스피커가 개인 영역에서 언어 모델의 성능을 향상시켜 이런 모델의 훈련을 가능하게 하는 것을 발견했다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kalbų modeliavimas yra pagrindinė gamtos kalbų apdorojimo užduotis. Mokant kalbos model į jautrios informacijos klausimais, diferencinis privatumas (DP) leidžia mums kiekybiškai įvertinti, kokiu mastu mūsų privatūs duomenys yra apsaugoti. Tačiau mokymo algoritmai, kuriais užtikrinamas diferencinis privatumas, dažnai lemia modelio kokybės blogėjimą. Mes tiriame galimybę mokytis kalbos modelio, kuris kartu yra aukštos kokybės ir privatumo išsaugojimas, pritaikydami viešojo pagrindo model į prie privataus korpuso. Mes manome, kad DP patobulinimas didina kalbų modelių privačioje srityje veiksmingumą, todėl tokių modelių mokymas yra įmanomas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Моделирањето јазик е клучна задача во природното обработување јазик. Кога тренираме јазички модел за чувствителни информации, диференцијалната приватност (ДП) ни овозможува да го квантификуваме степенот на заштита на нашите приватни податоци. Сепак, обуката на алгоритмите кои спроведуваат различен приватност честопати води до деградација во квалитетот на моделот. Ние ја проучуваме физибилитетот на учењето јазички модел кој е истовремено висок квалитет и зачувување на приватноста со прилагодување на јавниот модел на база на приватен корпус. Најдовме дека финетизирањето на ДП ја зголемува изведбата на јазичките модели во приватниот домен, што овозможува обука на вакви модели.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ഭാഷ മോഡലിങ്ങ് സ്വാഭാവിക ഭാഷയുടെ പ്രവര്‍ത്തനത്തില്‍ ഒരു കീസ്റ്റണ്‍ ജോലിയാണ്. സെന്‍സിറ്റീവ് വിവരങ്ങളെക്കുറിച്ച് ഒരു ഭാഷ മോഡല്‍ പരിശീലിക്കുമ്പോള്‍, വ്യത്യസ്ത സ്വകാര്യം (ഡിപിപി) നമ്മുടെ സ്വകാര്യ എന്നാലും, വ്യത്യസ്ത്രീയ സ്വകാര്യം പ്രവര്‍ത്തിപ്പിക്കുന്ന ആല്‍ഗോരിത്മുകള്‍ പലപ്പോഴും മോഡലിന്റെ ഗുണവി ഒരു ഭാഷ മോഡല്‍ പഠിപ്പിക്കുന്നതിന്റെ സ്വകാര്യം നമ്മള്‍ പഠിക്കുന്നു. അത് ഒരേ സമയത്ത് ഒരു പൊതുവിലെ ബേസ് മോഡലിനെ സംരക്ഷിക്കുന സ്വകാര്യ ഡോമെയിനിലെ ഭാഷ മോഡലുകളുടെ പ്രദര്‍ശനത്തിന്റെ പ്രദര്‍ശനത്തിന് നമുക്ക് കണ്ടെത്താനാണ് ഡിപിപി നല്ല ടൂണ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Холын модель бол байгалийн хэл үйлдвэрлэлийн түлхүүр чулуун ажил. Хэдий хэл загварыг мэдрэмжтэй мэдээллийн тухай суралцах үед, ялгаатай хувийн байдал (ДП) бидэнд хувийн өгөгдлийн хамгаалах хэмжээг тодорхойлж чадна. Гэхдээ өөр өөр өөрийн гэр бүлийн амьдралд хүргэх сургалтын алгоритм нь загварын сайн чанарын бууруулалттай болдог. Бид хэл загварыг сурах боломжтой байдлыг судалж, хувийн корпус дээр олон нийтийн суурь загварын загварын тулд хадгалах өндөр чанартай, хувийн амьдралыг хадгалах боломжтой. Бид ДП-ын хувийн хэл загварын үйл ажиллагааг нэмэгдүүлж, ийм загварын сургалтыг боломжтой болгодог.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Pemodelan bahasa adalah tugas batu kunci dalam pemprosesan bahasa alami. Apabila melatih model bahasa pada maklumat sensitif, privasi berbeza (DP) membolehkan kita kuantifikasikan darjah yang mana data peribadi kita dilindungi. Namun, algoritma latihan yang memaksa privasi perbezaan sering menyebabkan kerosakan dalam kualiti model. Kami mempelajari kemudahan belajar model bahasa yang bersamaan kualiti tinggi dan memelihara privasi dengan tuning model pangkalan awam pada korpus peribadi. Kami mendapati bahawa penyesuaian DP meningkatkan prestasi model bahasa dalam domain peribadi, membuat latihan model tersebut mungkin.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>L-immudellar tal-lingwi huwa kompitu ewlieni fl-ipproċessar tal-lingwi naturali. Meta jitħarreġ mudell lingwistiku dwar informazzjoni sensittiva, il-privatezza differenzjali (DP) tippermettilna nikkwantifikaw il-grad sa fejn id-dejta privata tagħna hija protetta. Madankollu, algoritmi ta’ taħriġ li jinfurzaw il-privatezza differenzjali spiss iwasslu għal degradazzjoni fil-kwalità tal-mudell. Aħna nistudjaw il-fattibbiltà tat-tagħlim ta’ mudell lingwistiku li fl-istess ħin huwa ta’ kwalità għolja u l-preservazzjoni tal-privatezza billi naġġustaw mudell ta’ bażi pubblika fuq korpus privat. Aħna nsibu li l-aġġustament tad-DP isaħħaħ il-prestazzjoni tal-mudelli lingwistiċi fid-dominju privat, u dan jagħmel it-taħriġ ta’ mudelli bħal dawn possibbli.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Taalmodellering is een sleuteltaak in de verwerking van natuurlijke taal. Bij het trainen van een taalmodel over gevoelige informatie stelt differential privacy (DP) ons in staat om te kwantificeren in hoeverre onze persoonlijke gegevens worden beschermd. Trainingsalgoritmes die differentiële privacy afdwingen leiden echter vaak tot aantasting van de modelkwaliteit. We bestuderen de haalbaarheid van het leren van een taalmodel dat tegelijkertijd hoogwaardig is en privacy behoudt door een openbaar basismodel af te stemmen op een privé corpus. We vinden dat DP fine-tuning de prestaties van taalmodellen in het privédomein verhoogt, waardoor het trainen van dergelijke modellen mogelijk wordt.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Språk- modellering er ein nøkkelstone- oppgåve i naturspråk- handsaming. Når opplæring av eit språk-modell på sensitivt informasjon, kan forskjellige privat (DP) bruka oss å kvantifisera kor grad privat data våre skal beskyttast. Men øvingsalgoritme som gjer forskjellige privat fører ofte til degradasjon i modellkvalitet. Vi studerer feilighet for å lære eit språk-modell som er samtidig høg kvalitet og privat som lagrar ved å tunera ein offentleg basemodell på ein privat korpus. Vi finn at DP finnstilling styrer utviklinga av språk-modeller i den private domenet, gjer opplæring av slike modeller mogleg.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelowanie języka jest kluczowym zadaniem w przetwarzaniu języka naturalnego. Podczas szkolenia modelu językowego dotyczącego informacji wrażliwych, różnicowa prywatność (DP) pozwala nam określić stopień ochrony naszych danych prywatnych. Jednak algorytmy szkoleniowe, które wymuszają różnicę prywatności, często prowadzą do degradacji jakości modelu. Badamy możliwość nauki modelu językowego, który jest jednocześnie wysokiej jakości i zachowania prywatności poprzez dostrojenie publicznego modelu bazy na korpusie prywatnym. Uważamy, że dopracowanie DP zwiększa wydajność modeli językowych w domenie prywatnej, umożliwiając szkolenie takich modeli.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A modelagem de linguagem é uma tarefa fundamental no processamento de linguagem natural. Ao treinar um modelo de linguagem em informações confidenciais, a privacidade diferencial (DP) nos permite quantificar o grau em que nossos dados privados são protegidos. No entanto, algoritmos de treinamento que impõem privacidade diferencial geralmente levam à degradação da qualidade do modelo. Estudamos a viabilidade de aprender um modelo de linguagem que seja simultaneamente de alta qualidade e preservação da privacidade, ajustando um modelo de base pública em um corpus privado. Descobrimos que o ajuste fino de DP aumenta o desempenho de modelos de linguagem no domínio privado, tornando possível o treinamento de tais modelos.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelarea limbajului este o sarcină cheie în procesarea limbajului natural. Atunci când instruim un model lingvistic privind informațiile sensibile, confidențialitatea diferențială (DP) ne permite să cuantificăm gradul în care datele noastre private sunt protejate. Cu toate acestea, algoritmii de formare care impun confidențialitatea diferențială duc adesea la degradarea calității modelului. Studiem fezabilitatea învățării unui model lingvistic care să fie simultan de înaltă calitate și păstrarea confidențialității prin ajustarea unui model de bază publică pe un corpus privat. Considerăm că ajustarea DP sporește performanța modelelor lingvistice în domeniul privat, făcând posibilă instruirea unor astfel de modele.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Языковое моделирование является ключевой задачей в обработке естественного языка. При обучении языковой модели конфиденциальной информации дифференциальная конфиденциальность (DP) позволяет нам количественно оценить степень защиты наших личных данных. Тем не менее, обучающие алгоритмы, обеспечивающие дифференциальную конфиденциальность, часто приводят к ухудшению качества модели. Мы изучаем возможность изучения языковой модели, которая одновременно является высококачественной и сохраняет конфиденциальность, настраивая публичную базовую модель на частном корпусе. Мы обнаружили, что точная настройка DP повышает производительность языковых моделей в частной сфере, делая возможным обучение таких моделей.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>භාෂාව මොඩිල් කරන්නේ ස්වාභාවික භාෂාව ප්‍රක්‍රියාසයේ යතුරු ස්ටෝන් වැඩක්. භාෂාව ප්‍රශ්නයක් සංවේදනය තොරතුරු ගැන, වෙනස් පුද්ගලිකතාවය (DP) අපිට පුද්ගලික දත්ත ආරක්ෂා කරන්න පුළුව නමුත්, ප්‍රධානය ඇල්ගෝරිත්ම්ස් වලින් වෙනස් පුද්ගලිකතාවට ප්‍රශ්නයක් ප්‍රශ්නයක් වෙනුවෙන් අපි භාෂා මොඩල් ඉගෙන ගන්න පුළුවන් විදියට අධ්‍යානය කරනවා ඒ වගේම පුළුවන් විශේෂතාවක් සහ පුළුවන් විශේෂතාව අපිට හොයාගන්න පුළුවන් කියලා DP හොඳ සංවේදනය විශේෂයෙන් භාෂා මොඩේල්ස් එක්ක පුළුවන් වෙන්න පුළු</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Jezikovno modeliranje je ključna naloga v obdelavi naravnega jezika. Pri usposabljanju jezikovnega modela o občutljivih informacijah nam diferencialna zasebnost (DP) omogoča količinsko opredelitev stopnje zaščite naših zasebnih podatkov. Vendar pa algoritmi usposabljanja, ki uveljavljajo različno zasebnost, pogosto vodijo v poslabšanje kakovosti modela. Preučujemo izvedljivost učenja jezikovnega modela, ki je hkrati visokokakovosten in ohranja zasebnost, z uglaševanjem javnega baznega modela na zasebnem korpusu. Ugotavljamo, da fino nastavitev DP povečuje učinkovitost jezikovnih modelov v zasebni domeni, kar omogoča usposabljanje takšnih modelov.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tusaale ahaan luuqadu waa shaqada dhagaxa ah oo ku qoran baaraandegista afka dabiicadda ah. Markii aad sameyneyso tusaale ahaan luqada oo ku saabsan macluumaadka jilicsan, gaarka loo leeyahay (DP) ayaa noogu ogolaa inaynu qiyaasto shahaadada ay macluumaadkayaga gaarka loo leeyahay. Si kastaba ha ahaatee tababaridda algorityada ku takhasusa gaarka kala duwan waxay inta badan ku hoggaamiyaan tusaale ahaan hoosaysiinta. Waxaynu baranaynaa awoodda barashada tusaale ahaan luqada, kaas oo isla markaasna la ilaaliyaa qiimo sare iyo gaar ahaanshaha, marka lagu sameynayo muusikada aasaaska dadweynaha oo gaarka ah. Waxaynu heli nahay in DP-ku kordhiyo sameynta noocyada luuqada ee gaarka loo leeyahay, in lagu tababariyo tusaalooyinkaas oo suurtagal ah.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelimi i gjuhës është një detyrë kryesore në procesimin natyror të gjuhës. Kur trajnojmë një model gjuhësh për informacion të ndjeshëm, privatësia diferenciale (DP) na lejon të kuantifikojmë shkallën në të cilën të dhënat tona private janë të mbrojtura. Megjithatë, stërvitja e algoritmeve që zbatojnë privatësinë diferenciale shpesh çon në degradim në kualitetin e modelit. Ne studiojmë realizueshmërinë e mësimit të një modeli gjuhësh që është njëkohësisht e cilësisë së lartë dhe ruajtjes së privatësisë duke rregulluar një model bazë publike në një korpus privat. Ne gjejmë se përmirësimi i DP rrit performancën e modeleve gjuhësore në fushën private, duke bërë të mundur trajnimin e modeleve të tilla.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modeliranje jezika je ključni zadatak u procesu prirodnog jezika. Kada obučavamo jezički model o osjetljivim informacijama, diferencijalna privatnost (DP) nam omogućava da kvantificiramo stupnju zaštite naših privatnih podataka. Međutim, algoritmi obuke koji primjenjuju diferencijalnu privatnost često vode do degradacije u kvaliteti modela. Proučavamo mogućnost učenja jezičkog model a koja je istovremeno visoka kvaliteta i privatnost koja čuva, uspoređujući javni model baze na privatnom korpusu. Našli smo da je DP-ova fino-tuning povećala izvodnju jezičkih modela u privatnom domenu, čineći obuku takvih modela mogućim.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Språkmodellering är en nyckeluppgift i naturlig språkbearbetning. När vi tränar en språkmodell på känslig information kan vi kvantifiera i vilken grad våra privata data skyddas. Utbildningsalgoritmer som upprätthåller differentierad integritet leder dock ofta till försämring av modellkvaliteten. Vi studerar möjligheten att lära sig en språkmodell som samtidigt håller hög kvalitet och bevarar integriteten genom att anpassa en offentlig basmodell på en privat korpus. Vi finner att DP finjustering ökar prestandan hos språkmodeller inom den privata domänen, vilket gör utbildning av sådana modeller möjlig.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Utengenezaji wa lugha ni kazi ya msingi katika upasuaji wa lugha asili. Wakati tukifundisha muundo wa lugha kuhusu taarifa za kiuchumi, faragha tofauti (DP) hutupatia kuhakikisha kiwango ambacho taarifa zetu binafsi zinalindwa. Hata hivyo, takwimu za mafunzo zinazolazimisha faragha tofauti mara nyingi hupelekea kupungua kwa kiwango cha mifano. Tunafundisha uwezekano wa kujifunza muundo wa lugha ambao kwa wakati ule unaohifadhi ubora wa juu na faragha kwa kutumia muundo wa msingi wa umma kwenye makampuni binafsi. Tunapata kwamba mafunzo ya vizuri ya DP yanaongezea utendaji wa mifano ya lugha katika eneo binafsi, na kufanya mafunzo ya mifano kama hiyo inawezekana.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>மொழி மாதிரி வடிவமைப்பு இயல்பான மொழி செயல்பாடு உணர்வுடைய தகவலின் மொழி மாதிரி பயிற்சி செய்யும் போது, வேறுபட்ட தனிப்பட்ட தனிப்பட்ட (DP) எங்கள் தனிப்பட்ட தரவுகள் எதை பாதுகாக்கப ஆயினும், வேறுபட்ட தனியார்ச்சியை செயல்படுத்தும் பயிற்சி பட்டியல்கள் பெரும்பாலும் மாதிரி தரமாக குறைக நாங்கள் ஒரு மொழி மாதிரி கற்றுக் கொள்ளும் சூழ்நிலையில் கற்றுக்கொள்ளும் சூழ்நிலை மற்றும் தனிப்பட்ட தனிப்பட்ட மாதிரியை ஒரு தன நாங்கள் கண்டுபிடிப்போம் என்று டிபிடி நன்றாக முன்னேறுதல் தனிப்பட்ட களத்தில் மொழி மாதிரிகளின் செயல்பாட்டை அதிகர</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dil modellerlemesi tebigy diller işlemekde bir taýdan täze täze. Dil nusgasyny hasaplanjak maglumaty barada okuw görkezilýän zaman, üýtgeşik hususiyetimiz (DP) bize häzirki maglumatymyzyň gaýd edilen derejesini bejermek üçin mümkin edýär. Ýöne, üýtgeşik wajyplygyny üýtgeden algoritmalar nusgasynda köplenç nusgasyna degradýar. Biz dil nusgasyny öwrenmek mümkinçiligini öwrenip otyrýarys. Şu wagt hem ýük-kaliwatly we ýüzünlik nusgasyny hususit korpusa düzenleyerek jemi baz nusgasyny çykarýarys. Biz DP ajaýyp şeklinde düzenlemek isleýän nusgalaryň üýtgetmegini özbaşdaky domandan üýtgedýäris we şol nusgalary mümkin edip biler.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>زبان موڈلینگ طبیعی زبان پرسس میں ایک کلی سنگ کام ہے. جب ایک زبان موڈل کی تعلیم کرتی ہے جو حساس معلومات کے بارے میں ہے، مختلف خصوصیت (DP) ہمیں اجازت دیتا ہے کہ درجہ کی مقدار کریں جس پر ہماری خصوصی ڈائٹی محافظت کی جاتی ہے. لیکن تعلیم الگوریتم جن کی مختلف خصوصیت کو مضبوط کر رہی ہے اکثر موڈل کی کیفیت میں دھوکا ہوا ہے۔ ہم ایک زبان موڈل کی تعلیم کے امکانات کو پڑھتے ہیں جو ایک دفعہ میں بہت بڑی کیفیت اور خصوصی کی حفاظت کرتی ہے ایک خصوصی کورپوس پر ایک عمومی بنی موڈل کو تنظیم کرتی ہے۔ ہم دیکھتے ہیں کہ ڈی.پی.ٹ.پی.ٹ.پی.ٹ.ٹ. کی زبان مدل کی عملکرد خصوصی دامین میں بڑھتی ہے، اس طرح طرح کی مدل کی آموزش ممکن بناتی ہے.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Language modeling is a keystone task in natural language processing. Til modelini sensitiv maʼlumot haqida o'rganishda, ajoyib shaxsiylik (DP) bizga shaxsiy maʼlumotni saqlashga ruxsat beradi. Lekin, to'g'ri shaxsiy shaxsiyatlarni ishlatish algoritlarini ko'p doim model sifatida o'zgartiradi. Biz tillar modelini o'rganish imkoniyatini o'rganamiz. Bu samtida o'sha shaxsiy darajada o'rganish va shaxsiylikni saqlash imkoniyatini o'rganamiz. Biz o'rganamiz, DDP yaxshi tuzilishi mumkin, shaxsiy domenadagi tilning modellarini bajarishini bajaradi, bu modellarni o'rganish imkoniyatlarini bajaradi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Lựa chọn ngôn ngữ là việc then chốt trong việc xử lý ngôn ngữ tự nhiên. Khi huấn luyện mô hình ngôn ngữ về thông tin nhạy cảm, DP cho phép chúng tôi xác định mức độ bảo vệ dữ liệu cá nhân. Tuy nhiên, thuật to án huấn luyện tác động đến tính riêng tư khác thường dẫn đến việc làm xấu chất lượng mô hình. Chúng tôi nghiên cứu khả năng học một mô hình ngôn ngữ có cùng chất lượng cao và bảo vệ sự riêng tư bằng cách thiết lập một cơ sở cá nhân. Chúng tôi thấy nền Tuyệt hảo của DP thúc đẩy khả năng của các mô hình ngôn ngữ trong lĩnh vực riêng tư, làm cho việc huấn luyện các mô hình như vậy có thể.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>言语者建模自然语言治之要务也。 敏感训言,差分隐私(DP)使我辈得量化吾私数保护如此。 然强制执行差分阴教算法常致模样。 吾论学一言之可行性,当因私语料库上以持高质量私之护。 吾见DP微调增私有域语言模形之性,使可得而教也。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.privatenlp-1.5</dd><dt>Volume:</dt><dd><a href=/volumes/2020.privatenlp-1/>Proceedings of the Second Workshop on Privacy in NLP</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/privatenlp/>PrivateNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>39–45</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.privatenlp-1.5>https://aclanthology.org/2020.privatenlp-1.5</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.privatenlp-1.5 title="To the current version of the paper by DOI">10.18653/v1/2020.privatenlp-1.5</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">kerrigan-etal-2020-differentially</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Gavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020. <a href=https://aclanthology.org/2020.privatenlp-1.5>Differentially Private Language Models Benefit from Public Pre-training</a>. In <i>Proceedings of the Second Workshop on Privacy in NLP</i>, pages 39–45, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.privatenlp-1.5>Differentially Private Language Models Benefit from Public Pre-training</a> (Kerrigan et al., PrivateNLP 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.privatenlp-1.5.pdf>https://aclanthology.org/2020.privatenlp-1.5.pdf</a></dd><dt class=acl-button-row>Video:</dt><dd class=acl-button-row><a href=https://slideslive.com/38939774 class="btn btn-attachment btn-sm"><i class="fas fa-video"></i>&nbsp;https://slideslive.com/38939774</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.privatenlp-1.5.pdf title="Open PDF of 'Differentially Private Language Models Benefit from Public Pre-training'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Differentially+Private+Language+Models+Benefit+from+Public+Pre-training" title="Search for 'Differentially Private Language Models Benefit from Public Pre-training' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Differentially Private Language Models Benefit from Public Pre-training'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://slideslive.com/38939774 title="Open video for 'Differentially Private Language Models Benefit from Public Pre-training'"><span class="align-self-center px-1"><i class="fas fa-video"></i></span>
<span class=px-1>Video</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Differentially Private Language Models Benefit from Public Pre-training](https://aclanthology.org/2020.privatenlp-1.5) (Kerrigan et al., PrivateNLP 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.privatenlp-1.5>Differentially Private Language Models Benefit from Public Pre-training</a> (Kerrigan et al., PrivateNLP 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Gavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020. <a href=https://aclanthology.org/2020.privatenlp-1.5>Differentially Private Language Models Benefit from Public Pre-training</a>. In <i>Proceedings of the Second Workshop on Privacy in NLP</i>, pages 39–45, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>