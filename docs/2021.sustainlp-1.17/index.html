<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Unsupervised Contextualized Document Representation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Unsupervised Contextualized Document Representation" name=citation_title><meta content="Ankur Gupta" name=citation_author><meta content="Vivek Gupta" name=citation_author><meta content="Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing" name=citation_conference_title><meta content="2021/11" name=citation_publication_date><meta content="https://aclanthology.org/2021.sustainlp-1.17.pdf" name=citation_pdf_url><meta content="166" name=citation_firstpage><meta content="173" name=citation_lastpage><meta content="10.18653/v1/2021.sustainlp-1.17" name=citation_doi><meta property="og:title" content="Unsupervised Contextualized Document Representation"><meta property="og:image" content="https://aclanthology.org/thumb/2021.sustainlp-1.17.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2021.sustainlp-1.17"><meta property="og:description" content="Ankur Gupta, Vivek Gupta. Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing. 2021."><link rel=canonical href=https://aclanthology.org/2021.sustainlp-1.17></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Unsupervised Contextualized Document Representation</a>
<a id=af_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Onondersteunde Konteksualiseerde Dokument Voorstelling</a>
<a id=am_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>ሰነዱን እንዳለ ምረጡ</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>تمثيل المستندات السياقية غير الخاضع للإشراف</a>
<a id=az_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Qeyd edilm톛y톛n Kontext D칬k칲man 캻zl톛ri</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Неконтролирано контекстуализирано представяне на документа</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>পর্যবেক্ষণ করা নথির প্রতিনিধি</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>སྔོན་བསྐྱུར་མེད་པའི་རྣམ་གྲངས་ཀྱི་ཡིག་ཆ་མཚོན་རྟགས</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Nepotrebna kontekstualizacija predstavljanja dokumenta</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Representació Contextualitzada sense supervisió</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Nekontrolovaná kontextová reprezentace dokumentu</a>
<a id=da_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Ikke-overvåget kontekstualiseret dokumentrepræsentation</a>
<a id=de_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Nicht überwachte kontextualisierte Dokumentdarstellung</a>
<a id=el_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Αναπροσώπευση εγγράφου χωρίς επιτήρηση</a>
<a id=es_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Representación de documentos contextualizados no supervisados</a>
<a id=et_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Järelevalveta kontekstualiseeritud dokumendi esitus</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>نمایش سند غیرقابل حفاظت</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Tarkkailematon kontekstualisoitu asiakirjaesitys</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Représentation contextualisée de documents non supervisée</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Ionadaíocht Doiciméad Comhthéacsúil Gan Maoirseacht</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>@ action</a>
<a id=he_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>מייצג מסמכים מקורקסטוליזי ללא השגחה</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>असुरक्षित संदर्भित दस्तावेज़ प्रतिनिधित्व</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Nepotrebna kontekstualizacija predstavljanja dokumenta</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Felügyelet nélküli kontextualizált dokumentumképviselet</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Comment</a>
<a id=id_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Perwakilan Dokumen Konteksual Tidak Disupervisi</a>
<a id=is_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Rappresentanza contestualizzata non sorvegliata</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>スーパーバイザーなしのコンテキスト化されたドキュメント表現</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>marker</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>კონტექსტუალური დოკუმენტის გამოსახულება</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Сәйкестік контекстуалды құжатты таңдау</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>무감독의 상하 문화 문서 표시</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Neaprižiūrimas kontekstinis dokumento atstovavimas</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Ненадгледувана контекстна претстава на документот</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>പരിശോധിച്ചിട്ടില്ലാത്ത രേഖയുടെ പ്രതിനിധി</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Хадгалагдаггүй Сүүлийн үеийн Документын төлөөлөл</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Perwakilan Dokumen Berkonteks Tidak Dikawal</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Unsupervised Contextualized Document Representation</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Niet-gecontextualiseerde documentweergave</a>
<a id=no_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Comment</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Niekontrolowana kontekstowa reprezentacja dokumentu</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Representação de Documento Contextualizado Não Supervisionado</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Reprezentarea contextualizată nesupravegheată a documentelor</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Неконтролируемое представление контекстуализированных документов</a>
<a id=si_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>සුරක්ෂා නැති සංවේදනය සඳහා ලිපින්ත ප්‍රතිස්ථානය</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Nenadzorovana kontekstualizirana predstavitev dokumenta</a>
<a id=so_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Xukummada aan la ilaalinayn</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Përfaqësimi i Dokumentit të Konteksualizuar i Pambikqyrur</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Представка документа неподржана контекстално</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Icke övervakad kontextualiserad dokumentrepresentation</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>Replacement of document</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>கண்காணிக்கப்படவில்லை</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>_Senedler</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>غیر محافظت کی کنٹکسٹولیز ڈوکومٹ ریسپرنسیٹ</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>@ info: whatsthis</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>KCharselect unicode block name</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2021.sustainlp-1.17.pdf>无监督者文档示</a></h2><p class=lead><a href=/people/a/ankur-gupta/>Ankur Gupta</a>,
<a href=/people/v/vivek-gupta/>Vivek Gupta</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. SCDV (Mekala et al., 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. How-ever, both techniques ignore the polysemyand contextual character of words. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso demonstrate our embeddings effective-ness on other tasks, such as concept match-ing and sentence similarity. In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Verskeie NLP-opdragte benodig die effektief herstelling van teksdokumente. Arora et al.,2017 wys dat eenvoudige gewigte stoor-aging van woordevektore dikwels uitgevoerde sneuringsmodelle. SCDV (Mekala et al., 2017) verder verbeter dit van setnings na docu-ments deur sagte en sparse cluster-ing te gebruik oor voorafreken woord vektore. Hoe-ooit, beide teknike ignoreer die poliseme en contextual karakter van woorde. In hierdie spaper, ons adres hierdie probleem deur die voorstel van SCDV+BERT( ctxd), ân eenvoudig en effektief ongesuperviseerde voorstelling wat con- textualiseerde BERT (Devlin et al., 2019) gebasewoord ingesluit vir woord sin disambigua- tion met SCDV sagte clustering toegang. Ons wys dat ons inbêding uitvoer oorspronklike SCDV, voorstrein BERT, en verskeie ander besonderhede op baie klassifikasie datastelle. Wealso wys ons inbêdings effektief-belangheid op ander taak, soos konsepte ooreenstemmende en setgelykheid. In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data and only few shots examples.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. SCDV (Mekala et 2017) ይህንን ከፍርድ ወደ ዶኪዎች አካባቢዎች እና ቀላል እና መቆጣጠር በቁጥጥር የተቆጠሩ ቃላት vectors በመቀጠል ይዘረጋል፡፡ እንዴት እንደሆነ፣ ሁለቱ ቴክኖክቶች የፖሊሲም እና የቃላትን ሁኔታ ጥያቄ ይተዋሉ፡፡ በቴስፓፓር፣ SCDV+BERT(ctxd) በተለየ እናስቀናለን፡፡ የኦሪጂ-የ SCDV፣ የብኤርቴን አስቀድሞ እና በብዙ መግለጫ ዳታዎች ላይ ብዙ ሌሎችን መደገፊያዎች እንዲያሳየው ምዕራብ ነው፡፡ አካባቢነታችንን ለሌሎች ስራዎችን በጥያቄ እና የሥርዓት ብጤት እና ማሳየትን አሳይተናል፡፡ በተጨማሪም፣ SCDV+BERT (ctxd) ውጤት-tune BERT እና በተለየ ቁጥጥር ላይ-ፓርቲዎች በተለየ ዳታ እና ጥቂት ነጥቦች ምሳሌዎች ብቻ እንደሆነ እናሳየዋለን፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>تحتاج العديد من مهام البرمجة اللغوية العصبية إلى إعادة إرسال المستندات النصية بشكل فعال ، يوضح أرورا وآخرون ، 2017 أن متوسط العمر المرجح لمتجهات الكلمات يتفوق في كثير من الأحيان على النماذج العصبية. يمتد SCDV (Mekala et al. ، 2017) هذا من الجمل إلى المستندات من خلال استخدام مجموعة ناعمة ومتفرقة على متجهات الكلمات المحسوبة مسبقًا. ومع ذلك ، تتجاهل كلتا التقنيتين الطابع متعدد المعاني والسياق للكلمات. في هذه الورقة ، نتناول هذه المشكلة من خلال اقتراح SCDV + BERT (ctxd) ، وهو تمثيل بسيط وفعال غير خاضع للإشراف يجمع بين BERT المنسق للنص (Devlin et al. ، 2019 ) تضمين الكلمات الأساسية لتوضيح معنى الكلمة باستخدام نهج التجميع الناعم SCDV. نلاحظ أن حفلات الزفاف لدينا تفوقت في الأداء على SCDV الأصلي ، والتدريب المسبق لـ BERT ، والعديد من الخطوط الأساسية الأخرى في العديد من مجموعات بيانات التصنيف. نوضح أيضًا فاعلية عمليات التضمين الخاصة بنا في مهام أخرى ، مثل مطابقة المفاهيم وتشابه الجمل. بالإضافة إلى ذلك ، نظهر أن SCDV + BERT (ctxd) يتفوق في الأداء على BERT وتطبيقات التضمين المختلفة في سيناريوهات ذات بيانات محدودة وقليل فقط أمثلة اللقطات.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bir neçə NLP işləri mətn belələrinin yenidən ifadə edilməsi lazımdır. Arora et al.,2017-ci ilə, sadəcə ağırlı vektörlərin ağırlığını göstərir. SCDV (Mekala et al., 2017) bunu cümlələrdən docu-ments vasitəsilə, ön-hesaplanmış söz vektörlərinin üstündə yumuşaq və küçük cluster-ing istifadə edərək uzaqlaşdırır. Necə olaraq, hər iki tekniki polisimlə müxtəlif sözlərin karakterini görməz. Bu səbəbdə, SCDV+BERT(ctxd) təbliğ etmək üçün bu məsələni çəkirik. Bu çox basit və efektiv təhlükəsizlik edilməmiş bir göstəricisidir ki, con-textualized BERT (Devlin et al., 2019) sözləri SCDV yumuşaq clustering approach ilə birləşdirir. Biz göstəririk ki, bizim inşallarımız bir çox klasifikasiya verilən qurğularda orijinal SCDV, BERT-dən əvvəl tələb edir və bir çox başqa dəyişiklik qurğularında. Wealso bizim inşallarımızı başqa işlərdə effektiv olmağımızı göstərir, məsələlər eşitmək və cümlələr kimi. Əvvəlcə, SCDV+BERT(ctxd) müxtəlif performances BERT və müxtəlif məlumatlarla müəyyən məlumatlarda fərqli məlumatlar və yalnız bir neçə fərqli məsəllərə istifadə edirik.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Няколко задачи от НЛП се нуждаят от ефективно представяне на текстови документи. Арора и др., 2017 демонстрират, че простата претеглена средна възраст на векторите на словото често надвишава перфектните неврални модели. SCDV (Mekala et al., 2017) допълнително разширява това от изречения до документи, като използва меко и рядко клъстериране върху предварително изчислени вектори на думи. Както и да е, двете техники игнорират полисемиятконтекстуалния характер на думите. В този раздел ние разглеждаме този проблем, като предлагаме просто и ефективно неконтролирано представяне, което съчетава контекстуализирано BERT (Девлин и др., 2019), базирано на вграждане на думи за изясняване на смисъла на думата с подхода на меко клъстериране SCDV. Ние показваме, че нашите вграждания превъзхождат оригиналните SCDV, предтренажните BERT и няколко други базови линии в много класификационни набори от данни. Ние също така демонстрираме ефективността на вграждането си върху други задачи, като съвпадение на концепции и сходство с изречения. В допълнение, ние показваме, че SCDV+BERT(ctxd) превъзхожда фината настройка на BERT и различните вграждащи приложения в сценарии с ограничени данни и само няколко примера за снимки.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>বেশ কয়েকটি এনএলপি কাজের লেখা ডকুমেন্টের কার্যকর পুনরায় প্রতিবেদন দরকার। আরোরা এন্ট আল, ২০১৭ প্রতিবাদ করেছে যে সাধারণত শব্দ ভেক্টরের গড় বয়সের মাধ্যমে সাধারণত উৎপাদনের মডেল। এসসিডিভি (মেকালা এন্ট ২০১৭) আরো এই বিষয়টি ডোকুর কারাদণ্ড থেকে বাড়িয়ে দিয়েছেন ডকুমেন্টের কাছ থেকে নম্রভাবে ব্যবহার করে গণনানোর আগে যেভাবেই, দুটো প্রযুক্তিগুলো পলিসেমিয়ান এবং প্রাক্তন বাক্যের চরিত্র উপেক্ষা করে। In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. ওয়েস্কো যে আমাদের বিভিন্ন অরিজি-নাল এসসিডিভি, প্রাক্তন ট্রেন বিবের্ট এবং অনেক বিভিন্ন গ্রাফিকেশন ডাটাসেটে বেশ কিছু বেসে আমরা অন্যান্য কাজের উপর আমাদের প্রতিযোগিতার কার্যকর প্রদর্শন করেছি, যেমন ধারণা মিল এবং শাস্তি একই রকম। এছাড়াও, আমরা দেখাচ্ছি যে SCDV+BERT (ctxd) আউটপার্সিফিন-tune BERT এবং বিভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন প্রোচিকের সাথে সীমিত তথ্য এবং শুধু</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>NLP ལ་བྱ་འགུལ་མང་པོ་ཞིག་ནི་ཡིག་གེ་ཡིག་གེ་ཡིག་གེ་ཚིག་ཡིག་ཆ་བསྐྱར་དུ་གཏོང་དགོས་པ Arora et al,2017 ཡིས་སྤྱི་ཚོགས་ཀྱི་རྒྱ་ཆེ་མཐོང་ཚད་ལྷག་པའི་གཞན་པ་ཚོའི་ནང་གི་མཐའ་འཁོར་གྱི་དཔེ་དབྱིབས་བཀོད་ཡོད། SCDV (Mekala et al., 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. གང་ལྟར་ཞིག་ཡིན་ན། ཐབས་ལམ་གཉིས་ཀྱིས་སྔོན་ཅིག་དང་འཕགས་རིས་ཡི་གེ་སྣང་མེད་བསྐྱུར་བྱེད། In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso་གིས་ང་ཚོའི་ནང་དུ་ཡོད་པའི་འཇུག་སྣོད་ནི་ལས་འཕགས་པ་གཞན་དང་མཉམ་དུ་བཀྲམ་སྟོན་ཐུབ། In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data and only few shots examples.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nekoliko zadataka NLP-a je potrebno efikasno ponovno režiranje tekstskih dokumenta. Arora et al., 2017. pokazuje da je jednostavno težino održavanje vektora riječi često iznad izvornih modela. SCDV (Mekala et al., 2017) dodatno proširi to od kazne na docu-ments koristeći meke i rezervne skustere preko pre-računalnih vektora riječi. Kako god, obje tehnike ignoriraju polisemijski kontekstualni karakter riječi. U toj oblasti, riješimo ovaj problem predloženjem SCDV+BERT(ctxd), jednostavnom i efikasnom nedovoljnom predstavljanju koja kombinira kontekstualiziranu BERT (Devlin et al., 2019) baznu ploču koja se uključuje za disambigua-ciju riječi sa mekim pristupom SCDV-a. Mi pokazujemo da naše ugrađenje iznosi originalni SCDV, pre vožnje BERT i nekoliko drugih bazena na mnogim podacima klasifikacije. Wealso pokazuje naše uključenje učinkovitosti na druge zadatke, kao što su koncept odgovaranja i sličnost rečenica. Osim toga, pokazujemo da je SCDV+BERT(ctxd) nadmažena izvedba BERT-a i različite ugrađivanje povreda u scenarijima s ograničenim podacima i samo nekoliko primjera snimaka.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Diverses tasques del NLP necessiten una representació efectiva dels documents de text. Arora et al., 2017 demostren que l'envelliment mitjà ponderat dels vectors de paraules sovint supera els models neuronals. SCDV (Mekala et al., 2017)ho extreu més des de frases a documentacions utilitzant grups suaus i escassos sobre vectors de paraules precomputats. Com sigui, les dues tècniques ignoren el caràcter polisèmic i contextual de les paraules. En aquest spaper, abordem aquest tema proposant SCDV+BERT(ctxd), un a representació senzilla i efectiva sense supervisió que combina el textualitzat BERT (Devlin et al., 2019) basat en incorporació de paraules de desambiguació de sentit amb l'enfocament de clustering suau SCDV. Ens mostren que les nostres incorporacions superen el SCDV origi nal, el BERT previ al tren i diverses altres línies en molts conjunts de dades de classificació. Wealso també demostra l'eficiència de les nostres incorporacions en altres tasques, com la comparació de conceptes i la similitud de frases. In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Několik úkolů NLP vyžaduje efektivní reprezentaci textových dokumentů. Arora et al.,2017 ukazují, že jednoduché vážené aver-aging slovních vektorů často překonává neurální modely. SCDV (Mekala et al., 2017)to dále rozšiřuje od vět do dokumentů pomocí měkkého a řídkého clusterování přes předem vypočítané slovní vektory. Obě techniky ovšem ignorují polyzemii a kontextový charakter slov. V této části řešíme tento problém navrhováním SCDV+BERT(ctxd), jednoduché a efektivní reprezentace bez dohledu, která kombinuje kontextualizované BERT (Devlin et al., 2019) založené na vložení slovního smyslu s přístupem měkkého shlukování SCDV. Naše vložení překonávají původní SCDV, BERT před tréninkem a několik dalších základních linek na mnoha klasifikačních datových sadách. Také demonstrujeme naši efektivitu vkládání na další úkoly, jako je například shoda konceptů a podobnost vět. Kromě toho ukazujeme, že SCDV+BERT(ctxd) překonává jemné ladění BERT a různé postupy vkládání v scénářích s omezenými daty a pouze málo příkladů snímků.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Flere NLP-opgaver kræver effektiv repræsentation af tekstdokumenter. Arora et al., 2017 viser, at simpel vægtet averaging af ordvektorer ofte overperformanceneurale modeller. SCDV (Mekala et al., 2017) udvider dette yderligere fra sætninger til dokumenter ved at anvende blød og sparsom klyngning over forudberegnede ordvektorer. Begge teknikker ignorerer imidlertid ordenes polysemogkontekstuelle karakter. I denne beskrivelse behandler vi dette problem ved at foreslå SCDV+BERT(ctxd), en enkel og effektiv ikke-overvåget repræsentation, der kombinerer kontekstualiseret BERT (Devlin et al., 2019) baseret ordindlejring for ordforståelse med SCDV soft clustering tilgang. Vi mener, at vores indlejringer på mange klassificeringsdatasæt overgår oprindelige SCDV, BERT forud for træningen og flere andre basislinjer. Vi skal også demonstrere vores indlejringer effektivitet på andre opgaver, såsom konceptmatching og sætning lighed. Desuden viser vi, at SCDV+BERT(ctxd) overgår resultaterne finjusteret BERT og forskellige indlejrings-processer i scenarier med begrænsede data og kun få billeder eksempler.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mehrere NLP-Aufgaben erfordern die effektive Repräsentation von Textdokumenten. Arora et al.,2017 zeigen, dass einfache gewichtete Abwertung von Wortvektoren häufig neuronale Modelle übertrifft. SCDV (Mekala et al., 2017)erweitert dies weiter von Sätzen zu Dokumenten, indem weiche und spärliche Clustering über vorberechnete Wortvektoren verwendet werden. Allerdings ignorieren beide Techniken den polysemischen und kontextuellen Charakter von Wörtern. In diesem Artikel behandeln wir dieses Problem, indem wir SCDV+BERT(ctxd) vorschlagen, eine einfache und effektive nicht-überwachte Darstellung, die kontextualisierte BERT (Devlin et al., 2019) basierte Einbettung für Wortsinn-Disambigua mit SCDV Soft Clustering Ansatz kombiniert. Wir zeigen, dass unsere Einbettungen bei vielen Klassifikationsdatensätzen die ursprünglichen SCDV, BERT-Vortrainings und mehrere andere Basislinien übertreffen. Wir demonstrieren unsere Einbettungseffektivität auch bei anderen Aufgaben wie Concept Matching und Satzähnlichkeit. Darüber hinaus zeigen wir, dass SCDV+BERT(ctxd) die Feinabstimmung von BERT und verschiedene Einbettungsapplikationen in Szenarien mit begrenzten Daten und nur wenigen Shots übertrifft.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Αρκετές εργασίες NLP χρειάζονται την αποτελεσματική αντιπροσώπευση εγγράφων κειμένου. Η Αρόρα κ.λ.,2017 αποδεικνύει ότι η απλή σταθμισμένη αντιστάθμιση των διανυσματικών λέξεων συχνά ξεπερνά τα νευρωνικά μοντέλα. Η SCDV (Mekala et al., 2017)επεκτείνει περαιτέρω αυτό από προτάσεις σε έγγραφα χρησιμοποιώντας μαλακή και αραιή συσσώρευση πάνω από προ-υπολογισμένα διανύσματα λέξεων. Ωστόσο, και οι δύο τεχνικές αγνοούν τον πολυσημιακό και περιεκτικό χαρακτήρα των λέξεων. Σε αυτό το άρθρο, αντιμετωπίζουμε αυτό το ζήτημα προτείνοντας μια απλή και αποτελεσματική μη εποπτευόμενη αναπαράσταση που συνδυάζει την ενσωμάτωση λέξεων με βάση το κείμενο με την προσέγγιση μαλακής ομαδοποίησης. Να δείξουμε ότι οι ενσωματώσεις μας ξεπερνούν την αρχική SCDV, την προ-εκπαίδευση BERT και αρκετές άλλες γραμμές βάσης σε πολλά σύνολα δεδομένων ταξινόμησης. Θα επιδείξουμε επίσης την αποτελεσματικότητα των ενσωματώσεων μας σε άλλες εργασίες, όπως η αντιστοίχιση εννοιών και η ομοιότητα των προτάσεων. Επιπλέον, δείχνουμε ότι το SCDV+BERT(ctxd) ξεπερνά τις επιδόσεις του BERT και τις διαφορετικές προσεγγίσεις ενσωμάτωσης σε σενάρια με περιορισμένα δεδομένα και ελάχιστα παραδείγματα λήψεων.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Varias tareas de PNL requieren la representación efectiva de los documentos de texto. Arora et al., 2017 demuestran que el promedio ponderado simple de los vectores de palabras con frecuencia supera a los modelos neuronales. SCDV (Mekala et al., 2017) amplía aún más esto de oraciones a documentos mediante el empleo de agrupamiento suave y disperso sobre vectores de palabras precalculados. Sin embargo, ambas técnicas ignoran la polisemia y el carácter contextual de las palabras. En este artículo, abordamos este tema proponiendo SCDV+BERT (ctxd), una representación simple y eficaz sin supervisión que combina la incrustación de palabras basada en BERT (Devlin et al., 2019) contextualizada para la desambiguación del sentido de las palabras con SCDV enfoque de clustering suave. Demuestramos que nuestras incorporaciones superan al SCDV original, preentrenan BERT y varias otras líneas de base en muchos conjuntos de datos de clasificación. También demostramos la eficacia de nuestras incorporaciones en otras tareas, como la correspondencia de conceptos y la similitud de oraciones. Además, mostramos que SCDV+BERT (ctxd) supera al BERT de ajuste fino y a los diferentes enfoques de incrustación en escenarios con datos limitados y ejemplos de solo unos pocos disparos.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mitmed uue uue tööprogrammi ülesanded vajavad tekstidokumentide tõhusat esitamist. Arora et al., 2017 näitavad, et sõnavaktorite lihtne kaalutud keskmine vananemine ületab sageli täiuslikke neuromudeleid. SCDV (Mekala jt., 2017) laiendab seda veelgi lausetelt dokumentidele, kasutades pehmet ja hõredat klastrit eelnevalt arvutatud sõnavaktorite suhtes. Kuid mõlemad tehnikad eiravad sõnade polüseemiat ja kontekstilist iseloomu. Selles osas käsitleme seda probleemi, pakkudes välja SCDV+BERT(ctxd), lihtsa ja tõhusa järelevalveta esinduse, mis ühendab kontekstualiseeritud BERT-i (Devlin jt., 2019) põhisõna põhisõna põhjendamise ja SCDV pehme klastrite lähenemisviisi. Näitame, et meie manustamised on paljudes klassifitseerimisandmekogumites paremad kui algsed SCDV-d, eelkoolitused BERT-d ja mitmed teised alusjooned. Samuti demonstreerime oma integreerimise tõhusust muudes ülesannetes, nagu kontseptsioonide sobitamine ja lausete sarnasus. Lisaks näitame, et SCDV+BERT(ctxd) on parem kui BERT ja erinevad manustamisprotsessid piiratud andmetega stsenaariumides ja ainult väheste võtete näidetes.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>چندتا کار NLP نیاز به بازگرداندن مدارک متن دارد. Arora et al.,۲۰۱۷ نشان می دهد که سنگین سنگین سنگین سنگین ویکتورهای کلمه اغلب از مدل های عملکرد غیر قابل توجه است. SCDV (Mekala et al., 2017) این را از جمله‌ها به دوک-ments به وسیله استفاده از کلاستر نرم و نرم بر روی ویکتورهای کلمه پیش‌محاسبه می‌کند. هرچقدر، هر دوی تکنیک ها شخصیت پلیسمی و متوسط کلمات را نادیده می دهند. در این مسئله، ما با پیشنهاد SCDV+BERT(ctxd) یک نمایش ساده و تاثیر غیر نظارت‌کننده‌ای درباره‌ی این مسئله را حل می‌کنیم که با توجه به عنوان حس نامبیگو-tion با دستور نرم کلاستر SCDV متصل شده BERT (Devlin et al., 2019) را ترکیب می‌کند. ما نشان می دهیم که وسیله‌های ما در مجموعه‌های بسیاری از اطلاعات‌شناسی‌های اولیه‌ی SCDV، BERT پیش‌آموزش، و چند تن دیگر را در مجموعه‌های مختلف انجام می‌دهند. ویلسو نشان می‌دهد که درگیری‌هایمان بر روی کارهای دیگر موثر است، مثل مشابه‌ای که مشابه‌ای و جمله‌ها دارند. علاوه بر این، ما نشان می دهیم که SCDV+BERT(ctxd) بیشتر از اجرای صفحه صفحه‌های صفحه‌ای که داده‌های محدودیت دارند و فقط چند مثال شلیک‌های مختلف دارند.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Useissa uuden työohjelman tehtävissä tekstiasiakirjojen tehokasta esittämistä tarvitaan. Arora et al., 2017 osoittavat, että sanavektorien yksinkertainen painotettu keskiarvo ylittää usein suorituksen neuromallit. SCDV (Mekala et al., 2017) laajentaa tätä lauseista dokumentteihin käyttämällä pehmeää ja harvaa klusterointia ennalta laskettujen sanavektorien päälle. Kummassakin tekniikassa ei kuitenkaan oteta huomioon sanojen monimuotoista ja kontekstuaalista luonnetta. Tässä artikkelissa käsittelemme tätä ongelmaa ehdottamalla SCDV+BERT(ctxd), yksinkertaista ja tehokasta valvomatonta esitystä, jossa yhdistyvät kontekstualisoitu BERT (Devlin et al., 2019) -pohjainen upotus sanan merkityksen selventämiseksi SCDV soft klustering -lähestymistapaan. Miten upotuksemme suoriutuvat SCDV:stä, BERT:stä ja useista muista lähtökohdista monissa luokitustietoaineistoissa. Meidän on myös osoitettava upotustemme tehokkuutta muihin tehtäviin, kuten konseptien vastaavuuteen ja lauseiden samankaltaisuuteen. Lisäksi osoitamme, että SCDV+BERT(ctxd) suoriutuu BERT:n hienosäätämisestä ja erilaisista upotussovelluksista skenaarioissa, joissa on rajallista dataa ja vain muutamia otoksia.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Plusieurs tâches de PNL nécessitent la représentation efficace de documents textuels. Arora et al., 2017 démontrent que la moyenne pondérée simple des vecteurs de mots surpasse souvent les modèles neuronaux. SCDV (Mekala et al., 2017) étend encore cela des phrases aux documents en utilisant un cluster-ing souple et clairsemé sur des vecteurs de mots précalculés. Cependant, les deux techniques ignorent la polysémie et le caractère contextuel des mots.Dans cet article, nous abordons ce problème en proposant SCDV+BERT (ctxd), une représentation non supervisée simple et efficace qui combine l'intégration de mots basée sur le BERT con-textualisé (Devlin et al., 2019) pour la désambiguïa-tion du sens des mots avec SCDV approche de clustering souple. Nous montrons que nos intégrations surpassent le SCDV d'origine, le BERT pré-entraînement et plusieurs autres lignes de base sur de nombreux ensembles de données de classification. Nous démontrons également l'efficacité de nos intégrations sur d'autres tâches, telles que la correspondance de concepts et la similarité de phrases. De plus, nous montrons que SCDV+BERT (ctxd) surpasse le BERT affiné et les différentes approches d'application d'intégration dans des scénarios avec des données limitées et seulement quelques exemples de plans.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Teastaíonn athsheoladh éifeachtach na ndoiciméad téacs ó roinnt tascanna NLP. Déanann SCDV (Mekala et al., 2017) é seo a leathnú tuilleadh ó abairtí go doiciméid trí bhraislí bog agus tanaí a úsáid thar veicteoirí focal réamhríofa. Mar sin féin, déanann an dá theicníocht neamhaird ar charachtar ilghnéitheach agus comhthéacsúil na bhfocal. Sa pháipéar seo, tugaimid aghaidh ar an tsaincheist seo trí SCDV+BERT(ctxd) a mholadh, léiriú simplí agus éifeachtach gan maoirsiú a chomhcheanglaíonn BERT comhthéacsúil (Devlin et al., 2019). ) basedword embedding for word sense disambigua-tion with SCDV bog cnuasach chuige. Léiríonn muid go sáraíonn ár leabaithe SCDV bunaidh, CRET réamhthraenála, agus go leor bonnlínte eile ar go leor tacar sonraí aicmithe. Léirímid freisin ár n-éifeachtacht leabú ar thascanna eile, mar mheaitseáil choincheapa agus cosúlacht abairtí. Chomh maith leis sin, léirímid go sáraíonn SCDV+BERT(ctxd) mion-thonn CRET agus cur chuige leabaithe éagsúla i gcásanna le sonraí teoranta agus gan ach beagán acu. samplaí shots.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kayya cikin aikin NLP, ana ƙayyade fara-ƙara wa takardar littãfi masu inganci. Arora et al.,2017 na nuna cewa yana da gwargwadon mai sauri ga sauri masu tsakanin shiryoyi na maganar ko da yawa suka sami misãlai. SCDV (Mekala et al., 2017) Kayya, duk tufãfi biyu suna ƙyãma ma ma'anar musamman da ma'anar sauri. In titopa, Munã jãyayya wannan masu al'amarin da Muke bukãtar da SCDV+BERT(ctxd), wani mai sauƙi da mai amfani da wanda bai zama mai tsaro ba, mai haɗãwa da con-text-naturated BERT (Devlin et al., 2019) bazaƙord wanda ke shigar da wa maganar sanyi desmbague-tion da SCDV zaɓallin matsayin mai ƙaranci. @ info: whatsthis Kayya, Mun nũna masu da amfani da masu akan aiki na dabam, kamar zato da suka yi daidai da maganar. Ina ƙaranci, Munã nuna SCDV+BERT(ctxd) outperformsfine-tune BERT da wasu taɓallu masu cikin tsarario da aka tsare data da kuma misãlai kaɗan kawai.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>מספר משימות NLP זקוקות לייצג יעיל של מסמכים טקסט. Arora et al., 2017 מראים שזקנה ממוצעת משקלת פשוטה של ווקטורים מילים לעתים קרובות יותר מודלים נוראלים. SCDV (Mekala et al., 2017) מחביר את זה יותר משפטים לדוקטורטים על ידי השימוש של קבוצות רכות ומנמכות מעל ווקטורי מילים מחשבים מראש. בכל מקרה, שתי הטכניקות מתעלמות מהפוליזמיה והאופי הקונקסטי של מילים. בספרד הזה, אנו מתייחסים לנושא הזה על ידי הצעה SCDV+BERT( ctxd), מייצג פשוט ויכולתי ללא פיקוח שמשולב את BERT (Devlin et al., 2019) המבוסס קונטקסטוליזציה למילה תחושה אנחנו מראים שהקישורים שלנו עולים מעל SCDV מקורי, BERT לפני הרכבת, וכמה קווים אחרים על קבוצות מידע מסווג רבים. ווילסו מציגים את היכולת של ההכניסות שלנו למשימות אחרות, כמו התאמה של מושג וכמויות משפטים. בנוסף, אנו מראים שSCDV+BERT(ctxd) יוצאים מהביצועים של BERT מסוים ומערכות אפ-אפ שונות בתסריטים עם נתונים מוגבלים ורק כמה דוגמאות צילומים.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>कई NLP कार्यों को पाठ दस्तावेज़ों के प्रभावी repre-sentation की आवश्यकता होती है। अरोड़ा एट अल., 2017 प्रदर्शित करता है कि शब्द वैक्टर के सरल भारित औसत-उम्र बढ़ने अक्सर तंत्रिका मॉडल से बेहतर प्रदर्शन करते हैं। SCDV (Mekala et al., 2017) आगे पूर्व-परिकलित शब्द वैक्टर पर नरम और विरल क्लस्टर-इंग को नियोजित करके वाक्यों से डॉक्यू-मेंट्स तक इसका विस्तार करता है। कैसे-कभी, दोनों तकनीकें शब्दों के पॉलीसेमी और प्रासंगिक चरित्र को अनदेखा करती हैं। इस पत्र में, हम इस मुद्दे को SCDV + BERT (ctxd) का प्रस्ताव करके संबोधित करते हैं, जो एक सरल और प्रभावी संयुक्त राष्ट्र-पर्यवेक्षित प्रतिनिधित्व है जो SCDV नरम क्लस्टरिंग दृष्टिकोण के साथ शब्द भावना disambigua-tion के लिए एम्बेडिंग के लिए con-textualized BERT (Devlin et al., 2019) को जोड़ता है। Weshow कि हमारे embeddings origi-nal SCDV, पूर्व ट्रेन BERT, और कई वर्गीकरण डेटासेट पर कई अन्य आधारों से बेहतर प्रदर्शन करते हैं। Wealdings भी अन्य कार्यों पर हमारे embeddings प्रभावी-नेस का प्रदर्शन, जैसे अवधारणा मिलान और वाक्य समानता के रूप में. इसके अलावा, हम दिखाते हैं कि SCDV + BERT (ctxd) सीमित डेटा और केवल कुछ शॉट्स उदाहरणों के साथ परिदृश्यों में विभिन्न एम्बेडिंग एपी-प्रोच और विभिन्न एम्बेडिंग एपी-प्रोच को मात देता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nekoliko zadataka NLP-a potrebno je učinkovito ponovno kazanje tekstskih dokumenta. Arora et al., 2017. pokazuje da je jednostavno težino održavanje vektora riječi često iznad izvornih modela. SCDV (Mekala et al., 2017) dalje to proširi od kazne na docu-ments koristeći meke i rezervne skupine preko predračunalnih vektora riječi. Kako god, obje tehnike ignoriraju polisemijski kontekstualni karakter riječi. U toj oblasti, riješimo ovaj problem predloženjem SCDV+BERT(ctxd), jednostavnom i učinkovitom nedovoljnom predstavljanju koji kombinira kontekstualiziranu BERT (Devlin et al., 2019) baznu stroj koja se uključuje za disambigua-ciju riječi s mekim pristupom SCDV-a. Mi pokazujemo da naše ugrađenje iznosi originalni SCDV, pre vožnje BERT i nekoliko drugih bazena na mnogim podacima klasifikacije. Wealso pokazuje naše uključenje učinkovitosti na druge zadatke, poput koncepta odgovaranja i sličnosti rečenica. Osim toga, pokazujemo da je SCDV+BERT(ctxd) nadmažena djelovanja BERT-a i različitih ugrađenih prolaza u scenarijima s ograničenim podacima i samo nekoliko primjera snimaka.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Számos NLP-feladatra van szükség a szöveges dokumentumok hatékony megismertetésére. Arora et al., 2017 bizonyítja, hogy a szóvektorok egyszerű súlyozott átlagos öregedése gyakran felülmúlja a teljesítményneurális modelleket. Az SCDV (Mekala et al., 2017) tovább bővíti ezt a mondatoktól a dokumentumokig azáltal, hogy puha és ritka klaszterezést alkalmaz előre kiszámított szóvektorokkal szemben. Mindkét technika azonban figyelmen kívül hagyja a szavak poliszemiás és kontextuális karakterét. Ebben a kérdésben az SCDV+BERT(ctxd), egy egyszerű és hatékony, felügyelet nélküli reprezentáció javaslatával foglalkozunk ezzel a problémával, amely ötvözi a szöveges BERT (Devlin et al., 2019) alapszó beágyazását a szó értelmezésének egyértelműsítéséhez az SCDV soft klaszterelési megközelítésével. Számos osztályozási adatkészleten felülmúlják az eredeti SCDV-t, a BERT-t és számos más alapvonalat. Beágyazásunk hatékonyságát más feladatokban is bemutatjuk, mint például a koncepciók összehasonlítása és a mondatok hasonlósága. Ezenkívül megmutatjuk, hogy az SCDV+BERT(ctxd) teljesítményét a BERT finomhangolásához és a különböző beágyazási eljárásokhoz korlátozott adatokkal és csak néhány felvételi példával rendelkező forgatókönyvekben.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Շատ ՆԼՊ-ի առաջադրանքներ կարիք ունեն տեքստի փաստաթղթերի արդյունավետ ներկայացումը: Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. ՍԿԴՎ (Meical et al., 2017) ավելի շատ տարածում է սա նախադասություններից դեպի փաստաթղթեր՝ օգտագործելով փափուկ և փոքր խմբավորումներ նախահաշվարկված բառերի վեկտորների վրա: Ինչևէ, երկու տեխնիկաները անտեսում են բառերի պոլիսեմիան և կոնտեքստոնալ բնույթը: Այս պայմաններում մենք լուծում ենք այս խնդիրը առաջարկելով, որ ՍԿԴՎ+ԲԵՌՏ( կկՏՔՍ), մի պարզ և արդյունավետ առանց վերահսկվող ներկայացուցիչ, որը համադրում է կոնտեքստալիզացված ԲԵՌՏ (Devin et al., 2019) բառի զգացմունքի դիզաբիգուգացիայի համար բառի դիզաբիգուգացիայի և ՍԿԴՎ Մենք ցույց ենք տալիս, որ մեր ներդրումները գերազանցում են սկզբնական ՍՔԴՎ-ը, BER-ը նախապատրաստման և բազմաթիվ այլ հիմքերը բազմաթիվ դասակարգման տվյալների համակարգերում: Վիելսոն ցույց է տալիս, որ մեր ներդրումները արդյունավետ են այլ խնդիրների վրա, ինչպիսիք են օրինակ համեմատությունը և նախադասությունների նմանությունը: In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 menunjukkan bahwa keseluruhan pemasaran berat sederhana vektor kata sering melebihi model sneural. SCDV (Mekala et al., 2017) melanjutkan ini dari kalimat ke dokumen dengan menggunakan berkumpul lembut dan ringan atas vektor kata pre-komputasi. How-ever, both techniques ignore the polysemyand contextual character of words. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso menunjukkan kemampuan kita pada tugas lain, seperti konsep match-ing dan kalimat yang sama. In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Diversi compiti del PNL richiedono una rappresentazione efficace dei documenti di testo. Arora et al.,2017 dimostrano che il semplice averaging ponderato dei vettori di parola spesso supera i modelli neurali prestazionali. SCDV (Mekala et al., 2017) estende ulteriormente questo concetto dalle frasi ai documenti impiegando cluster soft e sparso su vettori di parole pre-calcolati. Tuttavia, entrambe le tecniche ignorano il polisemie e il carattere contestuale delle parole. In questo articolo, affrontiamo questo problema proponendo SCDV+BERT(ctxd), una rappresentazione non supervisionata semplice ed efficace che combina l'embedding basato su parole con testo BERT (Devlin et al., 2019) per la disambiguazione del senso delle parole con l'approccio SCDV soft clustering. In molti set di dati di classificazione, le nostre incorporazioni superano le prestazioni iniziali SCDV, BERT pre-treno e molte altre linee di base. Dimostreremo inoltre l'efficacia delle nostre incorporazioni su altri compiti, come l'abbinamento dei concetti e la somiglianza delle frasi. Inoltre, mostriamo che SCDV+BERT(ctxd) supera le prestazioni di BERT e diverse applicazioni di incorporamento in scenari con dati limitati e solo pochi esempi di scatti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>いくつかのNLPタスクは、テキスト文書の効果的な繰り返し送信を必要とする。rora et al., 2017は、単純なワードベクトルの加重平均時効が、ニューラルモデルを頻繁に上回ることを実証している。 SCDV （ Mekala et al., 2017 ）は、これをさらに、事前に計算された単語ベクトルの上にソフトでまばらなクラスタリングを採用することによって、文から文に拡張します。 どのようにして、両方のテクニックは単語の多義性と文脈的性格を無視しています。本稿では、コンテキスト化されたBERT （ Devlin et al., 2019 ）ベースの単語センスの埋め込みとSCDVソフトクラスタリングアプローチを組み合わせた、シンプルで効果的な無監督表現であるSCDV + BERT （ ctxd ）を提案することで、この問題に対処します。 当社の埋め込みは、多くの分類データセット上で、ORIGI - NAL SCDV、事前トレーニングBERT、およびいくつかの他のベースラインよりも優れていることを示しています。 Wealsoは、コンセプトマッチングや文章の類似性など、他のタスクでの埋め込みの有効性を実証しています。さらに、SCDV + BERT （ ctxd ）は、限られたデータとわずかなショットの例だけのシナリオで、BERTやさまざまな埋め込みアプローチを細かく調整しています。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Awak dhéwé NLP sing disimpen nggawe ngubah werak-seneng nggawe dokumen teks Aara et al. forward expans this from words to docu ments-ments by hiring software and spase cluster ing lah Nan paten, kita diwurung ngobudhakan iki dadi saben nggunakae + (BERT(ct xx) text-box-mode Wacom action-type Label</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>რამდენიმე NLP დავალებები იჭირდება ტექსტის დოკუმენტების ეფექტიური განახლება. Arora et al.,2017-ს დემონსტრაცია, რომ სიტყვის გვექტორების მხოლოდ უფრო გავაკეთებული მოდელები. SCDV კაკ თ ეა ვ, ეგამარა რვჳნთკა თდნჲპთპარ ოჲლთჟვმთწრა თ კჲნრვკჟსრნა ოპთფთნა ნა ესმთრვ. ამ სპეპერში, ჩვენ ამ პრობლემას გადაწყენებთ SCDV+BERT(ctxd), საუკეთესო და ეფექტიური არ დააყენებული გამოსახულებელი გამოსახულება, რომელიც კონტექსტულიზებული BERT (Devlin et al., 2019) სპექტირებულია, რომელიც სიტყვის სიტყვის განს ჩვენ ჩვენი კლასიფიკაციის მონაცემები უფრო გავაკეთებთ origi-nal SCDV, pre-train BERT, და რამდენიმე სხვა კლასიფიკაციის მონაცემები. ვილჟჲ ევმონსრპთპა ნაქთრვ თნტვკრთგნჲჟრთ ნა ეპსდთრვ პაბჲრთ, კარჲ კჲნუვოუთ£ა ჟლთფნჲჟრ თ ჟლთფნჲჟრ ნა ოპვეგთე. დამატებით, ჩვენ გამოჩვენებთ, რომ SCDV+BERT(ctxd) ბერტი-ტუნი-ტუნი BERT და განსხვავებული სენარიოში ჩატვირთვა ყველაფერი სენარიოში, რომლებიც დავწერებული მონაცემები და მხოლოდ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Кейбір NLP тапсырмаларында мәтін құжаттарды қайта құру керек. Арора и ал.,2017 жылы сөздердің қарапайым жетілдірілген векторларының көпшілігін өзгерту үлгілерін көрсетеді. SCDV (Mekala et al., 2017) бұл сөздерді сөздерден документтерге қолданып, алдын- есептелген сөздер векторларының арқылы жабысқа және кеңістік кластерін қолдануға арналады. Қалай болса, екеуі техникалар полиземмен контексті сөздердің таңбаларын елемейді. Бұл мәселеге SCDV+BERT( ctxd) дегенді таңдап береміз. Бұл мәселеге контекстуализацияланған BERT (Devlin et al., 2019) бағдарламасын SCDV бағдарламалық кластерлік арқылы сөздердің сезімі үшін дембигуа- таңдау үшін қолданылатын кәдімгі және Біздің ендіруіміздің көпшілік классификациялық деректер қорларындағы көпшілікті SCDV, BERT алдындағы және бірнеше бағдарламалық жолдарды жасайды. Wealso біздің ендіруіміздің басқа тапсырмалардың эффективнігін көрсетеді, мысалы, сәйкестіктер мен сөйлеменің ұқсастығы. Қосымша, біз SCDV+BERT( ctxd) берттеулердің шектелген мәліметтері және тек бірнеше сценариялық сценарияларда әртүрлі бағдарламалардың шектелген жағдайды көрсетедік.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>여러 NLP 작업을 수행하려면 텍스트 문서를 효과적으로 표현해야 합니다.아로라 등은 2017년 단어 벡터의 단순가중평균값이 보통 자연모델보다 우수하다는 것을 증명했다.SCDV(Mekala et al., 2017)는 미리 계산된 단어의 벡터에 대해 연성 희소 집합을 실시하여 문장에서 문서로 확장한다.그러나 이 두 가지 기술은 모두 단어의 다의성과 상하문 특징을 소홀히 했다.본고에서 우리는 CDV+BERT(ctxd)를 제시함으로써 이 문제를 해결한다.ctxd는 간단하고 효과적인 무감독 표현으로 상하 문화 BERT(Devlin et al., 2019)의 DWORD를 바탕으로 단어의 뜻을 없애는 방법과SCDV 소프트 집합 방법을 결합시킨다.우리는 많은 분류 데이터 집합에 삽입된 것이 원시 SCDV, 예비 훈련 BERT, 기타 일부 기선보다 우수하다는 것을 보여 주었다.우리는 개념 일치와 문장 유사성 등 다른 임무에 대한 효과적인 삽입도 보여 주었다.그 밖에 SCDV+BERT(ctxd)는 데이터가 유한하고 소량의 렌즈만 있는 상황에서 BERT와 서로 다른 삽입 ap 방법보다 우수하다는 것을 증명했다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 m. rodo, kad paprastas svertinis žodžių vektorių vidurkis dažnai viršija neurologinius modelius. SCDV (Mekala et al., 2017 m.) toliau pratęsia nuo sakinių iki dokumentų, naudojant minkštą ir nedidelę klasterizaciją prieš iš anksto apskaičiuotus žodžių vektorius. Vis dėlto abu metodai ignoruoja žodžių polisemiją ir kontekstinį pobūdį. Šiame spaper sprendžiame šį klausimą pasiūlydami SCDV+BERT(ctxd), paprastą ir veiksmingą nepastebimą atstovavimą, kuris sujungia tekstualizuotą BERT (Devlin et al., 2019 m.), grindžiamą žodžių supratimo nesuderinamumu su SCDV minkšto klasterizavimo metodu. Mes parodysime, kad mūsų įdėjimai viršija pradinę SCDV, prieš traukinį BERT ir keletą kitų bazinių linijų daugelyje klasifikavimo duomenų rinkinių. Wealso taip pat įrodo, kad mes įtraukiame veiksmingumą į kitas užduotis, pvz., sąvokų derinimą ir sakinių panašumą. Be to, mes rodome, kad SCDV+BERT(ctxd) rezultatai yra suderinti su BERT ir skirtingi įtraukimo metodai į scenarijus, kuriuose duomenys yra riboti, ir tik keletas fotografijų pavyzdžių.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>На неколку задачи на НЛП им треба ефикасна претстава на текстовите документи. Арора и други, 2017 демонстрираат дека едноставното тежено просечно стареење на векторите на зборови често ги надминува норалните модели. SCDV (Mekala et al., 2017)понатаму го проширува ова од реченици до документи со употреба на меки и мали групирачки вектори над прекомпјутираните зборови. Сепак, двете техники го игнорираат полисемиот и контекстниот карактер на зборовите. Во овој спапер, го решаваме ова прашање предлагајќи SCDV+BERT( ctxd), едноставна и ефикасна ненадгледувана претстава која ги комбинира контекстулизираните BERT (Devlin et al., 2019) базирани зборови за раздвојување на зборовите со слабиот пристап на SCDV кластерирање. Ние покажуваме дека нашите внесувања ги надминуваат оригиналните SCDV, пред возењето BERT, и неколку други бази на многу податоци за класификација. Вилсо ја демонстрира нашата вмешаност ефикасност во другите задачи, како што се концептот на споредба и сличноста на речениците. Покрај тоа, покажуваме дека SCDV+BERT( ctxd) ги надминува резултатите од line- tune BERT и различните вградени ap- proaches во сценарија со ограничени податоци и само неколку примери од снимки.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Several NLP tasks need the effective repre-sentation of text documents. അരോറാ എറ്റ് അല്‍.,2017 വാക്ക് വെക്റ്ററുകളുടെ വയസ്സില്‍ എളുപ്പമുള്ള വയസ്സായിരുന്നു എന്ന് പ്രത്യക്ഷിക്കുന്നു. എസ്സിഡിവി (മെക്കാ എറ്റ് എൽ, 2017) വാക്കുകളില്‍ നിന്നും ഡോക്ടറിലേക്ക് മാറ്റുന്നത് മുമ്പ് കണക്കിലാക്കുന്ന വാക്ക് വെക്റ്ററുകള എപ്പോഴെങ്കിലും, രണ്ട് സാങ്കേതികവിദ്യകളും വാക്കുകളുടെ കൂട്ടത്തിലുള്ള പോലിസെമിന്റെയും സാധാരണ ക തിസ്പാപ്പരില്‍, SCDV+BERT( ctxd) പ്രാദേശിപ്പിക്കുന്നത് നമ്മള്‍ ഈ പ്രശ്നത്തെക്കുറിച്ച് വിശദീകരിക്കുന്നു. അത് കോണ്‍ട്ടെക്സ്ച്ചലേഷന്‍ ബെര്‍ട്ടി (ഡെവെല്‍ലിന്‍ എല്‍., 2019) വാക്ക് ഞങ്ങളുടെ അകത്തേക്ക് പോകുന്നത് ഓറിജി-നാള്‍ സിഡിവി, മുന്നോട്ട് ട്രെയിന്‍ ബെര്‍ട്ടി (BERT), പല വിഭാഗവിവരങ്ങളുടെ ഡാറ്റാസറ്റ മറ്റു ജോലികളില്‍ നമ്മുടെ അഭിപ്രായത്തിന്റെ പ്രധാനപ്പെടുത്തിവെക്കുന്ന പ്രകടനങ്ങള്‍ പ്രത്യക്ഷപ്പെടുത്തുന്നു. കൂടാതെ, SCDV+BERT(ctxd) പുറത്ത് പ്രദര്‍ശിപ്പിക്കുന്നത് ബെര്‍ട്ടിയില്‍ നിന്നും വ്യത്യസ്തമായ എപ്പ്-പ്രോചെക്സുകളും സിനേറ്ററിയോസില്‍ നി</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>НЛП-ын олон даалгаврууд текст баримтыг дахин дахин өгүүлэх хэрэгтэй. Арора и аль.,2017 онд энгийн хэмжээний векторуудын хөгжүүлэх нь ихэвчлэн үйл ажиллагааны загвараас илүүтэй. SCDV (Mekala et al., 2017) үүнийг өгүүлбэрээс docu-ments руу нэмэгдүүлдэг. Өмнөх тооцоологдсон үгний векторуудын хувьд бага болон богино кластерийг ашиглаж байна. Яаж ч гэсэн, хоёр техник нь полицемийн, орчин үгний харилцааны харилцааныг эсэргүүцдэг. Энэ асуудлыг бид SCDV+BERT(ctxd) гэсэн санал болгоход энгийн, эффективнэй, удирдлагагүй илтгэл зохион байгуулалт хийдэг. Энэ асуудлыг солилцуулсан BERT (Devlin et al., 2019) суурь зохион байгуулалт нь SCDV бага хэмжээний арга хэмжээнд зохион байгуулагддаг. Бид өөрсдийнхөө хөрөнгө оруулалтын эхний SCDV, BERT-ын өмнө дасгал хөдөлгөөн болон олон төрлийн өгөгдлийн санд олон өөр хэсгүүдийг хийдэг гэдгийг харуулж байна. Уилзо өөр ажил дээр үр дүнтэй байдлыг харуулж байна. Яг л ойлголт тоглох болон өгүүлбэртэй адилхан. Мөн бид SCDV+BERT(ctxd) бүтээгдэхүүний үр дүнг BERT болон хязгаарлагдсан өгөгдлийн хувилбар болон хэдэн зураг жишээлүүдийн хувилбарт өөр өөр нэгдэж байгааг харуулж байна.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Beberapa tugas NLP memerlukan penghantaran semula dokumen teks yang berkesan. Arora et al.,2017 menunjukkan bahawa keseluruhan umur vektor perkataan yang berat sederhana sering melebihi model sneural. SCDV (Mekala et al., 2017)melanjutkan ini dari kalimat ke dokumen dengan menggunakan kelompok lembut dan ringan atas vektor perkataan pra-komputer. Bagaimanapun, kedua-dua teknik mengabaikan aksara polisemyand kontekstual perkataan. Dalam spaper ini, kami mengatasi isu ini dengan melaporkan SCDV+BERT( ctxd), sebuah perwakilan mudah dan efektif tanpa mengawasi yang menggabungkan bentuk BERT (Devlin et al., 2019) berdasarkan teks untuk penyelesaian perkataan dengan pendekatan kelompok lembut SCDV. Kita tunjukkan bahawa penyembedding kita melampaui SCDV asal, BERT sebelum kereta api, dan beberapa garis lain pada banyak set data pengelasahan. Wealso menunjukkan keefektivitas penyembedding kita pada tugas lain, seperti konsep sepadan dan persamaan kalimat. Selain itu, kami menunjukkan bahawa SCDV+BERT(ctxd) melampaui prestasi BERT baris-tune dan proach-embedding berbeza dalam skenario dengan data terbatas dan hanya beberapa contoh tembakan.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Diversi kompiti tal-NLP jeħtieġu r-rappreżentazzjoni effettiva tad-dokumenti tat-test. Arora et al.,2017 juru li l-medja ppeżata sempliċi tat-tixjiħ tal-vetturi tal-kliem spiss taqbeż il-mudelli newrali. SCDV (Mekala et al., 2017) testendi aktar dan mis-sentenzi għad-dokumenti billi tuża raggruppament dgħajjef u żgħir fuq vetturi tal-kliem ikkumputati minn qabel. Madankollu, iż-żewġ tekniki jinjoraw il-karattru poliżiku u kuntestwali tal-kliem. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Nirriżultaw li l-inkorporazzjonijiet tagħna jaqbżu l-SCDV oriġinali, il-BERT ta’ qabel il-ferrovija, u diversi linji oħra fuq ħafna settijiet ta’ dejta ta’ klassifikazzjoni. Wealso juri wkoll l-effettività tal-inkorporazzjonijiet tagħna f’kompiti oħra, bħall-paragun tal-kunċetti u s-similarità tas-sentenzi. Barra minn hekk, aħna nuru li SCDV+BERT(ctxd) irriżultaw minn BERT b’aġġustament fiss u approċċi differenti ta’ inkorporazzjoni fix-xenarji b’dejta limitata u ftit eżempji ta’ shot biss.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Verschillende NLP-taken vereisen een effectieve representatie van tekstdocumenten. Arora et al.,2017 tonen aan dat eenvoudige gewogen aver-aging van woordvectoren vaak beter presteert dan neurale modellen. SCDV (Mekala et al., 2017)breidt dit verder uit van zinnen naar documenten door gebruik te maken van zachte en schaarse clustering over vooraf berekende woordvectoren. Beide technieken negeren echter het polysemische en contextuele karakter van woorden. In dit artikel pakken we dit probleem aan door SCDV+BERT(ctxd) voor te stellen, een eenvoudige en effectieve niet-begeleide representatie die gecontextualiseerde BERT (Devlin et al., 2019) basedword embedding combineert voor woordsense disambigua met SCDV soft clustering aanpak. We laten zien dat onze embeddings beter presteren dan originele SCDV, pre-train BERT en verschillende andere baselines op veel classificatiedatasets. We demonstreren ook onze effectiviteit bij andere taken, zoals conceptmatching en zinsgelijkenis. Daarnaast laten we zien dat SCDV+BERT(ctxd) beter presteert dan BERT en verschillende embedding-processen in scenario's met beperkte data en weinig shots voorbeelden.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Fleire NLP-oppgåver treng det effektive gjenoppretting av tekstdokument. Arora et al.,2017 viser at enkelt vekt lagring av ordvektorar ofte utfører utviklingsmedler. SCDV (Mekala et al., 2017) utvidar dette frå setningar til docu-ments ved å bruka måk og sparse cluster ing over forrekna ordvektorar. Kor mykje, begge teknikkar ignorerer polysemen og kontekst teikn av ord. I denne området er vi oppretta dette problemet ved å foreslå SCDV+BERT(ctxd), eit enkel og effektivt ikkje-oversikt representasjon som kombinerer con-textualisert BERT (Devlin et al., 2019) basedword som innebygger for ordfølelse disambigua-tion med SCDV-måk clustering tilnærming. Vis at innbyggingane våre utfører origi nal nal SCDV, før-treng BERT, og fleire andre ulike linjer på mange klassifikasjonsdata. Wealso viser innbygginga våre effektivt på andre oppgåver, slik som konsept for samsvar med setningar. I tillegg viser vi at SCDV+BERT(ctxd) er utgått av finn-tune BERT og ulike innbygging av ap-proaches i scenarioar med begrensede data og berre få eksemplar på fotografikk.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kilka zadań NLP wymaga skutecznej reprezentacji dokumentów tekstowych. Arora i al.,2017 pokazują, że proste ważone przeciwstarzenie wektorów słowa często przewyższa modele neuronowe. SCDV (Mekala et al., 2017)dodatkowo rozszerza to od zdań do dokumentów, stosując miękkie i rzadkie klastrowanie nad wstępnie obliczonymi wektorami słów. Jednakże obie techniki ignorują polizemię i kontekstowy charakter słów. W tym artykule rozwiązujemy ten problem, proponując SCDV+BERT(ctxd), prostą i skuteczną nienadzorowaną reprezentację, która łączy skontextualizowane BERT (Devlin et al., 2019) oparte na osadzeniu słów dla dyambiguacji sensu słowa z podejściem do miękkiego klastrowania SCDV. Nasze osadzenia przewyższają oryginalne SCDV, BERT i kilka innych linii bazowych na wielu zbiorach danych klasyfikacyjnych. Wykazujemy również efektywność osadzenia w innych zadaniach, takich jak dopasowanie koncepcji i podobieństwo zdań. Dodatkowo pokazujemy, że SCDV+BERT(ctxd) wydaje się lepiej dostroić BERT i różne procesy osadzania w scenariuszach z ograniczonymi danymi i niewieloma przykładami ujęć.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Várias tarefas de PNL precisam da representação eficaz de documentos de texto. Arora et al., 2017 demonstram que a média ponderada simples de vetores de palavras frequentemente supera os modelos neurais. SCDV (Mekala et al., 2017) estende ainda mais isso de frases para documentos, empregando agrupamento suave e esparso sobre vetores de palavras pré-computados. No entanto, ambas as técnicas ignoram a polissemia e o caráter contextual das palavras. Neste artigo, abordamos essa questão propondo SCDV+BERT(ctxd), uma representação não supervisionada simples e eficaz que combina BERT contextualizado (Devlin et al., 2019 ) incorporação de palavras para desambiguação de sentido de palavras com abordagem de clustering suave SCDV. Mostramos que nossos embeddings superam o SCDV original, BERT pré-treinamento e várias outras linhas de base em muitos conjuntos de dados de classificação. Também demonstramos a eficácia de nossa incorporação em outras tarefas, como correspondência de conceito e similaridade de sentença. exemplos de tiros.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mai multe sarcini PNL necesită reprezentarea eficientă a documentelor text. Arora et al., 2017 demonstrează că îmbătrânirea medie ponderată simplă a vectorilor de cuvânt depășește frecvent modelele neuronale performante. SCDV (Mekala et al., 2017) extinde în continuare acest lucru de la propoziții la documente prin utilizarea de cluster-ing soft și rar peste vectori de cuvinte pre-calculați. Cu toate acestea, ambele tehnici ignoră caracterul polisemiși contextual al cuvintelor. În acest articol, abordăm această problemă propunând SCDV+BERT(ctxd), o reprezentare simplă și eficientă fără supraveghere care combină încorporarea cuvintelor bazate pe BERT (Devlin et al., 2019) pentru dezambiguizarea sensului cuvintelor cu abordarea clusterizării soft SCDV. Încorporările noastre depăşesc performanţele SCDV iniţiale, BERT pre-train şi mai multe alte linii de bază pe multe seturi de date de clasificare. De asemenea, să demonstrăm eficiența încorporărilor noastre în alte sarcini, cum ar fi potrivirea conceptului și similitudinea propozițiilor. În plus, arătăm că SCDV+BERT(ctxd) depășește performanțele optime ale BERT și diferite proiecte de încorporare în scenarii cu date limitate și doar câteva exemple de fotografii.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Несколько задач NLP нуждаются в эффективном воспроизведении текстовых документов. Арора и др., 2017 демонстрируют, что простое взвешенное среднее старение векторов слов часто превосходитневрологические модели. SCDV (Mekala et al., 2017)дополнительно расширяет это от предложений до решений, используя мягкое и разреженное кластеризацию над предварительно вычисленными векторами слов. Тем не менее, обе методики игнорируют полисемию и контекстный характер слов. В этой статье мы решаем эту проблему, предлагая SCDV +BERT(ctxd), простое и эффективное неконтролируемое представление, которое сочетает встраивание слов на основе контекстуализированного BERT (Devlin et al., 2019) для расчленения смысла слова с подходом мягкой кластеризации SCDV. Показано, что наши вложения превосходят исходные SCDV, BERT перед тренировкой и некоторые другиеосновы на многих классификационных наборах данных. Мы также демонстрируем эффективность наших вложений в других задачах, таких как сопоставление концепций и сходство предложений. Кроме того,мы показываем, что SCDV+BERT(ctxd) превосходит тонкую настройку BERT и различные встраивающие аппроксимации в сценариях с ограниченными данными и только несколькими примерами снимков.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>NLP වැඩක් වෙනස් වැඩක් ලිපින්ත ලිපින්ත ලිපින්ත වාර්තාව ආපහු ප්‍රතික්‍රියාත්මක කරන්න ඕනි. Arora et al.,2017 පෙන්වන්නන්න පුළුවන් විදිහට වෙක්ටර් වලින් ප්‍රමාණයක් නැති විදිහට ප්‍රමාණය කරනවා කියලා. SCDV (Mecala et al., 2017) කොහොමද කියලා, දෙන්නම් තාක්ෂණය පොලිසිමියාන් වචන වගේ සංවේදනය අවධානය කරන්න. In this spaper, we address this challenge by preaching SCDV+BERT(ctxm), a strain and ffective un-supervised reposition that combinates Con-textuolized BERT (Devlin et al., 2019) basedword integrating for word sensing disombgua-tion with SCDV software cluster ing approach. අපි පෙන්වන්නම් අපේ ඇම්බෙන්ඩින්ග් ප්‍රධාන SCDV, BERT ප්‍රධාන ප්‍රධාන ප්‍රධානය, සහ වෙනස් විශේෂණ දත්ත සේට් වලි විල්සෝ පෙන්වන්නේ අනිත් වැඩවල් වලට අපේ සංවේදනය ප්‍රශ්නය කරනවා, ඒ වගේම සංවේදනය සහ වාක්ය වගේම. ඒ වගේම, අපි පෙන්වන්නේ SCDV+BERT(ctxm) නිර්භාවිත BERT වලින් වෙනස් ප්‍රශ්නයක් වලින් ප්‍රශ්නයක් තියෙනවා සීමාන්‍ය දත්ත සහ ප්‍රශ්නයක</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Več nalog novega delovnega programa zahteva učinkovito predstavitev besedilnih dokumentov. Arora et al., 2017 dokazujejo, da preprosta ponderirana povprečna staranja besednih vektorjev pogosto presega izvedbene nevronske modele. SCDV (Mekala et al., 2017) dodatno razširja to iz stavkov na dokumente z uporabo mehkega in redkega grozdnega združevanja nad vnaprej izračunanimi besednimi vektorji. Kljub temu obe tehniki ignorirata polikemijski in kontekstualni značaj besed. V tem dokumentu obravnavamo to vprašanje s predlogom SCDV+BERT(ctxd), preproste in učinkovite nenadzorovane predstavitve, ki združuje konteksturalizirano BERT (Devlin et al., 2019) osnovno vključevanje besednega pomena z mehkim grozdjem SCDV. Kako lahko naše vdelave presegajo originalne SCDV, BERT pred vlakom in več drugih baznih podatkov o klasifikaciji. Pokazali bomo tudi učinkovitost vgradnje pri drugih nalogah, kot sta ujemanje konceptov in podobnost stavkov. Poleg tega pokažemo, da SCDV+BERT(ctxd) izboljšujejo izboljšanje BERT in različnih vdelanih aplikacij v scenarijih z omejenimi podatki in le malo primerov posnetkov.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Shaqooyin badan oo NLP ah waxay u baahan yihiin dib u soo celinta warqadaha qoraalka. Arora et al.,2017 waxay muujiyaan in si fudud u miisaamay waxey si fudud u taqaan wadooyinka hadalka oo marar badan u samaysan qaababka Yurub. SCDV (Mekala et al., 2017) Si kastaba ha ahaatee labada teknikada ayaa ka jeeda qoraalka la yidhaahdo polysemi iyo xarafka qoraalka ah. Taaspaper, waxan ayaannu ka sheekaynaynaa suurtagalka SCDV+BERT(ctxd), kaas oo ah mid fudud oo faa'iido ah oo aan ilaalinayn, kaas oo ku soo ururiya con-textualized BERT (Devlin et al., 2019) Basdword oo ku qoran word disambigua-tion with SCDV approach soft cluster. Weshow in boosasheenna ay ka muuqato origi-nal SCDV, pre-train BERT, iyo qaar kale oo ku qoran koorasyo kala duduwan. Sidoo kale waxaynu muujinnaa shaqaalahayaga saameyn u leh shaqaalaha kale, tusaale ahaan fikrada u eg isku mid ahaanshaha iyo imtixaanka. Waxaa kale oo aan muujinnaa in SCDV+BERT(ctxd) outperformsfine-tune BERT iyo sidoo kale kooxaha ap-wacdiyo kala duduwan oo ay ku haystaan macluumaad xad ah iyo tusaalooyin yar oo ganacsi ah.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Disa detyra të NLP-së kanë nevojë për përfaqësim të efektshëm të dokumenteve teksti. Arora et al.,2017 demonstrojnë se mesatarja e peshuar e mesatare e vektorëve të fjalës shpesh kalon modelet neuronale. SCDV (Mekala et al., 2017)e zgjeron më tej këtë nga fjalët në dokumente duke përdorur grupe të buta dhe të pakta mbi vektorët e fjalëve të parallogaritur. How-ever, both techniques ignore the polysemyand contextual character of words. Në këtë spaper, ne e trajtojmë këtë çështje duke propozuar SCDV+BERT(ctxd), një përfaqësim i thjeshtë dhe efektiv pa mbikqyrur që kombinon tekstualizuar BERT (Devlin et al., 2019) të mbështetur në përfshirje për fjalë kuptimi të çambiguation me metodën e grupimit të butë SCDV. Ne tregojmë se përfshirjet tona kalojnë SCDV origjinale, para trenit BERT, dhe disa baza të tjera në shumë grupe të dhënash klasifikimi. Wealso demonstrate our embeddings effective-ness on other tasks, such as concept match-ing and sentence similarity. In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nekoliko zadataka NLP-a je potrebno efikasno ponovno rešiti tekstske dokumente. Arora et al.,2017. pokazuje da je jednostavno težino održavanje vektora rečenih vektora često iznad izvornih modela. SCDV (Mekala et al., 2017) dodatno proširi ovo od rečenica na docu-ments koristeći meke i rezervne skustere preko pre-računalnih vektora rečenica. Kako god, obe tehnike ignoriraju polisemijski kontekstualni karakter reèi. U toj oblasti, rješavamo se ovom pitanju predloženjem SCDV+BERT(ctxd), jednostavnom i efikasnom nedovoljnom predstavljanju koja kombinuje kontekstualiziranu BERT (Devlin et al., 2019) baznu ploču koja se uključuje za disambigua-ciju reèima sa mekim pristupom SCDV-a. Pokazujemo da naše uključenje iznosi originalni SCDV, pre vožnje BERT i nekoliko drugih bazena na mnogim klasifikacijskim podacima. Wealso pokazuje naše uključenje učinkovitosti na druge zadatke, kao što su koncept odgovaranja i sličnost rečenica. Osim toga, pokazujemo da je SCDV+BERT(ctxd) iznad izvedbi BERT-a i različite ugrađivanje jabuka u scenarijima sa ograničenim podacima i samo nekoliko primjera snimaka.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Flera nationella handlingsplaner kräver en effektiv återgivning av textdokument. Arora et al., 2017 visar att enkel viktad averaging av ordvektorer ofta överträffar performanceneurala modeller. SCDV (Mekala et al., 2017) utökar detta ytterligare från meningar till dokument genom att använda mjuk och gles klustring över förberäknade ordvektorer. Båda teknikerna ignorerar dock ordens polysemiska och kontextuella karaktär. I den här artikeln tar vi upp detta problem genom att föreslåSCDV+BERT(ctxd), en enkel och effektiv icke-övervakad representation som kombinerar kontextualiserad BERT (Devlin et al., 2019) baserad ordinbäddning för ordförnimmelse och SCDV mjuk klustring metod. Vi visar att våra inbäddningar presterar bättre än original SCDV, pre-train BERT och flera andra baslinjer på många klassificeringsdatauppsättningar. Vi ska också visa vår inbäddning effektivitet på andra uppgifter, såsom konceptmatchning och meningsliknande. Dessutom visar vi att SCDV+BERT(ctxd) presterar bättre än att finjustera BERT och olika inbäddningsapproacher i scenarier med begränsad data och endast få bilder exempel.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kazi kadhaa za NLP zinahitaji kuchukuliwa tena kwa ufanisi wa nyaraka za maandishi. Arora et al.,2017 imeonyesha kuwa rahisi ilikuwa na uzee wa wastani wa vectors wa maneno mara nyingi wakifanya mifano ya kijamii. SCDV (Mekala et al., 2017) inaongezea tena hii kutoka hukumu hadi madaktu kwa kutumia mstari mwepesi na kupunguza viungo vinavyotokea vector za maneno yaliyohesabiwa. Kwa namna gani, mbinu zote hizi zinazipuuza mhalisi na tabia za kawaida za maneno. Katika kipindi hiki, tunazungumzia suala hili kwa pendekezo la SCDV+BERT(ctxd), uwakilishi rahisi na wenye ufanisi usiotangaliwa ambao unaunganisha BERT (Devlin et al., 2019) kituo cha msingi kinachojumuisha kwa maneno yanayohusiana na mfumo wa mfumo wa ufundi wa SCDV. Weshow ambazo maeneo yetu yanaendesha SCDV ya asili ya origi, mafunzo ya kabla ya treni BERT, na mistari kadhaa ya watu kwenye seti nyingi za usambazaji. Pia tulionyesha maeneo yetu yenye ufanisi katika kazi nyingine, kama vile dhana inayofanana na hukumu inayofanana. Zaidi ya hayo, tunaonyesha kwamba SCDV+BERT(ctxd) kuonyesha usoni-tune BERT na wapiganaji mpya tofauti katika mitazamo yenye takwimu isiyo na mifano michache tu ya risasi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>பல NLP பணிகள் உரை ஆவணங்களின் விருப்பமான மீண்டும் உணர்வு தேவை. அரோரா மற்று அல்.,2017 சுலபமான வார்த்தை வெக்டர்கள் சராசரி வயதாக எடுத்தார்கள் என்பதை குறிப்பிடுகிறது. SCDV (மெக்கலா மற்றும் அல்., 2017) முன் கணக்கிடப்பட்ட வார்த்தை வெக்டார்களை பயன்படுத்தி மென் மற்றும் வெக்டார்களை பயன்படுத்தி இதை மேலும் வாக்க How-ever, both techniques ignore the polysemyand contextual character of words. திஸ்பேப்பரில், SCDV+BERT( ctxd), ஒரு எளிய மற்றும் பயன்படுத்தப்படாத கண்காணிக்கப்படாத பகுதியை நாம் இந்த பிரச்சினையை விளக்குகிறோம். இது கூட்டு நூல் நிறுவப்பட்ட BERT (டெப்லின் et al., 2019) க எங்கள் உள்ளீடுகள் ஓரிஜி-நாள் SCDV, முன் பயிற்சி BERT, மற்றும் பல வகுப்பு தரவுத்தளங்களில் பல மேற்கோடுக்கோடுகள். நாம் மற்ற பணிகளில் எங்கள் உள்ளடக்கங்களுக்கு வெளிப்படையான தெளிவாக்கம் காட்டுகிறோம், பொருத்தம் மற்றும் வாக்கியத்தி மேலும், SCDV+BERT(ctxd) வெளியிடும் செயல்பாடு-tune BERT மற்றும் வேறு வித்தியாசமான பிப் பகிர்வாசிகள் காண்பிக்கிறோம் மற்றும் எல்லாம் தகவல்</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>NLP birnäçe zada metin senediň täzeliklerini ýene-täzeliklerini gerek. 2017-nji Arora et al.2017'de basit ağırlı vektörler kelimelerinin çoğunlukla etkisiz modellerinden daha yüksek olduğunu gösteriyor. SCDV (Mekala et al., 2017)further extends this from sentence to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. Nähili bolsa, her iki teknikiň polisemiýa we konsekst sözlerin karakterini görmezden gelenok. Bu sahypada, biz bu meseleyi SCDV+BERT(ctxd) teklif eden basit ve etkili bir şekilde gözetlememiş bir temsil çözümleri birleştirerek, WIRT (Devlin et al., 2019) söz duygu üçin birleştirilen baz sözlerimizi çözerek, SCDV yumuşak klusterin yaklaşımı ile birleştirmekten emin bir şekilde çözeriz. Biziň düzümlerimiz origi nal SCDV, BERT öňünden otlydygyny we köp taýýarlama düzümlerinde birnäçe taýýarlama netijesinde çykýandygyny görkez. Wasp biziň daşarymyzyň içinde başga işlerde etkinlik gabdalygymyzy, meňzeş we sözleriň meňzeşligini görkez. Munuň üçin, SCDV+BERT(ctxd) BERT'yň üstüne çykyşynyň üstini görkez we senaryoýla çykyş berüvleri bilen diňe birnäçe eserler bar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>بہت سی NLP تاسکیوں کی تاسکیٹ ڈکومکیٹ کے مطابق دوبارہ سنتی کی ضرورت ہے. Arora et al.,2017 دکھاتا ہے کہ کلمات ویکتروں کی ساده وزن کی سنگی ہے، بہت سے زیادہ اضافہ کرنے والی موڈل. SCDV (Mekala et al., 2017) اس کو مفصل سے ڈاکو منٹ تک پہنچا دیتا ہے کہ نرم اور نرم کلسٹر کا استعمال کرتا ہے۔ کہیں بھی، دونوں تکنیک کلمات کی پولیسیم اور متوسط شخصیت کو غافل کرتے ہیں۔ اس جگہ میں ہم اس مسئلہ کو مشورہ کریں گے SCDV+BERT(ctxd) کی پیشنهاد سے، ایک ساده اور اثبات غیر نظارت کی تصویر جو con-textualized BERT (Devlin et al., 2019) بنسٹ ورڈ کو جمع کرتا ہے، جو کلمات کے لئے SCDV نرم کلسٹرینگ طریقے کے ساتھ مکمل ہوتا ہے۔ ہم نشان دیتے ہیں کہ ہمارے ایمبڈینگ آغاز-نال SCDV، پہلے ٹرین BERT اور بہت سی کلاسیکٹ ڈیٹ سٹ پر بہت سی دوسری صفحے ہیں۔ ویلسو ہمارے انڈینگ کو دوسرے کاموں پر اثرات کے ساتھ دکھاتا ہے، جیسے مفصل مطابق اور جماعت کی برابری. اور اس کے علاوہ ہم نشان دیتے ہیں کہ SCDV+BERT(ctxd) فین-ٹون BERT اور مختلف مصنوعی اندازے کے مطابق مختلف مصنوعی کے مطابق اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا ا</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Koʻp nechta NLP vazifalar matn hujjatlarini qayta takrorlash kerak. Arora et al.,2017 oddiy so'z vektorining o'rtacha o'rtacha o'smirligini ko'rsatish mumkin. Name Shunday qilib, ikkita teknologiya polisemiz va ma'lum so'zlarning har xil harfni o'zgartiradi. Bu muammolarni SCDV+BERT(ctxd) rivojlanish orqali, oddiy va effektiv nazorat qilmagan representiyatlarni boshqarish mumkin. Bu biz SCDV va SCDV soʻzni boshqarish usuli bilan birlashtirish uchun oddiy va ishlab chiqaruvchi bo'lgan BERT (Devlin et al., 2019). Weshow bizning tuzilishimiz origi-ichki SCDV, BERT oldin taqdim, va ko'p ta'lim maʼlumotlar tarkibidagi bir necha boshqaruvchilar. Va biz boshqa vazifalarga ishlashimizni tasavvur qilamiz, tushuning o'xshash va gapirish bir xil. Ko'rsatganda, SCDV+BERT(ctxd) outperformsfine-tune BERT (BERT) va scenariosdagi boshqa ap-prokaslari bilan chegara maʼlumot va faqat bir necha shot misollari bilan ishlatiladi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Một số công việc của Đảng NLP cần có sự đại diện hiệu quả của văn bản. Arora et Al,2007 chứng minh rằng giá trị trung bình đơn giản của giá trị cổ động từ trường phổ biến thường vượt trội. Tổ chức SCDV (Meuca et al., 2007) nới rộng từ câu này sang biên tập bằng cách sử dụng cụm từ mềm mỏng và mỏng hơn nhiều so với giá trị từ cổ xưa. Tuy nhiên, cả hai kỹ thuật lờ đi tính chất lục địa và ngữ cảnh của từ. Trong nghiên cứu này, chúng tôi giải quyết vấn đề này bằng cách đề xuất SCDV+BERT (ctyxd), một mô hình đơn giản và hiệu quả không được giám sát, kết hợp kết hợp kết cấu kết cấu kết cấu kết hợp BERT (Devlin et al., 209) nền từ ngữ: Chúng tôi làm cách khác để hoàn thành tập tin khủng long khai mạc, Trước mùa đông BERT, và nhiều nền khác trên nhiều tập tin phân loại. chứng minh sự tác hợp hiệu quả của chúng ta với các công việc khác, như khả năng kết hợp và kết án giống nhau. Ngoài ra, chúng tôi cho thấy nhóm SCDV+BERT(ctyxd) thành công vượt trội cho giải mã BERT và nhiều trường hợp khác nhau trong các kịch bản có giới hạn dữ liệu và chỉ vài ví dụ.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>诸NLP务对文本文档有效者更分之。 Arora等,2017年证,词向量简加权偏老化常优于神经。 SCDV(Mekala等,2017)于预算词向量上用软疏聚类,更广其句于文档。 此二术者,皆略单词之多义,与上下文之征也。 本文中,发SCDV + BERT(ctxd)以决之,此简而效者无监,将基于con-textualized BERT(Devlin等,2019)之基于单词词嵌SCDV软聚类,以成词义消歧义。 吾明嵌诸类集上优于始 SCDV、预练 BERT 及诸基线。 展我嵌有效性,如对句相似性。 此外又明数有限而少镜头示例者,SCDV+BERT(ctxd) 优于精细调 BERT 异者嵌 ap-proaches。</span></div></div><dl><dt>Anthology ID:</dt><dd>2021.sustainlp-1.17</dd><dt>Volume:</dt><dd><a href=/volumes/2021.sustainlp-1/>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Virtual</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/sustainlp/>sustainlp</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>166–173</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.sustainlp-1.17>https://aclanthology.org/2021.sustainlp-1.17</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2021.sustainlp-1.17 title="To the current version of the paper by DOI">10.18653/v1/2021.sustainlp-1.17</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">gupta-gupta-2021-unsupervised</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Ankur Gupta and Vivek Gupta. 2021. <a href=https://aclanthology.org/2021.sustainlp-1.17>Unsupervised Contextualized Document Representation</a>. In <i>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</i>, pages 166–173, Virtual. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2021.sustainlp-1.17>Unsupervised Contextualized Document Representation</a> (Gupta & Gupta, sustainlp 2021)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.sustainlp-1.17.pdf>https://aclanthology.org/2021.sustainlp-1.17.pdf</a></dd><dt>Code</dt><dd><a href=https://github.com/vgupta123/contextualize_scdv><i class="fab fa-github"></i>&nbsp;vgupta123/contextualize_scdv</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.sustainlp-1.17.pdf title="Open PDF of 'Unsupervised Contextualized Document Representation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Unsupervised+Contextualized+Document+Representation" title="Search for 'Unsupervised Contextualized Document Representation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-secondary d-flex flex-wrap justify-content-center" href="https://paperswithcode.com/paper/?acl=2021.sustainlp-1.17" title="Code for 'Unsupervised Contextualized Document Representation' on Papers with Code"><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-big" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg><span class="pl-sm-2 d-none d-sm-inline">Code</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Unsupervised Contextualized Document Representation'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Unsupervised Contextualized Document Representation](https://aclanthology.org/2021.sustainlp-1.17) (Gupta & Gupta, sustainlp 2021)</p><ul class=mt-2><li><a href=https://aclanthology.org/2021.sustainlp-1.17>Unsupervised Contextualized Document Representation</a> (Gupta & Gupta, sustainlp 2021)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Ankur Gupta and Vivek Gupta. 2021. <a href=https://aclanthology.org/2021.sustainlp-1.17>Unsupervised Contextualized Document Representation</a>. In <i>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</i>, pages 166–173, Virtual. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>