<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Can Recurrent Neural Networks Learn Nested Recursion? - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Can Recurrent Neural Networks Learn Nested Recursion?" name=citation_title><meta content="Jean-Phillipe Bernardy" name=citation_author><meta content="Linguistic Issues in Language Technology, Volume 16, 2018" name=citation_conference_title><meta content="2018/7" name=citation_publication_date><meta content="https://aclanthology.org/2018.lilt-16.1.pdf" name=citation_pdf_url><meta property="og:title" content="Can Recurrent Neural Networks Learn Nested Recursion?"><meta property="og:image" content="https://aclanthology.org/thumb/2018.lilt-16.1.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2018.lilt-16.1"><meta property="og:description" content="Jean-Phillipe Bernardy. Linguistic Issues in Language Technology, Volume 16, 2018. 2018."><link rel=canonical href=https://aclanthology.org/2018.lilt-16.1></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2018.lilt-16.1.pdf>Can <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> Learn Nested Recursion?</a>
<a id=af_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Kan Herhaalde Nurale Netwerke Leer Neste Rekursie?</a>
<a id=am_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>የአሁኑን ጊዜ የኔural መረብ መረብ ማር ይችላልን?</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>هل يمكن للشبكات العصبية المتكررة تعلم العودية المتداخلة؟</a>
<a id=az_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Yenidən gələn nöral ağları sıxıntıları öyrənə bilərmi?</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Могат ли повтарящите се неврални мрежи да научат гнездената рекурзия?</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>পুনরায় নিউরাল নেটওয়ার্ক কি নেস্টের পুনরুদ্ধার শিখতে পারে?</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>རྩིས་འབྱུང་བའི་དྲ་བ་དྲ་བ་སྤྱིར་བཏང་བའི་བསྐྱར་དུ་སླར་གྱི་རྒྱུ་དང་།</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Može li ponovne neurone mreže naučiti neodoljivu rekursiju?</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Les xarxes neurals recurrents poden aprendre recursió neumatizada?</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Mohou se opakované neuronové sítě naučit vnořené rekurze?</a>
<a id=da_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Kan tilbagevendende neurale netværk lære indlejret rekursion?</a>
<a id=de_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Können wiederkehrende neuronale Netzwerke verschachtelte Rekursion lernen?</a>
<a id=el_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Μπορούν τα επαναλαμβανόμενα νευρωνικά δίκτυα να μάθουν τη φωλιασμένη αναδρομή;</a>
<a id=es_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>¿Pueden las redes neuronales recurrentes aprender la recursión anidada?</a>
<a id=et_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Kas korduvad närvivõrgud saavad pesastatud rekursiooni õppida?</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>آیا شبکه‌های عصبی دوباره می‌توانند بازگشت نیز را یاد بگیرند؟</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Voiko toistuvat hermoverkot oppia pesiytyneen rekursion?</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Les réseaux de neurones récurrents peuvent-ils apprendre la récursivité imbriquée</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>An féidir le Líonraí Néaracha Athfhillteacha Atarlú Neadaithe a Fhoghlaim?</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>@ action: button</a>
<a id=he_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>האם רשתות נוירוליות מחדשות יכולות ללמוד מחזור מסודר?</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>क्या आवर्तक तंत्रिका नेटवर्क नेस्टेड पुनरावृत्ति सीख सकते हैं?</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Može li ponovne neuronske mreže naučiti neodoljivu rekursiju?</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Az ismétlődő ideghálózatok megtanulhatják a beágyazott regenerációt?</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Կարո՞ղ են կրկնվող նյարդային ցանցերը սովորել նեյրոնային կրկնվությունը:</a>
<a id=id_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Dapatkah Rangkaian Neural Sekali-kali belajar rekursi Nested?</a>
<a id=is_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Le reti neurali ricorrenti possono imparare la recursione nidificata?</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>再発性ニューラルネットワークはネストされた再帰を学ぶことができますか？</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>politenessoffpolite"), and when there is a change ("assertivepoliteness</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>შეიძლება შეიძლება გადასწავლა ნეიროლური ქსელები გასწავლა შემდეგ რეკურსი?</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Қайталанған нейралық желі қайталануды үйрене алады ба?</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>귀속 신경 네트워크는 끼워 넣는 귀속을 배울 수 있습니까?</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Ar pakartotiniai neurologiniai tinklai gali išmokti susigręžtą rekursiją?</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Can Recurrent Neural Networks Learn Nested Recursion?</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>നെയുറല്‍ നെറ്റ്വര്‍ക്കുകള്‍ വീണ്ടും പഠിക്കാന്‍ കഴിയുമോ?</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Дахин дахин сэтгэл мэдрэлийн сүлжээг дахин дахин сурах болов уу?</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Boleh Rangkaian Neural Berulang Belajar Perulangan Bersarang?</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Can Recurrent Neural Networks Learn Nested Recursion?</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Kunnen Recurrent Neural Networks Nested Recursion leren?</a>
<a id=no_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Kan gjentaande neuralnettverk læra neste rekursjon?</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Czy powtarzające się sieci neuronowe mogą nauczyć się zagnieżdżonej rekursji?</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>As redes neurais recorrentes podem aprender recursão aninhada?</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Pot rețelele neurale recurente să învețe recurența reciprocă?</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Могут ли рекуррентные нейронные сети изучать вложенную рекурсию?</a>
<a id=si_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>පුළුවන් නිර්මාණික ජාලවාර්ක්ෂිත වෙනුවෙන් ඉගෙන ගන්න?</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Ali se lahko ponavljajoča živčna omrežja naučijo gnezdene rekurzije?</a>
<a id=so_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Shabakada Neural ee ku soo deganaanshaha ma baran karaa dib u soo celinta Nested?</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>A mund rrjetet e përsëritura neuronale të mësojnë rekursionin e shtyllur?</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Može li ponovne neurone mreže naučiti neodoljivu rekursiju?</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Kan återkommande neurala nätverk lära sig Nested Recursion?</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Je, mtandao wa Neural unaweza kujifunza kurejeshwa?</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>நியூரால் வலைப்பின்னல் திரும்பச் செய்ய முடியுமா?</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Geçen netijesi ýene gaty gaýtalanany öwrenip bilermi?</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>کیا دوبارہ نیورل نیٹورک نیٹورک نیٹ پڑھنے کی تعلیم سکتے ہیں؟</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Can Recurrent Neural Networks Learn Nested Recursion?</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>Có thể tái sinh các thần kinh học không?</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2018.lilt-16.1.pdf>递归神经网络可以学嵌套递归乎?</a></h2><p class=lead><a href=/people/j/jean-phillipe-bernardy/>Jean-Phillipe Bernardy</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Context-free grammars (CFG) were one of the first formal tools used to model <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a>, and they remain relevant today as the basis of several frameworks. A key ingredient of CFG is the presence of nested recursion. In this paper, we investigate experimentally the capability of several <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNNs)</a> to learn nested recursion. More precisely, we measure an upper bound of their capability to do so, by simplifying the task to learning a generalized Dyck language, namely one composed of matching parentheses of various kinds. To do so, we present the RNNs with a set of random strings having a given maximum nesting depth and test its ability to predict the kind of closing parenthesis when facing deeper nested strings. We report mixed results : when generalizing to deeper nesting levels, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of standard RNNs is significantly higher than <a href=https://en.wikipedia.org/wiki/Randomness>random</a>, but still far from perfect. Additionally, we propose some non-standard stack-based models which can approach perfect <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, at the cost of <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kontekstvry gramme (CFG) was een van die eerste formele nutsprogramme wat gebruik word om natuurlike tale te model, en hulle bly vandag relevant as die basis van verskeie raamwerke. 'n Sleutel ingredient van CFG is die voorsiening van naaste herhaal. In hierdie papier, ons ondersoek eksperimenteel die kapasiteit van verskeie herhaalde neuralnetwerke (RNN) om te leer naaste herhaalde rekursie. More precise, we measure an upper bound of their ability to do so, by simplifying the task to learn a generalised Dyck language, namely one composed of matching parentheses of various kinds. Om dit te doen, laat ons die RNN voorsien met 'n stel van willekeurige strings wat 'n gegewe maksimum nestende diepte het en toets sy moontlik om die soort van sluit parentes te voorskou wanneer dieper neste strings gesig is. Ons rapporteer gemengde resultate: wanneer generalisering na dieper nesting vlakke, is die presisie van standaard RNN betekenlik hoër as willekeurige, maar nog ver van perfekte. Ons voorstel ook 'n paar nie-standaard stack-gebaseerde modele wat kan naby perfekte presisie, op die koste van kragtigheid.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የኮምፕዩተር ቋንቋዎች ምሳሌ የሚደረጉት የመጀመሪያይቱ ምርጫዎች (CFG) የኮምፕዩተር ቋንቋዎች አንዱ ናቸው፡፡ የCFG ቁልፍ ማቀናጃ የደረጃ መግቢያ ነው፡፡ በዚህ ፕሮግራም፣ በብዙ ጊዜ የሚቆጠሩ የደዌብ መረብ (RNNs) መግቢያን ለመማር ባለስልጣንን እናሳውቃለን፡፡ በአስተናጋሪም፣ ይህን ለማድረግ የሚችሉትን አቅራቢያ እናስቀምጣለን፡፡ ይህንን ለማድረግ፣ የRNNs ጥልቅ ጥልቅ የተሰጥነውን እናስቸጋጅላለን እና ወላጆችን ወደ ጥልቅ ግንብ በተገናኘ ጊዜ ምን ዓይነት እንዲቆርጥ እናስታውቃለን፡፡ We report mixed results: when generalizing to deeper nesting levels, the accuracy of standard RNNs is significantly higher than random, but still far from perfect. በተጨማሪም፣ የደረጃ ብዛት ፍጹም ትክክለኛ ሊደርስ የሚችሉትን የአካባቢ ሳይተካክሉ ዓይነቶች እናሳስባታለን፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>كانت القواعد النحوية الخالية من السياق (CFG) واحدة من أولى الأدوات الرسمية المستخدمة لنمذجة اللغات الطبيعية ، ولا تزال ذات صلة اليوم كأساس للعديد من الأطر. أحد المكونات الرئيسية لـ CFG هو وجود العودية المتداخلة. في هذا البحث ، قمنا بالتحقيق بشكل تجريبي في قدرة العديد من الشبكات العصبية المتكررة (RNNs) على تعلم العودية المتداخلة. بتعبير أدق ، نقيس الحد الأعلى لقدرتهم على القيام بذلك ، من خلال تبسيط المهمة لتعلم لغة Dyck معممة ، أي لغة مكونة من أقواس متطابقة من أنواع مختلفة. للقيام بذلك ، نقدم RNNs مع مجموعة من السلاسل العشوائية التي لها أقصى عمق تداخل معين ونختبر قدرتها على التنبؤ بنوع قوس الإغلاق عند مواجهة سلاسل متداخلة أعمق. لقد أبلغنا عن نتائج مختلطة: عند التعميم على مستويات متداخلة أعمق ، تكون دقة RNN القياسية أعلى بكثير من العشوائية ، ولكنها لا تزال بعيدة عن الكمال. بالإضافة إلى ذلك ، نقترح بعض النماذج غير القياسية القائمة على المكدس والتي يمكن أن تقترب من الدقة الكاملة على حساب المتانة.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Təbiətli dillərə modelləşdirmək üçün istifadə edilən ilk formal vasitələrdən biri idi və bu gün çoxlu frameworklərin əsası olaraq mövcuddur. CFG'nin anahtar məlumatı yuxarı dönüşünün varlığıdır. Bu kağızda, bir neçə tekrarlı nöral ağlarının (RNN) yenilənməsini öyrənmək üçün təcrübə edirik. Daha do ğrusu, biz onların bunu edə biləcək yetkinliklərinin üstünü ölçürük, işləri generalized Dyck dilini öyrənmək üçün asanlaşdırmaq üçün, bu deyil ki, varislər ata-anasından oluşan. Bunu etmək üçün, RNN'ləri daha derin çubuqlar ilə qarşılaşarkən, daha yaxın çubuqların qarşısında növbəsini qapılmaq bacarığı ilə göstəririk. Biz karışıq sonuçlarını bildiririk: daha derin yuxarı seviklərinə generalizasyon edərkən standart RNN dəqiqliyi rastlaşdırmaqdan daha yüksəkdir, amma hələ də mükəmməldən uzaqdır. Əksinə, biz standart stack-based modelləri təbliğ edirik ki, bu mükəmməl ədalətə yaxınlaşa bilər.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Граматиката без контекст е един от първите официални инструменти, използвани за моделиране на естествени езици и днес те остават релевантни като основа на няколко рамки. Ключова съставка на CFG е наличието на вложена рекурсия. В тази статия изследваме експериментално способността на няколко повтарящи се невронни мрежи (РНН) да научат гнездена рекурсия. По-точно измерваме горната граница на способността им да правят това, като опростим задачата за изучаване на обобщен език на Дик, а именно такъв, съставен от съвпадащи скоби от различни видове. За да направим това, представяме на RNN набор от произволни низове с дадена максимална дълбочина на гнездене и тестваме способността му да предвижда вида затварящи скоби при изправяне пред по-дълбоки вложени низове. Докладваме смесени резултати: при генерализиране на по-дълбоки нива на гнездене точността на стандартните RNN е значително по-висока от случайната, но все още далеч от перфектната. Освен това предлагаме някои нестандартни модели, базирани на стек, които могат да доближат перфектна точност на цената на здравината.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>স্বাভাবিক ভাষার মডেল করার জন্য প্রথম ফার্মিল টুল ব্যবহার করা হয়েছিল এবং তারা আজকের বেশ কয়েকটি ফ্রেমের ভিত্তিক হিসেবে প্রযুক্ সিএফজির একটি কী উপাদান হচ্ছে নেস্ট পুনরুদ্ধারের উপস্থিতি। এই কাগজটিতে আমরা বেশ কয়েকটি পুনরাবার নিউরেল নেটওয়ার্ক (আরএনএন) পুনরাবৃত্তি শিখার ক্ষমতার পরীক্ষায় তদন্ত করি। আরো পরিস্কার, আমরা তাদের ক্ষমতার উপরের বাটন পরিমাপ পরিমাপ করি, সাধারণ একটি জেনারেলিয়াল ডাইক ভাষা শিখার কাজ সাধারণ করে, যেটি বিভিন্ন ধরনের পিতা-মাতা এটা করার জন্য আমরা আরএনএন-এর সামনে উপস্থিত করি বিভিন্ন সংস্করণের সাথে যেখানে দেয়া হয়েছে সর্বোচ্চ বাড়ির গভীর এবং গভীর বাড়ির স্ট্রিঙ্গের মুখোমুখি হয় আমরা মিশ্র ফলাফলের সংবাদ প্রদান করি যে: যখন গভীরভাবে বাড়ির স্তরে সাধারণ করে, তখন স্ট্যান্ডার্ডার আরএনএন-এর সঠিকতা অনিয়মের চেয়ে বেশী বেশী, এছাড়াও আমরা কিছু স্ট্যাক-ভিত্তিক মডেল প্রস্তাব করি যা পোশাকের মূল্যের কাছে সম্পূর্ণ সঠিকভাবে পৌঁছাতে পারে।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Context-free grammars (CFG)are one of the first formal tools used to model natural languages. They remain relevant today as the basis of several frameworks. CFG འི་གཙོ་ཆེ་ཆུང་དེ་nested སླར་རིམ་ལ་གནས་ཡོད། ང་ཚོས་ཤོག་བྱས་འདིའི་ནང་དུ་ལྟ་ཞིབ་ཀློག་བྱེད་པའི་སྒེར་གྱི་རྐྱེན་འབྲེལ་གཞན་མང་པོ་ཞིག་དང་། ང་ཚོས་རང་ཉིད་ཀྱི་ཆེད་དུ་འཕགས་རིས་བྱས་ན་གྱི་ཆེ་མཐོང་ཚད་རྩིས་ཐོག་ཏེ། ལྟར་ན། ང་ཚོས་རྣམ་པ་ཚོའི་མཐུན་ཚད་ལྟར་མཐའ་བར་མཐུན་ཡོད་པའི་སྐབས་ཡིག ང་ཚོས་བརྗོད་བྱས་ཡོད་པའི་འབྱུང་བ་དག་བརྗོད། འོན་ཀྱང་། ང་ཚོས་བློ་རྣམས་མེད་སྔོན་གྱི་ཁག་ཅིག་གི་ཚད་གཞི་བརྟེན་ནས་ཕན་ཚུན་བདེ་ཞིག་དང་། རྩ་བ་སྐྱེན་ཚད་ལ་ཉེན་ར</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grammari bez konteksta (CFG) bili su jedan od prvih formalnih alata koji su koristili za modelu prirodnih jezika, i danas ostaju relevantni kao osnova nekoliko okvira. Ključni sastojak CFG-a je prisutnost nestane ponovnosti. U ovom papiru istražujemo eksperimentalno sposobnost nekoliko rekonstruiranih neuralnih mreža (RNN) da naučimo gnijezdo rekonstrukcije. Tačnije, mjerimo gornju vezu njihovih sposobnosti da to rade, jednostavljajući zadatak da naučimo generalizirani jezik Dyck a, a to je jedan sastavljen od odgovarajućih roditelja različitih vrsta. Da bismo to uradili, predstavljali smo RNN sa setom nasumičnih žica koji imaju određenu maksimalnu dubinu gnijezda i testirali svoju sposobnost da predvidi vrstu zatvaranja parenteze kada se suočavaju sa dubljim gnijezdama. Prijavljamo miješane rezultate: kada generaliziramo na dublje nivoe gnijezda, preciznost standardnih RNN-a je značajno veća od nasumičnih, ali još uvijek daleko od savršenog. Osim toga, predlažemo neke ne-standardne modele na stazi koji mogu pristupiti savršenoj preciznosti, uz cijenu pljačke.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Les gramàtiques sense contest (CFG) eren una de les primeres eines oficials utilitzades per modelar llengües naturals, i són actualment rellevants com a base de diversos marcs. Un ingredient clau de CFG és la presença de recursió ninjada. En aquest article investigam experimentalment la capacitat de diverses xarxes neurals recurrents (RNN) d'aprendre recursió ninjada. De manera més precisa, mesurem el límit superior de la seva capacitat de fer-ho, simplificant la tasca d'aprendre un llenguatge Dyck generalitzat, és a dir, un compost de parànceses de diferents tipus. Per fer-ho, presentem als RNN un conjunt de cordes aleatòries amb una profunditat de nidificació màxima i provem la seva habilitat de predir el tipus de paràntesis que tanca quan enfrontem cordes nidificades més profundes. Informem resultats mixts: quan s'generalitzem a nivells de nidificació més profunds, la precisió dels RNA estándar és significativament més alta que aleatòria, però encara lluny de perfecte. A més, proposem alguns models no estàndard basats en piles que poden abordar la precisió perfecta, al cost de la robustet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bezkontextové gramatiky (CFG) byly jedním z prvních formálních nástrojů používaných k modelování přirozených jazyků a zůstávají relevantní i dnes jako základ několika rámců. Klíčovou složkou CFG je přítomnost vnořené rekurze. V tomto článku experimentálně zkoumáme schopnost několika rekurentních neuronových sítí (RNN) naučit se vnořené rekurze. Přesněji měříme horní hranici jejich schopnosti tak učinit tím, že zjednodušíme úkol učit se zobecněný Dyckův jazyk, konkrétně takový, který se skládá z odpovídajících závork různých druhů. Za tímto účelem prezentujeme RNN sadu náhodných řetězců s danou maximální hloubkou vnoření a testujeme jejich schopnost předpovídat druh uzavírací závorky při čelem hlubších vnořených řetězců. Hlásíme smíšené výsledky: při zobecnění na hlubší úrovně vnoření je přesnost standardních RNN výrazně vyšší než náhodná, ale stále daleko od dokonalosti. Navíc navrhujeme některé nestandardní modely založené na zásobníku, které dokážou dosáhnout dokonalé přesnosti za cenu robustnosti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kontekstfri grammatik (CFG) var et af de første formelle værktøjer, der blev brugt til at modellere natursprog, og de forbliver relevante i dag som grundlag for flere rammer. En vigtig ingrediens i CFG er tilstedeværelsen af indlejret rekursion. I denne artikel undersøger vi eksperimentelt muligheden for flere tilbagevendende neurale netværk (RNN'er) til at lære indlejret rekursion. Mere præcist måler vi en øvre grænse af deres evne til at gøre det ved at forenkle opgaven til at lære et generaliseret Dyck sprog, nemlig et sammensat af matchende parenteser af forskellig art. For at gøre dette præsenterer vi RNN'erne med et sæt tilfældige strenge med en given maksimal indlejringsdybde og tester dens evne til at forudsige den slags lukkende parentes, når de står over for dybere indlejrede strenge. Vi rapporterer blandede resultater: Når man generaliserer til dybere indlejringsniveauer, er nøjagtigheden af standard RNN'er betydeligt højere end tilfældige, men stadig langt fra perfekt. Derudover foreslår vi nogle ikke-standard stack-baserede modeller, der kan nærme sig perfekt nøjagtighed, på bekostning af robusthed.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kontextfreie Grammatiken (CFG) waren eines der ersten formalen Werkzeuge, die zur Modellierung natürlicher Sprachen verwendet wurden, und sie sind bis heute als Grundlage mehrerer Frameworks relevant. Ein wesentlicher Bestandteil von CFG ist das Vorhandensein von verschachtelter Rekursion. In diesem Beitrag untersuchen wir experimentell die Fähigkeit mehrerer rezidivierender neuronaler Netze (RNNs), verschachtelte Rekursion zu erlernen. Genauer gesagt messen wir eine Obergrenze ihrer Fähigkeit, dies zu tun, indem wir die Aufgabe vereinfachen, eine generalisierte Dyck-Sprache zu lernen, nämlich eine, die aus passenden Klammern verschiedener Art besteht. Um dies zu tun, stellen wir den RNNs eine Reihe von zufälligen Strings mit einer gegebenen maximalen Verschachtelungstiefe vor und testen ihre Fähigkeit, die Art der schließenden Klammern vorherzusagen, wenn tiefere verschachtelte Strings gegenüberstehen. Wir berichten gemischte Ergebnisse: Bei Verallgemeinerung auf tiefere Verschachtelungsebenen ist die Genauigkeit von Standard-RNNs deutlich höher als zufällig, aber noch lange nicht perfekt. Darüber hinaus schlagen wir einige nicht standardisierte Stack-basierte Modelle vor, die perfekte Genauigkeit erreichen können, auf Kosten der Robustheit.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Οι γραμματικές χωρίς πλαίσιο ήταν ένα από τα πρώτα τυπικά εργαλεία που χρησιμοποιήθηκαν για τη μοντελοποίηση φυσικών γλωσσών και εξακολουθούν να ισχύουν σήμερα ως βάση διάφορων πλαισίων. Ένα βασικό συστατικό του είναι η παρουσία ένθετης αναδρομής. Σε αυτή την εργασία, διερευνούμε πειραματικά την ικανότητα πολλών επαναλαμβανόμενων νευρωνικών δικτύων (RNN) να μάθουν ένθετη αναδρομή. Πιο συγκεκριμένα, μετράμε ένα ανώτερο όριο της ικανότητάς τους να το κάνουν αυτό, απλοποιώντας το έργο εκμάθησης μιας γενικευμένης γλώσσας Ντυκ, δηλαδή μιας που αποτελείται από αντίστοιχες παρενθέσεις διαφόρων ειδών. Για να γίνει αυτό, παρουσιάζουμε τα RNN με ένα σύνολο τυχαίων συμβολοσειρών που έχουν ένα δεδομένο μέγιστο βάθος φωλιάσματος και εξετάζουμε την ικανότητά τους να προβλέπουν το είδος κλεισίματος παρένθεσης όταν αντιμετωπίζουν βαθύτερες φωλιές συμβολοσειρών. Αναφέρουμε μικτά αποτελέσματα: όταν γενικεύονται σε βαθύτερα επίπεδα ωοτοκίας, η ακρίβεια των τυποποιημένων είναι σημαντικά υψηλότερη από την τυχαία, αλλά ακόμα απέχει πολύ από τέλεια. Επιπλέον, προτείνουμε μερικά μη τυποποιημένα μοντέλα που μπορούν να προσεγγίσουν την τέλεια ακρίβεια, με κόστος της ανθεκτικότητας.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Las gramáticas sin contexto (CFG) fueron una de las primeras herramientas formales utilizadas para modelar lenguajes naturales, y siguen siendo relevantes hoy en día como base de varios marcos. Un ingrediente clave de CFG es la presencia de recursión anidada. En este artículo, investigamos experimentalmente la capacidad de varias redes neuronales recurrentes (RNN) para aprender la recursión anidada. Más precisamente, medimos un límite superior de su capacidad para hacerlo, simplificando la tarea para aprender un idioma dyck generalizado, es decir, uno compuesto por paréntesis coincidentes de varios tipos. Para ello, presentamos a los RNN un conjunto de cadenas aleatorias que tienen una profundidad de anidamiento máxima dada y probamos su capacidad para predecir el tipo de paréntesis de cierre cuando se enfrentan a cadenas anidadas más profundas. Presentamos resultados mixtos: al generalizar a niveles de anidamiento más profundos, la precisión de las RNN estándar es significativamente mayor que la aleatoria, pero aún está lejos de ser perfecta. Además, proponemos algunos modelos basados en pilas no estándar que pueden alcanzar una precisión perfecta, a costa de la robustez.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kontekstivabad grammatikad (CFG) olid üks esimesi formaalseid vahendeid, mida kasutati looduslike keelte modelleerimiseks, ning need on tänapäeval olulised mitmete raamistike alusena. CFG peamine koostisosa on pesastatud rekursiooni olemasolu. Käesolevas töös uurime eksperimentaalselt mitme korduva närvivõrgu (RNN) võimet õppida pesastatud rekursiooni. Täpsemalt, me mõõdame ülemise piiri nende võimekust seda teha, lihtsustades ülesannet õppida üldist Dyck keelt, nimelt üks koosneb sobivatest sulgudest erinevat liiki. Selleks esitame RNN-id juhuslike stringidega, millel on antud maksimaalne pesitsemissügavus ja testime nende võimet ennustada sulgu tüüpi sügavamate pesitsemissügavustega stringide ees. Teatame erinevatest tulemustest: sügavamatele pesitsustasemetele üldistamisel on standardsete RNN-ide täpsus märkimisväärselt suurem kui juhuslik, kuid siiski kaugeltki täiuslik. Lisaks pakume välja mõned mittestandardsed korstnapõhised mudelid, mis suudavad läheneda täiuslikule täpsusele, tugevuse hinnaga.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>گرامارهای بی محیط (CFG) یکی از اولین ابزارهای رسمی برای مدل زبانهای طبیعی استفاده می‌شود و امروز به عنوان بنیاد چندین فرم مربوط می‌شوند. یک عنصر کلید CFG موجود بازگشتی است. در این کاغذ، ما با آزمایش توانایی چند شبکه عصبی (RNN) را برای یاد گرفتن تکرار آزمایش تحقیق می‌کنیم. دقیقاً ما یک ارتباط بالا از توانایی آنها برای انجام این کار را اندازه می‌گیریم، با ساده کردن کار برای یادگیری زبان دیک ژنرال، یعنی یکی از پدر و مادر مختلف مختلف است. برای انجام این کار، ما RNN ها را با مجموعهٔ خط تصادفی نشان می‌دهیم که عمیق گسترش را دارند و توانایی آن را امتحان می‌کنیم تا زمانی که با خط‌های گسترش عمیق‌تر روبرو می‌شوند، از نوع بستن پدر‌ترس را پیش‌بینی کنیم. ما نتایج مختلف را گزارش می‌دهیم: زمانی که عمومی به سطح عمیق گسترش، دقیق RNN استاندارد بسیار بالاتر از تصادفی است، ولی هنوز دور از کامل است. به اضافه، ما چند مدل غیر استاندارد بنیاد استاندارد را پیشنهاد می کنیم که می توانند به دقیق کامل نزدیک شوند، به هزینه قوی.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kontekstivapaat kieliopit (CFG) olivat yksi ensimmäisistä luonnollisten kielten mallintamiseen käytetyistä virallisista työkaluista, ja ne ovat edelleen merkityksellisiä useiden viitekehysten perustana. CFG:n keskeinen ainesosa on sisäkkäisen rekursion läsnäolo. Tässä työssä tutkitaan kokeellisesti useiden toistuvien hermoverkkojen (RNN) kykyä oppia sisäkkäistä rekursiota. Tarkemmin sanottuna mittaamme ylärajan heidän kyvystään tehdä niin yksinkertaistamalla tehtävää oppia yleistetty Dyck kieli, nimittäin sellainen, joka koostuu eri suluissa. Tätä varten esitämme RNN:ille joukon satunnaisia merkkijonoja, joilla on tietty enimmäispesäsyvyys ja testaamme niiden kykyä ennustaa sulkeutumissulkuja kohdatessamme syvempiä sisäkkäisiä merkkijonoja. Raportoimme sekavia tuloksia: kun yleistetään syvemmälle pesimätasolle, vakioRNN:ien tarkkuus on huomattavasti suurempi kuin satunnaiset, mutta silti kaukana täydellisestä. Lisäksi ehdotamme joitakin epätavanomaisia pinopohjaisia malleja, jotka voivat lähestyä täydellistä tarkkuutta kestävyyden kustannuksella.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Les grammaires sans contexte (CFG) ont été l'un des premiers outils formels utilisés pour modéliser les langues naturelles, et elles restent pertinentes aujourd'hui en tant que base de plusieurs cadres. Un ingrédient clé de CFG est la présence d'une récursion imbriquée. Dans cet article, nous étudions expérimentalement la capacité de plusieurs réseaux neuronaux récurrents (RNN) à apprendre la récursion imbriquée. Plus précisément, nous mesurons une limite supérieure de leur capacité à le faire, en simplifiant la tâche d'apprentissage d'une langue dyck généralisée, à savoir une langue composée de parenthèses correspondantes de différentes sortes. Pour ce faire, nous présentons les RNN avec un ensemble de chaînes aléatoires ayant une profondeur d'imbrication maximale donnée et testons sa capacité à prédire le type de parenthèse fermante face à des chaînes imbriquées plus profondes. Nous rapportons des résultats mitigés : lors de la généralisation à des niveaux d'imbrication plus profonds, la précision des RNN standard est nettement supérieure à celle du hasard, mais encore loin d'être parfaite. De plus, nous proposons des modèles non standard basés sur des piles qui peuvent atteindre une précision parfaite, au détriment de la robustesse.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bhí gramadach saor ó chomhthéacs (CFG) ar cheann de na chéad uirlisí foirmiúla a úsáideadh chun teangacha nádúrtha a shamhaltú, agus tá siad ábhartha sa lá atá inniu ann mar bhunús do roinnt creataí. Príomh-chomhábhar de CFG is ea láithreacht atarlaithe neadaithe. Sa pháipéar seo, déanaimid imscrúdú turgnamhach ar chumas roinnt líonraí néaracha athfhillteacha (RNNanna) chun atarlú neadaithe a fhoghlaim. Níos cruinne, déanaimid teorainn uachtarach a gcumas é sin a dhéanamh a thomhas, tríd an tasc chun teanga ghinearálta Dyck a fhoghlaim a shimpliú, is é sin ceann atá comhdhéanta de lúibíní de chineálacha éagsúla a mheaitseáil. Chun é sin a dhéanamh, cuirimid sraith teaghráin randamacha i láthair na RNNanna a bhfuil uasdoimhneacht neadaithe tugtha acu agus tástálaimid a chumas an cineál lúibíní deiridh a thuar agus teaghráin neadaithe níos doimhne os comhair. Tuairiscímid torthaí measctha: agus sinn ag ginearálú go leibhéil neadaithe níos doimhne, tá cruinneas na RNNanna caighdeánacha i bhfad níos airde ná randamach, ach fós i bhfad ó bheith foirfe. Ina theannta sin, molaimid roinnt samhlacha neamhchaighdeánach atá bunaithe ar chruach ar féidir leo teacht i ngleic le cruinneas foirfe, ar chostas stóinsithe.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>KCharselect unicode block name @ action: button Daga wannan takardan, munã karatun karatun abincin masu iya iya samun tarakin neural da aka daura (RNNNs) zuwa an sanar da cewa masu bakwai. Ko da hakki, Munã cika wani girgije na sarki na awonsu da za ka aikata shi, kuma masu sauƙi da aikin in ka sanar da harshen Dyck da ɗabi'a, sami guda ta sami da kuma yana daidaita nau'in jama'a-biyu. Daga wannan, Munã halatar da RNNs da wasu rundun da aka ƙayyade kowaci da aka bai wa damƙari mafi ƙaranci, kuma Munã jarraba abincinsa da ya yi bayani ga jinin rufe mahaifa idan yana haɗi strife masu ƙaranci. Tuna rapi fassarar da aka haɗa: idan ana iya ƙara zuwa mafi ƙasƙanci, hakarin RNNs na da girma daga ranar da ba'a so, kuma amma yana da ƙayyade. Ina ƙaranci, Munã buɗa wasu misãlai masu banga-bango na'ura, masu iya kusantar da taƙaitacce kamid, kan kyautar tufi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>גרמטיקות ללא קשר (CFG) היו אחת מהכלים הרשמיים הראשונים שנמשכו למודל שפות טבעיות, והן נשארות רלוונטיות היום כבסיס של מספר מסגרות. מרכיב מפתח של CFG הוא נוכחות התחזור הקן. בעיתון הזה, אנו חוקרים בניסיון את היכולת של מספר רשתות עצביות חוזרות (RNN) ללמוד מחזור קן. יותר מדויק, אנו מדדים את הגבול העליון של היכולת שלהם לעשות את זה, על ידי הפשטת המשימה ללמוד שפת דיק כללית, כלומר אחד מורכב ממורות מתאימות מסוגים שונים. כדי לעשות זאת, אנחנו מציגים את RNN עם קבוצה של חוטים אקראיים שיש עמוק הקן מקסימום מסוים ולבדוק את היכולת שלו לחזות את סוג של הוורים סגורים כשמתמודדים עם חוטים הקן עמוקים יותר. אנו מדווחים על תוצאות מעורבות: כאשר הגנרליזציה לרמות הקן עמוקות יותר, מדויקת RNN סטנדרטי גבוהה באופן משמעותי מאשר אקראי, אבל עדיין רחוק ממשלם. בנוסף, אנו מציעים כמה דוגמנים לא סטנדרטיים מבוססים על ערימות שיכולים להתקרב לדיוקת מושלמת, במחיר החזקה.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>संदर्भ मुक्त व्याकरण (सीएफजी) प्राकृतिक भाषाओं को मॉडल करने के लिए उपयोग किए जाने वाले पहले औपचारिक उपकरणों में से एक थे, और वे आज भी कई ढांचे के आधार के रूप में प्रासंगिक हैं। सीएफजी का एक प्रमुख घटक नेस्टेड रीकर्सन की उपस्थिति है। इस पेपर में, हम नेस्टेड पुनरावृत्ति सीखने के लिए प्रयोगात्मक रूप से कई आवर्तक तंत्रिका नेटवर्क (RNNs) की क्षमता की जांच करते हैं। अधिक सटीक रूप से, हम एक सामान्यीकृत डाइक भाषा सीखने के लिए कार्य को सरल बनाकर, ऐसा करने की उनकी क्षमता के ऊपरी बाउंड को मापते हैं, अर्थात् विभिन्न प्रकार के मिलान कोष्ठक से बना एक। ऐसा करने के लिए, हम यादृच्छिक स्ट्रिंग्स के एक सेट के साथ RNNs को प्रस्तुत करते हैं जिसमें एक दी गई अधिकतम घोंसले की गहराई होती है और गहरे नेस्टेड स्ट्रिंग्स का सामना करते समय क्लोजिंग कोष्ठक के प्रकार की भविष्यवाणी करने की अपनी क्षमता का परीक्षण करते हैं। हम मिश्रित परिणामों की रिपोर्ट करते हैं: गहरे घोंसले के शिकार के स्तर पर सामान्यीकरण करते समय, मानक आरएनएन की सटीकता यादृच्छिक से काफी अधिक होती है, लेकिन अभी भी सही से बहुत दूर है। इसके अतिरिक्त, हम कुछ गैर-मानक स्टैक-आधारित मॉडल का प्रस्ताव करते हैं जो मजबूती की कीमत पर सही सटीकता तक पहुंच सकते हैं।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grammari bez konteksta (CFG) bili su jedan od prvih formalnih alata koje se koriste za modelu prirodnih jezika, i danas ostaju relevantni kao osnova nekoliko okvira. Ključni sastojak CFG-a je prisutnost nestane ponovnosti. U ovom papiru istražujemo eksperimentalno sposobnost nekoliko recidivnih neuralnih mreža (RNN) da naučimo gnjezdo recidivanje. Tačnije, mjerimo gornju vezu njihove sposobnosti da to rade, jednostavljajući zadatak učiti generalizirani jezik Dyck a, a to je jedan sastavljen od odgovarajućih roditelja različitih vrsta. Da bismo to učinili, predstavljali smo RNN sa setom nasumičnih žica koji imaju određenu maksimalnu dubinu gnijezda i testirali svoju sposobnost predvidjeti vrstu zatvaranja parenteze kada se suočavaju sa dubljim gnijezdama. Prijavljujemo miješane rezultate: kada generaliziramo na dublje nivoe gnijezda, preciznost standardnih RNN-a je značajno veća od nasumičnih, ali još uvijek daleko od savršenog. Osim toga, predlažemo neke ne-standardne modele koji mogu prići savršenoj točnosti, uz cijenu pljačke.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A kontextusmentes nyelvtanfolyamok (CFG) az egyik első formális eszköz volt, amelyet a természetes nyelvek modellezésére használtak, és ma is relevánsak maradnak számos keretrendszer alapjaként. A CFG kulcsfontosságú összetevője a beágyazott rekurzió jelenléte. Jelen tanulmányban kísérleti úton vizsgáljuk több visszatérő neurális hálózat (RNN) képességét a beágyazott rekurzió tanulására. Pontosabban megmérjük a képességük felső határát, egyszerűsítve az általánosított Dyck nyelv elsajátításának feladatát, nevezetesen különböző zárójelekből álló feladatot. Ehhez bemutatjuk az RNN-eket egy adott maximális beágyazási mélységgel rendelkező véletlenszerű karakterlánccal, és teszteljük annak képességét, hogy megjósolják a zárójelek fajtáját, ha mélyebb beágyazott karakterláncokkal néznek szembe. Vegyes eredményeket jelentünk: a mélyebb fészkelési szintekre való általánosítás során a standard RNN-ek pontossága jelentősen magasabb, mint a véletlenszerű, de még mindig messze nem tökéletes. Továbbá javasolunk néhány nem szabványos stack alapú modellt, amelyek tökéletes pontosságot közelítenek meg a robusztusság árán.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Առանց կոնտեքստի գրամագրությունները բնական լեզուների մոդելավորման առաջին պաշտոնական գործիքներից մեկն էին, և դրանք այսօր շարունակում են կարևոր լինել մի քանի շրջանակների հիմքում: ՖԿԳ-ի հիմնական բաղադրիչը խմբախմբախմբախմբախմբախմբախմբախմբախմբախմբախմբախմբախմբախմբախմբի գոյությունն է: Այս թղթի մեջ մենք փորձարկումներով ուսումնասիրում ենք մի քանի կրկնվող նյարդային ցանցերի (ՌՆՆ) կարողությունը սովորել բնակված կրկնվությունը: Ավելի ճշգրիտ, մենք չափում ենք նրանց հնարավորությունների վերին սահմանը, պարզաբանելով ընդհանուր Դիք լեզու սովորելու խնդիրը, հատկապես մեկը, որը կազմված է տարբեր տեսակի զուգահեռ կառուցվածքներից: Դա անելու համար մենք ներկայացնում ենք ՌՆԹ-ները պատահական լարերով, որոնք ունեն որոշակի մաքսավոր խորություն, և փորձում ենք այն ունակությունը կանխագուշակել փակցող անկյունների տեսակը, երբ դեմ են ավելի խորը անկյունների: Մենք զեկուցում ենք խառնված արդյունքներ. երբ ընդհանուր հասնում ենք ավելի խորը խմբավարման մակարդակներին, ստանդարտ ՌՆԹ-ների ճշգրտությունը շատ ավելի բարձր է, քան պատահական, բայց դեռևս հեռու կատարյալ: Ավելին, մենք առաջարկում ենք որոշ ոչ ստանդարտ կառուցվածքով հիմնված մոդելներ, որոնք կարող են մոտենալ կատարյալ ճշգրտությանը, հաստատության գնով:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gramatika tanpa konteks (CFG) adalah salah satu alat formal pertama yang digunakan untuk model bahasa alam, dan mereka tetap relevan hari ini sebagai dasar beberapa bingkai. Sebuah bahan kunci CFG adalah kehadiran rekursi sarang. Dalam kertas ini, kami menyelidiki secara eksperimen kemampuan beberapa jaringan saraf yang berulang-ulang (RNN) untuk belajar rekursi sarang. Lebih tepatnya, kita mengukur batas atas kemampuan mereka untuk melakukannya, dengan menyederhanakan tugas untuk belajar bahasa Dyck secara umum, yaitu salah satu yang terdiri dari kurungan yang cocok dari berbagai jenis. Untuk melakukannya, kami mempersembahkan RNN dengan set string acak yang memiliki kedalaman sarang maksimum yang diberikan dan menguji kemampuannya untuk memprediksi jenis kurungan tutup ketika menghadapi garis sarang yang lebih dalam. Kami melaporkan hasil campuran: ketika menyebar ke tingkat sarang lebih dalam, akurasi RNN standar jauh lebih tinggi dari acak, tapi masih jauh dari sempurna. Selain itu, kami mengusulkan beberapa model non-standard berdasarkan tumpukan yang dapat mendekati akurasi sempurna, dengan biaya kepekatan.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Le grammatiche senza contesto (CFG) sono state uno dei primi strumenti formali utilizzati per modellare i linguaggi naturali, e rimangono rilevanti oggi come base di diversi framework. Un ingrediente chiave del CFG è la presenza di ricorsi nidificati. In questo articolo, esaminiamo sperimentalmente la capacità di diverse reti neurali ricorrenti (RNN) di imparare la ricorsione nidificata. Più precisamente, misuriamo un limite superiore della loro capacità di farlo, semplificando il compito di imparare una lingua Dyck generalizzata, cioè quella composta da parentesi corrispondenti di vario tipo. Per fare ciò, presentiamo gli RNN con un insieme di stringhe casuali con una data profondità di nesting massima e testiamo la sua capacità di prevedere il tipo di parentesi di chiusura quando affrontano stringhe annidate più profonde. Riportiamo risultati contrastanti: quando si generalizza a livelli di nidificazione più profondi, l'accuratezza degli RNN standard è significativamente superiore a quella casuale, ma ancora tutt'altro che perfetta. Inoltre, proponiamo alcuni modelli stack-based non standard che possono avvicinarsi alla perfetta precisione, al costo della robustezza.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>文脈自由文法（ CFG ）は、自然言語をモデル化するために使用された最初の正式なツールの1つであり、いくつかのフレームワークの基礎として今日でも関連性があります。 CFGの重要な要素は、ネストされた再帰の存在です。 この論文では、いくつかの再帰的ニューラルネットワーク（ RNN ）がネストされた再帰を学習する能力を実験的に調査した。 より正確には、一般化されたDyck言語、すなわち様々な種類のカッコで構成された言語を学習する作業を単純化することによって、その能力の上限を測定します。 これを行うために、我々は、所与の最大ネスト深度を有するランダムな文字列のセットをRNNに提示し、より深いネストされた文字列に直面したときに閉括弧の種類を予測するその能力をテストする。 混合した結果を報告します。より深いネストレベルに一般化すると、標準的なRNNの精度はランダムよりも有意に高くなりますが、まだ完全ではありません。 さらに、堅牢性を犠牲にして、完璧な精度にアプローチできるいくつかの非標準的なスタックベースのモデルを提案します。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>graphs CFG Awak dhéwé éntuk karo pewiir iki, kita nguasakno perusahaan karo hal-hal dadi seneng alat (DNN) kanggo ngerasakno. Dayelih, kita kuwi nggambar aturan gawe kudu nggawe kapan karo koyo ngono, dadi supaya njawulang kanggo limo perkaraan diangkat dhèk, liyane wis dipoleh cara diangkat sing sampeyan karo koyok sinau. lining dolanan Awak dhéwé éntuk perintah kayané: pas gewisaké dilawak dhéwé, kabèh dumadhakan pangan ning DNN padha luwih luwih apik, njuk isih di luwih apik. Mungkin daftar, kita supoyo akeh model sing gak bener-ne basa gambar n'egal sing bisa dianteksi iki dadi sing apik buffet, iso nggawe barang apik trus</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>პირველი ფორმალური ხელსაწყობილობაში გამოიყენებული კონტექსტის გარეშე გრამიმარები (CFG) იყო პირველი ფორმალური ხელსაწყობილობაში, რომელიც მოდელური ენების მოდელში გამოყ Name ჩვენ ექსპერიმენტიურად ექსპერიმენტიურად განსხვავებთ რამდენიმე რეკურაციული ნეიროლური ქსელების შესაძლებლობა. უფრო მარტივია, ჩვენ მათი შესაძლებლობის მარტივი ზედაპირად განზომილებით, რომელიც განზომილებით განზომილებით დავასწავლოთ საუკეთესო სიტყვის, რომელიც ერთი განსხვავებულ პროცენტესების შე როგორც გავაკეთებთ, ჩვენ პრონეტები დახურებული პრონეტების კონფიგურაციის ნაწილი სტრიქონით, რომლებიც უფრო დიდი ნაწილი სტრიქონის მაქსიმალური სიგრძნობა და შევცვალოთ თავის შესა ჩვენ შეგვიყვანეთ შესაბამისი წარმოდგენები: როდესაც გენერალიზაცია უფრო დიდი ნესტის დონესზე, სტანდარტული RNN-ის წარმოდგენება უფრო მეტია, მაგრამ უფრო დამატებით, ჩვენ გვეყველა ნოსტანდარტური სტაკის მოდელები, რომლებიც შეუძლიათ გავაკეთოთ სრულიან წესიერება, რომლებიც ძალიან ძალიან ძალიან.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Контексті бос граммалар (CFG) табиғи тілдерді үлгілеу үшін қолданылатын алғашқы офимал құрылғылардың бірі болды. Бүгін олар бірнеше фреймдер негізінде қатынасыз болады. CFG кілттің көмегімен қайталану мүмкіндігі. Бұл қағазда біз тәжірибелі қайталану үшін бірнеше қайталанатын невралдық желілер (RNN) мүмкіндігін зерттеп көрдік. Ең дұрыс, біз олардың істеу мүмкіндіктерінің жоғарын өлшеміміз, тапсырманы жалпы дик тілді оқыту үшін қарапайым көмектесіп, бұл - әртүрлі түрлерге сәйкес келетін әкелер тілді. Бұл үшін, біз RNN-лерді кездейсоқ жолдарды келтіріп, келтірілген шегіндегі шегінде жабыс тереңдігін тексеріп, түсіндірілген жолдардың қаншалығын жабу мүмкіндігін тексеріп көрдік. Біз араластырылған нәтижелерді есептеп бердік: жалпы жүктеу деңгейіне көтергенде стандартты RNN деңгейінің дұрыстығы кездейсоқ деңгейіне көп жоғары, бірақ әлі әлі ә Қосымша, біз стандартты стак негіздеген үлгілерді таңдаймыз. Бұл дұрыстығын дұрыстыруға болады.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>컨텍스트 무관 문법(Context-free grammars, CFG)은 최초로 자연 언어 모델링에 사용된 정식 도구 중 하나로, 현재도 일부 프레임워크의 기초가 되고 있다.CFG의 중요한 요소 중 하나는 중첩 귀속의 존재이다.본고에서 우리는 실험을 통해 몇 가지 귀속신경망(RNN)이 귀속을 학습하는 능력을 연구했다.더 정확히 말하자면, 우리는 임무를 광범위한 Dyck 언어, 즉 각종 일치하는 괄호로 구성된 언어로 간소화함으로써 그들이 이렇게 하는 능력의 상한선을 평가한다.이를 위해 RNN에 최대 중첩 깊이를 지정하는 랜덤 문자열 세트를 제공하고, 더 깊은 중첩 문자열에 직면할 때 닫힌 괄호 유형을 예측하는 능력을 테스트했습니다.우리가 보고한 결과는 좋고 나쁨이 반반이다. 더욱 깊은 삽입 수준으로 확대되었을 때 표준 RNN의 정확성은 무작위 RNN보다 현저히 높았지만, 여전히 완벽하지는 않았다.그 밖에 우리는 비표준적인 창고 기반 모델을 제시했는데 이런 모델은 노봉성을 대가로 완벽한 정밀도에 접근할 수 있다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bekontekstinės gramatikos (CFG) buvo viena iš pirmųjų oficialių priemonių, naudojamų gamtinėms kalboms modeliuoti, ir šiandien jos tebėra svarbios kaip kelių sistemų pagrindas. Pagrindinė CFG sudedamoji dalis – nested recursion. Šiame dokumente eksperimentiškai tiriame kelių pasikartojančių nervinių tinklų (RNN) gebėjimą išmokti nested recursion. Tiksliau, mes išmatuojame jų gebėjimo tai padaryti viršutinę ribą, supaprastinant užduotį mokytis bendros disko kalbos, būtent kalbos, kurią sudaro įvairių rūšių antdėklės. Tam mes pristatysime RNN su atsitiktinių sričių rinkiniu, turinčiu tam tikrą didžiausią nestingo gylį, ir išbandome jo gebėjimą nuspėti uždarymo parenteziją, kai susiduriame su gilesnėmis nestingomis sričėmis. Mes pranešame apie mišrius rezultatus: generalizuojant į gilesnius nesting lygius standartinių RNR tikslumas yra gerokai didesnis nei atsitiktiniai, bet vis dar toli nuo tobula. Be to, siūlome kai kuriuos nestandartinius modelius, pagrįstus rinkiniais, kurie gali pasiekti visišką tikslumą, už patikimumą.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Граграмите без контекст (CFG) беа една од првите формални алатки кои се користат за моделирање на природните јазици, и остануваат релевантни денес како основа на неколку рамки. Клучна состојка на CFG е присуството на гнездената рекурсија. Во овој весник, експериментално ја истражуваме способноста на неколку рецидентни нервни мрежи (РНН) да научат рециденција во гнездото. Поточно, ја мериме горната граница на нивната способност да го направат тоа, со едноставување на задачата за учење генерализиран јазик Дик, имено оној составен од одговарање на parentheses од различни видови. За да го направиме тоа, ги претставуваме РНН со набор на случајни линии кои имаат одредена максимална длабочина на гнездото и ја тестираме својата способност да ја предвидат видот на затворање parenthesis кога се соочуваат со подлабоки гнездени линии. Пријавуваме мешани резултати: кога се генерализира на подлабоко ниво на гнездо, точноста на стандардните РНН е значително повисока од случајните, но сé уште далеку од совршено. Покрај тоа, предложуваме некои нестандардни модели базирани на групи кои можат да се приближат до совршена точност, по цена на robustness.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>സ്വാഭാവിക ഭാഷകള്‍ മോഡല്‍ ചെയ്യാന്‍ ഉപയോഗിക്കുന്ന ആദ്യത്തെ ഫോള്‍മാലില്‍ ഉപകരണങ്ങളില്‍ ഒന്നായി സിഎഫ്ജിയുടെ ഒരു താക്കോല്‍ ഉള്‍പ്പെടുത്തിയിരിക്കുന്നത് കെണ്‍സ്റ്റ് റിസ്റ്റര്‍ഷന്‍റെ സാ ഈ പത്രത്തില്‍, നമ്മള്‍ പരീക്ഷണത്തില്‍ അന്വേഷിക്കുന്നു കുറച്ച് പ്രാവര്‍ത്തമായ നെയൂറല്‍ വര്‍ക്കുകള്‍ക്കുള്ള കഴിവുകള്‍ കൂടുതല്‍ വ്യത്യസ്തമായ ഒരു ഡൈക്ക് ഭാഷ പഠിപ്പിക്കാനുള്ള ജോലിയെ ഞങ്ങള്‍ അളന്നുകൊടുക്കുന്നു, വ്യത്യസ്ത തരം മാതാപിതാക്കളുടെ കൂട്ടത്തില്‍ അങ്ങനെ ചെയ്യാന്‍ വേണ്ടി, നമ്മള്‍ RNNs കൂട്ടത്തില്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ ആഴത്തിന്റെ ആഴത്തില്‍ കൊടുക്കുന്ന നമ്മള്‍ മിഷ്ടപ്പെട്ട ഫലങ്ങള്‍ റിപ്പോര്‍ട്ട് ചെയ്യുന്നു: കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ നിലനില്‍ക്കുമ്പോള്‍, സാധാരണ RNNs-ന് കൂടാതെ, നമ്മള്‍ സ്റ്റാക്ക് അടിസ്ഥാനമില്ലാത്ത ചില മോഡലുകള്‍ പ്രായശ്ചിത്തം ചെയ്യുന്നു. അത് പൂര്‍ണ്ണമായ കൃത്യമായി</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Байгалийн хэл загварчлахад хэрэглэгдсэн анхны формал грамм (CFG) нь байгалийн хэл загварчлахад ашиглаж байгаа юм. Өнөөдөр эдгээр нь олон хэлбэрийн суурь болно. CFG-ын чухал хэсэг нь дахин сэргээгдэх байдал юм. Энэ цаасан дээр бид туршилтаар хэдэн дахин дахин дахин сэтгэл зүйн сүлжээний чадварыг судалж байна. Илүү тодорхой хэлбэрээр бид тэдний хийх чадварын дээд хэмжээг хэмжээгээр, ерөнхийлөгчийн Дик хэл суралцахын тулд ажлыг хялбарчилж, энэ нь өөр төрлийн эцэг эхчүүдийн холбоотой. Үүнийг хийхэд бид РНХ-г санамсаргүй хэлбэрээр тайлбарлаж, гүн гүнзгий хэлбэрээр байх үед эцэг эхчүүдийг хамгийн гүн гүнзгий хэлбэрээр хадгалах чадварыг шалгана. Бид төвөгдсөн үр дүнг тайлбарлаж байна: ихэвчлэн гүн гүнзгий хэмжээнд орших үед стандарт РНХ-ын тодорхойлолт санамсаргүйгээс их өндөр, гэхдээ төгс хэмжээнээс хол байдаг. Мөн бид стандарт тогтмол биш загваруудыг санал болгож, тэдгээр загваруудын үнэ цэнэтэй тохиромжтой үнэ цэнэтэй тохиромжтой.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grammar bebas konteks (CFG) adalah salah satu alat formal pertama yang digunakan untuk model bahasa alam, dan mereka tetap relevan hari ini sebagai dasar beberapa bingkai. Susunan kunci CFG adalah kehadiran rekursi tersarang. Dalam kertas ini, kami menyelidiki secara eksperimen kemampuan beberapa rangkaian saraf berulang (RNN) untuk belajar rekursi sarang. Lebih tepatnya, kita mengukur batas atas kemampuan mereka untuk melakukannya, dengan mempermudahkan tugas untuk mempelajari bahasa Dyck secara umum, iaitu satu yang terdiri dari kurungan yang sepadan dari berbagai jenis. Untuk melakukannya, kita memperkenalkan RNN dengan set rentetan rawak yang mempunyai kedalaman sarang maksimum yang diberikan dan menguji kemampuannya untuk meramalkan jenis kurungan tutup apabila menghadapi rentetan sarang yang lebih dalam. Kami laporkan keputusan campuran: apabila menyebarkan ke aras sarang yang lebih dalam, ketepatan RNN piawai jauh lebih tinggi daripada rawak, tetapi masih jauh dari sempurna. Selain itu, kami cadangkan beberapa model yang tidak-piawai berdasarkan tumpukan yang boleh mendekati ketepatan sempurna, dengan biaya kepekatan.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il-grammi ħielsa mill-kuntest (CFG) kienu waħda mill-ewwel għodod formali użati għall-mudell tal-lingwi naturali, u għadhom rilevanti llum bħala l-bażi ta’ diversi oqfsa. Ingredjent ewlieni tas-CFG huwa l-preżenza ta’ rikorrezzjoni fil-bejt. F’dan id-dokument, ninvestigaw b’mod sperimentali l-kapaċità ta’ diversi netwerks newrali rikorrenti (RNNs) li jitgħallmu rikorrenza mibnija. B’mod aktar preċiż, a ħna nqisu limitu ta’ fuq tal-kapaċità tagħhom li jagħmlu dan, billi nissimplifikaw il-kompitu li jitgħallmu lingwa Dyck ġeneralizzata, jiġifieri waħda magħmula minn parentesi li jaqblu ta’ diversi tipi. Biex nagħmlu dan, a ħna nippreżentaw l-RNNs b’sett ta’ strings każwali li għandhom fond massimu ta’ nidd partikolari u nittestjaw il-kapaċità tagħha li tbassar it-tip ta’ parenteżi tal-għeluq meta niffaċċjaw strings magħluqa aktar fil-fond. Aħna nirrappurtaw riżultati mħallta: meta niġeneralizzaw għal livelli aktar profondi ta’ nidd, il-preċiżjoni tal-RNNs standard hija ferm ogħla minn dik każwali, iżda għadha ’l bogħod mill-perfetta. Barra minn hekk, qed nipproponu xi mudelli mhux standard ibbażati fuq il-pil li jistgħu jmissu mal-preċiżjoni perfetta, bi spiża ta’ robustezza.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Contextvrije grammatica's (CFG) waren een van de eerste formele instrumenten die werden gebruikt om natuurlijke talen te modelleren, en ze blijven relevant als basis voor verschillende frameworks. Een belangrijk ingrediënt van CFG is de aanwezigheid van geneste recursie. In dit artikel onderzoeken we experimenteel de capaciteit van verschillende recidiverende neurale netwerken (RNN's) om geneste recursie te leren. Precies, we meten een bovengrens van hun vermogen om dit te doen, door de taak om een algemene Dyck taal te leren te vereenvoudigen, namelijk een die bestaat uit bijpassende haakjes van verschillende soorten. Om dit te doen, presenteren we de RNN's met een reeks willekeurige tekenreeksen met een gegeven maximale nestdiepte en testen we het vermogen om het soort afsluitende haakjes te voorspellen wanneer we geconfronteerd worden met diepere geneste tekenreeksen. We rapporteren gemengde resultaten: bij generaliseren naar diepere nesting niveaus, is de nauwkeurigheid van standaard RNN's aanzienlijk hoger dan willekeurig, maar nog lang niet perfect. Daarnaast stellen we enkele niet-standaard stack-gebaseerde modellen voor die perfecte nauwkeurigheid kunnen benaderen, ten koste van robuustheid.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kontekstfri grammar (CFG) var ein av dei første formelte verktøya som brukar for modellering av naturspråk, og dei blir relevante i dag som grunnlag av fleire rammeverktøy. Ein nøkkelingredient av CFG er tilstand til neste gjentaking. I denne papiret undersøker vi eksperimentert kapasiteten for fleire gjentaande neuralnettverk (RNN) for å lære neste rekursjon. Meir nøyaktig måleer vi eit øvre forbindelse av sine kapasiteten til å gjera det, ved å forenkla oppgåva til å lære ein generelt dykkspråk, som er eit samsvarande parentes av ulike typar. For å gjera det, presenterer vi RNN-ane med eit sett av tilfeldige strengar som har ein gitt maksimal nestende dybde og tester den muligheten til å foregå kva typen lukka parentesien når det står med dypere nesterte strengar. Vi rapporterer blandede resultat: når det genereliserer til dypere nesteringsnivåar, er nøyaktigheten av standard RNN betydelig høgare enn tilfeldig, men fortsatt langt frå perfekt. I tillegg foreslår vi nokre ikkje-standard stackbaserte modeller som kan nærme perfekt nøyaktighet ved kostnaden til kraftighet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gramatyki bezkontekstowe (CFG) były jednym z pierwszych narzędzi formalnych wykorzystywanych do modelowania języków naturalnych i pozostają aktualne jako podstawa kilku ram. Kluczowym składnikiem CFG jest obecność rekursji zagnieżdżonej. W niniejszym artykule badamy eksperymentalnie zdolność kilku powtarzających się sieci neuronowych (RNN) do nauki rekursji zagnieżdżonej. Dokładniej mierzymy górną granicę ich zdolności do tego, uproszczając zadanie nauki uogólnionego języka Dycka, a mianowicie takiego złożonego z różnego rodzaju pasujących nawiasów. Aby to zrobić, prezentujemy RNN z zestawem losowych ciągów o danej maksymalnej głębokości zagnieżdżania i testujemy jego zdolność do przewidywania rodzaju zamykającego się nawiasu w obliczu głębszych zagnieżdżonych ciągów. Raportujemy mieszane wyniki: podczas uogólniania do głębszych poziomów gnieżdżenia dokładność standardowych RNN jest znacznie wyższa niż losowa, ale wciąż daleka od doskonałości. Dodatkowo proponujemy niestandardowe modele oparte na stosach, które mogą osiągnąć doskonałą dokładność, kosztem solidności.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>As gramáticas livres de contexto (CFG) foram uma das primeiras ferramentas formais usadas para modelar linguagens naturais e continuam relevantes hoje como base de vários frameworks. Um ingrediente chave do CFG é a presença de recursão aninhada. Neste artigo, investigamos experimentalmente a capacidade de várias redes neurais recorrentes (RNNs) para aprender recursão aninhada. Mais precisamente, medimos um limite superior de sua capacidade de fazê-lo, simplificando a tarefa de aprender uma linguagem Dyck generalizada, ou seja, uma composta de parênteses correspondentes de vários tipos. Para isso, apresentamos as RNNs com um conjunto de strings aleatórias com uma dada profundidade de aninhamento máxima e testamos sua capacidade de prever o tipo de fechamento de parênteses ao enfrentar strings aninhadas mais profundas. Relatamos resultados mistos: ao generalizar para níveis de aninhamento mais profundos, a precisão de RNNs padrão é significativamente maior do que aleatória, mas ainda está longe de ser perfeita. Além disso, propomos alguns modelos baseados em pilha não padronizados que podem se aproximar da precisão perfeita, ao custo da robustez.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grammatica fără context (CFG) a fost unul dintre primele instrumente formale folosite pentru a modela limbile naturale și rămân relevante astăzi ca bază a mai multor cadre. Un ingredient cheie al CFG este prezența recursiei cuibărite. În această lucrare, investigăm experimental capacitatea mai multor rețele neurale recurente (RNN) de a învăța recursia cuibărită. Mai precis, măsurăm o limită superioară a capacității lor de a face acest lucru, simplificând sarcina de a învăța o limbă Dyck generalizată, și anume una compusă din paranteze potrivite de diferite tipuri. Pentru a face acest lucru, prezentăm RNN-urile cu un set de șiruri aleatorii având o adâncime maximă dată de cuibărire și testarea capacității sale de a prezice tipul de paranteze de închidere atunci când se confruntă cu șiruri mai adânci cuibărite. Raportăm rezultate mixte: atunci când generalizăm la niveluri mai profunde de cuibărire, precizia RNN standard este semnificativ mai mare decât aleatoriu, dar încă departe de a fi perfectă. În plus, propunem câteva modele non-standard bazate pe stivă, care pot aborda o precizie perfectă, cu prețul robusteții.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Контекстно-свободные грамматики (CFG) были одним из первых формальных инструментов, используемых для моделирования естественных языков, и они остаются актуальными сегодня в качестве основы нескольких структур. Ключевым ингредиентом CFG является наличие вложенной рекурсии. В этой статье мы экспериментально исследуем способность нескольких рекуррентных нейронных сетей (RNN) изучать вложенную рекурсию. Точнее, мы измеряем верхнюю границу их способности делать это, упрощая задачу изучения обобщенного языка Дайка, а именно языка, состоящего из соответствующих скобок различного рода. Для этого мы представляем RNN с набором случайных строк, имеющих заданную максимальную глубину вложенности, и проверяем его способность предсказывать вид закрывающей скобки при обращении к более глубоким вложенным строкам. Мы сообщаем смешанные результаты: при обобщении до более глубоких уровней вложенности точность стандартных RNN значительно выше, чем случайных, но все еще далека от совершенства. Кроме того, мы предлагаем некоторые нестандартные модели на основе стека, которые могут приближаться к идеальной точности, за счет надежности.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>සම්බන්ධතාවය නිදහස් ග්‍රාමාර්මාර්ස් (CFG) පළමු සාමාන්‍ය භාෂාවක් නිර්මාණය කරන්න ප්‍රධාන භාෂාවක් වලින් එකක් වුන CFG ගේ යතුරු අවශ්‍ය භාවිතාවක් තමයි ප්‍රතිස්ථානයක් තියෙන්නේ. මේ පැත්තට, අපි පරීක්ෂණයෙන් පරීක්ෂණය කරන්න පුළුවන් වෙනස් න්‍යූරල් ජාලය (RNN) වලින් පරීක්ෂණය කරන්න. විශේෂයෙන්, අපි ඔවුන්ගේ කරන්න පුළුවන්ගේ උපරිම සම්බන්ධයක් පරිමාණය කරනවා, සාමාන්‍ය විශේෂිත ඩික් භාෂාවක් ඉගෙන ගන්න වැඩක් සු එහෙම කරන්න, අපි RNN එක පෙන්වන්නේ අවසාන ස්ට්‍රින්ස් එක්ක සම්පූර්ණයක් තියෙන්නේ සම්පූර්ණයෙන් සම්පූර්ණයෙන් සම්පූර්ණයෙන් අඩ අපි වාර්තා කරන්නේ විවිධ ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රති තවත්, අපි ස්ථානය නොප්‍රමාණික ස්ටැක් අධාරිත විදිහට ප්‍රවේශ කරන්න පුළුවන් පුළුවන් සම්පූර්ණයෙන්ම සි</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Slovnice brez konteksta (CFG) so bile eno prvih formalnih orodij za modeliranje naravnih jezikov in so še danes pomembne kot osnova več okvirov. Ključna sestavina CFG je prisotnost gnezdene rekurzije. V prispevku eksperimentalno raziskujemo sposobnost več ponavljajočih se nevronskih omrežij (RNN) za učenje gnezdene rekurzije. Natančneje, merimo zgornjo mejo njihove zmožnosti za to s poenostavitvijo naloge učenja splošnega jezika Dyck, namreč tistega, ki ga sestavljajo ujemajoči se oklepaji različnih vrst. V ta namen predstavimo RNN-je z naborom naključnih nizov z določeno največjo globino gnezdenja in testiramo njihovo sposobnost napovedati vrsto zapiranja oklepajev, ko se soočamo z globljimi gnezdenimi nizi. Poročamo mešane rezultate: pri posploševanju na globlje ravni gnezdenja je natančnost standardnih RNN bistveno višja od naključnih, vendar še vedno daleč od popolnosti. Poleg tega predlagamo nekaj nestandardnih modelov, ki temeljijo na skladišču, ki se lahko približajo popolni natančnosti, na ceno robustnosti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Qoraam xor la’aan ah (CFG) waxay ahaayeen mid ka mid ah qalabka asalka ah ee ugu horeeyay ee loo isticmaalo tusaale ahaan luuqadaha asalka ah, oo maanta waxay ku muhiimsan yihiin sida asalka badan. CFG waa presence of redo. Qoraalkan waxaynu ku baaraanaynaa awoodda u baaraandegista shabakado neurada ah (RNNs) si aan u barno dib u soo celinta. More precisely, we measure an upper bound of their capability to do so, by simplifying the task to learning a generalized Dyck language, namely one composed of matching parentheses of various kinds. Sida darteed waxaynu soo bandhignaynaa RNNs oo ay leedahay xadooyin aad u fudud oo ay leedahay deegaan aad u badan, waxaana imtixaannaa awooddiisa uu horumarinayo in waalidiinta ay soo xidhaan marka ay soo jeedaan xadooyin deegaan. Waxaan wargelinaynaa arimaha isku xiran: marka aad u dhaqdhaqaaqdo heerarka deegaanka, saxda RNNs ayaa aad uga sarraysa fudud, laakiin weli way ka fog tahay kaamil. Sidoo kale waxaynu soo jeedaynaa tusaalooyin aan caadi ahayn oo aan caadi ahayn, taasoo u soo dhowaan kara si saxda ah, kharashka dharka.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gramatikat pa kontekst (CFG) ishin një nga mjetet e para zyrtare të përdorura për të modeluar gjuhët natyrore dhe mbeten të rëndësishme sot si bazë e disa kornizave. Një përbërës kyç i CFG është prania e rekursionit të folur. Në këtë letër, ne hetojmë eksperimentalisht aftësinë e disa rrjeteve neurale të përsëritura (RNN) për të mësuar përsëritjen e folur. Më saktësisht, ne matëm një kufi të lartë të aftësisë së tyre për ta bërë këtë, duke thjeshtuar detyrën për të mësuar një gjuhë të gjeneralizuar Dyck, në të vërtetë një të përbërë nga parentesa të përshtatshme të llojeve të ndryshme. Për ta bërë këtë, ne i prezantojmë RNN-të me një sërë vijash të rastësishme që kanë një thellësi maksimale të dhënë të foljes dhe testojmë aftësinë e saj për të parashikuar llojin e parentezisë së mbylljes kur përballen me vijash më të thella të foljes. Ne raportojmë rezultate të përziera: kur gjeneralizohemi në nivele më të thella të foljes, saktësia e RNN-ve standarte është ndjeshëm më e lartë se e rastësishme, por ende larg perfektit. Additionally, we propose some non-standard stack-based models which can approach perfect accuracy, at the cost of robustness.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grammari bez konteksta (CFG) bili su jedan od prvih formalnih alata koji su koristili za modelu prirodnih jezika, i danas ostaju relevantni kao osnova nekoliko okvira. Ključni sastojak CFG je prisustvo gnijezdane ponovnosti. U ovom papiru istražujemo eksperimentalno sposobnost nekoliko recidivnih neuralnih mreža (RNN) da naučimo nestalnu ponovnost. Više preciznije, mjerimo gornju vezu njihove sposobnosti da to rade, jednostavljajući zadatak da naučimo generalizovani jezik Dyck a, a to je jedan sastavljen od odgovarajućih roditelja različitih vrsta. Da bismo to uradili, predstavljali smo RNN sa setom nasumičnih žica koji imaju određenu maksimalnu dubinu gnijezda i testirali svoju sposobnost da predvidi vrstu zatvaranja parenteze kada se suočavamo sa dubljim gnijezdama. Prijavljujemo miješane rezultate: kada generalizujemo na dublje nivoe gnijezda, preciznost standardnih RNN-a je značajno veća od nasumičnih, ali i dalje od savršenog. Osim toga, predlažemo neke ne-standardne modele koji mogu pristupiti savršenoj tačnosti, po troškovi pljačke.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kontextfria grammatiker (CFG) var ett av de första formella verktygen som användes för att modellera naturliga språk, och de är fortfarande relevanta idag som grund för flera ramverk. En viktig ingrediens i CFG är närvaron av kapslad rekursion. I denna uppsats undersöker vi experimentellt förmågan hos flera återkommande neurala nätverk (RNN) att lära sig kapslad rekursion. Mer exakt mäter vi en övre gräns av deras förmåga att göra det, genom att förenkla uppgiften att lära sig ett generaliserat Dyck språk, nämligen ett bestående av matchande parenteser av olika slag. För att göra det presenterar vi RNN:erna med en uppsättning slumpmässiga strängar som har ett givet maximalt kapslingsdjup och testar dess förmåga att förutsäga vilken typ av stängande parentes när de möter djupare kapslade strängar. Vi rapporterar blandade resultat: vid generalisering till djupare häckningsnivåer är noggrannheten hos standard RNN betydligt högre än slumpmässigt, men fortfarande långt ifrån perfekt. Dessutom föreslår vi några icke-standardiserade stapelbaserade modeller som kan närma sig perfekt noggrannhet, på bekostnad av robusthet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Vyombo vya kwanza vilivyotumiwa kwa ajili ya kutengeneza lugha za asili, na bado vinaendelea kuwa na umuhimu leo kama msingi wa miundombi kadhaa. Kiungo muhimu cha CFG ni uwepo wa kurudishwa kwa makazi. Katika gazeti hili, tunachunguza kwa mtihani wa uwezo wa mitandao kadhaa ya kijamii yanayoendelea (RNNs) ili kujifunza upinzani wa makazi. Zaidi zaidi, tunapima mfumo wa juu wa uwezo wao wa kufanya hivyo, kwa kufurahisha kazi ya kujifunza lugha ya Dykk inayotengenezwa, yaani moja iliyoandaliwa na wazazi wa aina mbalimbali. Ili kufanya hivyo, tunawaunganisha RNN na mfululizo wa mistari yenye kiwango kikubwa cha makazi na tunajaribu uwezo wake wa kutabiri namna ya kufungwa kwa wazazi pale wanapokutana na mistari ya ndani. Tunatoa taarifa za matokeo yanayochanganyika: pale ambapo kwa ujumla kufikia kiwango cha makazi zaidi, uhakika wa RNNs wa kiwango cha kawaida ni mkubwa zaidi ya urahisi, lakini bado ni mbali na kamili. Kwa nyongeza, tunapendekeza baadhi ya mifano isiyo na msingi wa msingi ambao unaweza kufikia ukweli sahihi, kwa gharama za nguo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>உள்ளமைப்பு இலவச வரைப்படங்கள் (CFG) முதல் வடிவமைப்பு மொழிகளை மாதிரிக்க பயன்படுத்தப்பட்டுள்ள கருவிகளில் ஒன்றாக இருந்தன, மற்றும் இன்று CFG விசை பொருள் என்பது கூட்டு திரும்ப திரும்ப செலுத்தல் இருக்கிறது. இந்த காகிதத்தில், நாம் சோதனையில் செய்து சில திரும்பப்பட்ட திரும்பச் செலுத்தப்பட்ட புதிய வலைப்பின்னல்களின் இயல்ப மேலும், நாம் அதை செய்ய முடியும் ஒரு மேல் பிணைப்பை அளக்குகிறோம், பொதுவான டைக் மொழியை கற்றுக்கொள்ளும் செயலை எளிதாக்குகிறோம், அதாவது பல To do so, we present the RNNs with a set of random strings having a given maximum nesting depth and test its ability to predict the kind of closing parenthesis when facing deeper nested strings. நாம் கலப்பு முடிவுகளை அறிவிக்கிறோம்: ஆழமான கூட்டு மட்டத்திற்கு பொதுவாக்கும்போது, நிலையான RNs துல்லியமானது, ஆனால் இன்னும மேலும், நாம் சில நிலையான ஸ்டாக் அடிப்படையில் இல்லாத மாதிரிகளை பரிந்துரைக்கிறோம். அது சரியாக சரியாக நெருங்க முடி</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Täbiýe dillerini örenmek üçin ullanýan ilkinji formal çarpaklardan biri we bu gün birnäçe çarpaklaryň esasy bolup geçirilýär. CFG'yň aç aýtemy ýygnan tekrarlamak bolmalydyr. Bu kagyzda, birnäçe tekrarly netijeler (RNN) ýerleşmelerini öwrenmek üçin synanyşdyrylýarys. Daha dogry bir şekilde, biz olaryň üstlerini bu şekilde etjek ukyplaryny ölçüp, bu işi döredilmiş Dyk dilini öwrenmek üçin esasy bir şekilde örän çeşitli atalaryň arasynda ýerleşdirýäris. Böyle yapmak üçin, RNN'i biraz tesadüf düzgüleri ile berip, derin çöplük düzgüleri ile üzülerek oturan çöplüklerini tanımlamak için elimizde bulunuyoruz. Karışık netijeleri bildiriyoruz: jeneral çöplük derejesine çevrildikde standart RNN'in titizligi rastgelerden daha yüksekdir, ýöne bu şekilde mükemmel derecelerden uzakdır. Hem biz standart taýýarlanmaky bir nusgalary teklip edip, güýçli taýýarlanmaky üçin ajaýyp derejese gollaşabiler.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>کنٹکسٹ بیزار گرامر (CFG) سب سے پہلے قانونی ابزار تھے جو طبیعی زبانوں کی مدل کرنے کے لئے استعمال کئے جاتے تھے، اور آج وہ چند فرمورک کی بنیاد کے طور پر اہم رہتے ہیں. Name اس کاغذ میں ہم نے آزمائش کے ساتھ کئی دوبارہ تکرار نائرول نیٹورک (RNN) کے قابلیت کی تحقیق کی۔ بیشک ہم ان کے قابلیت کے اوپر سے اندازہ لیتے ہیں، اس طرح کام کو سادھا دیتے ہیں ایک عمومی دیک زبان کی تعلیم، یعنی ایک مختلف طریقوں کے ماں باپ کے مطابق پیدا کیا گیا ہے. اس کے لئے ہم RNS کو ایک مجموعہ سیٹ کے ساتھ پیش کرتے ہیں جن کے لئے مہربانی سیٹ گھاٹ کی گھاٹی ہے اور اس کی قابلیت کی آزمائش کریں کہ عمیق سیٹ سیٹ سیٹ کے سامنے موجود ہونے کے لئے پڑھنے کے پڑھنے کی طرح کی مخلوق کریں۔ ہم مختلف نتائج راپورٹ کرتے ہیں: جب عمومی نیسٹنگ سطح تک آگاہ ہوتے ہیں، استاندارڈ RNN کی دقیق رانڈ سے زیادہ بلند ہے، لیکن اب بھی کامل سے دور ہے. اور اضافہ، ہم ایسے بغیر استاندارڈ بنیادی موڈل کو پیشنهاد کرتے ہیں جو بالکل دقیق کے مطابق پہنچ سکتے ہیں، دقت کے مطابق۔</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>@ info: whatsthis Name Bu qogʻozda biz qaytadan qayta keladigan neyron tarmoqlari (RNNs) qo'llanmalarni o'rganish imkoniyatini tajriba qilamiz. Ko'proq, biz buni bajarish uchun eng yuqori qobiliyatni o'rganamiz, yani umumiy Dykk tilni o'rganish orqali oddiy qilamiz. Bu bir tomondan ota-onalar bir xil turli bo'lgan biri. Bunday qilish uchun, biz RNNS'larni eng katta qo'shish qo'shiq qo'shilga oshirish qobiliyatiga qarab, ota-onalarni qo'yish qanday qo'yish qobiliyatini predict qilamiz. Biz bir qanchalik natijalar haqida xabar beramiz: Qachon eng yuqori darajaga generaliz qilayotganda, standard RNNs haqida juda katta, lekin hammasi to ʻgʻri yo'q. Additionally, we propose some non-standard stack-based models which can approach perfect accuracy, at the cost of robustness.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Chương trình không ngữ cảnh là một trong những công cụ chính thức đầu tiên được dùng để mô tả ngôn ngữ tự nhiên, và chúng vẫn còn liên quan như cơ sở của một số nền tảng. Một thành phần quan trọng của CG là sự hiện diện của việc qui phục ấp. Trong tờ giấy này, chúng tôi nghiên cứu thử nghiệm khả năng của nhiều mạng thần kinh liên tục (RNN) để học tồn tại tại tại tại tại tổ. Chúng tôi đánh giá chính xác mức ràng buộc trên khả năng của họ, bằng cách đơn giản hóa nhiệm vụ học một ngôn ngữ liên bang Dyck, cụ thể là ngôn ngữ phù hợp các chủng tộc khác nhau. Để làm vậy, chúng tôi giới thiệu RNNs với một bộ chuỗi ngẫu nhiên có độ sâu làm tổ tối đa nhất định sẵn và thử khả năng dự đoán người giống ấp lại khi đối mặt với những chuỗi được làm tổ sâu hơn. Báo cáo kết quả hỗn hợp: khi phát tán tới mức làm tổ sâu hơn, độ chính xác của RNN tiêu chuẩn cao đáng kể hơn ngẫu nhiên, nhưng vẫn còn xa hoàn hảo. Thêm nữa, chúng tôi đề nghị một số mẫu không tiêu chuẩn dựa trên chồng có thể tiếp cận độ chính xác hoàn hảo, với giá của độ bền vững.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>上下文无关语法(CFG)始于自然语言建模之正器,今犹有相关性,数框架之基也。 CFG 之要,嵌套递归之存也。 本文之中,实验究数种递归神经网络(RNN)学嵌套递归之力。 正言以简学广义Dyck言语之务以量其能上限,则百匹之括号言也。 为供 RNN 给定最大嵌套深度随机字符串,试对更嵌套字符串占右括号力。 臣等奏对不同:当推广更嵌套水平,准RNN准确性显高随机,而犹远非全美。 又立堆栈之非标准,以鲁棒性为近美之精。</span></div></div><dl><dt>Anthology ID:</dt><dd>2018.lilt-16.1</dd><dt>Volume:</dt><dd><a href=/volumes/2018.lilt-16/>Linguistic Issues in Language Technology, Volume 16, 2018</a></dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd></dd><dt>Venue:</dt><dd><a href=/venues/lilt/>LILT</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>CSLI Publications</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd></dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2018.lilt-16.1>https://aclanthology.org/2018.lilt-16.1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">bernardy-2018-recurrent</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Jean-Phillipe Bernardy. 2018. <a href=https://aclanthology.org/2018.lilt-16.1>Can Recurrent Neural Networks Learn Nested Recursion?</a>. In <i>Linguistic Issues in Language Technology, Volume 16, 2018</i>. CSLI Publications.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2018.lilt-16.1>Can Recurrent Neural Networks Learn Nested Recursion?</a> (Bernardy, LILT 2018)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2018.lilt-16.1.pdf>https://aclanthology.org/2018.lilt-16.1.pdf</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2018.lilt-16.1.pdf title="Open PDF of 'Can Recurrent Neural Networks Learn Nested Recursion?'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Can+Recurrent+Neural+Networks+Learn+Nested+Recursion%3F" title="Search for 'Can Recurrent Neural Networks Learn Nested Recursion?' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Can Recurrent Neural Networks Learn Nested Recursion?'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Can Recurrent Neural Networks Learn Nested Recursion?](https://aclanthology.org/2018.lilt-16.1) (Bernardy, LILT 2018)</p><ul class=mt-2><li><a href=https://aclanthology.org/2018.lilt-16.1>Can Recurrent Neural Networks Learn Nested Recursion?</a> (Bernardy, LILT 2018)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Jean-Phillipe Bernardy. 2018. <a href=https://aclanthology.org/2018.lilt-16.1>Can Recurrent Neural Networks Learn Nested Recursion?</a>. In <i>Linguistic Issues in Language Technology, Volume 16, 2018</i>. CSLI Publications.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>