<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering" name=citation_title><meta content="Meryem M’hamdi" name=citation_author><meta content="Doo Soon Kim" name=citation_author><meta content="Franck Dernoncourt" name=citation_author><meta content="Trung Bui" name=citation_author><meta content="Xiang Ren" name=citation_author><meta content="Jonathan May" name=citation_author><meta content="Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies" name=citation_conference_title><meta content="2021/6" name=citation_publication_date><meta content="https://aclanthology.org/2021.naacl-main.283.pdf" name=citation_pdf_url><meta content="3617" name=citation_firstpage><meta content="3632" name=citation_lastpage><meta content="10.18653/v1/2021.naacl-main.283" name=citation_doi><meta property="og:title" content="X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering"><meta property="og:image" content="https://aclanthology.org/thumb/2021.naacl-main.283.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2021.naacl-main.283"><meta property="og:description" content="Meryem M’hamdi, Doo Soon Kim, Franck Dernoncourt, Trung Bui, Xiang Ren, Jonathan May. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."><link rel=canonical href=https://aclanthology.org/2021.naacl-main.283></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> and Question Answering<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>METRA</span>-<span class=acl-fixed-case>ADA</span>: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a>
<a id=af_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X- METRA- ADA: Cross- language Meta- Transfer leer aanpassing na Natuurlike Taal Verstaan en Fraag Antwoord</a>
<a id=am_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: የቋንቋ-ቋንቋ ማቀናጃ ለአዳራዊ ቋንቋ ማስታወቂያ እና ጥያቄ መልስ</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: تكيف تعلم التحويل متعدد اللغات عبر اللغات مع فهم اللغة الطبيعية والإجابة على الأسئلة</a>
<a id=az_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Təbiətli dil anlaşılması və sual cavab verməsi üçün çox dilli meta-Transfer öyrənməsi</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>Адаптация към разбирането на естествения език и отговаряне на въпроси</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>এক্স- মেট্রা- এডা: ক্রস-ভাষার মেটা- ট্রান্সফারের শিক্ষা প্রাকৃতিক ভাষায় বুঝতে এবং প্রশ্নের উত্তর</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Prejezička meta-prebacivanje učenja prilagođenja prirodnom razumijevanju jezika i odgovoru na pitanja</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Adaptació de l'aprenentatge translingüístic de metatransferències a l'entendre del llenguatge natural i resposta a preguntes</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptace na porozumění přirozenému jazyku a zodpovězení otázek</a>
<a id=da_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Tværsproget Meta-Transfer læring Tilpasning til natursprogforståelse og spørgsmål besvarelse</a>
<a id=de_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Cross-lingual Meta-Transfer Learning Anpassung an das Verstehen natürlicher Sprache und die Beantwortung von Fragen</a>
<a id=el_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>Χ-ΜΕΤΡΑ-ADA: Διγλωσσική μάθηση Μεταμεταφορά Προσαρμογή στη Φυσική Γλώσσα Κατανόησης και Απάντηση Ερωτήσεων</a>
<a id=es_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Aprendizaje de meta-transferencia multilingüe Adaptación a la comprensión del lenguaje natural y la respuesta a preguntas</a>
<a id=et_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Keeleülene metaülekanne õppimine Kohanemine loodusliku keele mõistmise ja küsimustele vastamisega</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: تغییرات یادگیری متا-انتقال متا-زبانی به درک زبان طبیعی و پاسخ سوال</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Monikielinen metasiirtooppiminen Sopeutuminen luonnolliseen kieleen Ymmärtäminen ja kyselyihin vastaaminen</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA : Apprentissage par méta-transfert multilingue Adaptation à la compréhension du langage naturel et à la réponse aux questions</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Foghlaim Meta-Aistrithe Thrasteangach Oiriúnú do Thuiscint Teanga Nádúrtha agus Freagairt Cheisteanna</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Ana karanta littafin Farawa na Meta-Transforms</a>
<a id=he_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: התאמה למטה-העברה בין שפות</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: क्रॉस-लिंगुअल मेटा-ट्रांसफर सीखने के अनुकूलन को प्राकृतिक भाषा की समझ और प्रश्न का उत्तर देने के लिए अनुकूलन</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Prejezička meta-prebacivanje učenja prilagođenja prirodnom razumijevanju jezika i odgovoru na pitanja</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Többnyelvű metatranszfer tanulás Alkalmazkodás a természetes nyelv megértéséhez és a kérdések megválaszolásához</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-Մետրա-ԱԴԱ. Խոսքերի միջև փոխանցվող մետափոխանցման ուսումնասիրությունը հարմարվում է բնական լեզվի հասկանալու և հարցերի պատասխանների հետ</a>
<a id=id_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Pelajaran Meta-Transfer saling bahasa Adaptasi ke Pemahaman Bahasa Alami dan Jawaban Pertanyaan</a>
<a id=is_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Meta-Transfer learning multilingue Adattamento alla comprensione del linguaggio naturale e risposta alle domande</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X - METRA - ADA ：クロスリンガルメタトランスファー学習自然言語理解と質問回答への適応</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>Name</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X- METRA- ADA: Түзіндік тілдерді түсініп және сұрақтарды жауап беру үшін бірнеше тілдерді мета- трансферлерді оқыту адаптациясы</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>크로스 언어 원 이동 학습 자연 언어 이해와 문제 대답에 적응</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Tarpkalbinis metaperdavimo mokymasis Prisitaikymas prie natūralios kalbos supratimo ir klausimų atsakymo</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>എക്സ്- മെട്രാ- ADA: ക്രോസ്- ഭാഷ മെറ്റ- മാറ്റാന്‍ പഠിക്കുന്നത് സ്വാഭാവികമായ ഭാഷയിലേക്കുള്ള അഡാപ്റ്റേഷന്‍ ബോധനമാ</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Түүнчлэн хэлний мета-трансфер суралцлагын адаптация байгалийн хэл ойлголт болон асуулт хариулт</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Penyesuaian Meta-Pemindahan Selasa Bahasa untuk Pemahaman Bahasa Alami dan Jawapan Pertanyaan</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Tagħlim translingwi ta’ Meta-Trasferiment Adattament għall-fehim tal-lingwa naturali u t-tweġibiet għall-mistoqsijiet</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Crosslingual Meta-Transfer leren Aanpassing aan het begrijpen van natuurlijke taal en het beantwoorden van vragen</a>
<a id=no_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Læringsadaptasjon til naturleg språk forståking og spørsmål</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Nauka metatransferowa międzyjęzyczna Adaptacja do rozumienia języka naturalnego i odpowiadania na pytania</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Adaptação de aprendizagem de metatransferência multilíngue para compreensão de linguagem natural e resposta a perguntas</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Învățare interlingvă Meta-Transfer Adaptare la înțelegerea limbii naturale și răspunsul la întrebări</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Кросс-лингвистическое мета-передача обучения Адаптация к пониманию естественного языка и ответы на вопросы</a>
<a id=si_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Cross-language Meta-Transfer</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Medjezično metatransferno učenje Prilagajanje razumevanju naravnega jezika in odgovarjanje na vprašanja</a>
<a id=so_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Waxbarashada afka kala duwan ee meta-wareejinta</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Ndryshimi ndërgjuhësor i mësimit të metatransferimit në kuptimin e gjuhës natyrore dhe përgjigjen e pyetjeve</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Prejezička meta-prevoz učenja prilagođenja prirodnom razumijevanju jezika i odgovoru na pitanja</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Cross-lingual Meta-Transfer lärande Anpassning till naturligt språk förståelse och frågesvar</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Kufunza Uhamiaji wa lugha ya Kimataifa na Kuhamasisha Ujumbe kwa lugha ya Kiasili</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X- METRA- ADA: Cross- language Meta- Transfer learning Adaptation to Natural Language Understanding and Question answers</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Dabiýal dil düşünemek we sorag jogap</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: Cross-lingual Meta-Transfer learning adaptation to Natural Language Understanding and Question Answering</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X- METRA- ADA: Foydalanuvchi va savol javobi</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA: vặn khóa học siêu truyền thống về học về ngôn ngữ tự nhiên và câu hỏi đáp ứng</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2021.naacl-main.283.pdf>X-METRA-ADA:跨语元移学自然语言解问答</a></h2><p class=lead><a href=/people/m/meryem-mhamdi/>Meryem M’hamdi</a>,
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>,
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>,
<a href=/people/t/trung-bui/>Trung Bui</a>,
<a href=/people/x/xiang-ren/>Xiang Ren</a>,
<a href=/people/j/jonathan-may/>Jonathan May</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a> is still inconsistent for <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typologically diverse languages</a> and across different benchmarks. Recently, <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a> has garnered attention as a promising technique for enhancing <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> under low-resource scenarios : particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks : multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Veelvuldige modele, soos M-BERT en XLM-R, het vergroot populariteit verkry, vanweë hul nul-skoot kruistale oordrag-leer kapasiteite. Maar hulle generelliseeringskapasiteit is nog steeds ongelukkig vir tipologies verskillende tale en oor verskillende benchmarke. Onlangs het meta-leer aandag gehou as 'n beloftende teknik vir verhoog van oordrag leer onder lae hulpbronne scenarios: spesiaal vir kruistale oordrag in Natuurlike Taal Verstaan (NLU). In hierdie werk voorstel ons X-METRA-ADA, 'n kruistale MEta-TRAnsfer leer ADAptasie toegang vir NLU. Ons toegang pas MAML, en 'n optimalisasie-gebaseerde meta-leer toegang, om te leer om aan nuwe tale te pas. Ons uitbreidig ons raamwerk evalueer op twee uitgelykende kruistale NLU-taak: multitaalske taak-orienteerde dialoog en tipologiese verskeie vraag antwoord. Ons wys dat ons toegang uitvoer naive fin-tuning, bereik rekenaktiewe prestasie op beide opdragte vir die meeste tale. Ons analisie openbaar dat X-METRA-ADA kan beperkte data vir vinniger aanpassing verwys.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>እንደ M-BERT እና XLM-R፣ የቋንቋ-ቋንቋ-ቋንቋ ተማሪ ችሎታቸውን በቁጥር የተመሳሳይ የቋንቋ ቋንቋዎች አካሄዱ፡፡ ነገር ግን የልዩ ቋንቋዎች እና ልዩ ልዩ መለያየት አካባቢዎች የሚቆጣጠር ኃይላቸው ገና የማይገባው ነው፡፡ Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). በዚህ ሥራ የሜትራ-ADA የሜታ-TRAnsfer ADAptation ቅድሚያ ለNLU የመማር የቋንቋ ቋንቋ መፍጠርን እናስጀጋለን፡፡ አዲስ ቋንቋዎች ለመተማር ማድረግ ማድረጉን ማህበራዊ መተማር ማድረጉን MAML ያሳድጋል፡፡ በሁለት የቋንቋ ቋንቋዎች የNLU ስራዎችን በሚያጋልጡ ሥርዓታችንን በብዙ ልዩ ቋንቋ እና በተለያዩ የጥያቄ መልስ እናሳውቃለን፡፡ እናሳያቸዋለን የሁለቱ ቋንቋዎች ላይ የፍላጎችን ትክክል እናደርጋለን፡፡ Analysያችን X-METRA-ADA ለፈጥነህ አካባቢ ዳታዎችን የሚያስቀምጥ ይችላል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>اكتسبت النماذج متعددة اللغات ، مثل M-BERT و XLM-R ، شعبية متزايدة ، نظرًا لقدراتها التعليمية على النقل عبر اللغات بدون طلقة. ومع ذلك ، فإن قدرتها على التعميم لا تزال غير متسقة بالنسبة للغات المتنوعة من حيث التصنيف وعبر معايير مختلفة. في الآونة الأخيرة ، حظي التعلم التلوي بالاهتمام باعتباره أسلوبًا واعدًا لتعزيز التعلم الانتقالي في ظل سيناريوهات الموارد المنخفضة: لا سيما بالنسبة للنقل عبر اللغات في فهم اللغة الطبيعية (NLU). في هذا العمل ، نقترح X-METRA-ADA ، نهج التعلم MEta-TRAnsfer التعلم عبر اللغات لـ NLU. يتكيف نهجنا مع MAML ، وهو نهج التعلم التلوي القائم على التحسين ، لتعلم التكيف مع اللغات الجديدة. نقوم بتقييم إطار عملنا على نطاق واسع في مهمتين صعبتين في NLU عبر اللغات: الحوار متعدد اللغات الموجه نحو المهام والإجابة على الأسئلة المتنوعة بشكل نمطي. نظهر أن نهجنا يتفوق في الأداء على الضبط الدقيق الساذج ، حيث يصل إلى أداء تنافسي في كلتا المهمتين لمعظم اللغات. يكشف تحليلنا أن X-METRA-ADA يمكنها الاستفادة من البيانات المحدودة للتكيف بشكل أسرع.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>M-BERT və XLM-R kimi çoxlu dil modelləri, sıfır-fərq dillərini öyrənmə qabiliyyətlərinə görə, məşhurluqlarını artırdı. Halbuki, onların generalizasyon qabiliyyəti typolojik müxtəlif dillərə və müxtəlif benchmarklərə uyğun deyildir. Son zamanlarda, meta-öyrənmə təsirlərini düşük ressurs scenariyalarının altında transfer öyrənməsini artırmaq üçün və ’ d verici tekniki olaraq təsirlərini öyrəndi: özellikle təbiətli dil anlama (NLU) içində çox çox dilli transfer üçün. Bu işdə X-METRA-ADA, NLU üçün çox dilli MEta-TRAnsfer öyrənən ADAptasyon metodlarını təklif edirik. Bizim yaxınlığımız MAML'i, optimizasiya tabanlı meta öyrənmə metodlarına uyğunlaşdırır, yeni dillərə uyğunlaşdırmağı öyrənir. Biz çerçivelərimizi iki dildən çox çətin NLU işlərində çox çətin değerləşdiririk: çoxlu dil işləri tərəfindən danışmış danışma və typolojik müxtəlif sual cavabı. Biz göstəririk ki, bizim tərəfimiz çox dillərin hər ikisinin müqayisədə müqayisədə olan hərəkətlərini təşkil edir. Bizim analizimiz X-METRA-ADA'nın daha hızlı uyğunlaşdırma üçün sınırlı məlumatları yaradıb edə biləcəyini göstərir.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Многоезичните модели, като например придобиват все по-голяма популярност, благодарение на техните възможности за обучение с нулев изстрел между езиците. Въпреки това, способността им за обобщаване все още е несъвместима за типологично разнообразни езици и в различни показатели. Наскоро метаобучението привлече внимание като обещаваща техника за подобряване на трансферното обучение при сценарии с ниски ресурси: особено за междуезичен трансфер в разбирането на естествения език (НЛУ). В тази работа предлагаме Х-МЕТРА-АДА, междуезичен подход за обучение за НЛУ. Нашият подход адаптира базиран на оптимизация мета-учене подход, за да се научи да се адаптира към нови езици. Ние обстойно оценяваме нашата рамка по две предизвикателни междуезични задачи на НЛУ: многоезичен диалог, ориентиран към задачите и типологично разнообразен отговор на въпроси. Показваме, че нашият подход превъзхожда наивното фино настройване, достигайки конкурентни резултати и при двете задачи за повечето езици. Нашият анализ показва, че Х-МЕТРА-АДА може да използва ограничени данни за по-бърза адаптация.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>মাল্টিভাষার মডেল, যেমন M-BERT এবং XLM-R, তাদের শুধুমাত্র শিক্ষা শিক্ষার কারণে জনপ্রিয়তা বৃদ্ধি পেয়েছে। তবে তাদের সাধারণ ভাষায় বিভিন্ন ভাষার জন্য তাদের জেনারেলেশনের ক্ষমতা এখনও অসম্পূর্ণ। Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). এই কাজে আমরা এক্স-মেট্রা-এডাকে প্রস্তাব করি, যা ক্রাশ-ভাষার মেটা-ট্রান্সফার এডিএপ্যাটেশন ক্ষেত্রে শিখতে পারে। আমাদের প্রতিযোগিতা ম্যাএমএলের সাথে সুনির্দিষ্ট ভিত্তিক মেটা শিক্ষার উপায়, নতুন ভাষায় যুক্ত করতে শিখতে। আমরা ব্যাপকভাবে আমাদের ফ্রেম মূল্য দিচ্ছি যে দুটি চ্যালেঞ্জের ব্যাপারে প্রতিযোগিতা করছি এনএলইউ কাজ: বহুভাষায় কাজের মুখোমুখি আমরা দেখাচ্ছি যে আমাদের প্রতিযোগিতা বেশীরভাগ ভাষার জন্য প্রতিযোগিতার প্রতিযোগিতা প্রদর্শন করছে। আমাদের বিশ্লেষণ প্রকাশ করেছে যে এক্স-মেট্রা-ADA দ্রুত আপেটশনের জন্য সীমিত তথ্য লাভ করতে পারে।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>སྐད་རིགས་ཀྱི་མིག་དཔེ་དཔེར་ན། M-BERT དང་ XLM-R ། མི་མང་ཆེ་ཤོས་ཀྱི་ཆ་རྐྱེན་ལ་བསྐྱེད་ཡོད། ཡིན་ནའང་། ཁོང་ཚོའི་སྤྱིར་བཏང་ན་ཆ་འཕྲིན་ཡིན་པའི་སྐད་རིགས་མི་འདྲ་བ་ཡིན། འཕྲལ་མ་དེ་ལྟ་བུ འོན་ཀྱང་། ང་ཚོས་X-METRA-ADA(translingual MEta-TRAnsfer)བརྡ་སྤྲོད་ཀྱི་ཐབས་ལམ་དེ་གིས་NLU་ལ་སྤྱོད་པའི་གཟུགས་བརྙན་ཞིག་ཡོད། ང་ཚོའི་ཐབས་ལམ་གྱིས་MAML་ལ་སྒྲིག་པ་ཞིག་གཙོ་བྱེད་ཀྱི་ཐབས་ལམ་ལྟར་བྱེད་ཀྱི་ཡོད། ང་ཚོས་རིགས་འདིའི་མཐུན་སྣུམ་གྱི་གནད ང་ཚོས་རང་གི་ཐབས་ལམ་དེ་མིན་ལྡན་མིན་ཐང་པོ་ཞིག་ཡོད་པ་ལས། སྐད་རིགས་ཆེ་ཤོས་ཀྱི་ལས་འགུལ་གྱི་རྒྱལ་སྐྱོར་དང ང་ཚོའི་དབྱེ་ཞིབ་ཀྱིས་X-METRA</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mnogjezički modeli, kao što su M-BERT i XLM-R, dobili su veću popularnost zbog njihovih sposobnosti za učenje preko jezika bez snimanja. Međutim, njihova sposobnost generalizacije još uvijek nije u skladu sa tipološki različitim jezicima i preko različitih kriterija. Nedavno je metaučenje privuklo pažnju kao obećavajuću tehniku za unapređenje učenja prijenosa pod scenarijem niskih resursa: posebno za cross-lingual transfer u razumijevanju prirodnog jezika (NLU). U ovom poslu predlažemo X-METRA-ADA, preko jezika MEta-TRAnsfer, pristup ADAptacije za NLU. Naš pristup prilagođuje MAML, optimizacijski metaučenjeni pristup, da nauči da se prilagodi novim jezicima. Mi široko procjenjujemo naš okvir na dva izazova preko jezika NLU zadataka: multijezički dijalog orijentiran na zadatak i tipološki različit odgovor na pitanje. Pokazujemo da naš pristup nadmašuje naivnu finalnu prilagodbu, postignući konkurentnu funkciju na obje zadatke za većinu jezika. Naša analiza otkriva da X-METRA-ADA može utjecati na ograničene podatke za brže adaptacije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Els models multilingües, com M-BERT i XLM-R, han guanyat una popularitat creixent, a causa de les seves capacitats d'aprenentatge translingüístic de transfer ència de zero. Tot i així, la seva habilitat de generalització encara és inconsistent en llengües tipològicament diverses i en diferents punts de referència. Recentment, el meta-aprenentatge ha guanyat atenció com una tècnica prometedora per millorar l'aprenentatge de transfer ència en escenaris de baix recursos: especialment per la transferència translingüística a l'Entensió de Llingua Natural (NLU). En aquesta feina, proposem X-METRA-ADA, un enfocament d'aprenentatge translingüístic de MEta-TRAnsfer adaptació per a NLU. El nostre enfocament adapta MAML, un enfocament de meta-aprenentatge basat en l'optimització, per aprendre a adaptar-se a les noves llengües. Evaluam ampliament el nostre marc en dues tasques translingües desafiables de la NLU: diàleg multilingüe orientat a tasques i resposta tipològicament diversa a preguntes. Mostrem que el nostre enfocament supera una perfecció naiva, arribant a un rendiment competitiu en les dues tasques de la majoria de llengües. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Vícejazyčné modely, jako například M-BERT a XLM-R, získaly rostoucí popularitu díky svým možnostem učení s nulovým přenosem mezi jazyky. Jejich generalizační schopnost je však stále nekonzistentní pro typologicky různé jazyky a napříč různými referenčními hodnotami. V poslední době si meta-learning získal pozornost jako slibná technika pro zlepšení transferového učení v rámci scénářů s nízkými zdroji: zejména pro přenos mezi jazyky v porozumění přírodním jazykům (NLU). V této práci navrhujeme X-METRA-ADA, cross-jazyčný MEta-TRAnsfer learning ADAPTation přístup pro NLU. Náš přístup přizpůsobuje MAML, meta-learningový přístup založený na optimalizaci, aby se naučil adaptovat na nové jazyky. Důkladně hodnotíme náš rámec na dvou náročných vícejazyčných úkolech NLU: vícejazyčný dialog orientovaný na úkoly a typologicky rozmanitý odpověď na otázky. Ukazujeme, že náš přístup překonává naivní jemné ladění a dosahuje konkurenčního výkonu u obou úkolů pro většinu jazyků. Naše analýza ukazuje, že X-METRA-ADA může využít omezené množství dat pro rychlejší adaptaci.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Flersprogede modeller, såsom M-BERT og XLM-R, har fået stigende popularitet på grund af deres nul-shot tværsprogede overførsel læringsevner. Men deres generaliseringsevne er stadig inkonsekvent for typologisk forskellige sprog og på tværs af forskellige benchmarks. For nylig har meta-læring fået opmærksomhed som en lovende teknik til at forbedre overførsel af læring under scenarier med lave ressourcer: især for tværsproget overførsel i Natural Language Understanding (NLU). I dette arbejde foreslår vi X-METRA-ADA, en tværsproget MEta-TRAnsfer læring ADAPTATION tilgang til NLU. Vores tilgang tilpasser MAML, en optimeringsbaseret meta-læringstilgang, til at lære at tilpasse sig nye sprog. Vi evaluerer grundigt vores rammer på to udfordrende tværsprogede NLU-opgaver: flersproget opgaveorienteret dialog og typologisk forskelligartet spørgsmål besvarelse. Vi viser, at vores tilgang overgår naiv finjustering og opnår konkurrencedygtige resultater på begge opgaver for de fleste sprog. Vores analyse afslører, at X-METRA-ADA kan udnytte begrænsede data til hurtigere tilpasning.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mehrsprachige Modelle, wie M-BERT und XLM-R, haben aufgrund ihrer Zero-Shot Cross-Lingual Transfer-Lernfähigkeit zunehmend an Popularität gewonnen. Ihre Verallgemeinerungsfähigkeit ist jedoch immer noch inkonsistent für typologisch unterschiedliche Sprachen und über verschiedene Benchmarks hinweg. In letzter Zeit hat Meta-Learning als vielversprechende Technik zur Verbesserung des Transferlernens unter ressourcenarmen Szenarien Aufmerksamkeit erregt: insbesondere für den translingualen Transfer im Natural Language Understanding (NLU). In dieser Arbeit schlagen wir X-METRA-ADA vor, einen mehrsprachigen MEta-TRAnsfer Lernansatz für NLU. Unser Ansatz adaptiert MAML, einen optimierungsbasierten Meta-Learning-Ansatz, um zu lernen, sich an neue Sprachen anzupassen. Wir evaluieren unser Framework umfassend auf zwei herausfordernde, sprachübergreifende NLU-Aufgaben: mehrsprachiger aufgabenorientierter Dialog und typologisch vielfältige Fragestellungen. Wir zeigen, dass unser Ansatz die naive Feinabstimmung übertrifft und für die meisten Sprachen eine wettbewerbsfähige Leistung bei beiden Aufgaben erreicht. Unsere Analyse zeigt, dass X-METRA-ADA begrenzte Daten für eine schnellere Anpassung nutzen kann.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Τα πολυγλωσσικά μοντέλα, όπως και τα έχουν αποκτήσει αυξανόμενη δημοτικότητα, λόγω των δυνατοτήτων εκμάθησης μεταξύ γλωσσών. Ωστόσο, η δυνατότητα γενικοποίησης τους εξακολουθεί να είναι ασυνεπής για τυπολογικά διαφορετικές γλώσσες και για διαφορετικά κριτήρια αναφοράς. Πρόσφατα, η μετα-μάθηση έχει συγκεντρώσει την προσοχή ως μια ελπιδοφόρα τεχνική για την ενίσχυση της μάθησης μεταφοράς σε σενάρια χαμηλού δυναμικού: ιδιαίτερα για τη διασυνοριακή μεταφορά στην κατανόηση φυσικής γλώσσας (ΝLU). Σε αυτή την εργασία, προτείνουμε μια προσέγγιση διακρατικής εκμάθησης ΜΕτα-TRAnsfer για τη NLU. Η προσέγγισή μας προσαρμόζει μια προσέγγιση μετα-μάθησης βασισμένη στη βελτιστοποίηση, για να μάθει να προσαρμόζεται στις νέες γλώσσες. Αξιολογούμε εκτενώς το πλαίσιο μας σε δύο απαιτητικές διγλωσσικές εργασίες: πολύγλωσσο διάλογο προσανατολισμένο στις εργασίες και τυπολογικά ποικίλες απαντήσεις σε ερωτήσεις. Δείχνουμε ότι η προσέγγισή μας ξεπερνά την αφελή τελειοποίηση, επιτυγχάνοντας ανταγωνιστικές επιδόσεις και στις δύο εργασίες για τις περισσότερες γλώσσες. Η ανάλυση μας αποκαλύπτει ότι το X-METRA-ADA μπορεί να αξιοποιήσει περιορισμένα δεδομένα για ταχύτερη προσαρμογή.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Los modelos multilingües, como M-BERT y XLM-R, han ganado cada vez más popularidad, debido a sus capacidades de aprendizaje de transferencia interlingüística de tiro cero. Sin embargo, su capacidad de generalización sigue siendo inconsistente para lenguajes tipológicamente diversos y en diferentes puntos de referencia. Recientemente, el meta-aprendizaje ha atraído la atención como una técnica prometedora para mejorar el aprendizaje por transferencia en escenarios de bajos recursos: particularmente para la transferencia interlingüística en la comprensión del lenguaje natural (NLU). En este trabajo, proponemos X-METRA-ADA, un enfoque de adaptación del aprendizaje de meta-transferencia multilingüe para NLU. Nuestro enfoque adapta MAML, un enfoque de meta-aprendizaje basado en la optimización, para aprender a adaptarse a los nuevos idiomas. Evaluamos exhaustivamente nuestro marco en dos desafiantes tareas de NLU multilingües: diálogo multilingüe orientado a tareas y respuestas a preguntas tipológicamente diversas. Demostramos que nuestro enfoque supera los ajustes ingenuos, alcanzando un rendimiento competitivo en ambas tareas para la mayoría de los idiomas. Nuestro análisis revela que X-METRA-ADA puede aprovechar los datos limitados para una adaptación más rápida.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mitmekeelsed mudelid, nagu M-BERT ja XLM-R, on suurenenud populaarsust tänu nende nullkeelsele ülekandeõppele. Kuid nende üldistamisvõime on tüpoloogiliselt erinevate keelte ja erinevate võrdlusaluste puhul endiselt vastuolus. Viimasel ajal on metaõpe pälvinud tähelepanu kui paljutõotav tehnika siirdeõppe parandamiseks vähese ressursiga stsenaariumides: eriti keeleülese ülekande puhul looduskeele mõistmises (NLU). Selles töös pakume välja X-METRA-ADA, keeleülese MEta-TRAnsferi õppe ADAptatsiooni lähenemisviisi NLU jaoks. Meie lähenemisviis kohandab MAML-i, optimeerimisel põhinevat metaõppe lähenemisviisi, et õppida kohanema uute keeltega. Hindame põhjalikult oma raamistikku kahel keeleülesel NLU ülesandel: mitmekeelne ülesandepõhine dialoog ja tüüpiliselt mitmekesine küsimustele vastamine. Näitame, et meie lähenemisviis ületab naiivset peenhäälestust, saavutades konkurentsivõimelise tulemuse mõlemal ülesandel enamiku keelte puhul. Meie analüüs näitab, et X-METRA-ADA saab kasutada piiratud andmeid kiiremaks kohanemiseks.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>مدل‌های زیادی زبان، مانند M-BERT و XLM-R، به دلیل توانایی یادگیری با انتقال کردن زبان‌های مختلف صفر، بیشتر شهرت را به دست آورده‌اند. با این حال، توانایی عمومی آنها هنوز برای زبان‌های مختلف نوع‌شناسی و از طریق برچسب‌های مختلف مختلف نیست. اخیرا، یادگیری متا به عنوان تکنیک قول‌دهنده برای افزایش یادگیری انتقال زیر سناریو منابع کم، توجه داده است: مخصوصا برای انتقال متوسط زبان در درک زبان طبیعی (NLU). در این کار، ما X-METRA-ADA را پیشنهاد می‌کنیم، یک طریق ADAptation برای NLU یاد گرفتن MEta-TRAnsfer متزبانی. دستور ما MAML را adapt می‌کند، یک دستور یادگیری meta-learning based on optimization, تا یاد بگیریم که به زبان جدید adapt می‌شود. ما چهارچوب خود را در دو مشکل کار NLU متوسط زبان ارزیابی می کنیم: گفتگو متوسط به کار زبان و جواب سوال متفاوت متفاوت متفاوت. ما نشان می دهیم که دسترسی ما به زیادی از زبان آرامش ساده انجام می دهد، و به اجرای مسابقه در هر دو کار برای بیشتر زبان رسیده است. تحلیل ما نشان می دهد که X-METRA-ADA می تواند داده های محدودیت را برای تغییر سریعتر تحمل کند.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Monikieliset mallit, kuten M-BERT ja XLM-R, ovat saaneet yhä enemmän suosiota kielten välisen siirtooppimisen ansiosta. Niiden yleistyskyky on kuitenkin edelleen epäjohdonmukainen typologisesti erilaisissa kielissä ja eri vertailuarvoissa. Viime aikoina metaoppiminen on herättänyt huomiota lupaavana keinona parantaa siirtooppimista vähävaraisissa skenaarioissa: erityisesti monikielisessä siirrossa luonnollisen kielen ymmärtämisessä (NLU). Tässä työssä ehdotamme X-METRA-ADA, monikielistä MEta-TRAnsfer oppimisen ADAptation lähestymistapaa NLU:lle. Lähestymistapamme mukauttaa MAML:ää, optimointiin perustuvaa meta-oppimista, oppimaan sopeutumaan uusiin kieliin. Arvioimme viitekehyksiämme laajasti kahdessa haastavassa monikielisessä NLU-tehtävässä: monikielisessä tehtäväkeskeisessä dialogissa ja typologisesti monipuolisessa kysymysvastauksessa. Osoitamme, että lähestymistapamme on naiivia hienosäätöjä parempi, saavuttaen kilpailukykyisen suorituskyvyn molemmissa tehtävissä useimmissa kielissä. Analyysimme osoittaa, että X-METRA-ADA voi hyödyntää rajallisia tietoja nopeampaan sopeutumiseen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Les modèles multilingues, tels que M-BERT et XLM-R, gagnent en popularité, en raison de leurs capacités d'apprentissage par transfert multilingue zero-shot. Cependant, leur capacité de généralisation est toujours incohérente pour les langues typologiquement diverses et selon les différents critères de référence. Récemment, le méta-apprentissage a attiré l'attention en tant que technique prometteuse pour améliorer l'apprentissage par transfert dans des scénarios à faibles ressources, en particulier pour le transfert interlinguistique dans la compréhension du langage naturel (NLU). Dans ce travail, nous proposons X-METRA-ADA, une approche multilingue d'adaptation de l'apprentissage par méta-transfert pour la NLU. Notre approche adapte MAML, une approche de méta-apprentissage basée sur l'optimisation, pour apprendre à s'adapter aux nouvelles langues. Nous évaluons en profondeur notre cadre sur deux tâches multilingues complexes de la NLU : le dialogue multilingue axé sur les tâches et la réponse aux questions typologiquement diversifiée. Nous montrons que notre approche surpasse le réglage fin naïf, atteignant des performances compétitives sur les deux tâches pour la plupart des langues. Notre analyse révèle que X-METRA-ADA peut exploiter des données limitées pour une adaptation plus rapide.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tá méadú ag teacht ar an éileamh atá ar shamhlacha ilteangacha, mar M-BERT agus XLM-R, mar gheall ar a gcumas foghlama aistrithe tras-teanga gan aon lámhaigh. Mar sin féin, tá a gcumas ginearálaithe fós ar neamhréir do theangacha atá éagsúil ó thaobh na clódóireachta de agus thar thagarmharcanna éagsúla. Le déanaí, tarraingíodh aird ar an meitea-fhoghlaim mar theicníc a bhfuil gealladh fúthu chun foghlaim aistrithe a fheabhsú faoi chásanna íseal-acmhainne: go háirithe maidir le haistriú tras-teangach i dTuiscint Teanga Nádúrtha (NLU). San obair seo, molaimid X-METRA-ADA, cur chuige tras-teangach oiriúnaithe foghlama MEta-Transfer don NLU. Déanann ár gcur chuige seo oiriúnú do MAML, cur chuige meitea-fhoghlaim atá bunaithe ar bharrfheabhsú, chun foghlaim conas oiriúnú do theangacha nua. Déanaimid meastóireacht fhairsing ar ár gcreat maidir le dhá thasc dhúshlánacha thrastheangacha NLU: dialóg ilteangach atá dírithe ar thascanna agus freagraí ilghnéitheacha tíopeolaíochta. Léirímid go sáraíonn ár gcur chuige mionchoigeartú naive, ag baint amach feidhmíocht iomaíoch sa dá thasc don chuid is mó de na teangacha. Léiríonn ár n-anailís gur féidir le X-METRA-ADA sonraí teoranta a ghiaráil le haghaidh oiriúnú níos tapúla.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Motoli masu yawa kamar misãlan M-BERT da XLM-R, sun ƙara umarni, sabo da abincin transference na fassarar-lugha na sifiri. A lokacin da, awonsu ya jenalisa yana bada da ba mai kamfata wa lugha masu turɓãya ko kuma a cikin wasu mistakardan dabam-dabam. A yanzu, karatun meta-ci ya garwaya aikin muhimmin muhimmin muhimmin wa ƙara-resource: kuma, has a'a, muhimmin a shige mistakarda cikin Lugha Haƙĩƙa (NLU). Daga wannan aikin, Munã bukãtar da X-METRA-ADA, wata takardar harshen mai tsawo na MEta-TRAnsfer da za'a sanar da adaptan zuwa NLU. Tsarakanmu na adatar da MAML, wata hanyarwa mai kwaɗayi ta wajen karatun meta-barci, dõmin ya yi kwaɗayi zuwa harshen sãbuwa. Ina iya ƙaddara firam a kan aikin biyu masu tsõratar da ke cikin linguin NLU: Tuna nũna cewa hanyarmu na samar tashi mai kawaici, kuma yana samun rabo da yin gaura a kan aikin dukansu masu cikin harshen. AnalyyinMu yana bayyana cewa X-METRA-ADA yana iya samun data wanda ke iya ƙaranci wa adadi sauri.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>דוגמנים רבים שפות, כמו M-BERT ו XLM-R, הרוויחו פופולריות גדולה, בגלל יכולות הלימודים של העברת שפות בצורה אפס. בכל אופן, היכולת הגנרליזציה שלהם עדיין לא תואמת לשפות טיפולוגיות מגוונות ובדרך רמזים שונים. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). בעבודה הזו, אנו מציעים X-METRA-ADA, גישה META-TRANSFER-למידה דרך שפתיים למידה התאמה ל NLU. הגישה שלנו מתאימה MAML, גישה מטה-למידה מבוססת על אופטימיזציה, כדי ללמוד להתאים לשפות חדשות. אנחנו מעריכים באופן רחב את המסגרת שלנו על שתי משימות NLU-שינותיות מרובות מאתגרות: דיאלוג מורכב למשימות מרובות ושיבות שונות טיפולוגיות. אנחנו מראים שהגישה שלנו מעלית התאמה נאיבית, מגיעה להופעה תחרותית על שני המשימות לרוב השפות. הניתוח שלנו מראה ש X-METRA-ADA יכול להשתמש במידע מוגבל להסתגלות מהר יותר.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>बहुभाषी मॉडल, जैसे कि एम-बर्ट और एक्सएलएम-आर, ने अपनी शून्य-शॉट क्रॉस-लिंगुअल ट्रांसफर सीखने की क्षमताओं के कारण बढ़ती लोकप्रियता हासिल की है। हालांकि, उनकी सामान्यीकरण क्षमता अभी भी टाइपोलॉजिकल रूप से विविध भाषाओं और विभिन्न बेंचमार्क ों के लिए असंगत है। हाल ही में, मेटा-लर्निंग ने कम संसाधन परिदृश्यों के तहत स्थानांतरण सीखने को बढ़ाने के लिए एक आशाजनक तकनीक के रूप में ध्यान आकर्षित किया है: विशेष रूप से प्राकृतिक भाषा समझ (एनएलयू) में क्रॉस-लिंगुअल हस्तांतरण के लिए। इस काम में, हम X-METRA-ADA का प्रस्ताव करते हैं, जो एनएलयू के लिए एक क्रॉस-लिंगुअल MEta-TRAnsfer सीखने वाला ADAptation दृष्टिकोण है। हमारा दृष्टिकोण MAML, एक अनुकूलन-आधारित मेटा-लर्निंग दृष्टिकोण को अनुकूलित करता है, नई भाषाओं के अनुकूल होने के लिए सीखने के लिए। हम बड़े पैमाने पर दो चुनौतीपूर्ण क्रॉस-लिंगुअल एनएलयू कार्यों पर हमारे ढांचे का मूल्यांकन करते हैं: बहुभाषी कार्य-उन्मुख संवाद और टाइपोलॉजिकल रूप से विविध प्रश्न का उत्तर देना। हम दिखाते हैं कि हमारा दृष्टिकोण भोले ठीक ट्यूनिंग से बेहतर प्रदर्शन करता है, अधिकांश भाषाओं के लिए दोनों कार्यों पर प्रतिस्पर्धी प्रदर्शन तक पहुंचता है। हमारे विश्लेषण से पता चलता है कि एक्स-मेट्रा-एडीए तेजी से अनुकूलन के लिए सीमित डेटा का लाभ उठा सकता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Većina jezičkih modela, poput M-BERT i XLM-R, dobili su veću popularnost zbog njihovih sposobnosti za učenje preko jezika bez pucnjave. Međutim, njihova sposobnost generalizacije još uvijek nije u skladu s tipološki različitim jezicima i raznim kriterijama. Nedavno je metaučenje privuklo pažnju kao obećavajuću tehniku za unapređenje učenja prijenosa u području scenarija niskih resursa: posebno za cross-lingual transfer u razumijevanju prirodnog jezika (NLU). U ovom poslu predlažemo X-METRA-ADA, cross-lingual MEta-TRAnsfer učenjem pristupa ADAptacije za NLU. Naš pristup prilagođuje MAML, optimizacijski metaučenjeni pristup, kako bi se naučio prilagoditi novim jezicima. Proširoko procjenjujemo naš okvir na dva izazovnog prekograničnog NLU zadatka: multijezički dijalog orientiranog na zadatke i tipološki različita odgovora na pitanja. Pokazujemo da naš pristup nadmašuje naivno ispravljanje, postizanje konkurentnih učinka na obje zadatke za većinu jezika. Naša analiza otkriva da X-METRA-ADA može utjecati na ograničene podatke za brže adaptacije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A többnyelvű modellek, mint például az M-BERT és az XLM-R, növekvő népszerűségre tettek szert, mivel zéró nyelvű transzfer tanulási képességeik vannak. Általánosítási képességük azonban továbbra is következetlen a tipológiailag különböző nyelvek és a különböző referenciaértékek között. Az utóbbi időben a metatanulás ígéretes technikáként szerezte meg a figyelmet az alacsony erőforrásokkal rendelkező forgatókönyvek közötti transzfertanulás fokozására: különösen a természetes nyelvi megértésben (NLU) többnyelvű transzfernél. Ebben a munkában javasoljuk az X-METRA-ADA-t, egy többnyelvű MEta-TRAnsfer tanulás ADAPTATION megközelítést az NLU számára. Megközelítésünk a MAML-t, az optimalizáláson alapuló metatanulási megközelítést alkalmazza, hogy megtanuljon alkalmazkodni az új nyelvekhez. Széles körben értékeljük keretrendszerünket két kihívást jelentő, többnyelvű feladatorientált párbeszéd és tipológiailag változatos kérdésekre. Megmutatjuk, hogy megközelítésünk felülmúlja a naiv finomhangolást, és versenyképes teljesítményt ér el mindkét feladatban a legtöbb nyelven. Elemzésünk kimutatja, hogy az X-METRA-ADA korlátozott adatokat tud kihasználni a gyorsabb alkalmazkodás érdekében.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Բազլեզու մոդելները, ինչպիսիք են M-BER-ը և XLM-R-ը, աճել են բնակչությունը, իրենց զրոյի միջլեզվի փոխանցման ուսուցման ունակությունների պատճառով: Այնուամենայնիվ, նրանց ընդհանուր ընդունակությունը դեռևս անհամապատասխան է տիպոլոգիապես բազմազան լեզուների և տարբեր հարաբերականների համար: Վերջերս մետասովորելը ուշադրություն դարձրել է որպես խոստացող տեխնիկա, որպեսզի բարելավվի ուսումնասիրությունը ցածր ռեսուրսներ ունեցող սցենարների ընթացքում, հատկապես բնական լեզվի հասկացության միջև լեզվի փոխանցման համար: Այս աշխատանքի ընթացքում մենք առաջարկում ենք X-Մետրա-ԱԴԱ-ը, երկլեզվային Մետրա-ԱԴԱ-ի ուսումնասիրության հարմարեցման մոտեցում ՆԼՄ-ի համար: Մեր մոտեցումը հարմարեցնում է MAML-ը, օպտիվացման հիմնված մետասովորման մոտեցումը, որպեսզի սովորենք հարմարեցնել նոր լեզուներին: Մենք էքսպենսիվ գնահատում ենք մեր կառուցվածքը երկու մարտահրավերների միջլեզվային ՆԼՀ-ի առաջադրանքների վրա' բազլեզվային առաջադրանքների վրա ուղղությամբ և տիպոլոգիապես տարբեր հարցերի պատասխանների վրա: Մենք ցույց ենք տալիս, որ մեր մոտեցումը գերազանցում է նայիվ բարելավումը, հասնելով մրցակցության արդյունքներին երկու խնդիրների համար լեզուների մեծ մասում: Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. Namun, kemampuan generalisasi mereka masih tidak konsisten untuk bahasa tipologi berbeda dan melalui benchmarks berbeda. Baru-baru ini, meta-belajar telah mendapatkan perhatian sebagai teknik yang berjanji untuk meningkatkan transfer belajar di bawah skenario sumber daya rendah: terutama untuk transfer saling bahasa dalam Natural Language Understanding (NLU). Dalam pekerjaan ini, kami mengusulkan X-METRA-ADA, pendekatan MEta-TRAnsfer berbahasa saling belajar Adaptasi untuk NLU. pendekatan kita menyesuaikan MAML, pendekatan meta-belajar berasaskan optimisasi, untuk belajar menyesuaikan bahasa baru. Kami secara ekstensif mengevaluasi cadangan kami pada dua tugas NLU saling bahasa menantang: dialog berbagai bahasa yang mengarientasi tugas dan menjawab pertanyaan tipologis berbeda. Kami menunjukkan bahwa pendekatan kita melebihi penyesuaian naif, mencapai prestasi kompetitif pada kedua tugas untuk kebanyakan bahasa. Analisi kami mengungkapkan bahwa X-METRA-ADA dapat menggunakan data terbatas untuk adaptasi lebih cepat.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I modelli multilingue, come M-BERT e XLM-R, hanno guadagnato crescente popolarità, grazie alle loro capacità di apprendimento cross-lingual transfer zero-shot. Tuttavia, la loro capacità di generalizzazione è ancora incoerente per lingue tipologicamente diverse e tra diversi parametri di riferimento. Recentemente, il meta-apprendimento ha attirato l'attenzione come una tecnica promettente per migliorare l'apprendimento di trasferimento in scenari a basso contenuto di risorse: in particolare per il trasferimento cross-lingual in Natural Language Understanding (NLU). In questo lavoro, proponiamo X-METRA-ADA, un approccio cross-lingual MEta-TRAnsfer apprendimento ADAptation per NLU. Il nostro approccio adatta MAML, un approccio meta-learning basato sull'ottimizzazione, per imparare ad adattarsi a nuove lingue. Valutiamo ampiamente il nostro framework su due impegnativi compiti di NLU cross-lingual: dialogo multilingue orientato ai compiti e risposta tipologicamente diversificata alle domande. Dimostriamo che il nostro approccio supera l'ingenua messa a punto, raggiungendo prestazioni competitive su entrambi i compiti per la maggior parte delle lingue. La nostra analisi rivela che X-METRA-ADA può sfruttare dati limitati per un adattamento più rapido.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>M - BERTやXLM - Rなどの多言語モデルは、ゼロショットのクロスリンガル転送学習機能により、ますます人気を集めています。 しかし、類型的に多様な言語と異なるベンチマークでは、それらの一般化能力は依然として一貫していません。 最近、メタラーニングは、特に自然言語理解（ NLU ）におけるクロスリンガル転送のための、低資源シナリオ下での転送学習を強化するための有望なテクニックとして注目を集めている。 この研究では、NLUのためのクロスリンガルMEta - TRAnsfer学習ADAptationアプローチであるX - METRA - ADAを提案します。 私たちのアプローチは、最適化ベースのメタラーニングアプローチであるMAMLを適応させ、新しい言語に適応することを学びます。 私たちは、多言語タスク指向のダイアログと類型的に多様な質問への回答という、2つの困難なクロスリンガルNLUタスクに関するフレームワークを幅広く評価します。 私たちは、私たちのアプローチが天真爛漫な微調整よりも優れており、ほとんどの言語で両方のタスクで競争力のあるパフォーマンスに達していることを示しています。 私たちの分析は、X - METRA - ADAが限られたデータを活用して、より迅速な適応を実現できることを明らかにしました。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mulalawat model, kaya M-BERT lan XLM-R, padha ugat popularno sing gak popularno kaya sistem sing gak perusahaan sesilyane kapasitasi telutung. politenessoffpolite, "), and when there is a change ("assertivepoliteness Suara, meta-ciliane kuwi nglanggar aturan kanggo teknik sing ngwalikno nggawe ngubah ilem nggawe barang kelas pengguna kuwi mau: supaya kuwi kanggo nggawe langgar tarjamahan kanggo langgambar terus Ninggang langgar (NLU). Nang barêng-barêng iki, kita supoyo X-METRA-AdA, akeh banter-langgar MEta-TRanser seneng nggambar adiptasi kanggo NLU. Awak dhéwé ngerti wiwigat MAML, meta-urip nggawe gerarané karo pertualisi, kanggo ngerwih kanggo langgambar Anyar. Awak dhéwé estetik luwih nggawe barang kelas telu nggawe barang kelas telu nggawe barang langgar-langgar NLU tasks: multi-language task-Oriented dialog dan Typlogically Awak dhéwé ngerasah bener tentang kanggo nggawe barang apik, iso nggawe ngerasah barang langga. Panjenenganipun anyar sumbarang X-METRA-ANA iso nggawe data limiter kanggo ngilanggar tarjamahan kanggo kalaayuni maneh.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>მრავალენგური მოდელები, როგორც M-BERT და XLM-R, უფრო მეტი პოლუბარიტება იქნება, რადგან ისინი უფრო მეტი ენგური გასწავლების შესაძლებლობა გადავიღეთ. მაგრამ მათი გენერალიზაციის შესაძლებლობა ისევ ტიპოლოგიურად განსხვავებული ენებისთვის და განსხვავებული ბენქმარკებისთვის არსებობს. მიმდინარე, მეტა-სწავლების შესაძლებლობელი ტექნონია, როგორც უნდა გავაკეთოთ ტრანსპერსონის სწავლების აღმოქმედებისთვის: განსაკუთრებულია კრესტრისონის ტრანსპერსონისთვის ნა ამ სამუშაოში, ჩვენ X-METRA-ADA-ს, სამუშაო ენგური MEta-TRAnsfer სწავლების ADAptation პროგორმა NLU-ს. ჩვენი პროგორმაცია MAML-ს აეპორტიმიზაცია, მეტა-სწავლების მიხედვით, რომ შესწავლოთ ახალი ენებისთვის აეპორტიზაციას. ჩვენ ჩვენი პარამეტრების განსაზღვრება ორი განსაზღვრებული მრავალენგური NLU დავალებებით: მრავალენგური დავალების დიალოგია და ტიპოლოგიურად განსხვავებული კითხვების განსახულება. ჩვენ გამოჩვენებთ, რომ ჩვენი წარმოდგენა უფრო ნავიგური წარმოდგენება, კონკრენტებური წარმოდგენება ორივე საქმებზე უფრო მეტი ენაზე. ჩვენი ანალიზაცია აღმოჩნდა, რომ X-METRA-ADA შეუძლია გავაკეთოთ უფრო სწრაფად ადამიანისთვის განსაზღვრებული მონაცემები.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>М-BERT және XLM-R секілді бірнеше тілдік үлгілері нөл тілдік аудару мүмкіндіктерінің себебі, нөл түрлі аудару мүмкіндіктері үшін бірнеше мәліметті көтерді. Бірақ олардың жалпы түрлендіру мүмкіндігі типтологиялық тілдерге және әртүрлі түрлендіру мәліметтеріне қатысты. Жуырда, мета оқыту көмегімен көмек ресурстар сценариясындағы транспорт оқыту үшін көмектесетін техникалық болып, өзіне Тәуелді тілді түсініктерінің көпшілікті аудару үшін (НLU). Бұл жұмыс ішінде біз X-METRA-ADA, NLU үшін бірнеше тілді MEta-TRAnsfer оқыту адаптациялық тәсілін ұсынамыз. Біздің тәсіліміз MAML-ды, оптимизациялау негіздеген мета оқыту тәсілігін, жаңа тілдерге адаптациялауды үйрену үшін қолданылады. Біз қоршауымызды екі тілді NLU тапсырмаларының көпшілігін бақылап, бірнеше тілді тапсырмалар бағытталған диалог және типтологиялық түрлі сұрақ жауаптары туралы дұрыстық оқу. Біз өзіміздің тәсіліміздің көпшілігіміздің екі тапсырмаларының көпшілігі үшін бақылау жұмысын жасайды. Біздің анализамыз X-METRA-ADA тез адаптациялау үшін шектелген деректерді көмектесе алады.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>M-BERT와 XLM-R과 같은 다국어 모델은 제로 언어 이동 학습 능력으로 점점 인기를 끌고 있다.그러나 서로 다른 유형의 언어와 서로 다른 기준에 대해 그들의 범화 능력은 여전히 일치하지 않는다.최근 몇 년 동안 원 학습은 저자원 상황에서 이동 학습을 추진하는 유망한 기술로서 주목을 받았다. 특히 자연 언어 이해에서의 다중 언어 이동이다.이 작업에서 우리는 X-METRA-ADA, NLU에 적용되는 다중 언어 모듈 이동 학습 적응 방법을 제시했다.우리의 방법은 최적화된 원 학습 방법인 MAML을 바탕으로 새로운 언어에 적응하는 것을 배운다.우리는 두 가지 다중 언어 NLU의 도전적인 임무의 구조를 광범위하게 평가했다. 그것이 바로 다중 언어 임무에 대한 대화와 다양한 질문에 대한 대답이다.우리는 우리의 방법이 간단한 마이크로스피커보다 우수하다는 것을 보여 주었고 대다수 언어의 두 가지 임무에서 경쟁력 있는 성능에 이르렀다.우리의 분석에 의하면 X-METRA-ADA는 유한한 데이터를 이용하여 더욱 빨리 적응할 수 있다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Pastaruoju metu metamokymasis sutelkė dėmesį kaip pažadėtiną metodą, skirtą gerinti mokymąsi perkeliant mokymąsi mažai išteklių turinčiais scenarijais: ypač kalbų perkėlimui į gamtos kalbų supratimą (NLU). Šiame darbe siūlome X-METRA-ADA, tarpkalbį MEta-Transfer mokymosi metodą, pritaikytą NLU. Mūsų metodas pritaikomas MAML, optimizavimu pagrįstam metamokymosi metodui, siekiant išmokti prisitaikyti prie naujų kalbų. Išsamiai vertiname savo sistemą dviem sudėtingomis tarpkalbinėmis NLU užduotimis: daugiakalbis į užduotis orientuotas dialogas ir tipologiškai įvairūs klausimų atsakymai. Mes parodome, kad mūsų požiūris yra naivus ir patobulintas, siekiant konkurencinių rezultatų abiejose užduotyse daugumoje kalbų. Mūsų analizė rodo, kad X-METRA-ADA gali sutelkti ribotus duomenis greičiau prisitaikyti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Мултијазичните модели, како што се M-BERT и XLM-R, добија зголемувачка популарност, поради нивните нултирани способности за трансфер на јазик. Сепак, нивната генерализациска способност е сé уште несогласна за типологички различни јазици и преку различни споредби. Неодамна метаучењето доби внимание како ветувачка техника за подобрување на префрлањето на учењето во скенарија со ниски ресурси: особено за прекујазички префрлање во Разбирањето на природниот јазик (НЛУ). Во оваа работа предложуваме Х-МЕТРА-АДА, крстојазичен пристап за учење на мета-трансфер Адаптација за НЛУ. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. Ги проценуваме нашите рамки за две предизвикувачки прекујазични задачи на НЛУ: мултијазички дијалог ориентиран на задачите и типологички различни одговори на прашања. Ние покажуваме дека нашиот пристап го надминува наивното финетизирање, достигнувајќи конкурентна перформанса на двете задачи за повеќето јазици. Нашата анализа открива дека X-METRA-ADA може да влијае на ограничени податоци за побрза адаптација.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>എംബെര്‍ട്ടിയെയും എക്സ്എല്‍എംആരിയെയും പോലുള്ള പല ഭാഷകളുടെ മോഡലുകള്‍ പൂര്‍ണ്ണമായി വര്‍ദ്ധിപ്പിച്ചിരിക്കുന്നു. അവരുടെ പൂര്‍ണ്ണമായ ക് എന്നാലും അവരുടെ സാധാരണ ഭാഷകള്‍ക്കും വ്യത്യസ്ത ഭാഷകള്‍ക്കും വ്യത്യസ്തമായ ബെന്‍മാര്‍ക്കുകള്‍ക്കും അവരുടെ സാധാരണ കഴിവു Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). ഈ പ്രവര്‍ത്തനത്തില്‍, നമ്മള്‍ എക്സ്-മെട്രാ-എഡാവിനെ പ്രൊദ്ദേശിപ്പിക്കുന്നു, ഒരു ക്രിസ്റ്റ ഭാഷ മെറ്റാ-ട്രാന്‍ഫെര്‍ എ നമ്മുടെ അടുത്തുള്ള പ്രായോഗ്യം MAML-ലേക്ക് ചേര്‍ക്കുന്നു, ഒരു മാറ്റ-അടിസ്ഥാനമായ മെറ്റ-പഠിക്കുന്ന പ്ര നമ്മുടെ ഫ്രെയിമെക്കുകള്‍ വിശാലമായി വിലാസപ്പെടുത്തുന്ന രണ്ട് വിലാസക്കാരികളായ NLU ജോലികളില്‍ നമ്മുടെ ഫ്രെയിമെക്കുകള്‍ വിലാസ ഞങ്ങള്‍ കാണിക്കുന്നത് നമ്മുടെ അടുത്തുള്ള അടുത്തുള്ള പ്രവര്‍ത്തനങ്ങള്‍ നൈവ് സുന്ദരമായി പ്രവര്‍ത്തിപ്പിക്കുന്ന നമ്മുടെ അന്വേഷണം കാണിച്ചു കൊണ്ടിരിക്കുന്നത് എക്സ്-മെട്രാ-ADA വേഗത്തില്‍ അഡാപ്റ്റേഷന്‍റെ പരിധികള്‍</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>M-BERT болон XLM-R зэрэг олон хэл загварууд тэдний хичээл хэлний шилжүүлэх чадварын тулд нэмэгдэж байна. Гэвч тэдний ерөнхийлөгчийн чадварыг бичил төрлийн хэл болон өөр төрлийн салбаруудын тухай харьцуулж чадахгүй байна. Саяхан мета суралцах нь бага баялаг боловсролын хувилбарын доор шилжүүлэх суралцааг илүү амлалтай техник болгон анхаарлаа хандуулсан. Ялангуяа Байгалийн хэл ойлголтын олон хэлний шилжүүлэх (НLU) хувилбар бол Энэ ажил дээр бид Х-МЕТРА-АДА, НЛУ-д ДОХ-ТРАНФЕР суралцах олон хэлний MEta-TRAnsfer суралцах арга замыг санал болгоно. Манай аргыг шинэ хэл дээр адаптацийг сурах боломжтой MAML-г сайжруулдаг. Бид хэлний олон хэлний даалгаварын тухай хоёр шаардлагатай НЛУ-ын даалгаварын талаар дүүрэн үнэлдэг. Бид олон хэлний даалгаварын талаар ярилцлага, хэлбэрийн өөр өөр асуулт хариулт. Бид харуулж байгаагаар бидний арга зам нь ихэнх хэлний хоёр даалгаврын даалгаврыг дамжуулж, өрсөлдөг үйл ажиллагаа хийдэг. Бидний шинжилгээнд X-METRA-ADA нь хурдан адилтгах боломжтой болгон хязгаарлагдсан мэдээллийг ашиглаж чадна.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Model berbilang bahasa, seperti M-BERT dan XLM-R, telah meningkat popularitas, disebabkan kemampuan belajar pemindahan bahasa secara sempurna. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Baru-baru ini, pembelajaran-meta telah mendapatkan perhatian sebagai teknik yang berjanji untuk meningkatkan pembelajaran pemindahan di bawah skenario sumber rendah: terutama untuk pemindahan saling bahasa dalam Pemahaman Bahasa Alami (NLU). Dalam kerja ini, kami cadangkan X-METRA-ADA, pendekatan pelajaran MEta-TRAnsfer salib bahasa untuk NLU. Pendekatan kami menyesuaikan MAML, pendekatan pembelajaran meta berdasarkan optimizasi, untuk belajar menyesuaikan kepada bahasa baru. Kami menilai secara ekstensif kerangka kami pada dua tugas NLU saling bahasa menantang: dialog berbagai bahasa yang mengarah tugas dan jawapan soalan yang berbeza secara tipologik. Kami menunjukkan bahawa pendekatan kita lebih melampaui penyesuaian naif, mencapai prestasi kompetitif pada kedua-dua tugas untuk kebanyakan bahasa. Analisis kami menunjukkan bahawa X-METRA-ADA boleh menggunakan data terbatas untuk penyesuaian lebih cepat.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il-mudelli multilingwi, bħal M-BERT u XLM-R, kisbu popolarità dejjem akbar, minħabba l-kapaċitajiet tagħhom ta’ tagħlim ta’ trasferiment translingwi mingħajr skop. Madankollu, il-kapaċità tagħhom ta’ ġeneralizzazzjoni għadha inkonsistenti għal lingwi tipoloġikament differenti u f’punti ta’ riferiment differenti. Dan l-a ħħar, it-tagħlim meta kiseb attenzjoni bħala teknika promettenti għat-titjib tat-tagħlim tat-trasferiment f’xenarji b’riżorsi baxxi: b’mod partikolari għat-trasferiment translingwi fil-Ftehim tal-Lingwa Naturali (NLU). F’din il-ħidma, nipproponu X-METRA-ADA, approċċ ta’ adattament għat-tagħlim translingwi tal-MEta-Trasferiment għall-NLU. L-approċċ tagħna jadatta MAML, approċċ meta-tagħlim ibbażat fuq l-ottimizzazzjoni, biex jitgħallem jadatta għal lingwi ġodda. Aħna jevalwaw b’mod estensiv il-qafas tagħna dwar żewġ kompiti translingwi ta’ NLU li jisfidaw: djalogu multilingwi orjentat lejn kompiti u tweġiba għal mistoqsijiet tipoloġikament diversifikata. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Meertalige modellen, zoals M-BERT en XLM-R, zijn steeds populairder geworden, vanwege hun zero-shot cross-lingual transfer learning mogelijkheden. Hun generaliseringsvermogen is echter nog steeds inconsistent voor typologisch diverse talen en voor verschillende benchmarks. Recent heeft meta-learning aandacht gekregen als een veelbelovende techniek voor het verbeteren van transferleren in scenario's met weinig middelen: met name voor cross-lingual transfer in Natural Language Understanding (NLU). In dit werk stellen we X-METRA-ADA voor, een meertalige MEta-TRAnsfer learning ADAPTation aanpak voor NLU. Onze aanpak past MAML, een op optimalisatie gebaseerde meta-learning aanpak, aan om zich aan te passen aan nieuwe talen. We evalueren ons framework uitgebreid op twee uitdagende cross-lingual NLU taken: meertalige taakgerichte dialoog en typologisch divers vragenantwoord. We laten zien dat onze aanpak beter presteert dan naïeve finetuning, waardoor we concurrerende prestaties bereiken bij beide taken voor de meeste talen. Uit onze analyse blijkt dat X-METRA-ADA beperkte data kan gebruiken voor snellere aanpassing.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Fleirspråk modeller, som M-BERT og XLM-R, har fått økt popularitet på grunn av sine null-snitt krysspråk-læringskapasiteten for å overføra. Den generelliseringsverdien er imidlertid inkonsistent for typologisk ulike språk og på ulike benchmarker. Nyleg har metalæring forstått oppmerksomhet som eit promising teknikk for å forbedra læring av overføringar under låg ressursscenario: spesielt for cross-language transfer in Natural Language Understanding (NLU). I denne arbeiden foreslår vi X-METRA-ADA, eit krysspråk MEta-TRAnsfer læring av ADAptasjonslinjen for NLU. Nærminga vårt tilpassar MAML, ein optimaliseringsbasert metalæringstilnærming for å lære å tilpassa nye språk. Vi evaluerer rammeverket vårt på to vanskeleg NLU-oppgåver: dialogvindauge med fleirspråk orientert oppgåve og typologisk forskjellig svar på spørsmål. Vi viser at tilnærminga vårt utfører naive fine-tuning, når det gjer konkurentivt utvikling på begge oppgåver for dei fleste språk. Analysen vårt viser at X-METRA-ADA kan levera begrensede data for raskare adaptasjon.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modele wielojęzyczne, takie jak M-BERT i XLM-R, zyskały coraz większą popularność ze względu na możliwości uczenia się wielojęzycznego transferu zero-shot. Jednak ich zdolność do uogólniania jest nadal niespójna dla zróżnicowanych typologicznie języków i różnych wskaźników odniesienia. Ostatnio meta-learning zyskał uwagę jako obiecująca technika poprawy uczenia się transferowego w scenariuszach niskich zasobów: zwłaszcza w przypadku transferu między językami w zakresie rozumienia języka naturalnego (NLU). W niniejszej pracy proponujemy X-METRA-ADA, wielojęzyczne podejście MEta-TRAnsfer learning ADAPTation dla NLU. Nasze podejście dostosowuje MAML, oparte na optymalizacji podejście meta-learning, aby nauczyć się adaptować do nowych języków. Szczegółowo oceniamy nasze ramy pod kątem dwóch trudnych zadań wielojęzycznych NLU: dialogu wielojęzycznego zorientowanego na zadania i typologicznie zróżnicowanego odpowiedzi na pytania. Pokazujemy, że nasze podejście przewyższa naiwne dostrajanie, osiągając konkurencyjną wydajność w obu zadaniach dla większości języków. Nasza analiza ujawnia, że X-METRA-ADA może wykorzystać ograniczone dane do szybszej adaptacji.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelos multilíngues, como M-BERT e XLM-R, ganharam popularidade crescente, devido às suas capacidades de aprendizado de transferência multilíngue zero-shot. No entanto, sua capacidade de generalização ainda é inconsistente para linguagens tipologicamente diversas e em diferentes benchmarks. Recentemente, a meta-aprendizagem ganhou atenção como uma técnica promissora para melhorar a aprendizagem por transferência em cenários de poucos recursos: particularmente para a transferência multilíngue em Compreensão da Linguagem Natural (NLU). Neste trabalho, propomos o X-METRA-ADA, uma abordagem de ADAptação de aprendizado de MEta-TRAnsfer multilíngue para NLU. Nossa abordagem adapta MAML, uma abordagem de meta-aprendizagem baseada em otimização, para aprender a se adaptar a novos idiomas. Avaliamos extensivamente nossa estrutura em duas tarefas NLU multilíngues desafiadoras: diálogo multilíngue orientado a tarefas e resposta a perguntas tipologicamente diversas. Mostramos que nossa abordagem supera o ajuste fino ingênuo, alcançando desempenho competitivo em ambas as tarefas para a maioria dos idiomas. Nossa análise revela que o X-METRA-ADA pode alavancar dados limitados para uma adaptação mais rápida.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelele multilingve, cum ar fi M-BERT și XLM-R, au câștigat popularitate din ce în ce mai mare, datorită capacităților lor de învățare translingvistică zero-shot. Cu toate acestea, capacitatea lor de generalizare este încă inconsecventă pentru limbi tipologice diverse și pentru diferite criterii de referință. Recent, meta-învățarea a atras atenția ca o tehnică promițătoare pentru îmbunătățirea învățării transferului în scenarii cu resurse reduse: în special pentru transferul translingvistic în înțelegerea limbilor naturale (NLU). În această lucrare, propunem X-METRA-ADA, o abordare translingvă MEta-TRAnsfer de învățare ADAPTATION pentru NLU. Abordarea noastră adaptează MAML, o abordare meta-învățare bazată pe optimizare, pentru a învăța să se adapteze la noi limbi. Evaluăm pe scară largă cadrul nostru în ceea ce privește două sarcini dificile translingve NLU: dialogul multilingv orientat spre sarcini și răspunsul tipologic divers la întrebări. Aratăm că abordarea noastră depășește reglarea fină naivă, atingând performanțe competitive în ambele sarcini pentru majoritatea limbilor. Analiza noastră arată că X-METRA-ADA poate utiliza date limitate pentru o adaptare mai rapidă.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Многоязычные модели, такие как M-BERT и XLM-R, набирают все большую популярность благодаря своим возможностям межъязыкового обучения с нулевым выстрелом. Однако их способность к обобщению все еще не соответствует типологически различным языкам и различным критериям. В последнее время мета-обучение привлекло внимание как перспективный метод для улучшения обучения передаче при сценариях с низким уровнем ресурсов: особенно для межъязыковой передачи в понимании естественного языка (NLU). В этой работе мы предлагаем X-METRA-ADA, кросс-лингвистический подход ADAptation обучения MEta-TRANSfer для NLU. Наш подход адаптирует MAML, основанный на оптимизации мета-учебный подход, чтобы научиться адаптироваться к новым языкам. Мы широко оцениваем нашу структуру по двум сложным межязычным задачам NLU: многоязычному диалогу, ориентированному на задачи, и типологически разнообразным ответам на вопросы. Мы показываем, что наш подход превосходит наивную настройку, достигая конкурентной производительности по обеим задачам для большинства языков. Наш анализ показывает, что X-METRA-ADA может использовать ограниченные данные для более быстрой адаптации.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ගොඩක් භාෂාවික මොඩේල්, M-BERT සහ XLM-R වගේම, ඔවුන්ගේ සුන්ධ ශූන්ය විශේෂ භාෂාවික විද්‍යාපනය ඉගෙන ගන්න සක්ෂමතා නමුත්, ඔවුන්ගේ සාමාන්‍ය ක්‍රියාත්මක විවිධ භාෂාව සහ වෙනස් බෙන්ච්මාර්ක් වලට තියෙන්නේ නැහැ. අලුත් වෙලාවේ, මෙටා-ඉගෙනීමේ අවධානයක් හොයාගත්තා ප්‍රශ්නයක් විදියට ප්‍රශ්නයක් වෙනුවෙන් ප්‍රශ්නයක් වෙනුවෙන් ප්‍රශ්නයක මේ වැඩේ අපි X-METRA-ADA සැලසුම් කරනවා, ක්‍රිස් භාෂාවක් MEta-TRAnsfer ඉගෙන ගන්න ADAption approach for NLU. අපේ ප්‍රවේශනය MAML වෙනුවෙන්, අළුත් භාෂාවට සමාන්‍ය වෙනුවෙන් ඉගෙන ගන්න හොඳයි. අපි ප්‍රශ්නයක් විශ්වාසයෙන් අපේ පරීක්ෂණය විශ්වාසයෙන් ප්‍රශ්නයක් දෙන්න ප්‍රශ්නයක් කරනවා: විශාල භාෂාත්මක වැඩ අපි පෙන්වන්නේ අපේ ප්‍රවේශනය නිර්භාවිත විදිහට ප්‍රවේශනය කරනවා කියලා, ගොඩක් භාෂාවට ප්‍රවේශනය කරන්න ප්‍ අපේ විශ්ලේෂණය පෙන්වන්නේ X-METRA-ADA පුළුවන් වේගයෙන් සීමාවිත දත්ත ප්‍රයෝජනය කරන්න.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Večjezični modeli, kot sta M-BERT in XLM-R, so postali vedno bolj priljubljeni zaradi svojih zmogljivosti za prenos medjezičnega učenja brez strela. Vendar pa je njihova sposobnost posploševanja še vedno neskladna za tipološko različne jezike in različne referenčne vrednosti. V zadnjem času je metaučenje pridobilo pozornost kot obetavna tehnika za izboljšanje transfernega učenja v scenarijih z nizkimi viri: zlasti za medjezični prenos v razumevanju naravnega jezika (NLU). V tem delu predlagamo X-METRA-ADA, večjezični pristop MEta-TRAnsfer učenja ADAptacije za NLU. Naš pristop prilagaja MAML, pristop meta učenja, ki temelji na optimizaciji, da se nauči prilagoditi novim jezikom. Naš okvir obsežno ocenjujemo glede dveh zahtevnih medjezičnih nalog NLP: večjezičnega dialoga, usmerjenega v naloge, in tipološko raznolikega odgovarjanja na vprašanja. Pokazujemo, da naš pristop presega naivne fine nastavitve in dosega konkurenčno uspešnost pri obeh nalogah za večino jezikov. Naša analiza kaže, da lahko X-METRA-ADA izkoristi omejene podatke za hitrejšo prilagajanje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tusaalada luuqadaha kala duduwan, sida M-BERT iyo XLM-R, waxay ku kordhaan aqoontooda barashada luqada kala duwan darteed. Si kastaba ha ahaatee awoodooda dhalashada weli ma xirna sida caadiga ah luqado kala duduwan iyo sidoo kale habboon kala duduwan. Mudankii u dhowaaday, waxbarashada meta-meta waxay u hagaajisay qalabka loo ballanqaaday in la koriyo barashada iskuulka hoos-hoos-nololeed: si gaar ah u wareejiyo luqada kala duwan ee afka dabiiciga (NLU). Markaas waxan, waxaan soo jeedaynaa X-METRA-ADA, taas oo ah mid af kala duduwan MEta-TRAnsfer barashada ADAptation qaab u ah NLU. Dhaqdhaqaalkayagu wuxuu habboonayaa MAML, qaab barashada meta-based ah, si aan u barto in luqadaha cusub lagu beddelo. Si ballan ah ayaannu ku qiimeynaynaa qoraalkayaga labada qasab oo af kala duduwan ee NLU: dialogue luuqado kala duduwan iyo jawaabta su'aalo kala duduwan. Waxaynu muujinnaa in dhaqdhaqaalahayagu uu soo bandhigaa tababar wanaagsan, oo uu ku gaadho tababar adag oo ku saabsan labada shaqooyin ee luuqadaha badan. Analyskayagu wuxuu muujiyaa in X-METRA-ADA uu soo diri karo macluumaad xad u leh si dhaqso loo beddelo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelet shumëgjuhësore, të tilla si M-BERT dhe XLM-R, kanë fituar popullaritet në rritje, për shkak të aftësive të tyre zero-shot të mësimit ndërgjuhësor transferimi. Megjithatë, aftësia e tyre e gjeneralizimit është ende e pakonsistentë për gjuhët tipologjikisht të ndryshme dhe nëpërmjet pikave të ndryshme. Kohët e fundit, meta-mësimi ka fituar vëmendje si një teknikë premtuese për përmirësimin e mësimit të transferimit në skenarë me burime të ulëta: veçanërisht për transferimin ndërgjuhësor në kuptimin e gjuhës natyrore (NLU). Në këtë punë, propozojmë X-METRA-ADA, një metodë ndërgjuhësore MEta-TRAnsfer për mësimin e adaptimit për NLU. Përqasja jonë përshtatet MAML, një përqasje metamësimi bazuar në optimizacion, për të mësuar të përshtatet ndaj gjuhëve të reja. Ne vlerësojmë në mënyrë të gjerë kuadrin tonë mbi dy detyra sfiduese ndërgjuhësore të NLU: dialog shumëgjuhësor i orientuar ndaj detyrave dhe përgjigje tipologjike të ndryshme pyetjesh. Ne tregojmë se qasja jonë mbizotëron rregullimin naiv, duke arritur performancën konkurruese në të dy detyrat për shumicën e gjuhëve. Analiza jonë zbulon se X-METRA-ADA mund të përdorë të dhëna të kufizuara për përshtatje më të shpejtë.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mnogjezički modeli, kao što su M-BERT i XLM-R, dobili su veću popularnost zbog njihovih sposobnosti za učenje prevođenja jezika bez snimanja. Međutim, njihova sposobnost generalizacije još uvijek nije u skladu sa tipološki različitim jezicima i preko različitih kriterija. Nedavno je metaučenje privuklo pažnju kao obećavajuću tehniku za unapređenje učenja prijenosa pod scenarijem niskih resursa: posebno za cross-lingual transfer u razumijevanju prirodnog jezika (NLU). U ovom poslu predlažemo X-METRA-ADA, preko jezika MEta-TRAnsfer, pristup ADAptacije za NLU. Naš pristup prilagođuje MAML, optimizacijski metaučenjeni pristup, da nauèimo da se prilagodimo novim jezicima. Prošireno procjenjujemo naš okvir na dva izazova međujezičkih NLU zadataka: multijezički dijalog orijentiranog zadataka i tipološki različita odgovora na pitanja. Pokazujemo da naš pristup nadmašuje naivno fino-tuniranje, postiže konkurentni izvor na oba zadatka za većinu jezika. Naša analiza otkriva da X-METRA-ADA može uticati na ograničene podatke za brže adaptacije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Flerspråkiga modeller, som M-BERT och XLM-R, har blivit allt populärare på grund av deras möjligheter att överföra flera språk. Deras generaliseringsförmåga är dock fortfarande inkonsekvent för typologiskt olika språk och över olika riktmärken. Nyligen har metainlärning fått uppmärksamhet som en lovande teknik för att förbättra transferinlärning under lågresursscenarier: särskilt för cross-lingual transfer in Natural Language Understanding (NLU). I detta arbete föreslår vi X-METRA-ADA, ett flerspråkigt MEta-TRAnsfer lärande ADAPTATION tillvägagångssätt för NLU. Vårt tillvägagångssätt anpassar MAML, ett optimeringsbaserat meta-lärande tillvägagångssätt, för att lära sig att anpassa sig till nya språk. Vi utvärderar grundligt vårt ramverk för två utmanande tvärspråkiga NLU-uppgifter: flerspråkig uppgiftsorienterad dialog och typologiskt varierande frågeställningar. Vi visar att vårt tillvägagångssätt överträffar naiv finjustering och uppnår konkurrenskraftig prestanda på båda uppgifterna för de flesta språk. Vår analys visar att X-METRA-ADA kan utnyttja begränsade data för snabbare anpassning.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mfano wa lugha nyingi, kama vile M-BERT na XLM-R, umeongezeka umaarufu, kwa sababu ya uwezo wao wa kuhamisha elimu kwa lugha za lugha sifuri. Hata hivyo, uwezo wao wa uzalishaji bado hauna uhakika kwa lugha mbalimbali na katika maeneo mbalimbali. Hivi karibuni, elimu ya meta imejikuta na hisia kama teknolojia ya kuahidini kuongeza kujifunza kwa ajili ya kuhamisha kwa kiwango cha asili chini ya rasilimali: has a kwa ajili ya uhamishaji wa lugha tofauti katika kuelewa lugha ya asili (NLU). Katika kazi hii, tunapendekeza X-METRA-ADA, lugha mbalimbali ya MEta-TRAnsfer kujifunza mbinu za ADAptation kwa ajili ya NLU. Hatua yetu inabadilisha MAML, mbinu yenye matumaini yenye mafunzo ya meta, ili kujifunza kupambana na lugha mpya. Tutathmini kwa kiasi kikubwa mfumo wetu kuhusu kazi mbili za changamoto za NLU yenye lugha mbalimbali: mazungumzo ya lugha mbalimbali na majibu ya maswali tofauti kwa kawaida. Tunaonyesha kwamba mbinu yetu inaonyesha michoro mazuri, na kufikia ufanisi wa ushindani katika kazi zote kwa lugha nyingi. Uchambuzi wetu unaonyesha kuwa X-METRA-ADA anaweza kutumia taarifa zisizo na mipaka kwa ajili ya kuboresha haraka.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>M-BERT மற்றும் XLM-R போன்ற பல மொழி மாதிரிகள் பூஜ்ஜியமாக்கப்பட்ட மொழி மொழி மாற்றும் கற்றுக்கொள்ளும் சக்திகள் காரணத்தால் அதிகப்படுத்த ஆனால், அவர்களுடைய பொதுவாக்குதல் இயல்பான மொழிகளுக்கு இன்னும் பொருத்தமானது மற்றும் வித்தியாசமான குறிப்புகளு சமீபத்தில், மெடா கற்றல் கவனத்தை குறைந்த மூலத்தின் கீழ் மாற்றும் கற்றத்தை மேம்படுத்துவதற்கு ஒரு வாக்களிக்கும் தொழில்நுட்பமான தொழில்நுட்பமாக மாற்றி இந்த வேலையில், நாம் எக்ஸ்-மெட்ரா-ADA, ஒரு கிரும்மொழி MEta-TRAnsfer ADAptation approach for NLU கற்றுக்கொள்வதை நினைவூட்டுகிறோம். நம்முடைய அணுக்கம் MAML ஒப்புக்கொள்கிறது, புதிய மொழிகளுடன் ஒப்புக்கொள்வதற்கு, மேம்பாட்டு அடிப்படையான மெடா கற நாம் விரிவாக எங்கள் சட்டத்தை இரண்டு சவாலிக்கும் மொழி NLU பணிகளின் மீது மதிப்பிட வேண்டும்: பல மொழி செயல் திசைக்கும் உரையாடல் மற்றும் We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. எங்கள் ஆராய்ச்சி தெரிவிக்கிறது எக்ஸ்-METRA-ADA வேகமான ஒழுங்குதலுக்கு வரம்பு தகவல்களை வழங்க முடியும்.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>M-BERT we XLM-R ýaly köp dil nusgalary, 0-atly çarpaz dil öwrenmesi beceriklerinden öňünden daşyrýarlar. Ýöne olaryň döredilik ukyplary typolojik dürli diller we farklı benchmarklaryň içinde durmaýar. Soňky wagtlar meta-öwrenmek üçin daşary golaý öwrenmesini iň az resurslar senaryýasynda köpräk terjime etmek üçin söz berýän teknik bolup üns çekdi: iň.a ýratynda tebigy dil düşünmesinde cross-dil terjime etmek üçin (NLU). Bu işde X-METRA-ADA, NLU için çoklu dilli MEta-TRAnsfer öwrenmesini teklif ediyoruz. Biziň ýaryşymyz MAML'i, optimizasynda tabanly meta öwrenmek ýaryşyny üýtgedýär, täze dillere üýtgetmek üçin öwrenmek üçin. Biz çerçevemizi iki dilli karışık NLU görevlerinde büyük değerlendiriyoruz: çoxlu dilli görev yönlendirilmiş dialog ve tipolojik çeşitli soru cevapı. Biziň ýaryşymyz köp dilleriň üçin döwürli şekilde täsir edip, ýaryşykly hereketlerimizi başarmaýandygyny görkezýäris. Biziň analýzymyz X-METRA-ADA çalt adaptasyýa üçin hadysy maglumaty etmäge mümkin edip biler.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>بہت سی زبان مدل، جیسے M-BERT اور XLM-R، ان کی صفر-شٹ کروسٹ زبان کی تعلیم قابلیت کے سبب اضافہ کی گئی ہے۔ لیکن ان کی عمومی تعلیم قابلیت ابھی تک ٹائیپولوژیکی مختلف زبانوں اور مختلف بنچمبارک کے لئے مختلف ہے. اچھے وقت، مٹا سکھونٹ نے دھوپ کی تخصیل کے طور پر انتقال کی تعلیم کم منبع سینارییوں کے نیچے بڑھنے کے لئے اظہار کیا ہے: مخصوصاً طبیعی زبان سمجھنے (NLU) میں cross-lingual transfer کے لئے۔ اس کام میں ہم X-METRA-ADA کی پیشنهاد کریں گے، ایک کرس زبان MEta-TRAnsfer کی ADAptation approach NLU کے لئے. ہمارا طریقہ MAML کو اچھی طریقہ سے آمادہ کرتا ہے، ایک مٹا سیکھنے کی طریقہ، نوی زبانوں کے ساتھ اچھی طریقہ سے آمادہ کرتا ہے۔ ہم اپنے فرمود کو دو مشکل زبانی NLU کے کاموں پر پھیلاتے ہیں: multilingual task-oriented dialog اور typologically diverse question replying. ہم دکھاتے ہیں کہ ہماری تقریبا بہت سی زبانوں کے لئے بہت سی کاموں پر مسابقات کے کام پہنچ رہی ہے۔ ہماری تحلیل ظاہر کرتا ہے کہ X-METRA-ADA بہت جلد اٹھانے کے لئے محدودہ ڈیٹا کو لکھ سکتا ہے.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>M-BERT va XLM-R kabi bir necha tillar modellari ularning o'rganish imkoniyatlarini zero o'zgartirish imkoniyatini sababchi bo'lgan odamlarga oshadi. Lekin ularning umumiy qobiliyatlari oddiy turli tillar va har xil xil bog'lamalar uchun muvaffaqiyatli emas. Yaqinda, meta ta'limni o'rganishni yaratish uchun o'rganishni qanchalik o'rganishni o'rganishga ishonchini o'rganish uchun ishlatiladi. Hullas, tabiiy tilda o'zgartirish (NLU) kabi tilni o'rganish uchun. Bu ishda, biz X-METRA-ADA, biz NLU uchun bir necha tillar MEta-TRAnsfer o'rganishni o'rganishni talab qilamiz. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. Biz ikkita tillar qo'llangan NLU vazifalarini kengaytirib qiymatmiz: muloqon muloqon muloqat bilan muloqat muloqat muloqat muloqat muloqat muloqat muloqat oynasini va oddiy savol javob beradi. Biz ko'rsatganimizni ko'rsatdikki, biz ko'pchilik tillar uchun bir xil vazifalarga yetarlicha bajarayotganimizni ko'rsatdik. Analytikiz X-METRA-ADA tez adaptlash uchun chegara maʼlumot yozib olish mumkin.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Các mô hình đa ngôn ngữ, như M-BERT và XLM-R, đã trở nên nổi tiếng nhờ vào khả năng học tập truyền ngôn ngữ khác nhau của họ. Tuy nhiên, khả năng tổng hợp của họ vẫn không khớp với ngôn ngữ khác nhau theo tiêu chuẩn khác nhau. Gần đây, meta-Learning đã được chú ý như một kỹ thuật hứa hẹn để phát triển học thuyên chuyển trong các tình huống với nguồn ít tài nguyên: đặc biệt cho việc chuyển giao ngôn ngữ rộng trong hiểu biết ngôn ngữ tự nhiên (Ntrường). Trong công việc này, chúng tôi đề nghị X-MENgựa-ADA, một cách tiếp cận phân biệt ngôn ngữ giữa MEta-TRAnsfer. Cách tiếp cận của chúng ta thích nghi MAML, một phương pháp phân biệt các meta-leaving, để học cách thích nghi với ngôn ngữ mới. Chúng tôi đánh giá rộng cơ sở của chúng tôi về hai công việc vượt ngôn ngữ rộng thử thách: hộp thoại chuyên mục đa dạng và trả lời các câu hỏi khác nhau. Chúng tôi cho thấy cách tiếp cận của chúng tôi hoàn thành những nghi thức ngây thơ, đạt được thành công trong cả hai nhiệm vụ cho hầu hết các ngôn ngữ. Phân tích của chúng tôi cho thấy X-METRA-ADA có thể thu thập dữ liệu giới hạn để thích nghi nhanh hơn.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>多言模样,如M-BERT与XLM-R,以其零镜头跨语迁学而益受欢迎。 然则多样化言异准,其泛化不一。 近者,元学之为下资也,强移学之一术而见关注:特于自然语言解(NLU)中跨语移也。 于此言之,吾等建X-METRA-ADA,一于NLU跨语MEta-TRAnsfer习ADAptation法。 吾法用MAML,一本于优化之元学,以学适新之言。 博论两挑战性之跨语NLU事框架:多言者,多样化对也。 吾法优于幼稚之微,多言两事,皆有竞争力性。 吾之分析表明,X-METRA-ADA可以有限之数更快应也。</span></div></div><dl><dt>Anthology ID:</dt><dd>2021.naacl-main.283</dd><dt>Volume:</dt><dd><a href=/volumes/2021.naacl-main/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/naacl/>NAACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>3617–3632</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.naacl-main.283>https://aclanthology.org/2021.naacl-main.283</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2021.naacl-main.283 title="To the current version of the paper by DOI">10.18653/v1/2021.naacl-main.283</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">mhamdi-etal-2021-x</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Meryem M’hamdi, Doo Soon Kim, Franck Dernoncourt, Trung Bui, Xiang Ren, and Jonathan May. 2021. <a href=https://aclanthology.org/2021.naacl-main.283>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a>. In <i>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, pages 3617–3632, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2021.naacl-main.283>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a> (M’hamdi et al., NAACL 2021)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.naacl-main.283.pdf>https://aclanthology.org/2021.naacl-main.283.pdf</a></dd><dt>Code</dt><dd><a href=https://github.com/meryemmhamdi1/meta_cross_nlu_qa><i class="fab fa-github"></i>&nbsp;meryemmhamdi1/meta_cross_nlu_qa</a></dd><dt>Data</dt><dd><a href=https://paperswithcode.com/dataset/mlqa>MLQA</a>,&nbsp;<a href=https://paperswithcode.com/dataset/tydi-qa>TyDi QA</a>,&nbsp;<a href=https://paperswithcode.com/dataset/tydiqa-goldp>TyDiQA-GoldP</a>,&nbsp;<a href=https://paperswithcode.com/dataset/xnli>XNLI</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.naacl-main.283.pdf title="Open PDF of 'X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=X-METRA-ADA+%3A+Cross-lingual+Meta-Transfer+learning+Adaptation+to+Natural+Language+Understanding+and+Question+AnsweringX-METRA-ADA%3A+Cross-lingual+Meta-Transfer+learning+Adaptation+to+Natural+Language+Understanding+and+Question+Answering" title="Search for 'X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-secondary d-flex flex-wrap justify-content-center" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.283" title="Code for 'X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering' on Papers with Code"><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-big" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg><span class="pl-sm-2 d-none d-sm-inline">Code</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering](https://aclanthology.org/2021.naacl-main.283) (M’hamdi et al., NAACL 2021)</p><ul class=mt-2><li><a href=https://aclanthology.org/2021.naacl-main.283>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a> (M’hamdi et al., NAACL 2021)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Meryem M’hamdi, Doo Soon Kim, Franck Dernoncourt, Trung Bui, Xiang Ren, and Jonathan May. 2021. <a href=https://aclanthology.org/2021.naacl-main.283>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question AnsweringX-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a>. In <i>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, pages 3617–3632, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>