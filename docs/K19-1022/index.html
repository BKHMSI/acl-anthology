<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Comparing Top-Down and Bottom-Up Neural Generative Dependency Models" name=citation_title><meta content="Austin Matthews" name=citation_author><meta content="Graham Neubig" name=citation_author><meta content="Chris Dyer" name=citation_author><meta content="Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)" name=citation_conference_title><meta content="2019/11" name=citation_publication_date><meta content="https://aclanthology.org/K19-1022.pdf" name=citation_pdf_url><meta content="227" name=citation_firstpage><meta content="237" name=citation_lastpage><meta content="10.18653/v1/K19-1022" name=citation_doi><meta property="og:title" content="Comparing Top-Down and Bottom-Up Neural Generative Dependency Models"><meta property="og:image" content="https://aclanthology.org/thumb/K19-1022.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/K19-1022"><meta property="og:description" content="Austin Matthews, Graham Neubig, Chris Dyer. Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). 2019."><link rel=canonical href=https://aclanthology.org/K19-1022></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/K19-1022.pdf>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a>
<a id=af_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Vergelyking Bo- Onderkant en Bo- Onderkant Neurale Genereerde Afhanklikheid Modelle</a>
<a id=am_title style=display:none href=https://aclanthology.org/K19-1022.pdf>gradient-editor-action</a>
<a id=ar_title style=display:none href=https://aclanthology.org/K19-1022.pdf>مقارنة نماذج التبعية العصبية التوليدية من أعلى إلى أسفل ومن أسفل إلى أعلى</a>
<a id=az_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Yuxarı Aşağı və Aşağı Nöral Generativ Bağılılıq Modelləri</a>
<a id=bg_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Сравняване на моделите на генеративна зависимост от неврола отгоре надолу и отдолу нагоре</a>
<a id=bn_title style=display:none href=https://aclanthology.org/K19-1022.pdf>উপর- নিচে এবং নিম্ন- নিউরেটিভ জেনারেটিভ নির্ভর মোডেল তুলনা করো</a>
<a id=bo_title style=display:none href=https://aclanthology.org/K19-1022.pdf>མགོ་རིང་འོག་ཏུ་དང་མགོ་རིང་མཐོང་ནུས་མིན་པའི་སྒེར་གྱི་རྟེན་འབྲེལ་མ་དཔེ་དབྱིབས</a>
<a id=bs_title style=display:none href=https://aclanthology.org/K19-1022.pdf>U usporedbi modela zavisnosti neuronske generacije</a>
<a id=ca_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Comparar models de dependencia neuronal generativa de dalt a baix i de baix</a>
<a id=cs_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Porovnání modelů neuronové generační závislosti shora dolů a zdola nahoru</a>
<a id=da_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Sammenligning top-down og bottom-up neural generative afhængighedsmodeller</a>
<a id=de_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Vergleich von Top-Down und Bottom-Up neuronalen generativen Abhängigkeitsmodellen</a>
<a id=el_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Σύγκριση μοντέλων γενετικής εξάρτησης από πάνω προς τα κάτω και από κάτω προς τα πάνω</a>
<a id=es_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Comparación de modelos de dependencia generativa neuronal descendente y ascendente</a>
<a id=et_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Ülalt alla ja alt üles neurogeneratiivse sõltuvuse mudelite võrdlemine</a>
<a id=fa_title style=display:none href=https://aclanthology.org/K19-1022.pdf>مقایسه مدل بستگی نسل عصبی بالا و پایین بالا</a>
<a id=fi_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Ylhäältä alas- ja alhaalta ylös -hermojen generatiivisen riippuvuuden mallien vertailu</a>
<a id=fl_title style=display:none href=https://aclanthology.org/K19-1022.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Comparaison des modèles de dépendance générative neuronale descendante et ascendante</a>
<a id=ga_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Comparáid a dhéanamh idir Múnlaí Spleáchais Néaracha ó Bhun Anuas agus ó Bhun Suas</a>
<a id=ha_title style=display:none href=https://aclanthology.org/K19-1022.pdf>@ action</a>
<a id=he_title style=display:none href=https://aclanthology.org/K19-1022.pdf>שיווה מודלים של תלויות ניאורליות גנרטיביות מעל למטה ומתחת למעלה</a>
<a id=hi_title style=display:none href=https://aclanthology.org/K19-1022.pdf>ऊपर-नीचे और नीचे-ऊपर तंत्रिका जननात्मक निर्भरता मॉडल की तुलना</a>
<a id=hr_title style=display:none href=https://aclanthology.org/K19-1022.pdf>U usporedbi modela zavisnosti Neuralne generacije vrha dolje i dolje</a>
<a id=hu_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Felülről lefelé és alulról felfelé generált függőségi modellek összehasonlítása</a>
<a id=hy_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Համեմատելով վերևից ներքև և ներքև գտնվող նյարդային գեներատիվ կախվածության մոդելները</a>
<a id=id_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Mengbandingkan Model Dependensi Neural Generatif Atas-Bawah dan Bawah-Up</a>
<a id=is_title style=display:none href=https://aclanthology.org/K19-1022.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Confronto dei modelli di dipendenza generativa neurale dall'alto verso il basso e dal basso verso l'alto</a>
<a id=ja_title style=display:none href=https://aclanthology.org/K19-1022.pdf>トップダウンとボトムアップの神経生成依存モデルの比較</a>
<a id=jv_title style=display:none href=https://aclanthology.org/K19-1022.pdf>bookmarks</a>
<a id=ka_title style=display:none href=https://aclanthology.org/K19-1022.pdf>ზემო- ქვემო და ქვემო- ზემო ნეიროლური დამხმარება მოდელების შემდგომარება</a>
<a id=kk_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Жоғарғы төменгі және төменгі жоғарғы нейрондық жасау тәуелдік үлгілерін салыстыру</a>
<a id=ko_title style=display:none href=https://aclanthology.org/K19-1022.pdf>위에서 아래로 신경 생성 의존 모델의 비교</a>
<a id=lt_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Palyginti iš viršaus į apačią ir iš apačią į viršų generuojančių nervų priklausomybės modelius</a>
<a id=mk_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Според моделите на генеративна нервна зависност од горе надолу и долу надолу</a>
<a id=ml_title style=display:none href=https://aclanthology.org/K19-1022.pdf>മുകളില്‍ നിന്നും താഴേക്കും മുകളില്‍ ന്യൂറല്‍ സൃഷ്ടിക്കുന്നതിന്റെ ആശ്രമമോഡലുകള്‍ തുല്യമാക്കുക</a>
<a id=mn_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Үүний доор болон доор доор нь мэдрэлийн төрөлхтний хамааралтай загваруудыг харьцуулах</a>
<a id=ms_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a>
<a id=mt_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Tqabbil ta’ Mudelli ta’ Dipendenza Ġenerattiva Newrali minn fuq għal isfel u minn isfel għal fuq</a>
<a id=nl_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Vergelijking van top-down en bottom-up neurale generatieve afhankelijkheidsmodellen</a>
<a id=no_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Samanliknar modeller for neuralgenerering av avhengighet</a>
<a id=pl_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Porównanie modeli zależności generacyjnej od góry i oddolnej od góry</a>
<a id=pt_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Comparando modelos de dependência generativa neurais de cima para baixo e de baixo para cima</a>
<a id=ro_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Compararea modelelor de dependenţă generativă neurală de sus în jos şi de jos în sus</a>
<a id=ru_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Сравнение нисходящей и восходящей моделей нейронной генеративной зависимости</a>
<a id=si_title style=display:none href=https://aclanthology.org/K19-1022.pdf>උඩ- පහළ හා පහළ- පහළ- උඩ නිර්මාණ ප්‍රමාණය විශේෂතා මඩේල්</a>
<a id=sk_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Primerjava modelov generativne odvisnosti od zgoraj navzdol in od spodaj navzgor</a>
<a id=so_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Isbarbarbardhigga Top-Down and Bottom-Up Neural Generative Dependence Models</a>
<a id=sq_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a>
<a id=sr_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Uspoređujući modele zavisnosti neuronske generacije</a>
<a id=sv_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Jämföra neurala generativa beroendemodeller uppifrån och ner och uppifrån</a>
<a id=sw_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Kulinganisha Top-Down and Bottom-Up Models of Creating Neural Dependence</a>
<a id=ta_title style=display:none href=https://aclanthology.org/K19-1022.pdf>மேல்- கீழ் மற்றும் கீழ் புதிய பொருள் உருவாக்கும் சார்பு மாதிரிகளை ஒப்பிடுகிறது</a>
<a id=tr_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Üst-Ast we Ast-Ast Niýal Jeşirmek Maýylyklary</a>
<a id=uk_title style=display:none href=https://aclanthology.org/K19-1022.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Top- Down اور Bottom- Up Neural Generative Dependency Models</a>
<a id=uz_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Yuqori- pastga va pastga yangi Umumiy</a>
<a id=vi_title style=display:none href=https://aclanthology.org/K19-1022.pdf>Đối chiếu các chế độ tự động phát triển từ trên xuống</a>
<a id=zh_title style=display:none href=https://aclanthology.org/K19-1022.pdf>校自上而下,与自下而上者神经生依模形</a></h2><p class=lead><a href=/people/a/austin-matthews/>Austin Matthews</a>,
<a href=/people/g/graham-neubig/>Graham Neubig</a>,
<a href=/people/c/chris-dyer/>Chris Dyer</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recurrent neural network grammars generate sentences using phrase-structure syntax and perform very well on both <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. To explore whether generative dependency models are similarly effective, we propose two new generative models of dependency syntax. Both models use <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural nets</a> to avoid making explicit <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independence assumptions</a>, but they differ in the order used to construct the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> : one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. We evaluate the two models on three typologically different languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. While both <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> improve <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance over a <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative baseline</a>, they are significantly less effective than non-syntactic LSTM language models. Surprisingly, little difference between the construction orders is observed for either <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> or <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Herhaalde neuralnetwerk gramme genereer setnings gebruik frase- struktuur sintaks en uitvoer baie goed op beide verwerking en taal modeling. Om te ondersoek of genereerbare afhanklikheidmodele gelyk effektief is, voorstel ons twee nuwe genereerbare modele van afhanklikheidsintaks. Beide modele gebruik herhaalde neurale netwerke om uitbreek te maak eksplisiese onveiligheid assumpsies, maar hulle verskil in die volgorde wat gebruik word om die bome te konstrukteer: een bou die boom onderste en die ander bo-onderste, wat profunde verander die estimatie probleme wat deur die leerder aangesig is. Ons evalueer die twee modele op drie tipologiese verskillende tale: Engels, Arabiese en Japanse. Alhoewel beide genereerde modele verbeter die verwerking van prestasie oor 'n diskriminasiewe basislien, is hulle betekenlik minder effektief as nie-sintaktieke LSTM taal modele. Verskil tussen die konstruksiebevele is onderwerp vir verwerking of taal modellering.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recurrent neural network grammars generate sentences using phrase-structure syntax and perform very well on both parsing and language modeling. አዲስ አዲስ አዲስ የፍላጎት ተሟጋቾች የሲንካስር ምሳሌዎች መሆኑን ለመመርመር እናሳውቃለን፡፡ ሁለቱም ሞላጆች የነፃነትን አሳብ ለመግለጥ የሚቆጠሩ የደዌብ መረብ ይጠይቃሉ፤ ነገር ግን ዛፎችን ለመሥራት በተለያዩ ይለያሉ፤ አንዱ የዛፉን መሠረትና ሁለተኛውን በላይ ይሠራል፣ ይህም በተማሪው የተቃውሞ ጉዳይ ይለውጣል፡፡ በሦስት በተለያዩ ቋንቋዎች ላይ ሁለቱን ምሳሌዎች እናረጋግጣለን፤ እንግሊዝኛ፣ ዐረብኛ እና ጃፓንኛ፡፡ ሁለቱም የዘረኝነት ሞዴሎች በተለያየ ብሔራዊ ደረጃዎች ላይ የፓርቲውን አካባቢ ሲያሳድጉ፣ ከLSTM ቋንቋ ምሳሌዎች የበለጠ ጥያቄ በጣም ጥቂት ናቸው፡፡ በማስደንቀቅ፣ የግንኙነቱ ትእዛዝ መካከለኛ ትንሽ ልዩነት ማኅበረሰብ ወይም ቋንቋ ምሳሌ ማሳየት ነው፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>تولد القواعد النحوية للشبكة العصبية المتكررة جملًا باستخدام بناء جملة بنية العبارات وتؤدي أداءً جيدًا في كل من التحليل ونمذجة اللغة. لاستكشاف ما إذا كانت نماذج التبعية التوليدية فعالة بالمثل ، نقترح نموذجين توليديين جديدين لبناء جملة التبعية. يستخدم كلا النموذجين شبكات عصبية متكررة لتجنب وضع افتراضات استقلالية صريحة ، لكنهما يختلفان في الترتيب المستخدم لبناء الأشجار: أحدهما يبني الشجرة من أسفل إلى أعلى والآخر من أعلى إلى أسفل ، مما يغير بشكل عميق مشكلة التقدير التي يواجهها المتعلم. نقوم بتقييم النموذجين على ثلاث لغات مختلفة نمطياً: الإنجليزية والعربية واليابانية. بينما يعمل كلا النموذجين التوليدين على تحسين أداء التحليل على أساس تمييزي ، إلا أنهما أقل فعالية بشكل ملحوظ من نماذج لغة LSTM غير النحوية. والمثير للدهشة أنه لوحظ اختلاف بسيط بين أوامر البناء سواء من حيث الإعراب أو النمذجة اللغوية.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tekrar nöral ağ grammaları fərz-strukturası sintaksi vasitəsilə cümlələri yaradır və dil modellərində də çox yaxşı işlər edir. Generativ bağımlılıq modellərinin eyni kimi etkin olduğunu keşfetmək üçün iki yeni generativ modellərin bağımlılıq sintaksi təklif edirik. İki modellər açıq bağımsızlıq iddialarını etmək üçün yenidən nöral ağlarını istifadə edirlər, amma ağacları in şa etmək üçün istifadə edilən müddətlərdə ixtilaf edirlər: birisi ağac aşağı və digər yuxarı aşağı inşa edir, ki öyrənənənin qarşısındakı hesab problemini dəyişdirir. Biz bu iki modeli üç tipolojik fərqli dildə değerlendiririk: İngilizce, ərəb və Japonca. Həqiqətən, hər ikisi generikat modellər diskriminasiyyətli səhifələr üzərində analizmə performansını yaxşılaşdırmaq üçün, sintaktik LSTM dili modellərindən çox daha çox etkilidir. İnşallah ki, inşaat əmrinin arasında az bir fərqli yoxdur ya analizə, ya da dil modelləri üçün gözləyir.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Повтарящите се граматики на невронната мрежа генерират изречения, използвайки синтаксиса на фраза-структура и се представят много добре както при анализирането, така и при езиковото моделиране. За да проучим дали моделите на генеративна зависимост са еднакво ефективни, предлагаме два нови генеративни модела на синтаксис на зависимост. И двата модела използват повтарящи се невронни мрежи, за да избегнат изрични предположения за независимост, но те се различават по реда, използван за изграждане на дърветата: единият изгражда дървото отдолу нагоре, а другият отгоре надолу, което дълбоко променя проблема с оценката, пред който е изправен обучаемият. Ние оценяваме двата модела на три типологично различни езика: английски, арабски и японски. Макар и двата генеративни модела да подобряват ефективността на анализа на дискриминационна база, те са значително по-малко ефективни от несинтактичните езикови модели. Изненадващо, малка разлика между поръчките за строеж се наблюдава както при анализиране, така и при езиково моделиране.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>বর্তমান নিউরেল নেটওয়ার্ক গ্রামার ব্যবহার করে শব্দ-কাঠামো সিন্ট্যাক্স ব্যবহার করে বাক্য সৃষ্টি করে এবং পার্সিং এবং ভাষা জেনারেটিভ নির্ভরশীল মডেল একই সাথে কার্যকর কিনা, আমরা নির্ভর সিন্ট্যাক্সের দুই নতুন জেনারেটিভ মডেল প্রস্তাব করি। Both models use recurrent neural nets to avoid making explicit independence assumptions, but they differ in the order used to construct the trees: one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. আমরা তিন ভিন্ন ভিন্ন ভাষায় এই দুটি মডেলের মূল্য দিচ্ছি: ইংরেজী, আরবী এবং জাপানীয়। যদিও উভয় জেনারেটিভ মডেল একটি বৈষম্যিক ভাষায় পার্সিং প্রদর্শনের প্রভাব উন্নত করে, তারা গুরুত্বপূর্ণ কার্যকর এলসিএম ভাষার মডেলের বিস্ময়কর, নির্মাণ নির্মাণ নির্দেশের মধ্যে কিছুটা পার্জিং অথবা ভাষা মডেলিং দেখা যাচ্ছে।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>རྩིས་འཁྲུལ་གྱི་ཉེན་སྐྱེས་འབྲེལ་གྱི་རྩིས་འབྲུ་བཞིན་པའི་ཚིག་རྟགས་སྒྲུབ་ཀྱིས་ཚིག་རྟགས་བཀོད་པ་དང་སྐད་རིགས་མ་དབ ལྟ་རྟོག་ནུས་པ་ཁག་གི་རྟེན་འབྲེལ་བ་མིག Both models use recurrent neural nets to avoid making explicit independence assumptions, but they differ in the order used to construct the trees: one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. ང་ཚོས་དབྱེ་རིགས་དང་། སྐད་རིགས་མི་འདྲ་བའི་མིག་གཟུགས་གཉིས་ཀྱི་རྩི་མོལ་ཞིབ་བྱས་པ་ཡིན། ཨིན་ཇིས་དང While both generative models improve parsing performance over a discriminative baseline, they are significantly less effective than non-syntactic LSTM language models. མཐའ་འཁོར་སྣང་མེད་པར། བཟོ་བརྩིགས་གྱི་བཀའ་བརྗོད་ལས་དབྱེ་སྟངས་གང་ཡང་ན་སྐད་རིགས་མ་དབྱེ་བ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Povratni gramari neuralne mreže stvaraju rečenice koristeći sintaks fraze strukture i vrlo dobro izvode na analizu i jezičkom modelima. Da bi istražili da li su modeli generične zavisnosti slično efikasni, predlažemo dva nove generična modela sintaksa ovisnosti. Obje modele koriste rekonstruirane neuralne mreže kako bi izbjegli izražavanje jasnih pretpostavki o nezavisnosti, ali se razlikuju u redovima koji se koristi za konstrukciju drveta: jedan izgradi drvo dolje i drugi vrh dolje, koji se duboko mijenja problem procjene s kojim se suočava učenik. Procjenjujemo dva modela na tri tipološki različita jezika: engleski, arapski i japanski. Iako su obje generativne modele poboljšale analizu učinkovitosti nad diskriminacijom početnom linijom, značajno su manje učinkoviti od neosintaktičnih jezičkih modela LSTM-a. Iznenađujuće, malo razlike između zapovijedi o konstrukciji primijećuje se za analizu ili modeliranje jezika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Les gramàtiques de xarxa neural recurrents generen frases utilitzant sintaxis d'estructura de frases i funcionen molt bé tant en l'analització com en la modelació de llenguatges. To explore whether generative dependency models are similarly effective, we propose two new generative models of dependency syntax. Ambdós models utilitzen xarxes neurals recurrents per evitar fer suposicions explícites de independència, però difereixen en l'ordre que s'utilitzen per construir els arbres: un construeix l'arbre de dalt a dalt i l'altre de dalt a baix, que canvia profundament el problema d'estimació que enfronta l'aprenent. We evaluate the two models on three typologically different languages: English, Arabic, and Japanese. While both generative models improve parsing performance over a discriminative baseline, they are significantly less effective than non-syntactic LSTM language models. Sorprenentment, hi ha poca diferència entre les ordres de construcció, tant per l'analització com per la modelació lingüística.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Opakované gramatiky neuronových sítí generují věty pomocí syntaxe frázové struktury a velmi dobře fungují jak při parsování, tak i při modelování jazyka. Abychom zjistili, zda jsou generační závislostní modely podobně efektivní, navrhujeme dva nové generační modely syntaxe závislostí. Oba modely používají recidivující neuronové sítě, aby se vyhnuly explicitním předpokladům o nezávislosti, ale liší se pořadím použitým k konstrukci stromů: jeden buduje strom zdola nahoru a druhý shora dolů, což výrazně mění problém odhadu, kterému žák čelí. Tyto dva modely hodnotíme na třech typologicky odlišných jazycích: angličtině, arabštině a japonštině. Zatímco oba generativní modely zlepšují výkon analýzy nad diskriminační základní linií, jsou výrazně méně efektivní než nesyntaktické jazykové modely LSTM. Překvapivě je pozorován malý rozdíl mezi stavebními zakázkami buď pro parsování nebo jazykové modelování.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tilbagevendende neurale netværksgrammater genererer sætninger ved hjælp af sætningsstruktur syntaks og fungerer meget godt på både parsing og sprogmodellering. For at undersøge, om generative afhængighedsmodeller er tilsvarende effektive, foreslår vi to nye generative modeller for afhængighedssyntaks. Begge modeller bruger tilbagevendende neurale net for at undgå at gøre eksplicitte uafhængighedsantagelser, men de adskiller sig i den rækkefølge, der anvendes til at konstruere træerne: den ene bygger træet nedefra og op og den anden top-down, hvilket dybt ændrer det estimeringsproblem, som eleven står over for. Vi evaluerer de to modeller på tre typologisk forskellige sprog: engelsk, arabisk og japansk. Mens begge generative modeller forbedrer fortolkningens ydeevne i forhold til en diskriminerende baseline, er de betydeligt mindre effektive end ikke-syntaktiske LSTM sprogmodeller. Overraskende nok observeres der kun lidt forskel mellem byggeordrerne for enten parsing eller sprogmodellering.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Wiederholte neuronale Netzwerkgrammatiken generieren Sätze mit Phrasenstruktur-Syntax und funktionieren sowohl beim Parsen als auch bei der Sprachmodellierung sehr gut. Um zu untersuchen, ob generative Abhängigkeitsmodelle ähnlich effektiv sind, schlagen wir zwei neue generative Abhängigkeitsmodelle vor. Beide Modelle verwenden wiederkehrende neuronale Netze, um explizite Unabhängigkeitsannahmen zu vermeiden, unterscheiden sich jedoch in der Reihenfolge, in der die Bäume konstruiert werden: Das eine baut den Baum von unten nach oben und das andere von oben nach unten, was das Schätzproblem des Lernenden grundlegend verändert. Wir bewerten die beiden Modelle anhand von drei typologisch unterschiedlichen Sprachen: Englisch, Arabisch und Japanisch. Während beide generativen Modelle die Parsing-Leistung über eine diskriminierende Basislinie verbessern, sind sie deutlich weniger effektiv als nicht-syntaktische LSTM-Sprachmodelle. Überraschenderweise werden bei Parsing oder Sprachmodellierung nur geringe Unterschiede zwischen den Bauaufträgen beobachtet.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Οι επαναλαμβανόμενες γραμματικές νευρωνικών δικτύων δημιουργούν προτάσεις χρησιμοποιώντας σύνταξη δομής φράσεων και αποδίδουν πολύ καλά τόσο στην ανάλυση όσο και στη μοντελοποίηση γλωσσών. Για να διερευνήσουμε αν τα μοντέλα γενεαλογικής εξάρτησης είναι εξίσου αποτελεσματικά, προτείνουμε δύο νέα γενεαλογικά μοντέλα σύνταξης εξάρτησης. Και τα δύο μοντέλα χρησιμοποιούν επαναλαμβανόμενα νευρωνικά δίχτυα για να αποφύγουν τις ρητές υποθέσεις ανεξαρτησίας, αλλά διαφέρουν στη σειρά που χρησιμοποιείται για την κατασκευή των δέντρων: το ένα χτίζει το δέντρο από κάτω προς τα πάνω και το άλλο από πάνω προς τα κάτω, γεγονός που αλλάζει ριζικά το πρόβλημα εκτίμησης που αντιμετωπίζει ο μαθητής. Αξιολογούμε τα δύο μοντέλα σε τρεις τυπολογικά διαφορετικές γλώσσες: Αγγλικά, Αραβικά και Ιαπωνικά. Ενώ και τα δύο παραγωγικά μοντέλα βελτιώνουν την απόδοση ανάλυσης σε μια διακριτική βάση βάσης, είναι σημαντικά λιγότερο αποτελεσματικά από τα μη συντακτικά γλωσσικά μοντέλα. Παραδόξως, παρατηρείται μικρή διαφορά μεταξύ των παραγγελιών κατασκευής είτε για ανάλυση είτε για γλωσσική μοντελοποίηση.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Las gramáticas recurrentes de redes neuronales generan oraciones utilizando la sintaxis de estructura de frases y funcionan muy bien tanto en el análisis como en el modelado del lenguaje. Para explorar si los modelos de dependencia generativa son igualmente efectivos, proponemos dos nuevos modelos generativos de sintaxis de dependencias. Ambos modelos utilizan redes neuronales recurrentes para evitar hacer suposiciones explícitas de independencia, pero difieren en el orden utilizado para construir los árboles: uno construye el árbol de abajo hacia arriba y el otro de arriba hacia abajo, lo que cambia profundamente el problema de estimación al que se enfrenta el alumno. Evaluamos los dos modelos en tres idiomas tipológicamente diferentes: inglés, árabe y japonés. Si bien ambos modelos generativos mejoran el rendimiento del análisis por encima de una línea de base discriminativa, son significativamente menos efectivos que los modelos de lenguaje LSTM no sintácticos. Sorprendentemente, se observa poca diferencia entre las órdenes de construcción para el análisis o el modelado del lenguaje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Korduvad närvivõrgu grammatikad genereerivad lauseid fraasistruktuuri süntaksi abil ja toimivad väga hästi nii parsimisel kui ka keele modelleerimisel. Et uurida, kas generatiivsed sõltuvusmudelid on sarnaselt efektiivsed, pakume välja kaks uut generatiivset sõltuvussüntaksi mudelit. Mõlemad mudelid kasutavad korduvaid närvivõrke, et vältida selgesõnaliste sõltumatuse eelduste tegemist, kuid nad erinevad puude ehitamise järjekorras: üks ehitab puu alt üles ja teine ülalt alla, mis muudab põhjalikult hindamisprobleemi, millega õppija silmitsi seisab. Hindame kahte mudelit kolmes tüpoloogiliselt erinevas keeles: inglise, araabia ja jaapani keeles. Kuigi mõlemad generatiivsed mudelid parandavad parsimise jõudlust diskrimineeriva algtasemega võrreldes, on need oluliselt vähem efektiivsed kui mittesüntaktilised LSTM keelemudelid. Üllataval kombel täheldatakse ehitustellimuste vahel vähe erinevusi kas parsimise või keele modelleerimise puhul.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>برنامه‌های شبکه عصبی دوباره با استفاده از ویژه‌سازی عبارت تولید می‌کنند و در مورد ویژه‌سازی و مدل‌سازی زبان خیلی خوب انجام می‌دهند. برای تحقیق کردن که آیا مدل بستگی نسبت به موجود موثر هستند، ما دو مدل جدید نسبت به نسبت بستگی را پیشنهاد می کنیم. هر دو مدل از شبکه‌های عصبی دوباره استفاده می‌کند تا از فرض کردن فرضیه‌های استفاده از استفاده نکنند، ولی آنها در فرض ساختن درختان تفاوت می‌کنند: یک درخت پایین و پایین پایین را ساخته می‌کند، که مشکل ارزیابی که توسط دانش آموزش روبرو می‌شود عمیقا تغییر می‌دهد. ما دو مدل را در سه زبان نوع‌شناسی متفاوت ارزیابی می‌کنیم: انگلیسی، عربی و ژاپنی. در حالی که هر دو مدل ژنراتیک عملکرد تجزیه کردن روی یک خط پایین جدایی بهتر می‌شود، آنها خیلی کمتر از مدل‌های زبان LSTM غیر سنتاکتیک موثر هستند. تعجب کننده است که تفاوت کوچک بین دستورات ساختمون برای شناسایی یا مدل زبانی مشاهده می شود.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Toistuvat neuroverkkogrammatikat tuottavat lauseita fraasirakenteen syntaksilla ja toimivat erittäin hyvin sekä jäsentämisessä että kielimallinnuksessa. Selvittääksemme, ovatko generatiiviset riippuvuusmallit yhtä tehokkaita, ehdotamme kahta uutta generatiivista riippuvuussuuntamallia. Molemmat mallit käyttävät toistuvia hermoverkkoja välttääkseen selkeitä itsenäisyysoletuksia, mutta ne eroavat toisistaan puiden rakentamisessa käytetyssä järjestyksessä: toinen rakentaa puun alhaalta ylös ja toinen ylhäältä alas, mikä muuttaa perusteellisesti oppijan kohtaamia arviointiongelmia. Arvioimme malleja kolmella eri kielellä: englanti, arabia ja japani. Vaikka molemmat generatiiviset mallit parantavat jäsennyssuorituskykyä diskriminatiiviseen lähtötilanteeseen verrattuna, ne ovat huomattavasti vähemmän tehokkaita kuin ei-syntaktiset LSTM-kielimallit. Yllättävää kyllä rakennustilausten välillä ei ole juurikaan eroa joko parsauksessa tai kielimallinnuksessa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Les grammaires de réseaux neuronaux récurrents génèrent des phrases utilisant la syntaxe de structure de phrase et fonctionnent très bien à la fois en analyse syntaxique et en modélisation du langage. Pour déterminer si les modèles de dépendance génératifs sont aussi efficaces, nous proposons deux nouveaux modèles génératifs de syntaxe de dépendance. Les deux modèles utilisent des réseaux neuronaux récurrents pour éviter de faire des hypothèses d'indépendance explicites, mais ils diffèrent dans l'ordre utilisé pour construire les arbres : l'un construit l'arbre de bas en haut et l'autre de haut en bas, ce qui modifie profondément le problème d'estimation auquel l'apprenant est confronté. Nous évaluons les deux modèles dans trois langues typologiquement différentes : l'anglais, l'arabe et le japonais. Alors que les deux modèles génératifs améliorent les performances d'analyse par rapport à une base discriminante, ils sont nettement moins efficaces que les modèles de langage LSTM non syntaxiques. Étonnamment, peu de différence entre les ordres de construction est observée pour l'analyse syntaxique ou la modélisation du langage.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gineann gramadach líonra néarach athfhillteach abairtí trí úsáid a bhaint as comhréir frása-struchtúr agus feidhmíonn siad go han-mhaith ar pharsáil agus samhaltú teanga. Chun iniúchadh a dhéanamh an bhfuil na samhlacha giniúna spleáchais chomh héifeachtach céanna, molaimid dhá shamhail ghiniúna nua de chomhréir spleáchais. Úsáideann an dá mhúnla líonta néaracha athfhillteacha chun boinn tuisceana soiléire neamhspleáchais a sheachaint, ach tá difríocht eatarthu san ord a úsáidtear chun na crainn a thógáil: tógann duine acu an crann ó bhun aníos agus an ceann eile ó bharr anuas, rud a athraíonn go mór an fhadhb meastacháin a bhíonn roimh an bhfoghlaimeoir. Déanaimid measúnú ar an dá mhúnla ar thrí theanga atá difriúil ó thaobh na clódóireachta de: Béarla, Araibis agus Seapáinis. Cé go gcuireann an dá mhúnla giniúna feabhas ar fheidhmíocht parsála thar bhunlíne idirdhealaitheach, tá siad i bhfad níos lú éifeachtaí ná samhlacha teanga LSTM neamh-chomhréireacha. Is ionadh é nach bhfuil mórán difríochta le sonrú idir na horduithe tógála maidir le parsáil nó samhaltú teanga.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>KCharselect unicode block name To, ka gane kowa misãlai masu inganci na jenatacce, masu amfani da kwamfyutan, za'a buƙata, misãlai biyu na daban-daban na sassautar da ɗabi'a. Dukan motsala biyu sunã yin amfani da waɗanan neural wanda ya sake zartar da shi, don ya bayyana zato mai inganci, kuma amma sun sãɓã wajen ƙayyade kamar an yi amfani da shi ga ya gina itãce: ɗayan yana samar itãciyar ƙasa da ƙasa da kuma gudan sama, da mai musanya muhimmin zartar muhimmanci mai ƙidãya da mai amfani da shi. Tuna ƙaddara misalin biyu a cikin lingui uku mai fasa-nau'i: Ingiriya, Larabci, da Japane. Waku da dukansu masu motsi biyu suka improve performance ga parse a kan wani basalin da yin ɓarna, sai su masu da muhimmi masu amfani da shi ne mafi ƙaranci daga misãlai na LSSM-da-syntactic. Ina yi mãmãki, za'a nuna difwalta kaɗan a tsakanin umarnin mazaɓa ko kuma don a sami misalin harshe.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>גרמטיקות רשת עצבית חוזרות יוצרות משפטים באמצעות סינטקס מבנה ביטויים ומפקדות היטב בין בדיקות וגם דגם שפה. כדי לחקור אם דוגמני תלויות דוריות הם יעילים באותה מידה, אנחנו מציעים שני דוגמנים דוריים חדשים של סינטקס תלויות. שני הדוגמנים משתמשים ברשתות עצביות חוזרות כדי להימנע מלעשות הנחות עצמאות ברורות, אבל הם שונים במסדר שנמשיך לבנות את העצים: אחד בונה את העץ מתחת למעלה והשני למעלה למטה אנו מעריכים את שני הדוגמנים בשלושה שפות טיפולוגיות שונות: אנגלית, ערבית, יפנית. למרות ששני דוגמנים דוריים משתפרים את ההעברה על בסיס דיסקרטיבי, הם פחות יעילים משמעותיים מאשר דוגמנים לשפה LSTM לא סינטקטיים. באופן מפתיע, הבדל הקטן בין פקודות הבנייה מתבונן עבור בדיקת או דוגמנית שפה.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>आवर्तक तंत्रिका नेटवर्क व्याकरण वाक्यांश-संरचना वाक्यविन्यास का उपयोग करके वाक्य उत्पन्न करते हैं और पार्सिंग और भाषा मॉडलिंग दोनों पर बहुत अच्छा प्रदर्शन करते हैं। यह पता लगाने के लिए कि क्या उत्पादक निर्भरता मॉडल समान रूप से प्रभावी हैं, हम निर्भरता वाक्यविन्यास के दो नए उत्पादक मॉडल का प्रस्ताव करते हैं। दोनों मॉडल स्पष्ट स्वतंत्रता धारणाओं को बनाने से बचने के लिए आवर्तक तंत्रिका जाल का उपयोग करते हैं, लेकिन वे पेड़ों के निर्माण के लिए उपयोग किए जाने वाले क्रम में भिन्न होते हैं: एक पेड़ को नीचे-ऊपर और दूसरा ऊपर-नीचे बनाता है, जो शिक्षार्थी द्वारा सामना की जाने वाली अनुमान समस्या को गहराई से बदलदेता है। हम तीन टाइपोलॉजिकल रूप से अलग-अलग भाषाओं पर दो मॉडलों का मूल्यांकन करते हैं: अंग्रेजी, अरबी और जापानी। जबकि दोनों उत्पादक मॉडल एक भेदभावपूर्ण आधार रेखा पर पार्सिंग प्रदर्शन में सुधार करते हैं, वे गैर-वाक्यात्मक एलएसटीएम भाषा मॉडल की तुलना में काफी कम प्रभावी होते हैं। हैरानी की बात है, निर्माण आदेशों के बीच थोड़ा अंतर या तो पार्सिंग या भाषा मॉडलिंग के लिए मनाया जाता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Povratni gramari neuralne mreže stvaraju rečenice koristeći sintaks fraze strukture i vrlo dobro izvode na analiziranju i jezičkom modelima. Da bi istražili da li su modeli generativne zavisnosti slično učinkoviti, predlažemo dva nove generativne modele syntakse ovisnosti. Obje modele koriste rekonstruirane neuralne mreže kako bi se izbjegli izražavati jasne pretpostavke nezavisnosti, ali se razlikuju u redovima koji se koristi za konstrukciju drveta: jedan izgradi drvo dolje i drugi vrh dolje, koji duboko mijenja problem procjene s učenikom. Procjenjujemo dva modela na tri tipološki različita jezika: engleski, arapski i japanski. Iako su obje generativne modele poboljšale učinkovitost analize na diskriminacijskom početku, značajno su manje učinkoviti od neosintaktičnih jezičkih modela LSTM-a. Iznenađujuće, malo razlike između zapovijedi o konstrukciji primjećuje se za analizu ili jezičke modele.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Az ismétlődő neurális hálózati nyelvtanfolyamok mondatokat hoznak létre kifejezésszerkezet szintaxisával, és nagyon jól teljesítenek mind az elemzésben, mind a nyelvmodellezésben. Annak vizsgálatára, hogy a generációs függőségi modellek hasonlóan hatékonyak-e, a függőségi szintaxis két új generációs modelljét javasoljuk. Mindkét modell visszatérő neurális hálókat használ annak érdekében, hogy elkerüljék a kifejezett függetlenségi feltételezéseket, de eltérnek a fák építéséhez használt sorrendben: az egyik alulról felfelé építi a fát, a másik felülről lefelé, ami alapvetően megváltoztatja a tanuló becslési problémáját. A két modellt három tipológiailag különböző nyelven értékeljük: angol, arab és japán. Bár mindkét generációs modell javítja az értelmezési teljesítményt egy diszkriminatív alapszinten, jelentősen kevésbé hatékonyak, mint a nem szintaktikus LSTM nyelvi modellek. Meglepő módon kevés különbséget figyelhetünk meg az építési megbízások között az elemzés vagy a nyelvi modellezés esetén.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Նյարդային ցանցի գրամագրությունները ստեղծում են նախադասություններ՝ օգտագործելով արտահայտության կառուցվածքի սինտաքսը և շատ լավ աշխատում են վերլուծության և լեզվի մոդելավորման վրա: Որպեսզի ուսումնասիրենք, թե արդյոք սերունդային կախվածության մոդելները նույնքան արդյունավետ են, մենք առաջարկում ենք կախվածության սինտաքսի երկու նոր սերունդային մոդել: Երկու մոդելները օգտագործում են կրկնօրինակ նյարդային ցանցեր, որպեսզի խուսափելի անկախության ենթադրություններ չկատարեն, բայց դրանք տարբերվում են ծառերի կառուցվածքի համակարգում. մեկը կառուցում է ծառը ներքև և մյուսը ներքև, ինչը խորապես փոխում է ուսանողի հանդիպման գն Մենք գնահատում ենք երկու մոդելները երեք տիպոլոգիապես տարբեր լեզուներով՝ անգլերեն, արաբերեն և ճապոներեն: Մինչդեռ երկու սերունդների մոդելները բարելավում են խտրականության հարաբերությունը, դրանք ավելի քիչ արդյունավետ են, քան ոչ սինտակտիկ LSMT լեզվի մոդելները: Զարմանալի է, որ կառուցվածքի հրամանների միջև փոքր տարբերություն է հետևում կամ վերլուծության, կամ լեզվի մոդելավորման համար:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recurrent neural network grammars generate sentences using phrase-structure syntax and perform very well on both parsing and language modeling. Untuk mengeksplorasi apakah model dependensi generatif sama-sama efektif, kami mengusulkan dua model dependensi generatif baru sintaks. Kedua model menggunakan jaringan saraf berkurang untuk menghindari membuat asumsi kemerdekaan eksplicit, tetapi mereka berbeda dalam urutan yang digunakan untuk membangun pohon: satu membangun pohon bawah-atas dan yang lain atas-bawah, yang mengubah secara mendalam masalah perhitungan yang dihadapkan oleh pelajar. Kami mengevaluasi dua model dalam tiga bahasa tipologi yang berbeda: Bahasa Inggris, Arab, dan Jepang. While both generative models improve parsing performance over a discriminative baseline, they are significantly less effective than non-syntactic LSTM language models. Mengejutkan, sedikit perbedaan antara perintah konstruksi diperhatikan baik untuk menghurai atau model bahasa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Le grammatiche di rete neurale ricorrenti generano frasi utilizzando sintassi frasari e funzionano molto bene sia nell'analisi che nella modellazione del linguaggio. Per esplorare se i modelli di dipendenza generativa sono altrettanto efficaci, proponiamo due nuovi modelli generativi di sintassi di dipendenza. Entrambi i modelli utilizzano reti neurali ricorrenti per evitare di fare ipotesi di indipendenza esplicita, ma differiscono nell'ordine utilizzato per costruire gli alberi: uno costruisce l'albero dal basso verso l'alto e l'altro dall'alto verso il basso, il che cambia profondamente il problema di stima affrontato dallo studente. Valutiamo i due modelli su tre lingue tipologicamente diverse: inglese, arabo e giapponese. Mentre entrambi i modelli generativi migliorano le prestazioni di analisi rispetto a una base di base discriminatoria, sono significativamente meno efficaci rispetto ai modelli LSTM non sintattici. Sorprendentemente, si osserva poca differenza tra gli ordini di costruzione sia per l'analisi che per la modellazione del linguaggio.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>再帰ニューラルネットワーク文法は、句構造構文を使用して文を生成し、構文解析と言語モデリングの両方で非常に優れたパフォーマンスを発揮します。生成依存性モデルが同様に効果的かどうかを探るために、2つの新しい生成依存性構文モデルを提案します。どちらのモデルも、明確な独立性の仮定を立てることを避けるために再帰的なニューラルネットを使用しますが、木を構築するために使用される順序が異なります。1つは木のボトムアップを構築し、もう1つはトップダウンを構築し、学習者が直面する推定問題を深刻に変化させます。私たちは、英語、アラビア語、日本語の3つの異なる言語で2つのモデルを評価します。両方の生成モデルは、識別ベースラインにわたって構文解析パフォーマンスを改善するが、それらは非構文LSTM言語モデルよりも有意に効果が低い。驚くべきことに、構築オーダー間の差異は、構文解析または言語モデリングのいずれにおいてもほとんど観察されない。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Laptop" and "Desktop Jejaring Awak dhéwé model sing gambar kelas kuwi tambah-iwakan sing bisa nggawe gerarané supoyo carane sing beraksi supoyo, maca wong wis diparahan kanggo nguasai dar alih sing diparahan kanggo nggawe punika: wong dhéwé nggawe akeh sekang carane lan kewong sangan carane sing ngedol padha kanggo tuka nggawe kesempatan uwong. Awak dhéwé éntukno sistem sing sampeyan telu, luwih basa luwih: Inggal, Perancis, lan Japang. Saying maneh</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>განვითარებული ნეიროლური ქსელის გრამიმარები გამოყენება ფრაზების სტრუქტურაციის სინტაქსის გამოყენება და ძალიან კარგი გამოყენება მარტივი პარაზაციის და ენ განსხვავებლად, თუ არა განსხვავებული დადარდამული მოდელები სხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვა მო ორივე მოდელები გამოყენებენ რეკურენტური ნეიროლური ქსელები, რომელიც გამოცდილობენ გამოცდილობული განსაზღვრებულება, მაგრამ ისინი განსხვავებენ სწორედ, რომელიც სახელის კონტურაციაში გამოყენებულია: ერთი შექმნის ქვემოთ და მეორ ჩვენ სამი ტიპოლოგიურად განსხვავებული ენების ორი მოდელის შესახებ: ანგლისურად, აპაბურად და იაპონურად. თუმცა ორივე განვითარებული მოდელები განაზღვრებას განაზღვრება დისკრიმინატიური ფესკლინტიური მხარეს, ისინი ძალიან უფრო ეფექტიური არა სინტაქტიური LSTM ენის მოდე სხვადასხვა, შექმნის მოწყობინებაში მალკი განსხვავება იყოს ან პანსტირებისთვის ან ენის მოდელირებისთვის.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Қайталанған невралдық желі граммалары сөздерді сөздер құрылып, талдау және тіл моделінде де дұрыс жасайды. Жалпы тәуелдік үлгілерінің ұқсас әсер етпегенін зерттеу үшін, біз тәуелдік синтаксисінің екі жаңа генерациялық үлгілерін ұсынамыз. Екі үлгілер қайталанатын невралдық желілерді түсінікті тәуелсіздік тапсырмаларын құру үшін қолданылады, бірақ олар ағаштарды құру үшін қолданылатын ретінде айырмашылық: бір ағаш төменгі және жоғары төменгі желінде құрады. Бұл оқытушы Біз үш типтологиялық тілдер үшін екі үлгі үлгілерді бағалаймыз: ағылшын, араб және жапон тілдер. Екі жасалған үлгілер дискриминациялық негізгі сызықтың талдауын жақсартқанда, олар синтактикалық LSTM тіл үлгілерінен әсер етеді. Құрылғы реттерінің арасындағы кішкентай айырмашылығы талдау не тіл моделігі үшін байқалады.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>귀속신경 네트워크 문법은 단어 구조 문법으로 문장을 생성하는데 문법 분석과 언어 모델링에 있어 모두 양호하다.생성 의존 모델이 똑같이 효과가 있는지 연구하기 위해 우리는 두 가지 새로운 생성 의존 문법 모델을 제시했다.이 두 모델 모두 귀속신경 네트워크를 사용하여 명확한 독립성 가설을 피하지만 나무를 구축하는 순서가 다르다. 하나는 아래에서 위로 나무를 구축하는 것이고 다른 하나는 위에서 아래로 나무를 구축하는 것이다. 이것은 학습자가 직면하는 평가 문제를 심각하게 변화시켰다.우리는 세 가지 유형의 서로 다른 언어에서 이 두 가지 모델, 즉 영어, 아랍어, 일본어를 평가한다.이 두 가지 생성 모델 모두 구분 기선보다 해석 성능을 높였지만 비문법 LSTM 언어 모델보다 효율이 현저히 낮았다.놀랍게도 해석이든 언어 모델링이든 구조 순서 사이에는 차이가 거의 없다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kartotinės neurologinio tinklo gramatikos sukuria sakinius naudojant frazių struktūros sintaksą ir labai gerai veikia tiek analizuojant, tiek kalbų modeliuojant. Siekiant ištirti, ar kartų priklausomybės modeliai yra panašiai veiksmingi, siūlome du naujus kartų priklausomybės sintakso modelius. Abu modeliai naudoja pakartotinius nervinius tinklus, kad būtų išvengta aiškių nepriklausomybės prielaidų, tačiau jie skiriasi pagal medžių statybos tvarką: vienas stato medį iš apačios į viršų ir kitą iš viršaus į apačią, o tai labai keičia mokslo patiriamą vertinimo problem ą. Vertiname du modelius trimis tipologiškai skirtingomis kalbomis: anglų, arabų ir japonų. Nors abiejų kartų modeliai gerina analizavimo rezultatus, palyginti su diskriminacine baze, jie yra žymiai mažiau veiksmingi nei nesusintaktiniai LSTM kalbos modeliai. Įspūdinga, kad analizuojant arba modeliuojant kalbas konstrukcijos nurodymai skiriasi nedaug.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Рекурентните граматики на нервната мрежа генерираат реченици користејќи синтаксис со фраза- структура и извршуваат многу добро во анализирањето и моделирањето на јазикот. За да истражуваме дали генеративните модели на зависност се слично ефикасни, предложуваме два нови генеративни модели на синтаксија на зависност. Двајцата модели користат рецидентни нервни мрежи за да избегнат да се прават експлицитни претпоставувања за независност, но се разликуваат во редот кој се користи за изградба на дрвјата: еден го изградува дрвото од дното нагоре и другиот од горе нагоре, што длабоко го менува проценувачкиот проблем со кој се соочува Ги проценуваме двата модели на три типологички различни јазици: англиски, арапски и јапонски. While both generative models improve parsing performance over a discriminative baseline, they are significantly less effective than non-syntactic LSTM language models. Изненадувачки, мала разлика помеѓу наредбите за изградба се забележа или за анализирање или моделирање на јазици.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>പാര്‍സിങ്ങ് ചെയ്യുന്നതിനും ഭാഷ മോഡലോഡിങ്ങിനും ഉപയോഗിച്ച് വാക്കുകള്‍ സൃഷ്ടിക്കുന്ന നെയൂറല്‍ നെറുല്‍ ശൃംഖല ഗ്രാ ജനറല്‍ ആശ്രയിക്കുന്ന മോഡലുകള്‍ ഒരേ പോലെയാണോ പ്രാവര്‍ത്തികമായിരിക്കുന്നതെന്ന് പരിശോധിക്കാന്‍, ആശ്രയിക്കുന്നതിന രണ്ടു മോഡലുകളും വ്യക്തമായ സ്വാതന്ത്ര്യ ഊഹിക്കാന്‍ ന്യൂറല്‍ നെറുല്‍ നെറ്റുകള്‍ ഉപയോഗിക്കുന്നു. പക്ഷെ മരങ്ങള്‍ നിര്‍മ്മിക്കാന്‍ ഉപയോഗിക്കുന്ന ക്രമീകരണത്തില്‍ അവര്‍ വ്യത് മൂന്നു സാധാരണ വ്യത്യസ്ത ഭാഷകളില്‍ രണ്ടു മോഡലുകളെ ഞങ്ങള്‍ വിലയിച്ചുകൊടുക്കുന്നു. ഇംഗ്ലീഷ്, അറബി, ജപ്പ രണ്ടു ജനററിവ് മോഡലുകളും ഒരു വ്യത്യസ്തമാക്കുന്ന ബേസ്ലൈനിലേക്ക് പാര്‍സിങ്ങ് പാര്‍സിങ്ങ് പ്രവര്‍ത്തനങ്ങള്‍ മുന്‍കൂട്ടുമ്പോള അത്ഭുതപ്പെട്ടു, നിര്‍മ്മാണിക്കുന്ന കല്‍പനകള്‍ക്കിടയില്‍ കുറച്ചു വ്യത്യാസം പാര്‍സിങ്ങ് അല്ലെങ്കി</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Дахин дахин мэдрэлийн сүлжээний грамм нь өгүүлбэр-бүтцийн синтаксисийг ашиглан өгүүлбэрийг бий болгодог бөгөөд хэл загварын талаар ажиллах болон загварын загварын талаар маш сайн хийдэг Бүтээгдэхүүний хамааралтай загваруудыг адилхан үр дүнтэй эсэхийг судалж үзэхийн тулд бид хоёр шинэ үйлдвэрлэл хамааралтай синтаксисын шинэ загварыг санал болгоно. Хоёр загвар нь дахин дахин сэтгэл зүйн сүлжээг ашигладаг. Яг тодорхой тогтнолтой байдлын тодорхойлолтуудыг гаргахын тулд тэд мод бүтээх загвараар өөрчлөгдөж байна. Нэг нь мод доор болон нөгөө дээд доор нь бүтээж байна. Энэ нь сурагчийн тулгарсан тооцоо Бид 3 төрлийн хэл дээр хоёр загварыг үнэлдэг: Англи, Араб, Япон. Бүтээлтийн загварууд хоёр нь хуваалцах үйл ажиллагааг ялгаатай суурь шугам дээр улам сайжруулдаг ч тэд синтактикгүй LSTM хэл загвараас илүү үр дүнтэй. Хамгийн гайхалтай нь барилгын захирамжуудын хоорондох бага ялгаа нь хэлний захирамжуудыг хуваалцах, эсвэл хэлний захирамжуудын хоорондох бага ялгаа байдаг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gramatika rangkaian saraf berulang menghasilkan kalimat menggunakan sintaks-struktur frasa dan berjalan dengan baik pada penghuraian dan pemodelan bahasa. Untuk mengeksplorasi sama ada model dependensi generatif sama-sama efektif, kami cadangkan dua model generatif baru sintaks dependensi. Both models use recurrent neural nets to avoid making explicit independence assumptions, but they differ in the order used to construct the trees: one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. Kami menilai dua model dalam tiga bahasa yang berbeza tipologi: Bahasa Inggeris, Arab, dan Jepun. Sementara kedua-dua model generatif meningkatkan prestasi penghuraian atas dasar diskriminatif, ia jauh lebih berkesan daripada model bahasa LSTM yang bukan sintaktik. Yang mengejutkan, sedikit perbezaan antara arahan pembinaan diperhatikan sama ada untuk penghuraian atau pemodelan bahasa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il-grammi rikorrenti tan-netwerk newrali jiġġeneraw sentenzi bl-użu tas-sintaks tal-istruttura tal-frażi u jwettqu prestazzjoni tajba kemm fl-analizzazzjoni kif ukoll fl-immudellar tal-lingwi. Biex jiġi esplorat jekk mudelli ta’ dipendenza ġenerattiva humiex effettivi b’mod simili, nipproponu żewġ mudelli ġenerattivi ġodda ta’ sintaks ta’ dipendenza. Both models use recurrent neural nets to avoid making explicit independence assumptions, but they differ in the order used to construct the trees: one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. Aħna jevalwaw iż-żewġ mudelli fuq tliet lingwi tipikament differenti: l-Ingliż, l-Għarbi u l-Ġappuniż. Filwaqt li ż-żewġ mudelli ġenerattivi jtejbu l-prestazzjoni tal-analiżi fuq linja bażi diskriminatorja, huma sinifikament inqas effettivi minn mudelli lingwistiċi LSTM mhux sintetiċi. Surprisingly, little difference between the construction orders is observed for either parsing or language modeling.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Recurrente neuronale netwerkgrammatica's genereren zinnen met behulp van frase-structure syntaxis en presteren zeer goed op zowel parsing als taalmodellering. Om te onderzoeken of generatieve afhankelijkheidsmodellen vergelijkbaar effectief zijn, stellen we twee nieuwe generatieve modellen voor afhankelijkheidssyntaxis voor. Beide modellen maken gebruik van terugkerende neurale netten om expliciete onafhankelijkheidsveronderstellingen te vermijden, maar ze verschillen in de volgorde die gebruikt wordt om de bomen te bouwen: de ene bouwt de boom bottom-up en de andere top-down, wat het schattingsprobleem van de leerling ingrijpend verandert. We evalueren de twee modellen op drie typologisch verschillende talen: Engels, Arabisch en Japans. Hoewel beide generatieve modellen de parseringsprestaties over een discriminerende baseline verbeteren, zijn ze aanzienlijk minder effectief dan niet-syntactische LSTM taalmodellen. Verrassend genoeg wordt er weinig verschil waargenomen tussen de bouworders voor parsing of taalmodellering.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gjennomsiktige neuralnettverksgrammar lagar setningar ved hjelp av frase- struktursyntaks og utfør veldig godt ved tolking og språk- modellering. For å utforske om genererte avhengighetsmodeller er likevel effektive, fører vi to nye genererte modeller av avhengighetssyntaks. Begge modeller brukar rekurserande neuralnettverk for å unngå å gjera eksplisitt uavhengighetsassumpsjon, men dei er ulike i rekkefølgja som brukar til å konstruera trær: ein bygger trær nedover og den andre øvre nedover, som endrar dårlig problemet med estimasjonen som lærer. Vi evaluerer dei to modelane på tre typologisk ulike språk: engelsk, arabisk og japansk. Selv om begge genererte modeller forbedrar tolkingsfunksjonen over ein diskriminert baseline, er dei mykje mindre effektive enn ikkje-syntaktiske LSTM-språk-modeller. Manglar at liten forskjell mellom konstruksjonsrekkefølgja er observert for anten tolking eller språk modellering.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Powtarzające się gramatyki sieci neuronowych generują zdania za pomocą składni struktury fraz i sprawdzają się bardzo dobrze zarówno w parsowaniu, jak i modelowaniu językowym. Aby zbadać, czy modele zależności generacyjnej są podobnie skuteczne, proponujemy dwa nowe modele składni zależności generacyjnej. Oba modele wykorzystują powtarzające się sieci neuronowe, aby uniknąć wyraźnych założeń niezależności, ale różnią się one kolejnością używaną do konstruowania drzew: jeden buduje drzewo oddolnie, a drugi górnie w dół, co głęboko zmienia problem szacunkowy, z którym boryka się uczący. Oceniamy dwa modele na trzech typologicznie różnych językach: angielskim, arabskim i japońskim. Podczas gdy oba modele generatywne poprawiają wydajność parsowania w odniesieniu do dyskryminacyjnej bazy bazowej, są one znacznie mniej skuteczne niż nieskładniowe modele językowe LSTM. Co zaskakujące, niewielka różnica między zleceniami konstrukcyjnymi jest obserwowana zarówno w przypadku parsowania, jak i modelowania językowego.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>As gramáticas de redes neurais recorrentes geram sentenças usando sintaxe de estrutura de frase e funcionam muito bem tanto na análise quanto na modelagem de linguagem. Para explorar se os modelos de dependência generativa são igualmente eficazes, propomos dois novos modelos generativos de sintaxe de dependência. Ambos os modelos usam redes neurais recorrentes para evitar suposições explícitas de independência, mas diferem na ordem usada para construir as árvores: um constrói a árvore de baixo para cima e o outro de cima para baixo, o que muda profundamente o problema de estimativa enfrentado pelo aprendiz. Avaliamos os dois modelos em três idiomas tipologicamente diferentes: inglês, árabe e japonês. Embora ambos os modelos generativos melhorem o desempenho de análise em uma linha de base discriminativa, eles são significativamente menos eficazes do que os modelos de linguagem LSTM não sintáticos. Surpreendentemente, pouca diferença entre as ordens de construção é observada tanto para análise sintática quanto para modelagem de linguagem.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grammaticele recurente ale rețelei neurale generează propoziții folosind sintaxa structurii frazelor și funcționează foarte bine atât în analizare, cât și în modelarea limbajului. Pentru a explora dacă modelele de dependență generativă sunt la fel de eficiente, propunem două noi modele generative de sintaxă a dependenței. Ambele modele folosesc rețele neurale recurente pentru a evita ipotezele explicite de independență, dar diferă în ordinea utilizată pentru a construi copacii: unul construiește copacul de jos în sus și celălalt de sus în jos, ceea ce schimbă profund problema estimării cu care se confruntă elevul. Evaluăm cele două modele pe trei limbi tipologice diferite: engleză, arabă și japoneză. În timp ce ambele modele generative îmbunătățesc performanța analizării față de o bază discriminatorie, ele sunt semnificativ mai puțin eficiente decât modelele LSTM nesintactice. În mod surprinzător, se observă o mică diferență între ordinele de construcție fie pentru analizare, fie pentru modelarea limbajului.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Рекуррентные грамматики нейронной сети генерируют предложения, используя синтаксис фраза-структура, и очень хорошо работают как при синтаксическом анализе, так и при языковом моделировании. Чтобы исследовать, являются ли генеративные модели зависимостей одинаково эффективными, мы предлагаем две новые генеративные модели синтаксиса зависимостей. Обе модели используют рекуррентные нейронные сети, чтобы избежать явных предположений о независимости, но они различаются в порядке, используемом для построения деревьев: одна строит дерево снизу вверх, а другая сверху вниз, что глубоко меняет проблему оценки, с которой сталкивается учащийся. Мы оцениваем две модели на трех типологически разных языках: английском, арабском и японском. Хотя обе генеративные модели улучшают производительность синтаксического анализа по сравнению с дискриминационной базовой линией, они значительно менее эффективны, чем несинтаксические языковые модели LSTM. Удивительно, но небольшая разница между порядками построения наблюдается как для синтаксического анализа, так и для языкового моделирования.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>වාර්තාව- සංවිධානය සහ භාෂාව මොඩලින් සඳහා වාර්තාව සඳහා ප්‍රයෝජනයක් නිර්මාණය කරනවා. පරීක්ෂණය කරන්න ප්‍රභාවිත විශේෂතාවක් සමාන්‍යයෙන් ප්‍රභාවිත විද්‍යාවක් තියෙන්නේ නැද්ද කියලා, අපි අළු දෙන්නම් මොඩල් දෙන්නම් ප්‍රතිශීල නිර්මාණ ජාලය භාවිත කරනවා ප්‍රතිශීල විශ්වාසය නිර්මාණය කරන්න, ඒත් එයාලා ගස් නිර්මාණය කරන්න ප්‍රතිශේෂයෙන් වෙනස් කරනවා: අපි වෙනස් භාෂාවල් තුනක් වගේ මොඩේල් දෙකක් විශ්වාස කරනවා: ඉංග්‍රීසි, අරාබික්, ජාපානිය දෙන්නම ප්‍රභාවිත මොඩේල්ස් දෙන්නම් විශේෂ විශේෂ ප්‍රභාව ප්‍රභාවිත විශේෂ කරනවා නම්, ඔවුන් විශේෂ විශ පුදුම විදිහට, නිර්මාණය නිර්මාණය අතරේ පොඩි වෙනස් තියෙන්නේ භාෂාවක් නිර්මාණය සඳහා විශේෂ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ponavljajoče se slovnice nevronskega omrežja ustvarjajo stavke z uporabo sintakse frazne strukture in delujejo zelo dobro pri razčlenjevanju in modeliranju jezika. Da bi raziskali, ali so generativni modeli odvisnosti podobno učinkoviti, predlagamo dva nova generativna modela sintakse odvisnosti. Oba modela uporabljata ponavljajoče se nevronske mreže, da bi se izognili izrecnim predpostavkam neodvisnosti, vendar se razlikujeta po vrstnem redu, ki se uporablja za gradnjo dreves: eden gradi drevo od spodaj navzgor in drugi od zgoraj navzdol, kar globoko spremeni problem ocenjevanja, s katerim se sooča učenec. Oba modela ocenjujemo na treh tipološko različnih jezikih: angleščini, arabščini in japonščini. Medtem ko oba generativna modela izboljšata učinkovitost razčlenitve v primerjavi z diskriminativno osnovno vrednostjo, sta bistveno manj učinkovita kot nesintaktična jezikovna modela LSTM. Presenetljivo je, da se med naročili gradnje opažajo majhne razlike bodisi za razčlenjevanje ali jezikovno modeliranje.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Shabakadda neurada ee ku soo dhowaaday waxay soo bandhigaan qeybo lagu isticmaalayo kaarista rasmiga ah oo lagu isticmaalo tusaale ahaan baardinta iyo luuqada. Si aan u baarayno in tusaalaha ku xiran ee geneeral-ku-xiran ay si siman u shaqeeyaan, waxaan soo jeedinayaa laba tusaale oo cusub oo dhaqan oo ah cashuurta ku xiran. Labada model waxay isticmaalaan shabakado neurada ah oo soo socda si aan uga dhigo malaabo xorriyad ah, laakiin waxay ku kala duwan yihiin sida loo dhiso geedaha: mid wuxuu dhisaa geedka hoose iyo tan kale hoose, kaas oo si weyn u beddela dhibaatada qiimeynta ee ay cilmiga u jeedo. Waxaannu qiimeynaynaa labada tusaale ee ku qoran saddex luuqadood oo kala duduwan: Ingiriis, Carabi, Jabanees. Inta lagu jiro labada model oo geneeral ah waxay bedeshaa horumarinta baaritaanka si takoorista ah, waxay si badan uga shaqeeyaan noocyada luqada LSTM ee aan la kooban. Si la yaab leh waxaa la soo jeedaa kala duwanaanshaha ku saabsan amarrada dhismaha ama tusaale ahaan baaritaanka ama luuqada.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gramatikat e rrjetit neural të përsëritur gjenerojnë fraza duke përdorur sintaksin e frazës-strukturës dhe funksionojnë shumë mirë në analizimin dhe modelimin e gjuhës. Për të eksploruar nëse modelet e varësisë gjenerative janë njëlloj të efektshëm, ne propozojmë dy modele të reja gjenerative të sintaksit të varësisë. Të dy modelet përdorin rrjete neurale të përsëritura për të shmangur bërjen e supozimeve të shprehura të pavarësisë, por ato ndryshojnë në rendin e përdorur për të ndërtuar pemët: një ndërton pemën poshtë lart dhe tjetrin poshtë lart, gjë që ndryshon thellësisht problem in e vlerësimit që përballet mësuesi. Ne vlerësojmë dy modelet në tre gjuhë tipologjikisht të ndryshme: anglisht, arabisht dhe japonez. Ndërsa të dy modelet gjenerative përmirësojnë analizimin e performancës lidhur me një bazë diskriminuese, ato janë më pak efektive se modelet e gjuhës LSTM jo sintaktike. Megjithatë, dallimi i vogël midis urdhërave të ndërtimit është vëzhguar për analizimin ose modelimin gjuhësor.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Povratni gramari neuralne mreže stvaraju rečenice koristeći sintaks fraze strukture i vrlo dobro izvode na analizu i jezičkom modelima. Da bi istražili da li su generični modeli ovisnosti slično efikasni, predlažemo dva nove generična modela sintaksa ovisnosti. Obje modele koriste rekonstruirane neuralne mreže kako bi izbjegli izražavanje jasnih pretpostavki nezavisnosti, ali se razlikuju u redovima koji se koristi za konstrukciju drveta: jedan izgradi drvo dole i drugi vrh dole, koji duboko mijenja problem procjene sa učenikom. Procjenjujemo dva modela na tri tipološki različita jezika: engleski, arapski i japanski. Iako obe generične modele poboljšavaju analizu učinkovitosti na diskriminacijskoj osnovnoj liniji, značajno su manje efikasni od nesintaktičnih jezičkih modela LSTM-a. Iznenađujuće, malo razlike između zapovijedi o konstrukciji posmatraju se za analizu ili jezičke modele.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Återkommande neurala nätverksgrammatiker genererar meningar med frasstruktur syntax och presterar mycket bra på både tolkning och språkmodellering. För att undersöka om generativa beroendemodeller är lika effektiva föreslår vi två nya generativa beroendemodeller. Båda modellerna använder återkommande neurala nät för att undvika att göra uttryckliga oberoende antaganden, men de skiljer sig åt i den ordning som används för att bygga träden: den ena bygger trädet nedifrån och upp och den andra uppifrån och ner, vilket förändrar skattningsproblemet för eleven. Vi utvärderar de två modellerna på tre typologiskt olika språk: engelska, arabiska och japanska. Även om båda generativa modeller förbättrar tolkningens prestanda jämfört med en diskriminerande baslinje, är de betydligt mindre effektiva än icke-syntaktiska LSTM språkmodeller. Överraskande nog observeras liten skillnad mellan byggorder för antingen tolkning eller språkmodellering.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Watumiaji wa mtandao wa neura unaoendelea wanatengeneza sentensi kwa kutumia mfumo wa mifumo na kutekeleza vizuri kwenye mifano ya wimbo na lugha. Kuchunguza kama modeli za kutegemea kwa ajili ya jenerali zinaweza kuwa na ufanisi, tunapendekeza mifano miwili mpya ya mfumo wa usajili wa kujitegemea. Viwili viwili vinatumia mtandao wa neura unaoendelea ili kuepuka kudhania uhuru wa wazi, lakini wanatofautiana na amri ya kujenga mti: moja hujenga mti wa chini na mwingine wa juu, ambalo hubadilisha tatizo la kadirino lililokabiliwa na mwanafunzi. Tunatathmini mifano miwili kwenye lugha tatu kwa kawaida tofauti: Kiingereza, Kiarabu na Kijapani. Wakati mifano yote ya kizazi inaboresha utendaji wa wimbo wa wimbo wa msingi wa kibaguzi, wanakuwa na ufanisi kidogo kuliko mifano ya lugha ya LSTM isiyo na ushirikiano. Inashangaza, tofauti kidogo kati ya amri za ujenzi huonekana kwa ajili ya kuuza au kutengeneza mifano ya lugha.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>தற்போதும் புதிய பிணைய பிணையத்தின் குறிப்புகள் சொற்றொடர்- structure syntax பயன்படுத்தி வாக்கியங்களை உருவாக்குகிறது மற்றும் பாடல பொதுவான சார்பு மாதிரிகள் அதே போல் வெளிப்படையாக இருக்குமா என்று கண்டறிய, சார்ந்த சார்ந்த ஒத்திசைவு வரிசையின இரண்டு மாதிரிகளும் திரும்ப புதிய புதிய வலைப்பின்னல்களை பயன்படுத்தி வெளிப்படையான சுதந்திரத்தை உருவாக்குவதற்கு தவிர்க்க, ஆனால் அவை வித்தியாசமாக இருக்கிறது, மரங்களை உர மூன்று வழக்கமான வித்தியாசமான மொழிகளில் இரண்டு மாதிரிகளை மதிப்பிடுகிறோம்: ஆங்கிலம், அரேபி, ஜப்பானிய இரண்டு பொதுவான மாதிரிகளும் ஒரு வித்தியாசமான அடிப்படைக்கோட்டில் பாடல் செயல்பாட்டை மேம்படுத்தும் போது, அவை முக்கியமாக ஒத்த ஆச்சரியமாக, கட்டும் கட்டளைகளுக்கிடையில் உள்ள சிறிய வேறுபாடு பாடல் அல்லது மொழி மாதிரியாக்கத்திற்கா</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Hata bellenilýär Döredijili bağlılık nusgalarynyň meňzeş şekilde etkisi bardygyny keşfetmek üçin, biz iki täze döredijili baglanylyk syntaksiniň täze nusgasyny teklif edip görýäris. Iki modeller ýene-täsirli neural şebekelerini açıklamak üçin üýtgetmek üçin ulanýarlar, ýöne agaçlary in şamak üçin üýtgeşirilýärler: biri agaç aşagyny we üstüni aşagyny inşaýar. Bu adam öwreniň gözlenen hasaplamada meseläni derinden üýtgedir. Biz iki nusga üç tipologik görnüşli dilde deňleýäris: Iňlisçe, Arapça we Japonça. Iki döredijili nusgalar diskriminçy baserden çykyş etmegini gowlaşdyryp bilýärler. Olar sintaktik LSTM dil nusgalaryndan has iň az täsirli. Gurama bilen inşaat düzenleriniñ arasyndaky kynçylyk üýtgeşigi ýa-da dil modellendirmesi üçin gözlenýär.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>دوبارہ نیورال نیٹ ورک گرامر فریز-ساختر سینٹکس کے مطابق عبارت پیدا کر رہے ہیں اور دونوں پارسینگ اور زبان موڈلینگ پر بہت اچھی طرح عمل کرتے ہیں. ان کے لئے تحقیق کرنا چاہیے کہ جنرائٹیوں اعتمادی موڈل برابر اثرات ہیں، ہم دو نو جنرائٹیوں موڈل ڈال دیتے ہیں۔ دونوں مدلکوں دوبارہ نیورال نیٹوں کو استعمال کرتے ہیں کہ صریح آزادی حدس سے منع کریں، لیکن وہ درختوں کو بنانے کے لئے استعمال کرنے کے لئے اختلاف کرتے ہیں: ایک درخت کے نیچے اوپر اور دوسرے نیچے نیچے نیٹوں کو بناتا ہے، جو علم والے کے سامنے مواجہ ہونے کے مسئلہ کو عمیق طور پر تغییر دیتا ہے. ہم نے تین ٹیپٹولوژیکی مختلف زبانوں پر دو نمڈل کا ارزش کیا: انگلیسی، عربی اور جاپانی. حالانکہ دونوں جنرائیٹ موڈلز ایک جدائی بیس لین پر پارسینٹ کی فعالیت کو بہتر کر رہے ہیں، وہ غیر سینٹکتیک LSTM زبان موڈلز سے بہت کم فائدہ ہیں۔ تعجب ہے کہ ساختاری دستور کے درمیان بہت کم تفاوت ہے یا پارسینگ یا زبان موڈلینگ کے لئے.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>@ info: whatsthis Umumiy ishlatuvchi modellari shunday ishlayotganligini aniqlash uchun biz ishlatadigan ikkita yangi generativ modellarini tasavvur qilamiz. Ikkita modellar davom etilgan tarmoq tarmoqlaridan foydalanadi, lekin ular daraxtni quyish uchun ajratilgan tarmoqni ajratib turadi: biri daraxtni pastga va boshqa yuqoriga yaratadi, bu ta'lim qo'yilgan qiymatni o'zgartiradi. Biz ikkita modelni o'qiymatimiz, har xil turli tillarda: Ingliz, араб, Япония. Bu ikkita generativ modellar taqdimlik asboblar tarkibida parsing natijasini bajaradi. Ular LSTM tili modellaridan juda katta ishlaydi. Aniqlik, quyidagi buyruqlar orasidagi bir ajratish yoki tillar modeli uchun ko'ra koʻrinadi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Các tạp chí mạng thần kinh liên tục tạo ra câu, sử dụng cấu trúc cụm từ, và thực hiện rất tốt trong việc phân tách và tạo mẫu ngôn ngữ. Để tìm hiểu xem các mô hình phụ thuộc tạo hóa có hiệu quả tương tự không, chúng tôi đề xuất hai mô hình gây tác động mới của cú chạm phụ thuộc. Cả hai mẫu dùng lưới thần kinh thường xuyên để tránh những giả định độc lập rõ ràng, nhưng chúng khác nhau trong trật tự được dùng để xây dựng cây: một người xây dựng cây ở trên và một người ở trên xuống, điều đó thay đổi sâu sắc vấn đề ước tính đối với học sinh. Chúng tôi đánh giá hai mẫu về ba loại ngôn ngữ khác nhau: Anh, Á Rập và Nhật. Trong khi cả hai mô- đun gây tác động cải thiện hiệu quả hơn so với mô hình ngôn ngữ LSD không cú pháp. Ngạc nhiên là, có rất ít sự khác biệt giữa các mệnh lệnh xây dựng cho việc phân tách hoặc tạo mẫu ngôn ngữ.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>递归神经网络语法以短语构语法成句,解析言建模善。 求生成与不同效,吾二新语法成模样。 二模皆用递归神经网络以避明独立性之设,然其次序不同:一自下而上,一自上而下,深变学者之虑也。 吾以三类型学异言质之:英语、阿拉伯语、日语。 虽二者成模基线皆增解析性,然其效明下非句法 LSTM 言。 令人讶之,解析语建模,工订单之异小也。</span></div></div><dl><dt>Anthology ID:</dt><dd>K19-1022</dd><dt>Volume:</dt><dd><a href=/volumes/K19-1/>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Hong Kong, China</dd><dt>Venue:</dt><dd><a href=/venues/conll/>CoNLL</a></dd><dt>SIG:</dt><dd><a href=/sigs/signll/>SIGNLL</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>227–237</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/K19-1022>https://aclanthology.org/K19-1022</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/K19-1022 title="To the current version of the paper by DOI">10.18653/v1/K19-1022</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">matthews-etal-2019-comparing</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Austin Matthews, Graham Neubig, and Chris Dyer. 2019. <a href=https://aclanthology.org/K19-1022>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a>. In <i>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</i>, pages 227–237, Hong Kong, China. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/K19-1022>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a> (Matthews et al., CoNLL 2019)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/K19-1022.pdf>https://aclanthology.org/K19-1022.pdf</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/K19-1022.pdf title="Open PDF of 'Comparing Top-Down and Bottom-Up Neural Generative Dependency Models'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Comparing+Top-Down+and+Bottom-Up+Neural+Generative+Dependency+Models" title="Search for 'Comparing Top-Down and Bottom-Up Neural Generative Dependency Models' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Comparing Top-Down and Bottom-Up Neural Generative Dependency Models'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Comparing Top-Down and Bottom-Up Neural Generative Dependency Models](https://aclanthology.org/K19-1022) (Matthews et al., CoNLL 2019)</p><ul class=mt-2><li><a href=https://aclanthology.org/K19-1022>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a> (Matthews et al., CoNLL 2019)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Austin Matthews, Graham Neubig, and Chris Dyer. 2019. <a href=https://aclanthology.org/K19-1022>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</a>. In <i>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</i>, pages 227–237, Hong Kong, China. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>