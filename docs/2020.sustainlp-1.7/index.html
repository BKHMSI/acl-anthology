<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Efficient Inference For Neural Machine Translation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Efficient Inference For Neural Machine Translation" name=citation_title><meta content="Yi-Te Hsu" name=citation_author><meta content="Sarthak Garg" name=citation_author><meta content="Yi-Hsiu Liao" name=citation_author><meta content="Ilya Chatsviorkin" name=citation_author><meta content="Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing" name=citation_conference_title><meta content="2020/11" name=citation_publication_date><meta content="https://aclanthology.org/2020.sustainlp-1.7.pdf" name=citation_pdf_url><meta content="48" name=citation_firstpage><meta content="53" name=citation_lastpage><meta content="10.18653/v1/2020.sustainlp-1.7" name=citation_doi><meta property="og:title" content="Efficient Inference For Neural Machine Translation"><meta property="og:image" content="https://aclanthology.org/thumb/2020.sustainlp-1.7.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.sustainlp-1.7"><meta property="og:description" content="Yi-Te Hsu, Sarthak Garg, Yi-Hsiu Liao, Ilya Chatsviorkin. Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing. 2020."><link rel=canonical href=https://aclanthology.org/2020.sustainlp-1.7></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Efficient Inference For Neural Machine Translation</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Name</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Efficient Inference For Neural Machine Translation</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>الاستدلال الفعال لترجمة الآلة العصبية</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>NĂ¶ral maĹźÄ±na Ă§evirilmÉ™si ĂĽĂ§ĂĽn ehtiyacÄ±</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Ефективно заключение за неврален машинен превод</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>নিউরাল মেশিন অনুবাদের জন্য কার্যকর ইনফেরেন্স</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>ནུས་ཡོད་པའི་མེ་འཁོར་གྱི་ལག་འཁྱེར་ལ་བསྒྱུར་ནུས་ཡོད་པ</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Učinjena šteta za neuronski prevod mašine</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferència eficient per a la traducció de màquines neurones</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Efektivní závěr pro neuronový strojový překlad</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Effektiv inferens til neural maskinoversættelse</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Effiziente Schlussfolgerung für neuronale maschinelle Übersetzung</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Αποτελεσματικό συμπέρασμα για τη νευρωνική μηχανική μετάφραση</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferencia eficiente para la traducción automática neuronal</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Tõhus järeldus neuroaalse masintõlke jaoks</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>تفاوت فعالی برای ترجمه ماشین عصبی</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Tehokas pĂ¤Ă¤telmĂ¤ hermojen konekĂ¤Ă¤nnĂ¶kselle</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inférence efficace pour la traduction automatique neuronale</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Tátal Éifeachtach d'Aistriúchán Meaisín Néarach</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>@ action</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>אינפרנסה יעילה לתרגום מכונת נוירולית</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>तंत्रिका मशीन अनुवाद के लिए कुशल अनुमान</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Učinjena šteta za neuronski prevod strojeva</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Hatékony fertőzés a neurális gépi fordításhoz</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Նյարդային մեքենայի թարգմանման արդյունավետ ինֆերանսը</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferensi Efisien untuk Translation Mesin Neural</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferenza efficiente per la traduzione automatica neurale</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>神経機械翻訳のための効率的な推論</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>layer-mode-effects</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Name</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Нейрондық машинаны аудару үшін әсер етілген қасиеттер</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>신경 기계 번역 중의 효율적인 추리</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Veiksmingas nervinių mašinų vertimas</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Ефикасна инференција за превод на неврална машина</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>നെയുറല്‍ മെഷീന്‍ പരിഭാഷപ്പെടുത്തുന്നതിനുള്ള പ്രയോജനപ്പെടുത്തല്‍</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Сэтгэл машины хөрөнгө оруулалтын үр дүнтэй нөлөөлдөг</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferensi Efisien untuk Terjemahan Mesin Neural</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Efficient Inference For Neural Machine Translation</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Efficiënte Inferentie voor Neurale Machine Translation</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Effektiv forskjell for neuralmaskinsomsetjing</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Skuteczne wnioski dla neuronowego tłumaczenia maszynowego</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferência eficiente para tradução automática neural</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferență eficientă pentru traducerea mașinii neurale</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Эффективный вывод для нейронного машинного перевода</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>න්‍යූරල් මැෂින් පරිවර්තනය වෙනුවෙන් ප්‍රශ්ණ විශ්වාස කරන්න</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Učinkovita ugotovitev za strojno prevajanje nevronov</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Efficient Inference For Neural machine Translation</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Inferencë Efikase për Translacionin e Makinës Neurale</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Efektan pogodak za neuronski prevod mašine</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Effektiv inferens för neural maskinöversättning</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Tafsiri yenye ufanisi</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>புதிய இயந்திரத்தின் மொழிபெயர்ப்புக்கான விளைவு</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Neural Maşynyň terjimesine ýeterlik ýeterlik</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>نیورال ماشین ترجمہ کے لئے اثر انفارنس</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Tarjima qilish</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>Sự liên hệ hiệu quả cho phiên dịch máy thần kinh</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.sustainlp-1.7.pdf>神经机器翻译高效推理</a></h2><p class=lead><a href=/people/y/yi-te-hsu/>Yi-Te Hsu</a>,
<a href=/people/s/sarthak-garg/>Sarthak Garg</a>,
<a href=/people/y/yi-hsiu-liao/>Yi-Hsiu Liao</a>,
<a href=/people/i/ilya-chatsviorkin/>Ilya Chatsviorkin</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Large Transformer models have achieved state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speed</a> without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109 % and 84 % speedup on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a> and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> respectively and reduce the number of parameters by 25 % while maintaining the same translation quality in terms of BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Name In hierdie werk, ons soek na die optimale kombinasie van bekende teknike om inferensie spoed te optimaliseer sonder om vertaling kwaliteit te offer. Ons doen 'n empiriese studie wat verskeie toegange stap en wys dat kombinasie van vervanging van dekoder self-aandag met eenvoudige herhaalde eenhede, Die aanvaar van 'n diep enkoder en 'n skaal dekoder-arkitektuur en meer-kop aanmerking kan tot 109% en 84% speeduig op CPU en GPU aanvaar en die nommer van parameters met 25% verduur terwyl die selfde vertaling-kwaliteit in terms van BLEU onderhou.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ትልቅ ትርጉም ሚድልቶች የ-የ-አርእስት ግንኙነትን አግኝተዋል እና በሜዳው የተመሳሳይ ሆኖአል፡፡ በዚህ ስራ፣ የተታወቀ ስህተት ማቀናጃ ጥቅም ሳይያሳርፍ ፍጥረትን ማሻሻል እናስፈልጋለን፡፡ የተለየ ልዩ ልቦችን የሚቆርጥ እና የድምፅ አካባቢ ተቃውሞ የሚለውጥ የራሱን ትኩረት በመለስ እናሳያልን፡፡ የጥልቅ ኮድ እና የጥቁር አካባቢ መሠረት እና የብዙራዊ አካባቢ ጉዳይ ጉዳይ በCPU እና GPU ላይ አቅራቢያ 109 በመቶ እና 84 በመቶው ይደርሳል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>حققت نماذج المحولات الكبيرة أحدث النتائج في الترجمة الآلية العصبية وأصبحت قياسية في هذا المجال. في هذا العمل ، نبحث عن مزيج مثالي من الأساليب المعروفة لتحسين سرعة الاستدلال دون التضحية بجودة الترجمة. نجري دراسة تجريبية تجمع بين الأساليب المختلفة وتوضح أن الجمع بين استبدال الاهتمام الذاتي لوحدة فك التشفير بوحدات متكررة مبسطة ، واعتماد مشفر عميق وبنية وحدة فك ترميز ضحلة وتقليم الانتباه متعدد الرؤوس يمكن أن يحقق ما يصل إلى 109٪ و 84٪ تسريع على CPU و GPU على التوالي وتقليل عدد المعلمات بنسبة 25٪ مع الحفاظ على نفس جودة الترجمة من حيث BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Büyük Transformer modelləri nöral maşına çevirilməsi ilə mümkün olduğu və sahədə standart oldular. Bu işdə, biz bilinmiş tekniklərin optimal kombinatsiyasını istəyirik ki, dəyişiklik sürətini qurbanlıq etmədən optimizləsin. Biz müxtəlif yaxınlıqları birləşdirən empirik təhsil etdik və dekoderin özünü təhsil etməsini basit təhsil edilən biriklərlə dəyişdiririk. Dərzini kodlayıcı və çətinli dekoder arhitektarını və çoxlu başlıqların gözləməsi CPU və GPU ilə müqayisədə 109%-ə və 84%-ə hızlandıra bilər və aynı tercümə keyfiyyətini BLEU ilə qoruyarkən parametru sayını 25%-ə əskilə bilər.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Големите трансформаторни модели са постигнали най-съвременни резултати в невронния машинен превод и са станали стандарт в областта. В тази работа търсим оптималната комбинация от известни техники за оптимизиране на скоростта на заключение, без да жертваме качеството на превода. Извършваме емпирично проучване, което подрежда различни подходи и демонстрира, че комбинацията от заместване на декодерното самовнимание с опростени повтарящи се единици, приемането на дълбок кодер и повърхностна декодерна архитектура и подрязването на вниманието с няколко глави може да постигне до 109% и 84% ускорение съответно на процесора и графичния процесор и да намали броя на параметрите с 25%, като същевременно поддържа същото качество на превода по отношение на Блеу.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. এই কাজে আমরা পরিচিত প্রযুক্তির সম্মিলনের অপেক্ষা করছি অনুবাদের মান ছাড়া ইনভেন্সের গতি বৃদ্ধি করার জন্য। আমরা একটি সম্মানিত গবেষণা করি যা বিভিন্ন উপায় স্থাপন করে এবং প্রদর্শন করে যে সুস্পষ্ট পুনরাবর্তন ইউনিটের সাথে নিজেকে আত্মমনোযোগ প্রতিস একটি গভীর এনকোডার গ্রহণ করা এবং একটি ধুলো কোডার আর্কিডেক্টার এবং বহুমাথার মনোযোগ প্রদান করা যায়, সিপিউ এবং জিপিউ-তে প্রায় ১০৯% এবং ৮৪% বেড়ে যাবে এবং বিলিউ এর মাধ্যমে একই অনুবাদের মান</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>རྩིས་པ་ཆེ་བའི་དབྱིབས་བཟོ་བྱེད་མ་དབྱིབས་སྣང་བའི་གནས་སྟངས་དང་མཐུན་རྐྱེན་གྱིས འོན་ཀྱང་། ང་ཚོས་གནས་ཚུལ་འདིའི་ནང་དུ་ཆེས་ཤུགས་ཀྱི་མཉམ་དུ་མཐུན་རྐྱེན་ཚད་མེད་སྤྲོད་ཀྱི་མཚམས་མཐུན་དང་། ང་ཚོས་རང་ཉིད་ཀྱི་གནད་དོན་དག་གི་ཐབས་ལམ་མ་འདྲ་བརྩལ་བ་ཞིག་བྱེད་ཀྱི་རྩོལ་ཞིག་བྱས་ནས། - adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Veliki modeli transformera postigli su rezultate umjetnosti u prevodu neuralnih strojeva i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Provodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevoda u smislu BLEU-a.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. En aquest treball busquem la combinació optima de tècniques conegudes per optimitzar la velocitat de inferència sense sacrificar la qualitat de traducció. Realitzem un estudi empíric que agrupa diversos enfocaments i demostra que la combinació de substituir l'autoatenció del decodificador per unitats recurrents simplificades, L'adopció d'un codificador profund i d'una arquitectura de decodificació superficial i una pruning d'atenció multicapa poden aconseguir fins al 109% i l'84% de velocitat en CPU i GPU respectivament i reduir el nombre de paràmetres un 25% mantenint la mateixa qualitat de traducció en termes de BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modely velkých transformátorů dosáhly nejmodernějších výsledků v oblasti neuronového strojového překladu a staly se standardem v oboru. V této práci hledáme optimální kombinaci známých technik pro optimalizaci rychlosti inference bez snížení kvality překladu. Provádíme empirickou studii, která shromažďuje různé přístupy a demonstruje, že kombinace nahrazení sebepozornosti dekodéru zjednodušenými opakujícími se jednotkami, Přijetím hlubokého snímače a mělké architektury dekodéru a vícehlavového prořezávání pozornosti lze dosáhnout až 109% a 84% urychlení procesoru a GPU a snížit počet parametrů o 25% při zachování stejné kvality překladu z hlediska BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Store Transformermodeller har opnået state-of-the-art resultater inden for neural maskinoversættelse og er blevet standard på området. I dette arbejde leder vi efter den optimale kombination af kendte teknikker til at optimere inferencehastigheden uden at gå på kompromis med oversættelseskvaliteten. Vi gennemfører en empirisk undersøgelse, der stabler forskellige tilgange og demonstrerer, at kombinationen af at erstatte dekoder selvopmærksomhed med forenklede tilbagevendende enheder, Vedtagelse af en dyb encoder og en lavvandet dekoderkarkitektur og opmærksomhedsbeskæring med flere hoveder kan opnå op til 109% og 84% hastighed på henholdsvis CPU og GPU og reducere antallet af parametre med 25% samtidig med at den samme oversættelseskvalitet med hensyn til BLEU opretholdes.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Große Transformatormodelle haben modernste Ergebnisse in der neuronalen maschinellen Übersetzung erzielt und sind in diesem Bereich Standard geworden. In dieser Arbeit suchen wir nach der optimalen Kombination bekannter Techniken, um die Inferenzgeschwindigkeit zu optimieren, ohne die Übersetzungsqualität zu beeinträchtigen. Wir führen eine empirische Studie durch, die verschiedene Ansätze stapelt und zeigt, dass die Kombination aus dem Ersetzen der Selbstaufmerksamkeit des Decoders durch vereinfachte wiederkehrende Einheiten, Durch die Verwendung eines tiefen Encoders und einer flachen Decoderarchitektur und des Aufmerksamkeitsschnitts mit mehreren Köpfen kann eine Beschleunigung von bis zu 109% bzw. 84% auf CPU und GPU erreicht und die Anzahl der Parameter um 25% reduziert werden, während die Übersetzungsqualität in Bezug auf BLEU beibehalten wird.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Τα μεγάλα μοντέλα μετασχηματιστών έχουν επιτύχει αποτελέσματα τελευταίας τεχνολογίας στη νευρωνική μηχανική μετάφραση και έχουν γίνει πρότυπο στον τομέα. Σε αυτή την εργασία, αναζητούμε τον βέλτιστο συνδυασμό γνωστών τεχνικών για τη βελτιστοποίηση της ταχύτητας συμπερασμάτων χωρίς να θυσιάζουμε την ποιότητα της μετάφρασης. Διεξάγουμε μια εμπειρική μελέτη που συσσωρεύει διάφορες προσεγγίσεις και αποδεικνύει ότι ο συνδυασμός αντικατάστασης της αυτοπροσοχής του αποκωδικοποιητή με απλοποιημένες επαναλαμβανόμενες μονάδες, υιοθετώντας έναν βαθύ κωδικοποιητή και μια ρηχή αρχιτεκτονική αποκωδικοποιητή και κλαδέματος προσοχής πολλαπλών κεφαλών μπορούν να επιτύχουν έως 109% και 84% επιτάχυνση στην ΚΜΕ και τη GPU αντίστοιχα και να μειώσουν τον αριθμό των παραμέτρων κατά 25% διατηρώντας την ίδια ποιότητα μετάφρασης όσον αφορά την BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Los modelos Transformer de gran tamaño han logrado resultados de vanguardia en la traducción automática neuronal y se han convertido en un estándar en el campo. En este trabajo, buscamos la combinación óptima de técnicas conocidas para optimizar la velocidad de inferencia sin sacrificar la calidad de la traducción. Llevamos a cabo un estudio empírico que combina varios enfoques y demuestra que la combinación de reemplazar la autoatención del decodificador con unidades recurrentes simplificadas, la adopción de un codificador profundo y una arquitectura de decodificador poco profunda y la reducción de atención de múltiples cabezales puede lograr una aceleración de hasta el 109% y el 84% en CPU y GPU respectivamente, y reducir el número de parámetros en un 25% manteniendo la misma calidad de traducción en términos de BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Suurte transformaatorite mudelid on saavutanud tipptasemel tulemusi neuromasintõlkes ja muutunud valdkonnas standardiks. Selles töös otsime teadaolevate tehnikate optimaalset kombinatsiooni järelduste kiiruse optimeerimiseks ilma tõlkekvaliteeti ohverdamata. Viime läbi empiirilise uuringu, mis koondab erinevaid lähenemisviise ja näitab, et dekooderi enesetähelepanu asendamise kombinatsioon lihtsustatud korduvate üksustega, Sügava kodeerija ja madala dekooderi arhitektuuri kasutuselevõtmine ning mitme peaga tähelepanu vähendamine võib saavutada protsessori ja graafikaprotsessori kiiruse vastavalt 109% ja 84% ning vähendada parameetrite arvu 25% võrra, säilitades samal ajal BLEU-s sama tõlkekvaliteedi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>مدل‌های تغییر‌پذیر بزرگ به حالت هنر نتیجه‌های تغییر‌پذیر ماشین عصبی رسیده‌اند و در زمینه استاندارد شده‌اند. در این کار، ما دنبال ترکیب بهترین تکنیک‌های شناخته می‌گردیم تا سرعت آلودگی را بدون قربانی کیفیت ترکیب بهترین کنیم. ما یک مطالعه امپراتیک را انجام می دهیم که تقریبا مختلف را جمع می کند و نشان می دهد که ترکیب توجه خود را با واحدهای ساده تکرار می دهد، با پذیرفتن یک کودهر عمیق و یک معماری دکوردر عمیق و حفظ توجه بسیاری از سرها می تواند تا 109 درصد و 84 درصد سرعت بر CPU و GPU را به طور مستقل رسید و تعداد پارامتر را به 25 درصد کاهش دهد در حالی که با حفظ یک کیفیت ترجمه را به عنوان BLEU نگه می دارد.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Suuret muuntajamallit ovat saavuttaneet huippuluokan tuloksia neurokonekäännöksessä ja niistä on tullut alan standardi. Tässä työssä etsimme tunnettujen tekniikoiden optimaalista yhdistelmää päättelynopeuden optimoimiseksi kääntämisen laadusta tinkimättä. Teemme empiirisen tutkimuksen, joka pinoaa erilaisia lähestymistapoja ja osoittaa, että yhdistelmä dekooderin itsetunnon korvaaminen yksinkertaistetuilla toistuvilla yksiköillä, Syväkooderin ja matalan dekooderiarkkitehtuurin käyttöönotto ja monipäinen huomioleikkaus voivat nopeuttaa suoritinta 109% ja grafiikkasuoritinta 84% ja vähentää parametrien määrää 25% säilyttäen samalla BLEU:n käännöslaadun.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Les modèles de grands transformateurs ont obtenu des résultats de pointe en matière de traduction automatique neuronale et sont devenus la norme dans le domaine. Dans ce travail, nous recherchons la combinaison optimale de techniques connues pour optimiser la vitesse d'inférence sans sacrifier la qualité de la traduction. Nous menons une étude empirique qui empile différentes approches et démontre que la combinaison du remplacement de l'attention personnelle du décodeur par des unités récurrentes simplifiées, l'adoption d'un encodeur profond et d'une architecture de décodeur superficielle et l'élagage de l'attention multi-têtes peut atteindre une accélération allant jusqu'à 109 % et 84 % sur le processeur et le GPU respectivement et de réduire le nombre de paramètres de 25 % tout en maintenant la même qualité de traduction en termes d'UEBL.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tá torthaí úrscothacha bainte amach ag samhlacha Trasfhoirmeoirí Móra san aistriúchán meaisín néarach agus tá siad tar éis éirí caighdeánach sa réimse. Sa saothar seo, féachaimid don chomhcheangal is fearr de theicnící aitheanta chun an luas tátail a bharrfheabhsú gan caighdeán an aistriúcháin a íobairt. Déanaimid staidéar eimpíreach a chruann cineálacha cur chuige éagsúla agus a thaispeánann gur féidir suas le 109% agus 84% luas suas le 109% agus 84% a bhaint amach le hionadú féin-aird an díchódóra le haonaid athfhillteacha simplithe, trí ionchódóir domhain agus ailtireacht díchódóra éadomhain a ghlacadh. LAP agus GPU faoi seach agus laghdaítear líon na bparaiméadar faoi 25% agus an caighdeán aistriúcháin céanna á choinneáil i dtéarmaí BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>@ action: button Daga wannan aikin, munã dãkin komai mai amfani da shiryoyin ayuka da aka sani wajen kwamfyuta saukarwa na kasancẽwa idan ba da tsarin fassarar ta ba. Tuna sami wani littãfi na tamkar da ke samun hanyõyi dabam-daban kuma ke nuna cewa da za'a bada sauri-rayi kanaga-raye da sunayen da aka sauce, @ info: whatsthis</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>דוגמנים גדולים של טרנספורר השיגו תוצאות חדשות בתרגום מכונות עצביות והפכו לסטנדרטים בשטח. בעבודה הזו, אנו מחפשים שילוב אופטימלי של טכניקות ידועות כדי לאופטימיזם מהירות המסקנה בלי להקריב איכות התרגום. אנו מבצעים מחקר אמפירי שמעריך גישות שונות ומוכיח שילוב של החלפת תשומת לב עצמית של מפענח עם יחידות חדשות פשוטות, באמצעות קודד עמוק וארכיטקטורת קודד גבוהה ומעטפת תשומת לב רב-ראשית יכולה להשיג עד 109% ו-84% מהירות על CPU ו-GPU בהתאם ולפחות את מספר הפרמטרים ב-25% בזמן לשמור על אותו איכות התרגום במונחים של BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>बड़े ट्रांसफॉर्मर मॉडल ने तंत्रिका मशीन अनुवाद में अत्याधुनिक परिणाम प्राप्त किए हैं और क्षेत्र में मानक बन गए हैं। इस काम में, हम अनुवाद की गुणवत्ता का त्याग किए बिना अनुमान की गति को अनुकूलित करने के लिए ज्ञात तकनीकों के इष्टतम संयोजन की तलाश करते हैं। हम एक अनुभवजन्य अध्ययन करते हैं जो विभिन्न दृष्टिकोणों को ढेर करता है और यह दर्शाता है कि सरलीकृत आवर्तक इकाइयों के साथ डिकोडर आत्म-ध्यान को बदलने का संयोजन, एक गहरी एन्कोडर और एक उथले डिकोडर आर्किटेक्चर और मल्टी-हेड ध्यान प्रूनिंग को अपनाने से सीपीयू और जीपीयू पर क्रमशः 109% और 84% स्पीडअप प्राप्त हो सकता है और BLEU के संदर्भ में एक ही अनुवाद गुणवत्ता को बनाए रखते हुए पैरामीटर की संख्या को 25% तक कम किया जा सकता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Veliki modeli transformera postigli su rezultate umjetnosti u prevodu neuralnih strojeva i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Provodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevoda u smislu BLEU-a.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A nagy transzformátor modellek korszerű eredményeket értek el a neurális gépi fordításban, és szabványossá váltak a területen. Ebben a munkában az ismert technikák optimális kombinációját keressük, hogy optimalizáljuk a következtetési sebességet anélkül, hogy feláldoznánk a fordítási minőséget. Egy empirikus tanulmányt végzünk, amely különböző megközelítéseket halmoz össze, és bebizonyítja, hogy a dekóder önfigyelem helyettesítése egyszerűsített visszatérő egységekkel, A mélykódoló és a sekély dekódoló architektúra alkalmazása, valamint a többfejű figyelem levágása akár 109%-os, illetve 84%-os gyorsulást érhet el a CPU és a GPU esetében, és 25%-kal csökkentheti a paraméterek számát, miközben ugyanazon fordítási minőséget tart fenn a BLEU tekintetében.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Մեծ տրանֆերմերների մոդելները հասել են նորագույն արդյունքներին նյարդային մեքենայի թարգմանման մեջ և դառնում են դաշտում ստանդարտ: Այս աշխատանքում մենք փնտրում ենք հայտնի մեթոդների օպտիմալ համադրումը, որպեսզի օպտիմացվի եզրակացության արագությունը առանց թարգմանման որակի զոհաբերելու: We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Խելամիտ կոդերի, մակերեսային կոդերի ճարտարապետության և բազմագլխավոր ուշադրության կրճատման ընդունելը կարող է հասնել մինչև 109 և 84 տոկոսի արագություն համակարգչային համակարգի և GPU-ի վրա և նվազեցնել պարամետրերի թիվը 25 տոկոսով, մինչդեռ պահպանել նույն թարգմանման որակ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Model Transformer Besar telah mencapai hasil terbaik dalam terjemahan mesin saraf dan telah menjadi standar di lapangan. Dalam pekerjaan ini, kita mencari kombinasi optimal dari teknik yang dikenal untuk optimisasi kecepatan inferensi tanpa mengorbankan kualitas terjemahan. Kami melakukan sebuah studi empiris yang mengumpulkan berbagai pendekatan dan menunjukkan bahwa kombinasi menggantikan perhatian diri dekoder dengan unit rekuren sederhana, Mengadopsi koder dalam dan arsitektur dekoder rendah dan pemotongan perhatian multi-kepala dapat mencapai sampai 109% dan 84% speedup pada CPU dan GPU secara respektif dan mengurangi jumlah parameter dengan 25% sementara mempertahankan kualitas terjemahan yang sama dalam terma BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I modelli Large Transformer hanno raggiunto risultati all'avanguardia nella traduzione automatica neurale e sono diventati standard nel campo. In questo lavoro, cerchiamo la combinazione ottimale di tecniche conosciute per ottimizzare la velocità di inferenza senza sacrificare la qualità della traduzione. Conduciamo uno studio empirico che impila vari approcci e dimostra che la combinazione di sostituire l'auto-attenzione decodificatore con unità ricorrenti semplificate, L'adozione di un encoder profondo e di un'architettura decodificatore poco profonda e la potatura dell'attenzione multi-testa può raggiungere una velocità fino al 109% e all'84% rispettivamente su CPU e GPU e ridurre il numero di parametri del 25% mantenendo la stessa qualità di traduzione in termini di BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>大型変圧器モデルは、ニューラル機械翻訳で最先端の結果を達成し、この分野で標準となっています。この研究では、翻訳品質を犠牲にすることなく推論速度を最適化するために、既知の技術の最適な組み合わせを探します。様々なアプローチを積み重ね、デコーダの自己注目を簡略化されたリカレントユニットに置き換え、深いエンコーダと浅いデコーダアーキテクチャを採用し、マルチヘッド注目の枝刈りを行うことで、CPUとGPUでそれぞれ最大109 ％と84 ％のスピードアップを達成し、BLEUの点で同じ翻訳品質を維持しながらパラメータの数を25 ％削減できることを実証する実証研究を行っています。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>string" in "context_BAR_stringLink Nang barêng-barêng iki, kita sampeyan kanggo ngerasakno ampliwat karo teknik sing berarti ujian kanggo ngerasakno luwih apik, lan akeh nyong ngerasakno. Awak dhéwé éntuk éntuk empir sing nggawe barang nggawe gerakan sampeyan karo ngono nggambar perusahaan winih dhéwé kuwi nggawe gerakan kelas perusahaan karo perusahaan sugih deep</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>დიდი ტრანფორმენტერის მოდელები გავაკეთეთ სიცოცხლის შედეგები ნეიროლური მაქინის გაგრძელებაში და გავაკეთეთ სტანდარტულება. ამ სამუშაოში, ჩვენ ძირებთ უცნობილი ტექნოგიების ოპტიმალური კომბინეცია, რომ ინფრენციის სიჩქარე ოპტიმიზრებად დავიწყებთ, უცნობიერებელი გა ჩვენ ემპერიკალური სწავლობას, რომელიც განსხვავებული მიზეზების შესაძლებლობა დავწყება და გამოჩვენება, რომ სწორედ განსხვავებული განსხვავებული განსხვავებული განსხვავებული ერთეზ შეიძლება გავაკეთოთ დიბოლო კოდერი და დიბოლო კოდერის აქტიქტურაცია და მრავალთან დაახლოების აღმოჩენა 109% და 84% სიჩქარე CPU და GPU-ზე და 25% პარამეტრის რაოდენობას დაახლოებით, როცა BLEU-ის განმავლობაში იგივე გადაწყვეტილების</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Үлкен түрлендіру үлгілері невралдық компьютердің аудармасының күйін жеткізді және өрісте стандартты болды. Бұл жұмыста біз белгілі техникалардың оптималдығын іздейміз, аудармалардың сапасын көмектесу үшін инференциялық жылдамдығын оптимизациялау үшін. Біз әртүрлі жағдайларды топтастырып, декодтардың өзіне қайталанатын бірліктермен ауыстыруын көрсетеді. Тіпті кодерді және көпшілік декодер архитектурасын қолдану және көпшілік басып тұрғысын қолдану мүмкін процессорды және GPU арқылы 109% және 84% жылдамдығына жеткізе алады және бір аудармалы сапатты BLEU арқылы қалаған параметрлердің санын 25% деп аза</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>대형 변압기 모형은 신경기계 번역 분야에서 가장 선진적인 성과를 거두었고 이미 이 분야의 표준이 되었다.이 작업에서 우리는 이미 알고 있는 기술의 가장 좋은 조합을 찾아 추리 속도를 최적화하는 동시에 번역의 질을 희생하지 않는다.우리는 실증 연구를 실시하여 각종 방법을 총결하고 간소화된 중복 단원으로 자신의 주의를 대체하는 것을 증명했다.깊이 인코더와 얕은 디코더 구조, 다중 주의 가지치기로 각각 CPU와 GPU에서 109%와 84%의 가속을 실현하고 파라미터 수량을 25% 줄이는 동시에 BLEU에서 같은 번역 품질을 유지할 수 있다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dideli transformatoriai pasiekė pažangiausius rezultatus, susijusius su nervinių mašin ų vertimu, ir tapo standartiniais šioje srityje. Šiame darbe siekiame optimalaus žinomų metodų derinio siekiant optimizuoti išvados greitį nepažeidžiant vertimo kokybės. Atliekame empirinį tyrimą, kuriame nustatomi įvairūs metodai ir įrodoma, kad dekoderių savarankiškumo pakeitimas supaprastintais pakartotiniais vienetais, Priėmus gilų kodavimo kodą ir plokščią dekoderių architektūrą bei daugiakalbį dėmesį, galima atitinkamai pasiekti iki 109 % ir 84 % greičio CPU ir GPU ir sumažinti parametrų skaičių 25 %, išlaikant tą pačią vertimo kokybę BLEU atžvilgiu.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Големите трансформски модели постигнаа најсовремени резултати во преводот на невропските машини и станаа стандардни на теренот. Во оваа работа бараме оптимална комбинација на познати техники за оптимизација на брзината на конференцијата без жртвување на квалитетот на превод. Правиме емпириска студија која собира различни пристапи и демонстрира дека комбинацијата на замена на самото внимание на декодерот со едноставни рецидентни единици, Примената на длабокиот кодер и ниска архитектура на декодерот и привлекувањето на повеќето глави на вниманието може да достигне до 109 отсто и 84 отсто брзина на процесорот и GPU, односно, и да го намали бројот на параметри за 25 отсто, при што ќе се одржи истиот квалитет на превод во поглед на</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>വലിയ ട്രാന്‍സ്ഫോര്‍മാന്‍സ്ഫോര്‍മാറ്റര്‍ മോഡലുകള്‍ ന്യൂറല്‍ മെഷീന്‍ പരിഭാഷണത്തിന്റെ അവസ്ഥ നേരിട്ടുണ്ട്. പിന്നെ ഈ ജോലിയില്‍, പരിചയപ്പെട്ട ട ടെക്നിക്കങ്ങളുടെ ഐപ്റ്റമില്ലാത്ത ഒരുമിച്ചിട്ടുണ്ടാക്കാന്‍ ഞങ്ങള്‍ നോക്കുന്നു, വ്യത്യസ്ത വഴികള്‍ സ്ഥാപിക്കുന്നു എന്നിട്ട് സ്വയം ആത്മാര്‍ത്ഥ്യം മാറ്റുന്നതിനുള്ള സ്വയം ശ്രദ്ധ കൂട്ടുന്നതിനെ കൂട്ടി ഒരു ആഴത്തെ കോഡെര്‍ എടുക്കുന്നതും ഒരു തണുത്ത ഡെക്കോഡേര്‍ ആര്‍ക്കെക്ട്രെക്റ്റിക്കേറ്റര്‍ ശ്രദ്ധ കാണിക്കുന്നതും സിപിയുവിലും 84% വേഗത്തില്‍ എത്തുന്നതും ബിലിയുവിന്റെ വിഭാ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Том шилжүүлэгч загварууд мэдрэлийн машины хөрөнгө оруулалт болон стандарт болж байна. Энэ ажлын тулд бид мэддэг техникуудын хамтдаа халдварын хурдыг илүү сайжруулахын тулд илүү сайжруулагддаг. Бид өөр өөр арга барилгыг багтаж, өөрийн анхаарлыг хялбарчлан дахин дахин дахин дахин дахин дахин дахин дахин анхаарлаа орлуулж, Гүн гүнзгий кодер болон гүнзгий декодер архитектур болон олон толгой анхаарлын удирдлага нь CPU болон GPU дээр хурдан 109% болон 84% хүртэл хүрэх боломжтой болно. БЛЕУ-ын хувьд адилхан орчуулах чадварыг 25% багасгаж байна.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Model Transformer Besar telah mencapai keputusan-state-of-the-art dalam terjemahan mesin saraf dan telah menjadi piawai dalam medan. Dalam kerja ini, kita mencari kombinasi optimal teknik yang diketahui untuk optimize kelajuan kesimpulan tanpa mengorbankan kualiti terjemahan. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Mengadopsi pengekod dalam dan arkitektur pengekod rendah dan pemotongan perhatian berbilang-kepala boleh mencapai hingga 109% dan 84% kecepatan pada CPU dan GPU secara berdasarkan dan mengurangkan bilangan parameter dengan 25% sementara menjaga kualiti terjemahan yang sama dalam terma BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il-mudelli tat-Transformer il-kbar kisbu riżultati l-aktar avvanzati fit-traduzzjoni tal-magni newrali u saru standard fil-qasam. F’dan ix-xogħol, aħna qed ifittxu l-aħjar kombinazzjoni ta’ tekniki magħrufa biex nimmassimizzaw il-veloċità ta’ inferenza mingħajr ma nisakrifikaw il-kwalità tat-traduzzjoni. Għandna nagħmlu studju empiriku li jimpjega diversi approċċi u juri li l-kombinazzjoni ta’ sostituzzjoni tal-awtonomija tad-dekoder b’unitajiet rikorrenti ssimplifikati, L-adozzjoni ta’ kodifikatur profond u arkitettura ta’ dekoder baxx u pruning ta’ attenzjoni b’ħafna ras jistgħu jilħqu sa 109% u 84% velodup fuq CPU u GPU rispettivament u jnaqqsu n-numru ta’ parametri b’25% filwaqt li jżommu l-istess kwalità ta’ traduzzjoni f’termini ta’ BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grote Transformatormodellen hebben state-of-the-art resultaten behaald in neurale machinevertaling en zijn standaard in het veld geworden. In dit werk zoeken we naar de optimale combinatie van bekende technieken om de inferentiesnelheid te optimaliseren zonder afbreuk te doen aan de vertaalkwaliteit. We voeren een empirische studie uit die verschillende benaderingen stapelt en aantoont dat de combinatie van het vervangen van decoder zelfaandacht door vereenvoudigde terugkerende eenheden, Door gebruik te maken van een diepe encoder en een ondiepe decoderarchitectuur en multi-head attentie snoeien kan een versnelling tot 109% en 84% van respectievelijk CPU en GPU worden bereikt en het aantal parameters met 25% worden verminderd terwijl dezelfde vertaalkwaliteit in termen van BLEU wordt gehandhaafd.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Stor transformeringsmodeller har oppnådd tilstanden av kunsten i omsetjinga av neuralmaskina og har blitt standard i feltet. I dette arbeidet ser vi etter optimalt kombinasjon av kjende teknikk for å optimalisera infeksjonsfartet utan å oftasta omsetjingskvalitet. Vi gjer eit empirisk studie som stakkar ulike tilnærmingar og demonstrerer at kombinasjonen av å byta ut dekoder selvmerksomhet med enkelte rekurserte einingar, Eit dyp koder og ein sårba dekoderarkitektur og fleire hovuddekoder kan oppnå opp til 109 % og 84 % raskare på CPU og GPU, og redusere talet på parametrar med 25 % mens det gjeld samme omsetjingskvalitet under BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Duże modele transformatorów osiągnęły najnowocześniejsze wyniki w neuronowym tłumaczeniu maszynowym i stały się standardem w tej dziedzinie. W niniejszej pracy poszukujemy optymalnego połączenia znanych technik w celu optymalizacji szybkości wnioskowania bez obniżania jakości tłumaczenia. Przeprowadzamy badanie empiryczne, które łączy różne podejścia i pokazuje, że połączenie zastępowania samoobserwacji dekodera uproszczonymi jednostkami powtarzającymi się, Zastosowanie głębokiego kodera i płytkiej architektury dekodera oraz wielogłowicowego przycinania uwagi może osiągnąć do 109% i 84% przyspieszenie procesora i GPU odpowiednio oraz zmniejszyć liczbę parametrów o 25% przy zachowaniu tej samej jakości tłumaczenia pod względem BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Grandes modelos de transformadores alcançaram resultados de última geração em tradução automática neural e se tornaram padrão no campo. Neste trabalho, procuramos a combinação ideal de técnicas conhecidas para otimizar a velocidade de inferência sem sacrificar a qualidade da tradução. Conduzimos um estudo empírico que empilha várias abordagens e demonstra que a combinação da substituição da autoatenção do decodificador por unidades recorrentes simplificadas, a adoção de um codificador profundo e uma arquitetura de decodificador rasa e a poda de atenção multi-head podem atingir até 109% e 84% de aceleração em CPU e GPU, respectivamente, e reduzem o número de parâmetros em 25%, mantendo a mesma qualidade de tradução em termos de BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelele de transformare mari au obținut rezultate de ultimă oră în traducerea mașinii neurale și au devenit standard în domeniu. În această lucrare, căutăm combinația optimă de tehnici cunoscute pentru a optimiza viteza inferenței fără a sacrifica calitatea traducerii. Realizăm un studiu empiric care stivuiește diferite abordări și demonstrează că combinația de înlocuire a auto-atenției decodorului cu unități recurente simplificate, Adoptarea unui encoder profund și a unei arhitecturi de decodare superficială și reducerea atenției cu mai multe capete pot atinge o viteză de până la 109% și, respectiv, 84% pe CPU și GPU și reduce numărul de parametri cu 25%, menținând în același timp aceeași calitate a traducerii în ceea ce privește BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Модели крупных трансформаторов достигли самых современных результатов в нейронном машинном переводе и стали стандартными в данной области. В этой работе мы ищем оптимальное сочетание известных методов для оптимизации скорости вывода без ущерба для качества перевода. Мы проводим эмпирическое исследование, которое суммирует различные подходы и демонстрирует, что сочетание замены самовнимания декодера на упрощенные повторяющиеся единицы, принятия глубокого кодера и неглубокой архитектуры декодера и многоголовочной обрезки внимания может достичь до 109% и 84% ускорения на CPU и GPU соответственно и уменьшить количество параметров на 25% при сохранении того же качества трансляции в терминах BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>විශාල ප්‍රවර්තනයක් විදිහට ප්‍රමාණයක් ලැබුනා න්‍යූරල් මැෂින් පරිවර්තනයේ ස්ථිතිය- of- the- art ප්‍රතිප මේ වැඩේ අපි හොයාගෙන ඉන්නේ දන්න තාක්ෂණිකාවගේ හොඳම සම්බන්ධයක් හොයාගෙන ඉන්නේ අනුවාර්ථ වේගයක් නැති අපි ඉම්පිරිකාලික අධ්‍යානයක් කරනවා වගේම විවිදියට ප්‍රතික්‍රියා කරනවා ඒ වගේම ප්‍රතික්‍රියා කරනවා කියලා සාමාන්‍ය ව ගොඩක් ඇන්කෝඩර් එකක් සහ ගොඩක් ඩිකෝඩර් ස්ථාපනයක් සහ ගොඩක් හෙඩක් අවධානයක් ඉන්න පුළුවන් CPU සහ GPU වලින් ඉක්මනට 109% සහ 84% ඉක්මනට සම්පූර්ණයෙන් ඉන්න සහ 25%</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modeli velikih transformatorjev so dosegli najsodobnejše rezultate v nevronskem strojnem prevajanju in postali standardni na tem področju. V tem delu iščemo optimalno kombinacijo znanih tehnik za optimizacijo hitrosti sklepanja brez žrtvovanja kakovosti prevajanja. Izvedli smo empirično študijo, ki zloži različne pristope in dokazuje, da kombinacija zamenjave samopozornosti dekoderja s poenostavljenimi ponavljajočimi enotami, S sprejetjem globokega kodirnika in plitve arhitekture dekodirnika ter večglavnega obrezovanja pozornosti lahko dosežete do 109% oziroma 84% pospešitev na CPU oziroma GPU ter zmanjšate število parametrov za 25%, hkrati pa ohranjate enako kakovost prevajanja v smislu BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tusaale wayn oo turjubaal ah waxay gaadheen xaalad-farshaxan, waxayna ka heleen tarjumaadda maskinada neurada ah, waxayna noqdeen standard duurka. Shaqodaas waxaynu raadinnaa iskuulka suurtagalka ah ee loo yaqaan si aan u faa’iideyn karin dhaqdhaqaaq la’aanta tarjumaadda. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Waxyaabaha aad u dheer kartid iyo dhismaha aad u deynta iyo xannaaneynta madaxa kala duduwanba waxay gaadhi karaan ugu badnaan 109% iyo 84% si gaar ah CPU iyo GPU, waxayna hoos u dhigi karaan tirada tirada ah 25% inta ay xajisanayso isku qiimaha tarjumaadka oo ah BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Modelet e mëdha të Transformës kanë arritur rezultate më të larta në përkthimin e makinave nervore dhe janë bërë standarde në fushë. Në këtë punë, ne kërkojmë kombinimin optimal të teknikave të njohura për të optimizuar shpejtësinë e përfundimit pa sakrifikuar cilësinë e përkthimit. Ne kryejmë një studim empirik që grumbullon metoda të ndryshme dhe demonstron se kombinimi i zëvendësimit të vetëvëmendjes së dekoderit me njësitë e thjeshta të përsëritura, - miratimi i një kodifikuesi të thellë dhe një arkitekture dekoderi të thellë dhe shtrëngimi i vëmendjes me shumë koka mund të arrijë deri në 109% dhe 84% shpejtësi respektivisht në CPU dhe GPU dhe të reduktojë numrin e parametrave me 25% duke mbajtur të njëjtën cilësi përkthimi në termat e BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Veliki modeli transformera postigli su rezultate umetnosti u prevodu neuralne mašine i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Vodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevođenja u smislu BLEU-a.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Stora Transformermodeller har uppnått toppmoderna resultat inom neural maskinöversättning och har blivit standard inom området. I detta arbete letar vi efter den optimala kombinationen av kända tekniker för att optimera inferenshastigheten utan att offra översättningskvaliteten. Vi genomför en empirisk studie som staplar olika tillvägagångssätt och visar att kombinationen av att ersätta avkodare självuppmärksamhet med förenklade återkommande enheter, Genom att använda en djup kodare och en ytlig avkodningsarkitektur och flerskalig uppmärksamhet kan man uppnå upp till 109% respektive 84% snabbare på CPU respektive GPU och minska antalet parametrar med 25% samtidigt som man bibehåller samma översättningskvalitet när det gäller BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mradi mkubwa wa Tafsiri umefanikiwa kuwa na hali ya sanaa na matokeo ya utafsiri wa mashine ya kisasa na yamekuwa kiwango cha kawaida katika uwanja. Katika kazi hii, tunatafuta muunganiko bora wa mbinu zinazofahamika kuboresha kiwango cha uchunguzi bila kutoa sifa za tafsiri. Tunafanya utafiti wa msisitizo ambao unaweka hatua mbalimbali na kuonyesha kuwa muunganiko wa kubadilisha nafuu kwa vitengo rahisi vya kurudi, Kwa kuchukua kiwango cha ndani na ujenzi mdogo wa decodi na uchunguzi wa kichwa vingi unaweza kufikia asilimia 109 na asilimia 84 kwa kiwango cha CPU na GPU na kupunguza idadi ya parameter kwa asilimia 25 wakati wakiendelea kutangaza kiwango hicho cha tafsiri kwa mujibu wa BLEU.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>பெரிய மாற்று மாற்றும் மாதிரிகள் நிலைமை- கலை மாற்றியமைத்து புதிய இயந்திரம் மொழிமாற்றி புலத்தில் இயல்பான மாத இந்த வேலையில், நாம் தெரியும் தொழில்நுட்பத்தின் வேகத்தை அதிகப்படுத்த வேண்டும் என்று தேடுகிறோம் மொழிபெயர்ப் நாம் சுலபமான திரும்ப அலகுகளை மாற்றும் குறியீட்டு தன்னுடைய கவனத்தை மாற்றும் குறியீட்டை மாற்றும் சுலபமான திரும்ப அலக ஒரு ஆழமான குறியீட்டு மற்றும் ஒரு மழுமையான குறியீட்டாளர் அமைப்பு மற்றும் பல தலைப்பு கவனத்தை புரிந்து பிபியு மற்றும் GPU மற்றும் 84% வேகத்தை பெற முடியும் மற்றும் பிலியூவி</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ullakan Transformer nusgalary neural maşynyň terjimesinde ýetip bardylar we sahypada standart boldy. Bu işde, biz bilinen teknikleriň optimal birleşigini terjime etmek üçin azalyş ýigrimizi bejermek üçin gözleýäris. Biz empirik bir aralygy ýerine ýetirýäris we munuň birnäçe nusgalaryny ýerleşdirýändigini görkezýäris. Garyp ködleme we çukultyk arhitektura we köp kelläp üns berişi 109% we 84% CPU we GPU-a süýtgelip biler we ayn terjime howpsaly BLEU-a garaşyp biler parameterleriň sanyny 25% tarapyndan azaltyp biler.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>بڑے ترنسفورر نمڈلوں نے نئورل ماشین ترجمہ کے نتیجے پہنچ گئے ہیں اور کھیل میں استاندارڈ ہوگئے ہیں. اس کام میں، ہم جانے والی تکنیک کی اچھی ترکیب کے لئے تلاش کرتے ہیں کہ اس کے ذریعہ مہربانی کی سرعت مہربانی کریں بغیر ترکیب کی کیفیت کے۔ ہم ایک مصریح تحقیق کرتے ہیں جو مختلف طریقوں کو ٹکڑے رکھتا ہے اور دکھاتے ہیں کہ دکور کی اپنا توجه سادھا دوبارہ واحدوں سے بدل دینے کی ترکیب ہے، سی پی یو اور جی پی یو پر چڑھا ہوا تھا اور ایک گہرے ڈیکوڈر معماری اور بہت سی سروں کی توجه پرینگ کے ذریعے سی پی یو اور جی پی یو پر چڑھا ہوا تھا اور اس کی تعداد 25% کے ذریعے کم کر سکتا ہے جب کہ بلیو کے اندر ایک ہی ترجمہ کیفیت کی حفاظت کرتی ہے۔</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. Bu vazifanda, biz tanlangan teknikalarning optimal birlashtirishni istaysizmi, tarjima sifatini tozalash mumkin. Biz bir tashkilotni bajaramiz, har xil usullarni qo'yish va o'sha narsalarni o'zgartirish qo'shimcha o'zimni o'zgartirish bilan soddalashtirish uchun o'zgartirish mumkin, AQSH</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Các mô hình biến hình lớn đã đạt được kết quả tối tân trong việc dịch chuyển cỗ máy thần kinh và trở thành tiêu chuẩn trên chiến trường. Trong công việc này, chúng tôi tìm kiếm sự kết hợp tối ưu tiên của kỹ thuật được biết để tối ưu tiên tốc độ nhận biết mà không hy sinh chất lượng dịch. Chúng tôi thực hiện một nghiên cứu có kinh nghiệm mang theo nhiều phương pháp khác nhau và chứng minh rằng kết hợp của việc thay thế bộ lọc bằng đơn vị thường xuyên, Một máy mã hóa sâu và một cấu trúc giải mã nông cạn và việc cắt giảm tập trung đa đầu có thể tăng tốc lên đến 109=và84=.='trên hai bộ vi xử lý cộng cộng đồng và giảm bớt số lượng các tham số bằng 25t. Trong khi vẫn giữ nguyên chất dịch bản phân loại tương tự.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>大Transformer先神经机器翻译,以为率土。 于此之中,求已知之最,以不牺牲译优化推理速度。 一实考之,累累其方,证将解码器自意代为循环单元,用深度编码器浅层解码器架构及多头意修合,可于CPUGPU上各得达109%84%之速,并减参数数25%,兼同BLEU转质。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.sustainlp-1.7</dd><dt>Volume:</dt><dd><a href=/volumes/2020.sustainlp-1/>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/sustainlp/>sustainlp</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>48–53</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.sustainlp-1.7>https://aclanthology.org/2020.sustainlp-1.7</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.sustainlp-1.7 title="To the current version of the paper by DOI">10.18653/v1/2020.sustainlp-1.7</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">hsu-etal-2020-efficient</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Yi-Te Hsu, Sarthak Garg, Yi-Hsiu Liao, and Ilya Chatsviorkin. 2020. <a href=https://aclanthology.org/2020.sustainlp-1.7>Efficient Inference For Neural Machine Translation</a>. In <i>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</i>, pages 48–53, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.sustainlp-1.7>Efficient Inference For Neural Machine Translation</a> (Hsu et al., sustainlp 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.sustainlp-1.7.pdf>https://aclanthology.org/2020.sustainlp-1.7.pdf</a></dd><dt class=acl-button-row>Video:</dt><dd class=acl-button-row><a href=https://slideslive.com/38939429 class="btn btn-attachment btn-sm"><i class="fas fa-video"></i>&nbsp;https://slideslive.com/38939429</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.sustainlp-1.7.pdf title="Open PDF of 'Efficient Inference For Neural Machine Translation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Efficient+Inference+For+Neural+Machine+Translation" title="Search for 'Efficient Inference For Neural Machine Translation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Efficient Inference For Neural Machine Translation'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://slideslive.com/38939429 title="Open video for 'Efficient Inference For Neural Machine Translation'"><span class="align-self-center px-1"><i class="fas fa-video"></i></span>
<span class=px-1>Video</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Efficient Inference For Neural Machine Translation](https://aclanthology.org/2020.sustainlp-1.7) (Hsu et al., sustainlp 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.sustainlp-1.7>Efficient Inference For Neural Machine Translation</a> (Hsu et al., sustainlp 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Yi-Te Hsu, Sarthak Garg, Yi-Hsiu Liao, and Ilya Chatsviorkin. 2020. <a href=https://aclanthology.org/2020.sustainlp-1.7>Efficient Inference For Neural Machine Translation</a>. In <i>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</i>, pages 48–53, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>