<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Representation Learning for NLP (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Representation Learning for NLP (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w19-43>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li></ul></div></div><div id=w19-43><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-43.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-43/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4300/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4301/>Deep Generalized Canonical Correlation Analysis</a></strong><br><a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/b/biman-gujral/>Biman Gujral</a>
|
<a href=/people/d/dee-ann-reisinger/>Dee Ann Reisinger</a>
|
<a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/r/raman-arora/>Raman Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4301><div class="card-body p-3 small">We present Deep Generalized Canonical Correlation Analysis (DGCCA) a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> are maximally informative of each other. While methods for nonlinear two view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks : <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic transcription</a> from acoustic & articulatory measurements, recommending hashtags and recommending friends on a dataset of Twitter users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4305/>Multilingual NMT with a Language-Independent Attention Bridge<span class=acl-fixed-case>NMT</span> with a Language-Independent Attention Bridge</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4305><div class="card-body p-3 small">In this paper, we propose an architecture for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> from each language for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and develops into a language-agnostic meaning representation that can efficiently be used for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We present a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4306/>Efficient <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> with Automatic Relevance Determination in Recurrent Neural Networks</a></strong><br><a href=/people/m/maxim-kodryan/>Maxim Kodryan</a>
|
<a href=/people/a/artem-grachev/>Artem Grachev</a>
|
<a href=/people/d/dmitry-ignatov/>Dmitry Ignatov</a>
|
<a href=/people/d/dmitry-vetrov/>Dmitry Vetrov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4306><div class="card-body p-3 small">Reduction of the number of parameters is one of the most important goals in <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a>. In this article we propose an adaptation of Doubly Stochastic Variational Inference for Automatic Relevance Determination (DSVI-ARD) for neural networks compression. We find this method to be especially useful in language modeling tasks, where large number of parameters in the input and output layers is often excessive. We also show that DSVI-ARD can be applied together with encoder-decoder weight tying allowing to achieve even better sparsity and performance. Our experiments demonstrate that more than 90 % of the weights in both encoder and decoder layers can be removed with a minimal quality loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4310/>Specializing Distributional Vectors of All Words for Lexical Entailment</a></strong><br><a href=/people/a/aishwarya-kamath/>Aishwarya Kamath</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4310><div class="card-body p-3 small">Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g. WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first post-processing method that specializes vectors of all vocabulary words including those unseen in the resources for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep feed-forward neural network</a> : its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4311/>Composing Noun Phrase Vector Representations</a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a>
|
<a href=/people/r/richard-crouch/>Richard Crouch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4311><div class="card-body p-3 small">Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks. While there seems to be a consensus about the usefulness of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and how to learn them, it is still unclear which <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> can capture the meaning of phrases or even whole sentences. Recent work has shown that simple operations outperform more complex <a href=https://en.wikipedia.org/wiki/Deep_learning>deep architectures</a>. In this work, we propose two novel <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> for computing noun phrase vector representations. First, we propose that the semantic and not the syntactic contribution of each component of a <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrase</a> should be considered, so that the resulting composed vectors express more of the phrase meaning. Second, the composition process of the two phrase vectors should apply suitable dimensions&#8217; selection in a way that specific semantic features captured by the phrase&#8217;s meaning become more salient. Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets. Our results show that these <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4312 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4312" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4312/>Towards Robust Named Entity Recognition for Historic German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/s/stefan-schweter/>Stefan Schweter</a>
|
<a href=/people/j/johannes-baiter/>Johannes Baiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4312><div class="card-body p-3 small">In this paper we study the influence of using language model pre-training for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Historic German. We achieve new state-of-the-art results using carefully chosen training data for <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. For a low-resource domain like <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer-learning</a> with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4313/>On Evaluating Embedding Models for Knowledge Base Completion</a></strong><br><a href=/people/y/yanjie-wang/>Yanjie Wang</a>
|
<a href=/people/d/daniel-ruffinelli/>Daniel Ruffinelli</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/s/samuel-broscheit/>Samuel Broscheit</a>
|
<a href=/people/c/christian-meilicke/>Christian Meilicke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4313><div class="card-body p-3 small">Knowledge graph embedding models have recently received significant attention in the literature. These models learn latent semantic representations for the entities and relations in a given <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> ; the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> can be used to infer missing knowledge. In this paper, we study the question of how well recent <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> perform for the task of knowledge base completion, i.e., the task of inferring new facts from an <a href=https://en.wikipedia.org/wiki/Complete_knowledge_base>incomplete knowledge base</a>. We argue that the entity ranking protocol, which is currently used to evaluate knowledge graph embedding models, is not suitable to answer this question since only a subset of the model predictions are evaluated. We propose an alternative entity-pair ranking protocol that considers all model predictions as a whole and is thus more suitable to the task. We conducted an experimental study on standard datasets and found that the performance of popular embeddings models was unsatisfactory under the new protocol, even on datasets that are generally considered to be too easy. Moreover, we found that a simple rule-based model often provided superior performance. Our findings suggest that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4314/>Constructive Type-Logical Supertagging With Self-Attention Networks</a></strong><br><a href=/people/k/konstantinos-kogkalidis/>Konstantinos Kogkalidis</a>
|
<a href=/people/m/michael-moortgat/>Michael Moortgat</a>
|
<a href=/people/t/tejaswini-deoskar/>Tejaswini Deoskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4314><div class="card-body p-3 small">We propose a novel application of self-attention networks towards <a href=https://en.wikipedia.org/wiki/Grammar_induction>grammar induction</a>. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of the grammar&#8217;s type system along with its <a href=https://en.wikipedia.org/wiki/Denotational_semantics>denotational semantics</a>. This lifts the <a href=https://en.wikipedia.org/wiki/Closed-world_assumption>closed world assumption</a> commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4317/>An Empirical Study on Pre-trained Embeddings and <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Bot Detection</a></strong><br><a href=/people/a/andres-garcia-silva/>Andres Garcia-Silva</a>
|
<a href=/people/c/cristian-berrio/>Cristian Berrio</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>José Manuel Gómez-Pérez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4317><div class="card-body p-3 small">Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are learned from large and well-formed text corpora from e.g. encyclopedic resources, <a href=https://en.wikipedia.org/wiki/Book>books</a> or <a href=https://en.wikipedia.org/wiki/News>news</a>. However, a significant amount of the text to be analyzed nowadays is Web data, often from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper we consider the research question : How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>? To answer this question, we focus on bot detection in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4318/>Probing Multilingual Sentence Representations With X-Probe<span class=acl-fixed-case>X</span>-Probe</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4318><div class="card-body p-3 small">This paper extends the task of probing sentence representations for linguistic insight in a multilingual domain. In doing so, we make two contributions : first, we provide datasets for multilingual probing, derived from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, in five languages, viz. English, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Second, we evaluate six sentence encoders for each language, each trained by mapping sentence representations to English sentence representations, using sentences in a parallel corpus. We discover that cross-lingually mapped representations are often better at retaining certain linguistic information than <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> derived from English encoders trained on natural language inference (NLI) as a downstream task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4320/>Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4320><div class="card-body p-3 small">In this paper, we propose Multilingual Meta-Embeddings (MME), an effective method to learn multilingual representations by leveraging monolingual pre-trained embeddings. MME learns to utilize information from these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> via a self-attention mechanism without explicit language identification. We evaluate the proposed embedding method on the code-switching English-Spanish Named Entity Recognition dataset in a multilingual and cross-lingual setting. The experimental results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4321/>Investigating Sub-Word Embedding Strategies for the Morphologically Rich and Free Phrase-Order Hungarian<span class=acl-fixed-case>H</span>ungarian</a></strong><br><a href=/people/b/balint-dobrossy/>Bálint Döbrössy</a>
|
<a href=/people/m/marton-makrai/>Márton Makrai</a>
|
<a href=/people/b/balazs-tarjan/>Balázs Tarján</a>
|
<a href=/people/g/gyorgy-szaszak/>György Szaszák</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4321><div class="card-body p-3 small">For morphologically rich languages, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> provide less consistent semantic representations due to higher variance in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>word forms</a>. Moreover, these <a href=https://en.wikipedia.org/wiki/Language>languages</a> often allow for less constrained word order, which further increases variance. For the highly agglutinative Hungarian, semantic accuracy of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> measured on word analogy tasks drops by 50-75 % compared to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We observed that <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learn morphosyntax quite well instead. Therefore, we explore and evaluate several sub-word unit based embedding strategies character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor) to boost semantic consistency in Hungarian word vectors. The effect of changing <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding dimension</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>context window size</a> have also been considered. Morphological analysis based lemmatization was found to be the best strategy to improve embeddings&#8217; semantic accuracy, whereas adding character n-grams was found consistently counterproductive in this regard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4322/>A Self-Training Approach for Short Text Clustering</a></strong><br><a href=/people/a/amir-hadifar/>Amir Hadifar</a>
|
<a href=/people/l/lucas-sterckx/>Lucas Sterckx</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4322><div class="card-body p-3 small">Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations of the short texts. Low-dimensional continuous representations or <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> can counter that sparseness problem : their high representational power is exploited in deep clustering algorithms. While deep clustering has been studied extensively in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, relatively little work has focused on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The method we propose, learns discriminative features from both an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> and a <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a>, then uses assignments from a <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering algorithm</a> as supervision to update weights of the encoder network. Experiments on three short text datasets empirically validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4323 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4323/>Improving <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>Word Embeddings</a> Using Kernel PCA<span class=acl-fixed-case>PCA</span></a></strong><br><a href=/people/v/vishwani-gupta/>Vishwani Gupta</a>
|
<a href=/people/s/sven-giesselbach/>Sven Giesselbach</a>
|
<a href=/people/s/stefan-ruping/>Stefan Rüping</a>
|
<a href=/people/c/christian-bauckhage/>Christian Bauckhage</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4323><div class="card-body p-3 small">Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections ; however, they fail to do so with small datasets. Extensions such as <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data. In this paper, we introduce a new approach to warm-start embedding models with morphological information, in order to reduce <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> and enhance their performance. We use word embeddings generated using both <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices. This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities. Evaluating our models on word similarity and analogy tasks in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time. Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies. Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4324" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4324/>Assessing Incrementality in Sequence-to-Sequence Models</a></strong><br><a href=/people/d/dennis-ulmer/>Dennis Ulmer</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/e/elia-bruni/>Elia Bruni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4324><div class="card-body p-3 small">Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. The most recent successes are predominantly due to the use of different variations of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language : continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to assess the behavior of RNNs with and without an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and identify key differences in the way the different model types process sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4325/>On Committee Representations of Adversarial Learning Models for Question-Answer Ranking</a></strong><br><a href=/people/s/sparsh-gupta/>Sparsh Gupta</a>
|
<a href=/people/v/vitor-carvalho/>Vitor Carvalho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4325><div class="card-body p-3 small">Adversarial training is a process in <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> that explicitly trains models on <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial inputs</a> (inputs designed to deceive or trick the learning process) in order to make it more robust or accurate. In this paper we investigate how representing adversarial training models as <a href=https://en.wikipedia.org/wiki/Committee>committees</a> can be used to effectively improve the performance of Question-Answer (QA) Ranking. We start by empirically probing the effects of adversarial training over multiple QA ranking algorithms, including the state-of-the-art Multihop Attention Network model. We evaluate these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> on several benchmark datasets and observe that, while adversarial training is beneficial to most baseline algorithms, there are cases where it may lead to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and performance degradation. We investigate the causes of such degradation, and then propose a new representation procedure for this adversarial learning problem, based on committee learning, that not only is capable of consistently improving all baseline algorithms, but also outperforms the previous state-of-the-art algorithm by as much as 6 % in NDCG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4327 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4327/>Best Practices for Learning Domain-Specific Cross-Lingual Embeddings</a></strong><br><a href=/people/l/lena-shakurova/>Lena Shakurova</a>
|
<a href=/people/b/beata-nyari/>Beata Nyari</a>
|
<a href=/people/c/chao-li/>Chao Li</a>
|
<a href=/people/m/mihai-rotaru/>Mihai Rotaru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4327><div class="card-body p-3 small">Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>linear projection</a> from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4329 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4329/>Learning Word Embeddings without Context Vectors</a></strong><br><a href=/people/a/alexey-zobnin/>Alexey Zobnin</a>
|
<a href=/people/e/evgenia-elistratova/>Evgenia Elistratova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4329><div class="card-body p-3 small">Most word embedding algorithms such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> or <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> construct two sort of vectors : for words and for contexts. Naive use of vectors of only one sort leads to poor results. We suggest using indefinite inner product in skip-gram negative sampling algorithm. This allows us to use only one sort of vectors without loss of quality. Our context-free cf algorithm performs on par with SGNS on word similarity datasets</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4331 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4331/>Modality-based Factorization for Multimodal Fusion</a></strong><br><a href=/people/e/elham-j-barezi/>Elham J. Barezi</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4331><div class="card-body p-3 small">We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (M+1)-way tensor to consider the high-order relationships between M modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> avoiding <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. We have applied this method to three different multimodal datasets in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, personality trait recognition, and <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>. We are able to recognize relationships and relative importance of different modalities in these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and achieves a 1 % to 4 % improvement on several evaluation measures compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for all three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.<i>M</i>+1)-way tensor to consider the high-order relationships between <i>M</i> modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>