<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>The Workshop on Narrative Understanding, Storylines, and Events (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>The Workshop on Narrative Understanding, Storylines, and Events (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020nuse-1>Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li></ul></div></div><div id=2020nuse-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2020.nuse-1/>Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nuse-1.0/>Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events</a></strong><br><a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/alejandro-jaimes/>Alejandro Jaimes</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/l/lara-j-martin/>Lara J. Martin</a>
|
<a href=/people/b/ben-miller/>Ben Miller</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nuse-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nuse-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929742 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nuse-1.3/>Improving the Identification of the Discourse Function of News Article Paragraphs</a></strong><br><a href=/people/d/deya-banisakher/>Deya Banisakher</a>
|
<a href=/people/w/w-victor-yarlott/>W. Victor Yarlott</a>
|
<a href=/people/m/mohammed-aldawsari/>Mohammed Aldawsari</a>
|
<a href=/people/n/naphtali-rishe/>Naphtali Rishe</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nuse-1--3><div class="card-body p-3 small">Identifying the discourse structure of documents is an important task in understanding <a href=https://en.wikipedia.org/wiki/Writing>written text</a>. Building on prior work, we demonstrate an improved approach to automatically identifying the discourse function of paragraphs in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. We start with the hierarchical theory of news discourse developed by van Dijk (1988) which proposes how paragraphs function within <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. This discourse information is a level intermediate between phrase- or sentence-sized discourse segments and document genre, characterizing how individual paragraphs convey information about the events in the storyline of the article. Specifically, the theory categorizes the relationships between narrated events and (1) the overall storyline (such as Main Events, Background, or Consequences) as well as (2) commentary (such as Verbal Reactions and Evaluations). We trained and tested a linear chain conditional random field (CRF) with new features to model van Dijk&#8217;s labels and compared it against several machine learning models presented in previous work. Our model significantly outperformed all <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and prior approaches, achieving an average of 0.71 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> which represents a 31.5 % improvement over the previously best-performing <a href=https://en.wikipedia.org/wiki/Support_vector_machine>support vector machine model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nuse-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nuse-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929744 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nuse-1.5/>Extensively Matching for Few-shot Learning Event Detection</a></strong><br><a href=/people/v/viet-dac-lai/>Viet Dac Lai</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nuse-1--5><div class="card-body p-3 small">Current event detection models under supervised learning settings fail to transfer to new event types. Few-shot learning has not been explored in event detection even though it allows a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to perform well with high <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> on new event types. In this work, we formulate event detection as a few-shot learning problem to enable to extend event detection to new event types. We propose two novel <a href=https://en.wikipedia.org/wiki/Loss_factor>loss factors</a> that matching examples in the support set to provide more <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training signals</a> to the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. Moreover, these <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training signals</a> can be applied in many metric-based few-shot learning models. Our extensive experiments on the ACE-2005 dataset (under a few-shot learning setting) show that the proposed method can improve the performance of few-shot learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nuse-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nuse-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929748 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nuse-1.9/>Annotating and quantifying narrative time disruptions in modernist and hypertext fiction</a></strong><br><a href=/people/e/edward-kearns/>Edward Kearns</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nuse-1--9><div class="card-body p-3 small">This paper outlines work in progress on a new method of annotating and quantitatively discussing <a href=https://en.wikipedia.org/wiki/Narrative>narrative techniques</a> related to time in fiction. Specifically those techniques are <a href=https://en.wikipedia.org/wiki/Analepsis>analepsis</a>, <a href=https://en.wikipedia.org/wiki/Prolepsis>prolepsis</a>, narrative level changes, and <a href=https://en.wikipedia.org/wiki/Stream_of_consciousness>stream-of-consciousness</a> and free-indirect-discourse narration. By counting the frequency and extent of the usage of these <a href=https://en.wikipedia.org/wiki/List_of_narrative_techniques>techniques</a>, the <a href=https://en.wikipedia.org/wiki/Narrative>narrative characteristics</a> of different works from different time periods and genres can be compared. This project uses <a href=https://en.wikipedia.org/wiki/Literary_modernism>modernist fiction</a> and <a href=https://en.wikipedia.org/wiki/Hypertext_fiction>hypertext fiction</a> as its case studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nuse-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nuse-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939705 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nuse-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nuse-1.10/>Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types</a></strong><br><a href=/people/b/belen-saldias/>Belen Saldias</a>
|
<a href=/people/d/deb-roy/>Deb Roy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nuse-1--10><div class="card-body p-3 small">Sharing personal narratives is a fundamental aspect of <a href=https://en.wikipedia.org/wiki/Social_behavior>human social behavior</a> as it helps share our life experiences. We can tell stories and rely on our background to understand their context, similarities, and differences. A substantial effort has been made towards developing storytelling machines or inferring characters&#8217; features. However, we do n&#8217;t usually find <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that compare narratives. This task is remarkably challenging for machines since they, as sometimes we do, lack an understanding of what similarity means. To address this challenge, we first introduce a corpus of real-world spoken personal narratives comprising 10,296 narrative clauses from 594 <a href=https://en.wikipedia.org/wiki/Videotape>video transcripts</a>. Second, we ask non-narrative experts to annotate those clauses under Labov&#8217;s sociolinguistic model of personal narratives (i.e., action, orientation, and evaluation clause types) and train a classifier that reaches 84.7 % F-score for the highest-agreed clauses. Finally, we match stories and explore whether people implicitly rely on Labov&#8217;s framework to compare narratives. We show that actions followed by the narrator&#8217;s evaluation of these are the aspects non-experts consider the most. Our approach is intended to help inform <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> aimed at studying or representing <a href=https://en.wikipedia.org/wiki/Personal_narrative>personal narratives</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nuse-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nuse-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nuse-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929754 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nuse-1.14/>On-The-Fly Information Retrieval Augmentation for <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a></a></strong><br><a href=/people/h/hai-wang/>Hai Wang</a>
|
<a href=/people/d/david-mcallester/>David McAllester</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nuse-1--14><div class="card-body p-3 small">Here we experiment with the use of <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> as an augmentation for pre-trained language models. The <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> used in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> can be viewed as form of <a href=https://en.wikipedia.org/wiki/Episodic_memory>episodic memory</a> which grows over time. By augmenting GPT 2.0 with <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> we achieve a zero shot 15 % relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>