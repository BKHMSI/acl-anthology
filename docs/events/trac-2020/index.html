<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Trolling, Aggression and Cyberbullying (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Trolling, Aggression and Cyberbullying (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020trac-1>Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li></ul></div></div><div id=2020trac-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.trac-1/>Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.0/>Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/b/bornini-lahiri/>Bornini Lahiri</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/v/vanessa-murdock/>Vanessa Murdock</a>
|
<a href=/people/d/daniel-kadar/>Daniel Kadar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.5/>Aggression Identification in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : a Transfer Learning Based Approach</a></strong><br><a href=/people/f/faneva-ramiandrisoa/>Faneva Ramiandrisoa</a>
|
<a href=/people/j/josiane-mothe/>Josiane Mothe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--5><div class="card-body p-3 small">The way people communicate have changed in many ways with the outbreak of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. One of the aspects of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is the ability for their information producers to hide, fully or partially, their identity during a discussion ; leading to <a href=https://en.wikipedia.org/wiki/Cyber-aggression>cyber-aggression</a> and interpersonal aggression. Automatically monitoring <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> in order to help moderating it is thus a very hot topic. In this paper, we propose to use the transformer based language model BERT (Bidirectional Encoder Representation from Transformer) (Devlin et al., 2019) to identify aggressive content. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is also used to predict the level of <a href=https://en.wikipedia.org/wiki/Aggression>aggressiveness</a>. The evaluation part of this paper is based on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provided by the TRAC shared task (Kumar et al., 2018a). When compared to the other participants of this shared task, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved the third best performance according to the weighted F1 measure on both Facebook and Twitter collections.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.7/>A Comparative Study of Different State-of-the-Art Hate Speech Detection Methods in Hindi-English Code-Mixed Data<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Mixed Data</a></strong><br><a href=/people/p/priya-rani/>Priya Rani</a>
|
<a href=/people/s/shardul-suryawanshi/>Shardul Suryawanshi</a>
|
<a href=/people/k/koustava-goswami/>Koustava Goswami</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/t/theodorus-fransen/>Theodorus Fransen</a>
|
<a href=/people/j/john-philip-mccrae/>John Philip McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--7><div class="card-body p-3 small">Hate speech detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media communication</a> has become one of the primary concerns to avoid conflicts and curb undesired activities. In an environment where multilingual speakers switch among multiple languages, hate speech detection becomes a challenging task using methods that are designed for monolingual corpora. In our work, we attempt to analyze, detect and provide a comparative study of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> in a code-mixed social media text. We also provide a Hindi-English code-mixed data set consisting of Facebook and Twitter posts and comments. Our experiments show that <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> trained on this code-mixed corpus perform better.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.trac-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.9/>Bagging BERT Models for Robust Aggression Identification<span class=acl-fixed-case>BERT</span> Models for Robust Aggression Identification</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--9><div class="card-body p-3 small">Modern transformer-based models with hundreds of millions of parameters, such as BERT, achieve impressive results at text classification tasks. This also holds for aggression identification and offensive language detection, where deep learning approaches consistently outperform less complex models, such as <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>decision trees</a>. While the complex models fit training data well (low bias), they also come with an unwanted high variance. Especially when fine-tuning them on small datasets, the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance varies significantly for slightly different training data. To overcome the high variance and provide more robust predictions, we propose an ensemble of multiple fine-tuned BERT models based on bootstrap aggregating (bagging). In this paper, we describe such an ensemble system and present our submission to the shared tasks on aggression identification 2020 (team name : Julian). Our submission is the best-performing <a href=https://en.wikipedia.org/wiki/System>system</a> for five out of six subtasks. For example, we achieve a weighted F1-score of 80.3 % for task A on the test dataset of English social media posts. In our experiments, we compare different model configurations and vary the number of models used in the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>. We find that the F1-score drastically increases when ensembling up to 15 models, but the returns diminish for more models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.10/>Scmhl5 at TRAC-2 Shared Task on Aggression Identification : Bert Based Ensemble Learning Approach<span class=acl-fixed-case>TRAC</span>-2 Shared Task on Aggression Identification: Bert Based Ensemble Learning Approach</a></strong><br><a href=/people/h/han-liu/>Han Liu</a>
|
<a href=/people/p/pete-burnap/>Pete Burnap</a>
|
<a href=/people/w/wafa-alorainy/>Wafa Alorainy</a>
|
<a href=/people/m/matthew-williams/>Matthew Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--10><div class="card-body p-3 small">This paper presents a system developed during our participation (team name : scmhl5) in the TRAC-2 Shared Task on aggression identification. In particular, we participated in English Sub-task A on three-class classification (&#8216;Overtly Aggressive&#8217;, &#8216;Covertly Aggressive&#8217; and &#8216;Non-aggressive&#8217;) and English Sub-task B on binary classification for Misogynistic Aggression (&#8216;gendered&#8217; or &#8216;non-gendered&#8217;). For both sub-tasks, our method involves using the pre-trained Bert model for extracting the text of each instance into a 768-dimensional vector of embeddings, and then training an ensemble of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on the embedding features. Our method obtained <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.703 and <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F-measure</a> of 0.664 for Sub-task A, whereas for Sub-task B the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> was 0.869 and <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F-measure</a> was 0.851. In terms of the rankings, the weighted F-measure obtained using our method for Sub-task A is ranked in the 10th out of 16 teams, whereas for Sub-task B the weighted F-measure is ranked in the 8th out of 15 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.14/>Spyder : Aggression Detection on Multilingual Tweets<span class=acl-fixed-case>S</span>pyder: Aggression Detection on Multilingual Tweets</a></strong><br><a href=/people/a/anisha-datta/>Anisha Datta</a>
|
<a href=/people/s/shukrity-si/>Shukrity Si</a>
|
<a href=/people/u/urbi-chakraborty/>Urbi Chakraborty</a>
|
<a href=/people/s/sudip-kumar-naskar/>Sudip Kumar Naskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--14><div class="card-body p-3 small">In the last few years, <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and aggressive comments have covered almost all the social media platforms like <a href=https://en.wikipedia.org/wiki/Facebook>facebook</a>, <a href=https://en.wikipedia.org/wiki/Twitter>twitter</a> etc. As a result hatred is increasing. This paper describes our (Team name : Spyder) participation in the Shared Task on <a href=https://en.wikipedia.org/wiki/Aggression>Aggression Detection</a> organised by TRAC-2, Second Workshop on <a href=https://en.wikipedia.org/wiki/Internet_troll>Trolling</a>, <a href=https://en.wikipedia.org/wiki/Aggression>Aggression</a> and <a href=https://en.wikipedia.org/wiki/Cyberbullying>Cyberbullying</a>. The Organizers provided datasets in three languages <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>. The task was to classify each instance of the test sets into three categories Overtly Aggressive (OAG), Covertly Aggressive (CAG) and Non-Aggressive (NAG). In this paper, we propose three different models using Tf-Idf, sentiment polarity and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning based classifiers</a>. We obtained <a href=https://en.wikipedia.org/wiki/F-number>f1 score</a> of 43.10 %, 59.45 % and 44.84 % respectively for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>.<b>Team name:</b>\n <b>Spyder</b>) participation in the Shared Task on Aggression Detection organised by TRAC-2, Second Workshop on Trolling, Aggression and Cyberbullying. The Organizers provided datasets in three languages &#8211; English, Hindi and Bengali. The task was to classify each instance of the test sets into three categories &#8211; &#8220;Overtly Aggressive&#8221; (OAG), &#8220;Covertly Aggressive&#8221; (CAG) and &#8220;Non-Aggressive&#8221; (NAG). In this paper, we propose three different models using Tf-Idf, sentiment polarity and machine learning based classifiers. We obtained f1 score of 43.10%, 59.45% and 44.84% respectively for English, Hindi and Bengali.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.15/>BERT of all trades, master of some<span class=acl-fixed-case>BERT</span> of all trades, master of some</a></strong><br><a href=/people/d/denis-gordeev/>Denis Gordeev</a>
|
<a href=/people/o/olga-lykova/>Olga Lykova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--15><div class="card-body p-3 small">This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Our team name was Ms8qQxMbnjJMgYcw. The competition consisted of 2 subtasks in 3 languages (Bengali, English and Hindi) where the participants&#8217; task was to classify aggression in short texts from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and decide whether it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> placed first in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and second in Bengali gendered text classification competition tasks with 0.87 and 0.93 in F1-score respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.17/>FlorUniTo@TRAC-2 : Retrofitting Word Embeddings on an Abusive Lexicon for Aggressive Language Detection<span class=acl-fixed-case>F</span>lor<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>T</span>o@<span class=acl-fixed-case>TRAC</span>-2: Retrofitting Word Embeddings on an Abusive Lexicon for Aggressive Language Detection</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--17><div class="card-body p-3 small">This paper describes our participation to the TRAC-2 Shared Tasks on Aggression Identification. Our team, FlorUniTo, investigated the applicability of using an abusive lexicon to enhance word embeddings towards improving detection of aggressive language. The embeddings used in our paper are word-aligned pre-trained vectors for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, to reflect the languages in the shared task data sets. The embeddings are retrofitted to a multilingual abusive lexicon, HurtLex. We experimented with an LSTM model using the original as well as the transformed embeddings and different language and setting variations. Overall, our <a href=https://en.wikipedia.org/wiki/System>systems</a> placed toward the middle of the <a href=https://en.wikipedia.org/wiki/List_of_Formula_One_World_Drivers&#8217;_Champions>official rankings</a> based on <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1 score</a>. However, the results on the development and test sets show promising improvements across languages, especially on the misogynistic aggression sub-task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.trac-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.19/>Multilingual Joint Fine-tuning of Transformer models for identifying Trolling, Aggression and Cyberbullying at TRAC 2020<span class=acl-fixed-case>TRAC</span> 2020</a></strong><br><a href=/people/s/sudhanshu-mishra/>Sudhanshu Mishra</a>
|
<a href=/people/s/shivangi-prasad/>Shivangi Prasad</a>
|
<a href=/people/s/shubhanshu-mishra/>Shubhanshu Mishra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--19><div class="card-body p-3 small">We present our team &#8216;3Idiots&#8217; (referred as &#8216;sdhanshu&#8217; in the official rankings) approach for the Trolling, Aggression and Cyberbullying (TRAC) 2020 shared tasks. Our approach relies on fine-tuning various Transformer models on the different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We also investigated the utility of task label marginalization, joint label classification, and joint training on multilingual datasets as possible improvements to our models. Our team came second in English sub-task A, a close fourth in the English sub-task B and third in the remaining 4 sub-tasks. We find the multilingual joint training approach to be the best trade-off between computational efficiency of model deployment and <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s evaluation performance. We open source our approach at https://github.com/socialmediaie/TRAC2020.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.trac-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.20/>Aggression and Misogyny Detection using <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> : A Multi-Task Approach<span class=acl-fixed-case>BERT</span>: A Multi-Task Approach</a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/p/parth-patwa/>Parth Patwa</a>
|
<a href=/people/s/srinivas-pykl/>Srinivas PYKL</a>
|
<a href=/people/p/prerana-mukherjee/>Prerana Mukherjee</a>
|
<a href=/people/a/amitava-das/>Amitava Das</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--20><div class="card-body p-3 small">In recent times, the focus of the NLP community has increased towards offensive language, aggression, and hate-speech detection. This paper presents our system for TRAC-2 shared task on Aggression Identification (sub-task A) and Misogynistic Aggression Identification (sub-task B). The data for this shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is provided in three different languages-English, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>. Each data instance is annotated into one of the three aggression classes-Not Aggressive, Covertly Aggressive, Overtly Aggressive, as well as one of the two misogyny classes-Gendered and Non-Gendered. We propose an end-to-end neural model using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> on top of BERT that incorporates a multi-task learning paradigm to address both the sub-tasks simultaneously. Our team, na14, scored 0.8579 weighted F1-measure on the English sub-task B and secured 3rd rank out of 15 teams for the task. The code and the model weights are publicly available at https://github.com/NiloofarSafi/TRAC-2. Keywords : <a href=https://en.wikipedia.org/wiki/Aggression>Aggression</a>, <a href=https://en.wikipedia.org/wiki/Misogyny>Misogyny</a>, <a href=https://en.wikipedia.org/wiki/Abusive_language>Abusive Language</a>, Hate-Speech Detection, BERT, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a>, <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.trac-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--trac-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.trac-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.trac-1.24/>Lexicon-Enhancement of Embedding-based Approaches Towards the Detection of Abusive Language</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/j/jason-scott/>Jason Scott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--trac-1--24><div class="card-body p-3 small">Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focuses on detecting personal attacks in <a href=https://en.wikipedia.org/wiki/Online_chat>online conversations</a>. As previous research on this task has largely used <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> based on <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, we explore the use of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> to enhance embedding-based methods in an effort to see how these methods apply in the particular task of detecting personal attacks. The methods implemented and experimented with in this paper are quite different from each other, not only in the type of lexicons they use (sentiment or semantic), but also in the way they use the knowledge from the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, in order to construct or to change embeddings that are ultimately fed into the learning model. The sentiment lexicon approaches focus on integrating <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> (in the form of sentiment embeddings) into the <a href=https://en.wikipedia.org/wiki/Machine_learning>learning model</a>. The <a href=https://en.wikipedia.org/wiki/Semantic_lexicon>semantic lexicon approaches</a> focus on transforming the original <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> so that they better represent relationships extracted from a <a href=https://en.wikipedia.org/wiki/Semantic_lexicon>semantic lexicon</a>. Based on our experimental results, semantic lexicon methods are superior to the rest of the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> in this paper, with at least 4 % macro-averaged F1 improvement over the baseline.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>