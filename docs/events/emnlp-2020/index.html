<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020emnlp-main>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a>
<span class="badge badge-info align-middle ml-1">268&nbsp;papers</span></li><li><a class=align-middle href=#2020emnlp-demos>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2020emnlp-tutorials>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020alw-1>Proceedings of the Fourth Workshop on Online Abuse and Harms</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2020blackboxnlp-1>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2020clinicalnlp-1>Proceedings of the 3rd Clinical Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#2020cmcl-1>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020codi-1>Proceedings of the First Workshop on Computational Approaches to Discourse</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020deelio-1>Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020eval4nlp-1>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2020findings-emnlp>Findings of the Association for Computational Linguistics: EMNLP 2020</a>
<span class="badge badge-info align-middle ml-1">149&nbsp;papers</span></li><li><a class=align-middle href=#2020insights-1>Proceedings of the First Workshop on Insights from Negative Results in NLP</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020intexsempar-1>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020louhi-1>Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020nlpbt-1>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020nlpcovid19-2>Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#2020nlpcss-1>Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2020nlposs-1>Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020privatenlp-1>Proceedings of the Second Workshop on Privacy in NLP</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020scai-1>Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI)</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020sdp-1>Proceedings of the First Workshop on Scholarly Document Processing</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2020sigtyp-1>Proceedings of the Second Workshop on Computational Research in Linguistic Typology</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020splu-1>Proceedings of the Third International Workshop on Spatial Language Understanding</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2020spnlp-1>Proceedings of the Fourth Workshop on Structured Prediction for NLP</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020sustainlp-1>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#2020wmt-1>Proceedings of the Fifth Conference on Machine Translation</a>
<span class="badge badge-info align-middle ml-1">50&nbsp;papers</span></li><li><a class=align-middle href=#2020wnut-1>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a>
<span class="badge badge-info align-middle ml-1">30&nbsp;papers</span></li></ul></div></div><div id=2020emnlp-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2020.emnlp-main/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.0/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></strong><br><a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939172 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.3/>Quantitative argument summarization and beyond : Cross-domain key point analysis</a></strong><br><a href=/people/r/roy-bar-haim/>Roy Bar-Haim</a>
|
<a href=/people/y/yoav-kantor/>Yoav Kantor</a>
|
<a href=/people/l/lilach-eden/>Lilach Eden</a>
|
<a href=/people/r/roni-friedman/>Roni Friedman</a>
|
<a href=/people/d/dan-lahav/>Dan Lahav</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--3><div class="card-body p-3 small">When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> has traditionally focused on creating textual summaries, which lack this quantitative aspect. Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments. The current work advances key point analysis in two important respects : first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert. Second, we demonstrate that the applicability of key point analysis goes well beyond <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation data</a>. Using <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on publicly available argumentation datasets, we achieve promising results in two additional domains : <a href=https://en.wikipedia.org/wiki/Survey_methodology>municipal surveys</a> and <a href=https://en.wikipedia.org/wiki/User_review>user reviews</a>. An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.5/>BLEU might be Guilty but References are not Innocent<span class=acl-fixed-case>BLEU</span> might be Guilty but References are not Innocent</a></strong><br><a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/i/isaac-caswell/>Isaac Caswell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--5><div class="card-body p-3 small">The quality of automatic metrics for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to <a href=https://en.wikipedia.org/wiki/German_language>German</a>, but also for <a href=https://en.wikipedia.org/wiki/Back-translation>Back-translation</a> and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a> with all modern evaluation metrics we look at, including embedding-based methods. To complete this picture, we reveal that multi-reference BLEU does not improve the <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a> for high quality output, and present an alternative multi-reference formulation that is more effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938786 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.7/>Simulated multiple reference training improves low-resource machine translation</a></strong><br><a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--7><div class="card-body p-3 small">Many valid translations exist for a given sentence, yet <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a> of the reference sentence from a <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphraser</a> and training the MT model to predict the <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphraser&#8217;s distribution</a> over possible tokens. We demonstrate the effectiveness of <a href=https://en.wikipedia.org/wiki/Speech-language_pathology>SMRT</a> in low-resource settings when translating to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938798 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.8/>Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing</a></strong><br><a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--8><div class="card-body p-3 small">We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the <a href=https://en.wikipedia.org/wiki/Paraphraser>paraphraser</a> as a multilingual NMT system, treating <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> as a zero-shot translation task (e.g., <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>). This results in the <a href=https://en.wikipedia.org/wiki/Paraphraser>paraphraser</a>&#8217;s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is simple and intuitive, and does not require human judgements for training. Our single <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a> where the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> had no training data). We also explore using our model for the task of <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality estimation</a> as a metricconditioning on the source instead of the referenceand find that it significantly outperforms every submission to the WMT 2019 shared task on <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality estimation</a> in every language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.11.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939163 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.11/>Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering</a></strong><br><a href=/people/p/pratyay-banerjee/>Pratyay Banerjee</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--11><div class="card-body p-3 small">The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. We propose <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939179 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.12/>More Bang for Your Buck : Natural Perturbation for Robust Question Answering</a></strong><br><a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--12><div class="card-body p-3 small">Deep learning models for <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic tasks</a> require <a href=https://en.wikipedia.org/wiki/Big_data>large training datasets</a>, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Such <a href=https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)>perturbations</a> have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. Further, they help address the issue that even <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input. To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create (cost ratio under 60 %), it is more effective to use them for training BOOLQ models : such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> exhibit 9 % higher robustness and 4.5 % stronger <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>, while retaining performance on the original BOOLQ dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938809 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.14/>Information-Theoretic Probing with Minimum Description Length</a></strong><br><a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--14><div class="card-body p-3 small">To measure how well pretrained representations encode some linguistic property, it is common to use <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a probe, i.e. a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained to predict the property from the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a>. Despite widespread adoption of <a href=https://en.wikipedia.org/wiki/Sensor>probes</a>, differences in their <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> fail to adequately reflect differences in <a href=https://en.wikipedia.org/wiki/Mental_representation>representations</a>. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates the amount of effort needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines : variational coding and online coding. We show that these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> agree in results and are more informative and stable than the standard probes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938801 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.17/>Repulsive Attention : Rethinking Multi-head Attention as <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian Inference</a><span class=acl-fixed-case>B</span>ayesian Inference</a></strong><br><a href=/people/b/bang-an/>Bang An</a>
|
<a href=/people/j/jie-lyu/>Jie Lyu</a>
|
<a href=/people/z/zhenyi-wang/>Zhenyi Wang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/c/changwei-hu/>Changwei Hu</a>
|
<a href=/people/f/fei-tan/>Fei Tan</a>
|
<a href=/people/r/ruiyi-zhang/>Ruiyi Zhang</a>
|
<a href=/people/y/yifan-hu/>Yifan Hu</a>
|
<a href=/people/c/changyou-chen/>Changyou Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--17><div class="card-body p-3 small">The neural attention mechanism plays an important role in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. In particular, multi-head attention extends single-head attention by allowing a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model&#8217;s representation power</a>. In this paper, for the first time, we provide a novel understanding of multi-head attention from a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian perspective</a>. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s expressiveness</a>. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions : why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938864 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.18/>KERMIT : Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations<span class=acl-fixed-case>KERMIT</span>: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations</a></strong><br><a href=/people/f/fabio-massimo-zanzotto/>Fabio Massimo Zanzotto</a>
|
<a href=/people/a/andrea-santilli/>Andrea Santilli</a>
|
<a href=/people/l/leonardo-ranaldi/>Leonardo Ranaldi</a>
|
<a href=/people/d/dario-onorati/>Dario Onorati</a>
|
<a href=/people/p/pierfrancesco-tommasino/>Pierfrancesco Tommasino</a>
|
<a href=/people/f/francesca-fallucchi/>Francesca Fallucchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--18><div class="card-body p-3 small">Syntactic parsers have dominated <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> is used in <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938866 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.26/>Incremental Processing in the Age of Non-Incremental Encoders : An Empirical Assessment of Bidirectional Models for Incremental NLU<span class=acl-fixed-case>NLU</span></a></strong><br><a href=/people/b/brielen-madureira/>Brielen Madureira</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--26><div class="card-body p-3 small">While humans process language incrementally, the best <a href=https://en.wikipedia.org/wiki/Encoder>language encoders</a> currently used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The omni-directional BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> like GPT-2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938970 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.28/>Dialogue Response Ranking Training with Large-Scale Human Feedback Data</a></strong><br><a href=/people/x/xiang-gao/>Xiang Gao</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--28><div class="card-body p-3 small">Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133 M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939351 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.31/>AutoQA : From Databases To QA Semantic Parsers With Only Synthetic Training Data<span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>QA</span>: From Databases To <span class=acl-fixed-case>QA</span> Semantic Parsers With Only Synthetic Training Data</a></strong><br><a href=/people/s/silei-xu/>Silei Xu</a>
|
<a href=/people/s/sina-semnani/>Sina Semnani</a>
|
<a href=/people/g/giovanni-campagna/>Giovanni Campagna</a>
|
<a href=/people/m/monica-lam/>Monica Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--31><div class="card-body p-3 small">We propose AutoQA, a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a <a href=https://en.wikipedia.org/wiki/Database_schema>database schema</a> and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses <a href=https://en.wikipedia.org/wiki/Automatic_paraphrasing>automatic paraphrasing</a> combined with template-based parsing to find alternative expressions of an attribute in different <a href=https://en.wikipedia.org/wiki/Part_of_speech>parts of speech</a>. It also uses a novel filtered auto-paraphraser to generate correct <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> of entire sentences. We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9 % when tested on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural questions</a>, which is only 6.4 % lower than a model trained with expert natural language annotations and paraphrase data collected from <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdworkers</a>. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>answer accuracy</a>, 16.4 % higher than the state-of-the-art zero-shot models and only 5.2 % lower than the same <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained with human data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938815 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.33/>What Have We Achieved on Text Summarization?</a></strong><br><a href=/people/d/dandan-huang/>Dandan Huang</a>
|
<a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/s/sen-yang/>Sen Yang</a>
|
<a href=/people/g/guangsheng-bao/>Guangsheng Bao</a>
|
<a href=/people/k/kun-wang/>Kun Wang</a>
|
<a href=/people/j/jun-xie/>Jun Xie</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--33><div class="card-body p-3 small">Deep learning has led to significant improvement in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by <a href=https://en.wikipedia.org/wiki/Automatic_summarization>automatic summarizers</a> and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency ; 2) milestone techniques such as copy, coverage and hybrid extractive / abstractive methods do bring specific improvements but also demonstrate limitations ; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938716 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.34/>Q-learning with <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> for Edit-based Unsupervised Summarization<span class=acl-fixed-case>Q</span>-learning with Language Model for Edit-based Unsupervised Summarization</a></strong><br><a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/a/akifumi-wachi/>Akifumi Wachi</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/r/ryuki-tachibana/>Ryuki Tachibana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--34><div class="card-body p-3 small">Unsupervised methods are promising for abstractive textsummarization in that the <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> with an edit-based summarization. The method combines two key <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> to form an Editorial Agent and Language Model converter (EALM). The <a href=https://en.wikipedia.org/wiki/Agency_(philosophy)>agent</a> predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> enables us not only to develop a competitive method but also to make the latest techniques in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> available for unsupervised summarization. We also conduct <a href=https://en.wikipedia.org/wiki/Qualitative_research>qualitative analysis</a>, providing insights into future study on unsupervised summarizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939194 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.37/>TernaryBERT : Distillation-aware Ultra-low Bit BERT<span class=acl-fixed-case>T</span>ernary<span class=acl-fixed-case>BERT</span>: Distillation-aware Ultra-low Bit <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/l/lu-hou/>Lu Hou</a>
|
<a href=/people/y/yichun-yin/>Yichun Yin</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--37><div class="card-body p-3 small">Transformer-based pre-training models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have achieved remarkable performance in many natural language processing tasks. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939198 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.38/>Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks</a></strong><br><a href=/people/t/trapit-bansal/>Trapit Bansal</a>
|
<a href=/people/r/rishikesh-jha/>Rishikesh Jha</a>
|
<a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--38><div class="card-body p-3 small">Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. However, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is still data inefficient when there are few labeled examples, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> can be low. Data efficiency can be improved by optimizing pre-training directly for future <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> with few examples ; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize ; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>. Furthermore, we show how the self-supervised tasks can be combined with <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised tasks</a> for <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>, providing substantial accuracy gains over previous supervised meta-learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939206 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.39/>Efficient Meta Lifelong-Learning with Limited Memory</a></strong><br><a href=/people/z/zirui-wang/>Zirui Wang</a>
|
<a href=/people/s/sanket-vaibhav-mehta/>Sanket Vaibhav Mehta</a>
|
<a href=/people/b/barnabas-poczos/>Barnabas Poczos</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--39><div class="card-body p-3 small">Current <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language processing models</a> work well on a single <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as <a href=https://en.wikipedia.org/wiki/Lifelong_learning>lifelong learning</a>. State-of-the-art lifelong language learning methods store past examples in <a href=https://en.wikipedia.org/wiki/Episodic_memory>episodic memory</a> and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments : (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1 % memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and <a href=https://en.wikipedia.org/wiki/Negative_transfer>negative transfer</a> at the same time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938787 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.40/>Do n’t Use English Dev : On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings<span class=acl-fixed-case>E</span>nglish Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings</a></strong><br><a href=/people/p/phillip-keung/>Phillip Keung</a>
|
<a href=/people/y/yichao-lu/>Yichao Lu</a>
|
<a href=/people/j/julian-salazar/>Julian Salazar</a>
|
<a href=/people/v/vikas-bhardwaj/>Vikas Bhardwaj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--40><div class="card-body p-3 small">Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results : still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938923 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.41/>A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/k/katsuki-chousa/>Katsuki Chousa</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--41><div class="card-body p-3 small">We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token&#8217;s context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938782 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.44" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.44/>Unsupervised Discovery of Implicit Gender Bias</a></strong><br><a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--44><div class="card-body p-3 small">Despite their prevalence in society, social biases are difficult to identify, primarily because <a href=https://en.wikipedia.org/wiki/Judgement>human judgements</a> in this domain can be unreliable. We take an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> to identifying gender bias against women at a comment level and present a model that can surface text likely to contain <a href=https://en.wikipedia.org/wiki/Bias>bias</a>. Our main challenge is forcing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to focus on signs of <a href=https://en.wikipedia.org/wiki/Implicit_stereotype>implicit bias</a>, rather than other artifacts in the data. Thus, our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> involves reducing the influence of <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a> through propensity matching and <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a>. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on <a href=https://en.wikipedia.org/wiki/Human_physical_appearance>appearance</a> and <a href=https://en.wikipedia.org/wiki/Sexualization>sexualization</a>. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939143 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.45/>Condolence and Empathy in <a href=https://en.wikipedia.org/wiki/Online_community>Online Communities</a></a></strong><br><a href=/people/n/naitian-zhou/>Naitian Zhou</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--45><div class="card-body p-3 small">Offering condolence is a natural reaction to hearing someone&#8217;s distress. Individuals frequently express distress in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, where some communities can provide support. However, not all <a href=https://en.wikipedia.org/wiki/Condolence>condolence</a> is equaltrite responses offer little actual support despite their good intentions. Here, we develop <a href=https://en.wikipedia.org/wiki/Computer>computational tools</a> to create a massive <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 11.4 M expressions of distress and 2.8 M corresponding offerings of <a href=https://en.wikipedia.org/wiki/Condolences>condolence</a> in order to examine the dynamics of <a href=https://en.wikipedia.org/wiki/Condolences>condolence</a> online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from <a href=https://en.wikipedia.org/wiki/Social_psychology>social psychology</a>, we analyze the language of condolence and develop a new dataset for quantifying the <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> in a <a href=https://en.wikipedia.org/wiki/Condolences>condolence</a> using appraisal theory. Finally, we demonstrate that the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> of condolence individuals find most helpful online differ substantially in their <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from those seen in <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>interpersonal settings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938649 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.49" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.49/>Event Extraction by Answering (Almost) Natural Questions</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--49><div class="card-body p-3 small">The problem of <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity recognition</a> as a preprocessing / concurrent step, causing the well-known problem of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. To avoid this issue, we introduce a new <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> for <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms <a href=https://en.wikipedia.org/wiki/Prior_probability>prior methods</a> substantially ; in addition, it is capable of extracting <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event arguments</a> for roles not seen at training time (i.e., in a zero-shot learning setting).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938669 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.50/>Connecting the Dots : Event Graph Schema Induction with Path Language Modeling</a></strong><br><a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--50><div class="card-body p-3 small">Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938847 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.51/>Joint Constrained Learning for Event-Event Relation Extraction</a></strong><br><a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--51><div class="card-body p-3 small">Understanding <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939118 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.53/>Semi-supervised New Event Type Induction and Event Detection</a></strong><br><a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--53><div class="card-body p-3 small">Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types. We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations. A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution. Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938839 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.60/>Learning to Represent Image and Text with Denotation Graph</a></strong><br><a href=/people/b/bowen-zhang/>Bowen Zhang</a>
|
<a href=/people/h/hexiang-hu/>Hexiang Hu</a>
|
<a href=/people/v/vihan-jain/>Vihan Jain</a>
|
<a href=/people/e/eugene-ie/>Eugene Ie</a>
|
<a href=/people/f/fei-sha/>Fei Sha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--60><div class="card-body p-3 small">Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> aligned with linguistic expressions that describe the <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>. In this paper, we propose learning representations from a set of implied, visually grounded expressions between <a href=https://en.wikipedia.org/wiki/Image>image</a> and text, automatically mined from those <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In particular, we use denotation graphs to represent how specific <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>linguistic analysis tools</a>. We propose <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to incorporate such <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> into <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>learning representation</a>. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the <a href=https://en.wikipedia.org/wiki/Flickr>Flickr30 K</a> and the COCO datasets are publically available on https://sha-lab.github.io/DG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.62.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938964 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.62/>Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think !</a></strong><br><a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--62><div class="card-body p-3 small">Modeling expressive cross-modal interactions seems crucial in <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal tasks</a>, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>function projection</a> modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models ; thus, performance improvements, even when present, often can not be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939282 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.63" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.63/>MUTANT : A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering<span class=acl-fixed-case>MUTANT</span>: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering</a></strong><br><a href=/people/t/tejas-gokhale/>Tejas Gokhale</a>
|
<a href=/people/p/pratyay-banerjee/>Pratyay Banerjee</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a>
|
<a href=/people/y/yezhou-yang/>Yezhou Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--63><div class="card-body p-3 small">While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and <a href=https://en.wikipedia.org/wiki/Prior_probability>priors</a> in <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on VQA-CP with a 10.57 % improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.<i>MUTANT</i>, a training paradigm that exposes the model to perceptually similar, yet semantically distinct <i>mutations</i> of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, <i>MUTANT</i> does not rely on the knowledge about the nature of train and test answer distributions. <i>MUTANT</i> establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938861 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.66/>TOD-BERT : Pre-trained Natural Language Understanding for Task-Oriented Dialogue<span class=acl-fixed-case>TOD</span>-<span class=acl-fixed-case>BERT</span>: Pre-trained Natural Language Understanding for Task-Oriented Dialogue</a></strong><br><a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--66><div class="card-body p-3 small">The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938993 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.69/>Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact</a></strong><br><a href=/people/y/yugo-murawaki/>Yugo Murawaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--69><div class="card-body p-3 small">Analyzing the evolution of dialects remains a challenging problem because <a href=https://en.wikipedia.org/wiki/Language_contact>contact phenomena</a> hinder the application of the standard <a href=https://en.wikipedia.org/wiki/Tree_model>tree model</a>. Previous statistical approaches to this problem resort to <a href=https://en.wikipedia.org/wiki/Genetic_admixture>admixture analysis</a>, where each dialect is seen as a mixture of latent ancestral populations. However, such ancestral populations are hardly interpretable in the context of the <a href=https://en.wikipedia.org/wiki/Tree_model>tree model</a>. In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions. We argue that the proposed model has higher affinity with the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree model</a> because a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> can alternatively be represented as a set of geographical distributions. Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938966 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.75/>Multi-task Learning for Multilingual Neural Machine Translation</a></strong><br><a href=/people/y/yiren-wang/>Yiren Wang</a>
|
<a href=/people/c/chengxiang-zhai/>ChengXiang Zhai</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--75><div class="card-body p-3 small">While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks ; the proposed approach outperforms massive scale models trained on single task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939237 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.86/>IIRC : A Dataset of Incomplete Information Reading Comprehension Questions<span class=acl-fixed-case>IIRC</span>: A Dataset of Incomplete Information Reading Comprehension Questions</a></strong><br><a href=/people/j/james-ferguson/>James Ferguson</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--86><div class="card-body p-3 small">Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13 K questions over paragraphs from <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We follow recent modeling work on various reading comprehension datasets to construct a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a> for this dataset, finding that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves 31.1 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, while estimated <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a> is 88.4 %. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.89.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938835 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.89" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.89/>ToTTo : A Controlled Table-To-Text Generation Dataset<span class=acl-fixed-case>ToTTo</span>: A Controlled Table-To-Text Generation Dataset</a></strong><br><a href=/people/a/ankur-parikh/>Ankur Parikh</a>
|
<a href=/people/x/xuezhi-wang/>Xuezhi Wang</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/d/dipanjan-das/>Dipanjan Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--89><div class="card-body p-3 small">We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task : given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We present systematic analyses of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938972 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.90/>ENT-DESC : Entity Description Generation by Exploring Knowledge Graph<span class=acl-fixed-case>ENT</span>-<span class=acl-fixed-case>DESC</span>: Entity Description Generation by Exploring Knowledge Graph</a></strong><br><a href=/people/l/liying-cheng/>Liying Cheng</a>
|
<a href=/people/d/dekun-wu/>Dekun Wu</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/z/zhanming-jie/>Zhanming Jie</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--90><div class="card-body p-3 small">Previous works on knowledge-to-text generation take as input a few <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF triples</a> or key-value pairs conveying the knowledge of some entities to generate a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language description</a>. Existing datasets, such as WIKIBIO, WebNLG, and <a href=https://en.wikipedia.org/wiki/E2E>E2E</a>, basically have a good alignment between an input triple / pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of <a href=https://en.wikipedia.org/wiki/Information_loss>information loss</a> and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph information</a> more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939115 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.92" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.92/>Online Back-Parsing for AMR-to-Text Generation<span class=acl-fixed-case>AMR</span>-to-Text Generation</a></strong><br><a href=/people/x/xuefeng-bai/>Xuefeng Bai</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--92><div class="card-body p-3 small">AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard <a href=https://en.wikipedia.org/wiki/Code>decoders</a>. Experiments on two AMR benchmarks show the superiority of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/System>system</a> based on graph Transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939186 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.93/>Reading Between the Lines : Exploring Infilling in Visual Narratives</a></strong><br><a href=/people/k/khyathi-raghavi-chandu/>Khyathi Raghavi Chandu</a>
|
<a href=/people/r/ruo-ping-dong/>Ruo-Ping Dong</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--93><div class="card-body p-3 small">Generating long form narratives such as <a href=https://en.wikipedia.org/wiki/Narrative>stories</a> and procedures from multiple modalities has been a long standing dream for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on <a href=https://en.wikipedia.org/wiki/Procedure_code>procedures</a> which is higher than the state-of-the-art on <a href=https://en.wikipedia.org/wiki/Visual_storytelling>visual storytelling</a>. We also demonstrate the effects of interposing new text with missing images during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. The code and the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938805 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.94/>Acrostic Poem Generation</a></strong><br><a href=/people/r/rajat-agarwal/>Rajat Agarwal</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--94><div class="card-body p-3 small">We propose a new task in the area of <a href=https://en.wikipedia.org/wiki/Computational_creativity>computational creativity</a> : acrostic poem generation in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Acrostic poems are <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> that contain a <a href=https://en.wikipedia.org/wiki/Hidden_message>hidden message</a> ; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints : given an input word, 1) the initial letters of each line should spell out the provided word, 2) the <a href=https://en.wikipedia.org/wiki/Poetry>poem&#8217;s semantics</a> should also relate to it, and 3) the <a href=https://en.wikipedia.org/wiki/Poetry>poem</a> should conform to a <a href=https://en.wikipedia.org/wiki/Rhyme_scheme>rhyming scheme</a>. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional <a href=https://en.wikipedia.org/wiki/Poetry>poems</a>. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> generated by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> are indeed closely related to the provided prompts, and that pretraining on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> can boost performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938834 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.95/>Local Additivity Based Data Augmentation for Semi-supervised NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/z/zhenghui-wang/>Zhenghui Wang</a>
|
<a href=/people/r/ran-tian/>Ran Tian</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--95><div class="card-body p-3 small">Named Entity Recognition (NER) is one of the first stages in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep language understanding</a> yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations : Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between <a href=https://en.wikipedia.org/wiki/Sampling_(signal_processing)>sampled training data</a>, LADA creates an infinite amount of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. We have publicly released our code at https://github.com/GT-SALT/LADA</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938850 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.96" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.96/>Grounded Compositional Outputs for Adaptive Language Modeling</a></strong><br><a href=/people/n/nikolaos-pappas/>Nikolaos Pappas</a>
|
<a href=/people/p/phoebe-mulcaire/>Phoebe Mulcaire</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--96><div class="card-body p-3 small">Language models have emerged as a central component across <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, and a great deal of progress depends on the ability to cheaply adapt <a href=https://en.wikipedia.org/wiki/Natural_language_processing>them</a> (e.g., through finetuning) to new domains and tasks. A <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>&#8217;s vocabularytypically selected before training and permanently fixed lateraffects its size and is part of what makes it resistant to such <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a>. Prior work has used compositional input embeddings based on <a href=https://en.wikipedia.org/wiki/Surface_(mathematics)>surface forms</a> to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency : our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more accurate for <a href=https://en.wikipedia.org/wiki/Word_frequency>low-frequency words</a>.<i>vocabulary</i>&#8212;typically selected before training and permanently fixed later&#8212;affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.98/>SetConv : A New Approach for Learning from Imbalanced Data<span class=acl-fixed-case>S</span>et<span class=acl-fixed-case>C</span>onv: <span class=acl-fixed-case>A</span> <span class=acl-fixed-case>N</span>ew <span class=acl-fixed-case>A</span>pproach for <span class=acl-fixed-case>L</span>earning from <span class=acl-fixed-case>I</span>mbalanced <span class=acl-fixed-case>D</span>ata</a></strong><br><a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/y/yi-fan-li/>Yi-Fan Li</a>
|
<a href=/people/y/yu-lin/>Yu Lin</a>
|
<a href=/people/c/charu-aggarwal/>Charu Aggarwal</a>
|
<a href=/people/l/latifur-khan/>Latifur Khan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--98><div class="card-body p-3 small">For many real-world classification problems, e.g., sentiment classification, most existing <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> can later be trained on a balanced class distribution. We prove that our proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939203 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.100/>Improving Bilingual Lexicon Induction for Low Frequency Words</a></strong><br><a href=/people/j/jiaji-huang/>Jiaji Huang</a>
|
<a href=/people/x/xingyu-cai/>Xingyu Cai</a>
|
<a href=/people/k/kenneth-church/>Kenneth Church</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--100><div class="card-body p-3 small">This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in <a href=https://en.wikipedia.org/wiki/Low_frequency>low frequency regime</a>, and secondly, exacerbated hubness at <a href=https://en.wikipedia.org/wiki/Low_frequency>low frequency</a>. Based on the observation, we further propose two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to address these two factors, respectively. The larger issue is hubness. Addressing that improves <a href=https://en.wikipedia.org/wiki/Inductive_reasoning>induction accuracy</a> significantly, especially for low-frequency words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939213 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.101/>Learning VAE-LDA Models with Rounded Reparameterization Trick<span class=acl-fixed-case>VAE</span>-<span class=acl-fixed-case>LDA</span> Models with Rounded Reparameterization Trick</a></strong><br><a href=/people/r/runzhi-tian/>Runzhi Tian</a>
|
<a href=/people/y/yongyi-mao/>Yongyi Mao</a>
|
<a href=/people/r/richong-zhang/>Richong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--101><div class="card-body p-3 small">The introduction of VAE provides an efficient <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for the learning of generative models, including generative topic models. However, when the <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a> is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize <a href=https://en.wikipedia.org/wiki/Dirichlet_distribution>Dirichlet distributions</a> for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940160 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.103/>Scaling Hidden Markov Language Models<span class=acl-fixed-case>M</span>arkov Language Models</a></strong><br><a href=/people/j/justin-chiu/>Justin Chiu</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--103><div class="card-body p-3 small">The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the <a href=https://en.wikipedia.org/wiki/Emission_spectrum>emission structure</a>. However, this separation makes it difficult to fit HMMs to large datasets in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient <a href=https://en.wikipedia.org/wiki/Exact_inference>exact inference</a>, a compact parameterization, and effective <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>. Experiments show that this approach leads to <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939325 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.104/>Coding Textual Inputs Boosts the Accuracy of <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/a/abdul-rafae-khan/>Abdul Rafae Khan</a>
|
<a href=/people/j/jia-xu/>Jia Xu</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--104><div class="card-body p-3 small">Natural Language Processing (NLP) tasks are usually performed word by word on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual inputs</a>. We can use arbitrary <a href=https://en.wikipedia.org/wiki/Symbol_(formal)>symbols</a> to represent the linguistic meaning of a word and use these <a href=https://en.wikipedia.org/wiki/Symbol_(formal)>symbols</a> as inputs. As alternatives to a text representation, we introduce <a href=https://en.wikipedia.org/wiki/Soundex>Soundex</a>, MetaPhone, <a href=https://en.wikipedia.org/wiki/NYSIIS>NYSIIS</a>, <a href=https://en.wikipedia.org/wiki/Logogram>logogram</a> to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, and develop fixed-output-length coding and its extension using <a href=https://en.wikipedia.org/wiki/Huffman_coding>Huffman coding</a>. Each of those <a href=https://en.wikipedia.org/wiki/Code_name>codings</a> combines different character / digital sequences and constructs a new <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> based on <a href=https://en.wikipedia.org/wiki/Code_name>codewords</a>. We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs. Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. The source code is available at https://github.com/abdulrafae/coding_nmt.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939305 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.107" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.107/>Named Entity Recognition for Social Media Texts with Semantic Augmentation</a></strong><br><a href=/people/y/yuyang-nie/>Yuyang Nie</a>
|
<a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/x/xiang-wan/>Xiang Wan</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/b/bo-dai/>Bo Dai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--107><div class="card-body p-3 small">Existing approaches for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> suffer from data sparsity problems when conducted on short and informal texts, especially <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated social media content</a>. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account. In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such <a href=https://en.wikipedia.org/wiki/Information>information</a>, respectively. Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939330 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.108/>Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations</a></strong><br><a href=/people/j/jianfei-yu/>Jianfei Yu</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/l/ling-min-serena-khoo/>Ling Min Serena Khoo</a>
|
<a href=/people/h/hai-leong-chieu/>Hai Leong Chieu</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--108><div class="card-body p-3 small">The prevalent use of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task. Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads. We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively. Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939332 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.109/>Social Media Attributions in the Context of Water Crisis</a></strong><br><a href=/people/r/rupak-sarkar/>Rupak Sarkar</a>
|
<a href=/people/s/sayantan-mahinder/>Sayantan Mahinder</a>
|
<a href=/people/h/hirak-sarkar/>Hirak Sarkar</a>
|
<a href=/people/a/ashiqur-khudabukhsh/>Ashiqur KhudaBukhsh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--109><div class="card-body p-3 small">Attribution of natural disasters / <a href=https://en.wikipedia.org/wiki/Collective_responsibility>collective misfortune</a> is a widely-studied political science problem. However, such <a href=https://en.wikipedia.org/wiki/Research>studies</a> typically rely on <a href=https://en.wikipedia.org/wiki/Survey_methodology>surveys</a>, or <a href=https://en.wikipedia.org/wiki/Expert_witness>expert opinions</a>, or external signals such as <a href=https://en.wikipedia.org/wiki/Opinion_poll>voting outcomes</a>. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>attribution factors</a>. We present a novel prediction task of attribution tie detection of identifying the <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>factors</a> (e.g., <a href=https://en.wikipedia.org/wiki/Urban_planning>poor city planning</a>, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> constructed from <a href=https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos>YouTube comments</a> (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>attribution ties</a> that achieves a reasonable performance (accuracy : 87.34 % on <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>attribution detection</a> and 81.37 % on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.<i>attribution tie detection</i> of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34% on attribution detection and 81.37% on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938774 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.110/>On the Reliability and Validity of Detecting Approval of Political Actors in Tweets</a></strong><br><a href=/people/i/indira-sen/>Indira Sen</a>
|
<a href=/people/f/fabian-flock/>Fabian Flöck</a>
|
<a href=/people/c/claudia-wagner/>Claudia Wagner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--110><div class="card-body p-3 small">Social media sites like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> possess the potential to complement surveys that measure political opinions and, more specifically, political actors&#8217; approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users&#8217; political opinions based on their content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors&#8217; approval by benchmarking them across several datasets. We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on minimal custom data. We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust. Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians&#8217; approval from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939008 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.114/>Predicting Clinical Trial Results by Implicit Evidence Integration</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/c/chuanqi-tan/>Chuanqi Tan</a>
|
<a href=/people/m/mosha-chen/>Mosha Chen</a>
|
<a href=/people/x/xiaozhong-liu/>Xiaozhong Liu</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--114><div class="card-body p-3 small">Clinical trials provide essential guidance for practicing <a href=https://en.wikipedia.org/wiki/Evidence-based_medicine>Evidence-Based Medicine</a>, though often accompanying with unendurable costs and risks. To optimize the design of <a href=https://en.wikipedia.org/wiki/Clinical_trial>clinical trials</a>, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> that implicitly contain PICOs and results as evidence. Specifically, we pre-train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to predict the disentangled results from such implicit evidence and fine-tune the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the baselines by large margins, e.g., with a 10.7 % relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> composed of <a href=https://en.wikipedia.org/wiki/Clinical_trial>clinical trials</a> related to COVID-19.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.115.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939013 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.115/>Explainable Clinical Decision Support from Text</a></strong><br><a href=/people/j/jinyue-feng/>Jinyue Feng</a>
|
<a href=/people/c/chantal-shaib/>Chantal Shaib</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--115><div class="card-body p-3 small">Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> usability in a clinical decision support context. From domain-expert evaluations, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates informative rationales that have promising real-life applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939383 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.116/>A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization<span class=acl-fixed-case>C</span>hinese Medical Procedure Entity Normalization</a></strong><br><a href=/people/j/jinghui-yan/>Jinghui Yan</a>
|
<a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/l/lu-xiang/>Lu Xiang</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--116><div class="card-body p-3 small">Medical entity normalization, which links medical mentions in the text to entities in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. The existing <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> relying on the <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies : category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938643 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.117/>Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/akshay-smit/>Akshay Smit</a>
|
<a href=/people/s/saahil-jain/>Saahil Jain</a>
|
<a href=/people/p/pranav-rajpurkar/>Pranav Rajpurkar</a>
|
<a href=/people/a/anuj-pareek/>Anuj Pareek</a>
|
<a href=/people/a/andrew-y-ng/>Andrew Ng</a>
|
<a href=/people/m/matthew-lungren/>Matthew Lungren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--117><div class="card-body p-3 small">The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance</a>, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938878 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.118/>Benchmarking Meaning Representations in Neural Semantic Parsing</a></strong><br><a href=/people/j/jiaqi-guo/>Jiaqi Guo</a>
|
<a href=/people/q/qian-liu/>Qian Liu</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/z/zhenwen-li/>Zhenwen Li</a>
|
<a href=/people/x/xueqing-liu/>Xueqing Liu</a>
|
<a href=/people/t/tao-xie/>Tao Xie</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--118><div class="card-body p-3 small">Meaning representation is an important component of <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(M)>them</a>. Thus, the impact of meaning representation on <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> is less understood. Furthermore, existing work&#8217;s performance is often not comprehensively evaluated due to the lack of readily-available execution engines. Upon identifying these gaps, we propose, a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a> and <a href=https://en.wikipedia.org/wiki/Execution_(computing)>execution engines</a> over three datasets four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar rules</a> heavily impact the performance of different <a href=https://en.wikipedia.org/wiki/Semantics>meaning representations</a>. Our <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, execution engines and implementation can be found on : https://github.com/JasperGuo/Unimer.<tex-math>\\times</tex-math> four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github.com/JasperGuo/Unimer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938902 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.119/>Analogous Process Structure Induction for Sub-event Sequence Prediction</a></strong><br><a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--119><div class="card-body p-3 small">Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as buying a car can be used in the context of a new but analogous process such as buying a house. Nevertheless, most event understanding work in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is still at the ground level and does not consider <a href=https://en.wikipedia.org/wiki/Abstraction>abstraction</a>. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938691 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.123/>Semantically Inspired AMR Alignment for the <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese Language</a><span class=acl-fixed-case>AMR</span> Alignment for the <span class=acl-fixed-case>P</span>ortuguese Language</a></strong><br><a href=/people/r/rafael-anchieta/>Rafael Anchiêta</a>
|
<a href=/people/t/thiago-pardo/>Thiago Pardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--123><div class="card-body p-3 small">Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> are concepts and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> are relations among them. Most of AMR parsing methods require alignment between the nodes of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and the words of the sentence. However, this alignment is not provided by manual annotations and available automatic aligners focus only on the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, not performing well for other languages. Aiming to fulfill this gap, we developed an alignment method for the <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese language</a> based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, improving AMR parsers, and achieving competitive results with a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> designed for the <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938841 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.126/>Table Fact Verification with Structure-Aware Transformer</a></strong><br><a href=/people/h/hongzhi-zhang/>Hongzhi Zhang</a>
|
<a href=/people/y/yingyao-wang/>Yingyao Wang</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/x/xuezhi-cao/>Xuezhi Cao</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/z/zhongyuan-wang/>Zhongyuan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--126><div class="card-body p-3 small">Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform <a href=https://en.wikipedia.org/wiki/Symbolic_reasoning>symbolic reasoning</a>. Pre-trained language models trained on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to combine symbolic and linguistic reasoning is also explored for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our method outperforms <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> with 4.93 % on TabFact, a large scale table verification dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938659 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.127/>Double Graph Based Reasoning for Document-level Relation Extraction</a></strong><br><a href=/people/s/shuang-zeng/>Shuang Zeng</a>
|
<a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--127><div class="card-body p-3 small">Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. Our code is available at https://github.com/PKUnlp-icler/GAIN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939215 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.130/>Knowledge Graph Alignment with Entity-Pair Embedding</a></strong><br><a href=/people/z/zhichun-wang/>Zhichun Wang</a>
|
<a href=/people/j/jinjian-yang/>Jinjian Yang</a>
|
<a href=/people/x/xiaoju-ye/>Xiaoju Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--130><div class="card-body p-3 small">Knowledge Graph (KG) alignment is to match entities in different <a href=https://en.wikipedia.org/wiki/Knowledge_Graph>KGs</a>, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> in low-dimensional vector spaces, and then obtain entity alignments by computations on their <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a>. Although continuous improvements have been achieved by recent work, the performances of existing <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> are still not satisfactory. In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment. Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs ; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities. To get desirable <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> is used to generate similarity features of entity-pairs from their attributes ; and a graph neural network is employed to propagate the similarity features and get the final <a href=https://en.wikipedia.org/wiki/Embedding>embeddings of entity-pairs</a>. Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939015 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.134/>Beyond [ CLS ] through Ranking by Generation<span class=acl-fixed-case>CLS</span>] through Ranking by Generation</a></strong><br><a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/x/xiaofei-ma/>Xiaofei Ma</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/z/zhiheng-huang/>Zhiheng Huang</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--134><div class="card-body p-3 small">Generative models for <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a>, where ranking of documents is viewed as the task of generating a query from a document&#8217;s language model, were very successful in various IR tasks in the past. However, with the advent of modern <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938676 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.136/>Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning</a></strong><br><a href=/people/y/yuning-mao/>Yuning Mao</a>
|
<a href=/people/y/yanru-qu/>Yanru Qu</a>
|
<a href=/people/y/yiqing-xie/>Yiqing Xie</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--136><div class="card-body p-3 small">While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization (MDS)</a>. We observe two major challenges when adapting SDS advances to MDS : (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations ; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Additionally, the explicit <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>redundancy measure</a> in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939229 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.137/>Improving Neural Topic Models using Knowledge Distillation<span class=acl-fixed-case>I</span>mproving <span class=acl-fixed-case>N</span>eural <span class=acl-fixed-case>T</span>opic <span class=acl-fixed-case>M</span>odels using <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>D</span>istillation</a></strong><br><a href=/people/a/alexander-miserlis-hoyle/>Alexander Miserlis Hoyle</a>
|
<a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--137><div class="card-body p-3 small">Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.140.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938750 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.140/>Incorporating <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>Multimodal Information</a> in Open-Domain Web Keyphrase Extraction</a></strong><br><a href=/people/y/yansen-wang/>Yansen Wang</a>
|
<a href=/people/z/zhen-fan/>Zhen Fan</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--140><div class="card-body p-3 small">Open-domain Keyphrase extraction (KPE) on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a> is a fundamental yet complex NLP task with a wide range of practical applications within the field of <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a>. In contrast to other document types, <a href=https://en.wikipedia.org/wiki/Web_design>web page designs</a> are intended for easy navigation and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information finding</a>. Effective designs encode within the layout and formatting signals that point to where the important information can be found. In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task. In particular, we leverage both <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical and visual features</a> (e.g., size, <a href=https://en.wikipedia.org/wiki/Font>font</a>, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection. Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models. A qualitative post-hoc analysis illustrates how these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> function within the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.143.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938697 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.143" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.143/>Multimodal Routing : Improving Local and Global Interpretability of Multimodal Language Analysis</a></strong><br><a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/m/martin-ma/>Martin Ma</a>
|
<a href=/people/m/muqiao-yang/>Muqiao Yang</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--143><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> can be expressed through multiple sources of information known as <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modalities</a>, including <a href=https://en.wikipedia.org/wiki/Tone_(linguistics)>tones of voice</a>, facial gestures, and <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>. Recent <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> with strong performances on human-centric tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> are often black-box, with very limited interpretability. In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality factors. Moreover, the weight assignment by <a href=https://en.wikipedia.org/wiki/Routing>routing</a> allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938824 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.145/>BiST : Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>ST</span>: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues</a></strong><br><a href=/people/h/hung-le/>Hung Le</a>
|
<a href=/people/d/doyen-sahoo/>Doyen Sahoo</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--145><div class="card-body p-3 small">Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved <a href=https://en.wikipedia.org/wiki/Sensory_cue>visual cues</a> are used as <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938852 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.147/>GraphDialog : Integrating <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>Graph Knowledge</a> into End-to-End Task-Oriented Dialogue Systems<span class=acl-fixed-case>G</span>raph<span class=acl-fixed-case>D</span>ialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems</a></strong><br><a href=/people/s/shiquan-yang/>Shiquan Yang</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/s/sarah-erfani/>Sarah Erfani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--147><div class="card-body p-3 small">End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems : one is how to effectively incorporate external knowledge bases (KBs) into the learning framework ; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. To exploit the relations between entities in KBs, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> combines multi-hop reasoning ability based on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938667 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.150" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.150/>Multi-turn Response Selection using Dialogue Dependency Relations</a></strong><br><a href=/people/q/qi-jia/>Qi Jia</a>
|
<a href=/people/y/yizhu-liu/>Yizhu Liu</a>
|
<a href=/people/s/siyu-ren/>Siyu Ren</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a>
|
<a href=/people/h/haifeng-tang/>Haifeng Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--150><div class="card-body p-3 small">Multi-turn response selection is a task designed for developing dialogue agents. The performance on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> has a remarkable improvement with pre-trained language models. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into <a href=https://en.wikipedia.org/wiki/Thread_(computing)>threads</a> based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode <a href=https://en.wikipedia.org/wiki/Thread_(computing)>threads</a> and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8 *, with competitive results on UbuntuV2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939216 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.155/>LOGAN : Local Group Bias Detection by Clustering<span class=acl-fixed-case>LOGAN</span>: Local Group Bias Detection by Clustering</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--155><div class="card-body p-3 small">Machine learning techniques have been widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. However, as revealed by many recent studies, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between <a href=https://en.wikipedia.org/wiki/Protected_group>protected groups</a> and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how <a href=https://en.wikipedia.org/wiki/Bias>biases</a> are embedded in a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In fact, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with similar aggregated performance between different groups on the entire data may behave differently on instances in a <a href=https://en.wikipedia.org/wiki/Region>local region</a>. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939084 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.160/>Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents<span class=acl-fixed-case>D</span>omain-<span class=acl-fixed-case>S</span>pecific <span class=acl-fixed-case>L</span>exical <span class=acl-fixed-case>G</span>rounding in <span class=acl-fixed-case>N</span>oisy <span class=acl-fixed-case>V</span>isual-<span class=acl-fixed-case>T</span>extual <span class=acl-fixed-case>D</span>ocuments</a></strong><br><a href=/people/g/gregory-yauney/>Gregory Yauney</a>
|
<a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--160><div class="card-body p-3 small">Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as kitchen and bedroom, and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond <a href=https://en.wikipedia.org/wiki/Object_detection>object detection</a> and image tagging baselines when evaluated on labeled subsets of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The proposed method is particularly effective for local contextual meanings of a word, for example associating <a href=https://en.wikipedia.org/wiki/Granite>granite</a> with <a href=https://en.wikipedia.org/wiki/Countertop>countertops</a> in the <a href=https://en.wikipedia.org/wiki/Real_estate>real estate dataset</a> and with rocky landscapes in a <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939320 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.162/>Vokenization : Improving Language Understanding with Contextualized, Visual-Grounded Supervision</a></strong><br><a href=/people/h/hao-tan/>Hao Tan</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--162><div class="card-body p-3 small">Humans learn <a href=https://en.wikipedia.org/wiki/Language>language</a> by listening, <a href=https://en.wikipedia.org/wiki/Speech>speaking</a>, <a href=https://en.wikipedia.org/wiki/Writing>writing</a>, <a href=https://en.wikipedia.org/wiki/Reading>reading</a>, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named vokenization that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call vokens). The vokenizer is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939230 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.165/>FedED : Federated Learning via Ensemble Distillation for Medical Relation Extraction<span class=acl-fixed-case>F</span>ed<span class=acl-fixed-case>ED</span>: Federated Learning via Ensemble Distillation for Medical Relation Extraction</a></strong><br><a href=/people/d/dianbo-sui/>Dianbo Sui</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a>
|
<a href=/people/y/yuantao-xie/>Yuantao Xie</a>
|
<a href=/people/w/weijian-sun/>Weijian Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--165><div class="card-body p-3 small">Unlike other domains, medical texts are inevitably accompanied by <a href=https://en.wikipedia.org/wiki/Privacy>private information</a>, so sharing or copying these <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with <a href=https://en.wikipedia.org/wiki/Privacy>privacy protection</a>. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in <a href=https://en.wikipedia.org/wiki/Information_privacy>privacy protection</a>, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> based on knowledge distillation. Such a <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> uses the uploaded predictions of <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble local models</a> to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939349 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.167/>A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression<span class=acl-fixed-case>A</span> <span class=acl-fixed-case>P</span>redicate-<span class=acl-fixed-case>F</span>unction-<span class=acl-fixed-case>A</span>rgument <span class=acl-fixed-case>A</span>nnotation of <span class=acl-fixed-case>N</span>atural <span class=acl-fixed-case>L</span>anguage for <span class=acl-fixed-case>O</span>pen-<span class=acl-fixed-case>D</span>omain <span class=acl-fixed-case>I</span>nformation e<span class=acl-fixed-case>X</span>pression</a></strong><br><a href=/people/m/mingming-sun/>Mingming Sun</a>
|
<a href=/people/w/wenyue-hua/>Wenyue Hua</a>
|
<a href=/people/z/zoey-liu/>Zoey Liu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/k/kangjie-zheng/>Kangjie Zheng</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--167><div class="card-body p-3 small">Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works ; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> for all OIE strategies. The <a href=https://en.wikipedia.org/wiki/OIX>OIX</a> is an OIE friendly expression of a sentence without <a href=https://en.wikipedia.org/wiki/Information_loss>information loss</a>. The generation procedure of <a href=https://en.wikipedia.org/wiki/OIX>OIX</a> contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of <a href=https://en.wikipedia.org/wiki/OIX>OIX</a> as inference operations focusing on more critical problems. Based on the same platform of <a href=https://en.wikipedia.org/wiki/OIX>OIX</a>, the OIE strategies are reusable, and people can select a set of <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to assemble their <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939140 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.171/>Understanding the Mechanics of SPIGOT : Surrogate Gradients for Latent Structure Learning<span class=acl-fixed-case>SPIGOT</span>: Surrogate Gradients for Latent Structure Learning</a></strong><br><a href=/people/t/tsvetomila-mihaylova/>Tsvetomila Mihaylova</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--171><div class="card-body p-3 small">Latent structure models are a powerful tool for modeling language data : they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT a variant of STE for structured models. Our perspective leads to new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939290 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.177/>Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses<span class=acl-fixed-case>NMT</span> with Hybrid Losses</a></strong><br><a href=/people/p/prathyusha-jwalapuram/>Prathyusha Jwalapuram</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/y/youlin-shen/>Youlin Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--177><div class="card-body p-3 small">Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation model</a>. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> has failed to adequately learn from, we improve the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939221 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.182/>Adversarial Attack and Defense of Structured Prediction Models</a></strong><br><a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/l/liwen-zhang/>Liwen Zhang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--182><div class="card-body p-3 small">Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> have attracted a lot of research in recent years. However, most of the existing approaches focus on <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification problems</a>. In this paper, we investigate attacks and defenses for structured prediction tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models : the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial training</a>, making its prediction more robust and accurate. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parsing</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.183.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939300 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.183" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.183/>Position-Aware Tagging for Aspect Sentiment Triplet Extraction</a></strong><br><a href=/people/l/lu-xu/>Lu Xu</a>
|
<a href=/people/h/hao-li/>Hao Li</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--183><div class="card-body p-3 small">Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end model</a> with a novel position-aware tagging scheme that is capable of jointly extracting the <a href=https://en.wikipedia.org/wiki/Multiple_birth>triplets</a>. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938900 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.184" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.184/>Simultaneous Machine Translation with Visual Context</a></strong><br><a href=/people/o/ozan-caglayan/>Ozan Caglayan</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/v/veneta-haralampieva/>Veneta Haralampieva</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--184><div class="card-body p-3 small">Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> and highest quality possible. The <a href=https://en.wikipedia.org/wiki/Translation>translation</a> thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.186.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939057 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.186/>The Secret is in the <a href=https://en.wikipedia.org/wiki/Electromagnetic_spectrum>Spectra</a> : Predicting Cross-lingual Task Performance with Spectral Similarity Measures</a></strong><br><a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--186><div class="card-body p-3 small">Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand : e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks : BLI, <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, POS tagging and MT. We hypothesize that statistics of the <a href=https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)>spectrum</a> of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two <a href=https://en.wikipedia.org/wiki/Embedding>embedding spaces</a>, based on the relevant statistics of their individual spectra. We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> capture complementary information to typologically driven language distance measures, and the combination of <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> from the two families yields even higher task performance correlations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.188/>AnswerFact : Fact Checking in Product Question Answering<span class=acl-fixed-case>A</span>nswer<span class=acl-fixed-case>F</span>act: Fact Checking in Product Question Answering</a></strong><br><a href=/people/w/wenxuan-zhang/>Wenxuan Zhang</a>
|
<a href=/people/y/yang-deng/>Yang Deng</a>
|
<a href=/people/j/jing-ma/>Jing Ma</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--188><div class="card-body p-3 small">Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during <a href=https://en.wikipedia.org/wiki/Online_shopping>online shopping</a>. However, the misinformation in the answers on those <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in <a href=https://en.wikipedia.org/wiki/E-commerce>E-commerce business</a>. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and various existing fact checking methods, showing that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms all baselines on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939080 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.190/>What do Models Learn from Question Answering Datasets?</a></strong><br><a href=/people/p/priyanka-sen/>Priyanka Sen</a>
|
<a href=/people/a/amir-saffari/>Amir Saffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--190><div class="card-body p-3 small">While models have reached superhuman performance on popular <a href=https://en.wikipedia.org/wiki/Question_answering>question answering (QA) datasets</a> such as SQuAD, they have yet to outperform <a href=https://en.wikipedia.org/wiki/Human>humans</a> on the task of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> itself. In this paper, we investigate if <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are learning <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is robust to all of our experiments and identify shortcomings in both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> through <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. We also release code to convert QA datasets to a shared format for easier experimentation at https://github.com/amazon-research/qa-dataset-converter</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938858 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.193/>Neural Deepfake Detection with Factual Structure of Text</a></strong><br><a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/z/zenan-xu/>Zenan Xu</a>
|
<a href=/people/r/ruize-wang/>Ruize Wang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/j/jiahai-wang/>Jiahai Wang</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--193><div class="card-body p-3 small">Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity graph</a>, which is further utilized to learn <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and <a href=https://en.wikipedia.org/wiki/Written_language>human-written text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--194 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938833 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.194" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.194/>MultiCQA : Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>CQA</span>: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale</a></strong><br><a href=/people/a/andreas-ruckle/>Andreas Rücklé</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--194><div class="card-body p-3 small">We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938960 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.195" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.195/>XL-AMR : Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques<span class=acl-fixed-case>XL</span>-<span class=acl-fixed-case>AMR</span>: Enabling Cross-Lingual <span class=acl-fixed-case>AMR</span> Parsing with Transfer Learning Techniques</a></strong><br><a href=/people/r/rexhina-blloshmi/>Rexhina Blloshmi</a>
|
<a href=/people/r/rocco-tripodi/>Rocco Tripodi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--195><div class="card-body p-3 small">Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the existing <a href=https://en.wikipedia.org/wiki/English_language>English AMR parsers</a> are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing : we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xl-amr.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--196 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939049 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.196" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.196/>Improving AMR Parsing with Sequence-to-Sequence Pre-training<span class=acl-fixed-case>AMR</span> Parsing with Sequence-to-Sequence Pre-training</a></strong><br><a href=/people/d/dongqin-xu/>Dongqin Xu</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/m/muhua-zhu/>Muhua Zhu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--196><div class="card-body p-3 small">In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, i.e., <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Syntactic_parsing>syntactic parsing</a>, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than <a href=https://en.wikipedia.org/wiki/Complex_analysis>complex models</a>. We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938955 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.197/>Hate-Speech and Offensive Language Detection in <a href=https://en.wikipedia.org/wiki/Roman_Urdu>Roman Urdu</a><span class=acl-fixed-case>R</span>oman <span class=acl-fixed-case>U</span>rdu</a></strong><br><a href=/people/h/hammad-rizwan/>Hammad Rizwan</a>
|
<a href=/people/m/muhammad-haroon-shakeel/>Muhammad Haroon Shakeel</a>
|
<a href=/people/a/asim-karim/>Asim Karim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--197><div class="card-body p-3 small">The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, <a href=https://en.wikipedia.org/wiki/Annotation>annotated datasets</a>, and <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> for this task. In this study, we : (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is more beneficial as compared to training embedding from scratch and that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits greater <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> as compared to the baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939035 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.199/>Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets</a></strong><br><a href=/people/n/nedjma-ousidhoum/>Nedjma Ousidhoum</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/d/dit-yan-yeung/>Dit-Yan Yeung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--199><div class="card-body p-3 small">Work on bias in <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> typically aims to improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance while relatively overlooking the quality of the data. We examine <a href=https://en.wikipedia.org/wiki/Selection_bias>selection bias</a> in <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.203.OptionalSupplementaryMaterial.tgz data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938871 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.203/>Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems<span class=acl-fixed-case>NMT</span> by Finetuning Subword Systems</a></strong><br><a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--203><div class="card-body p-3 small">Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939088 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.207" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.207/>Not Low-Resource Anymore : Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation<span class=acl-fixed-case>B</span>engali-<span class=acl-fixed-case>E</span>nglish Machine Translation</a></strong><br><a href=/people/t/tahmid-hasan/>Tahmid Hasan</a>
|
<a href=/people/a/abhik-bhattacharjee/>Abhik Bhattacharjee</a>
|
<a href=/people/k/kazi-samin/>Kazi Samin</a>
|
<a href=/people/m/masum-hasan/>Masum Hasan</a>
|
<a href=/people/m/madhusudan-basak/>Madhusudan Basak</a>
|
<a href=/people/m/m-sohel-rahman/>M. Sohel Rahman</a>
|
<a href=/people/r/rifat-shahriyar/>Rifat Shahriyar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--207><div class="card-body p-3 small">Despite being the seventh most widely spoken language in the world, <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a> has received much less attention in machine translation literature due to being low in resources. Most publicly available <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> for <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a> are not large enough ; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a> and propose two novel methods for parallel corpus creation on low-resource setups : aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural models</a>, we achieve an improvement of more than 9 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive <a href=https://en.wikipedia.org/wiki/Quality_control>quality control</a>. We release the segmenter, <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>, and the evaluation set, thus elevating <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a> from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https://github.com/csebuetnlp/banglanmt.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939388 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.210/>Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information</a></strong><br><a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/x/xiao-pan/>Xiao Pan</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--210><div class="card-body p-3 small">We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com / linzehui / mRASP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938739 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.211/>Losing Heads in the Lottery : Pruning Transformer Attention in Neural Machine Translation</a></strong><br><a href=/people/m/maximiliana-behnke/>Maximiliana Behnke</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--211><div class="card-body p-3 small">The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a> are not confident in their decisions and can be pruned. However, removing them before training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Turkish_language>TurkishEnglish</a>. The pruned model is 1.5 times as fast at <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with EnglishGerman student model gaining an additional 10 % speed-up with 75 % encoder attention removed and 0.2 BLEU loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938760 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.212/>Towards Enhancing <a href=https://en.wikipedia.org/wiki/Faithfulness>Faithfulness</a> for Neural Machine Translation</a></strong><br><a href=/people/r/rongxiang-weng/>Rongxiang Weng</a>
|
<a href=/people/h/heng-yu/>Heng Yu</a>
|
<a href=/people/x/xiangpeng-wei/>Xiangpeng Wei</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--212><div class="card-body p-3 small">Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training strategy</a> with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and decoder to guide <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938800 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.217" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.217/>Can Automatic Post-Editing Improve NMT?<span class=acl-fixed-case>NMT</span>?</a></strong><br><a href=/people/s/shamil-chollampatt/>Shamil Chollampatt</a>
|
<a href=/people/r/raymond-hendy-susanto/>Raymond Hendy Susanto</a>
|
<a href=/people/l/liling-tan/>Liling Tan</a>
|
<a href=/people/e/ewa-szymanska/>Ewa Szymanska</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--217><div class="card-body p-3 small">Automatic post-editing (APE) aims to improve <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translations</a>, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>artificial training data</a>, and <a href=https://en.wikipedia.org/wiki/Domain_specificity>domain specificity</a> for the APE task. We release this new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.220.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938710 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.220/>Some Languages Seem Easier to Parse Because Their Treebanks Leak</a></strong><br><a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--220><div class="card-body p-3 small">Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously discussed : If we abstract away from words and dependency labels, how many <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> in the test data were seen in the training data? We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938796 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.222/>Modularized Syntactic Neural Networks for Sentence Classification</a></strong><br><a href=/people/h/haiyan-wu/>Haiyan Wu</a>
|
<a href=/people/y/ying-liu/>Ying Liu</a>
|
<a href=/people/s/shaoyun-shi/>Shaoyun Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--222><div class="card-body p-3 small">This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a <a href=https://en.wikipedia.org/wiki/Syntax_tree>syntax tree</a> usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees. In MSNN, each <a href=https://en.wikipedia.org/wiki/Node_(computer_science)>node</a> of a <a href=https://en.wikipedia.org/wiki/Syntax_tree>syntax tree</a> is modeled by a label-related syntax module. Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representation</a>. We design a tree-parallel mini-batch strategy for efficient <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and predicting. Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938958 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.226/>MEGATRON-CNTRL : Controllable Story Generation with External Knowledge Using Large-Scale Language Models<span class=acl-fixed-case>MEGATRON</span>-<span class=acl-fixed-case>CNTRL</span>: Controllable Story Generation with External Knowledge Using Large-Scale Language Models</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/m/mostofa-patwary/>Mostofa Patwary</a>
|
<a href=/people/m/mohammad-shoeybi/>Mohammad Shoeybi</a>
|
<a href=/people/r/raul-puri/>Raul Puri</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a>
|
<a href=/people/a/animashree-anandkumar/>Anima Anandkumar</a>
|
<a href=/people/b/bryan-catanzaro/>Bryan Catanzaro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--226><div class="card-body p-3 small">Existing pre-trained large language models have shown unparalleled generative capabilities. However, <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>they</a> are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a <a href=https://en.wikipedia.org/wiki/Knowledge_retrieval>knowledge retriever</a>, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a>. The empirical results show that our model generates more fluent, consistent, and coherent stories with less <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a> and higher <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> compared to prior work on the ROC story dataset. We showcase the controllability of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> by replacing the <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> used to generate stories and re-running the generation process. Human evaluation results show that 77.5 % of these stories are successfully controlled by the new <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>. Furthermore, by scaling our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality of generation</a> (from 74.5 % to 93.0 % for <a href=https://en.wikipedia.org/wiki/Consistency_(statistics)>consistency</a>) and <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> (from 77.5 % to 91.5 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939097 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.228/>Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples</a></strong><br><a href=/people/l/lihao-wang/>Lihao Wang</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--228><div class="card-body p-3 small">A sequence-to-sequence (seq2seq) learning with <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>, and to enhance the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938791 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.231/>Multilingual AMR-to-Text Generation<span class=acl-fixed-case>AMR</span>-to-Text Generation</a></strong><br><a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--231><div class="card-body p-3 small">Generating text from <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge : that of generating into languages with varied word order and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological properties</a>. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. Our multilingual models surpass <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> that generate into one language in eighteen languages, based on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. We analyze the ability of our multilingual models to accurately capture <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and word order using human evaluation, and find that <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a> judge our generations to be fluent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.232.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938828 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.232/>Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation</a></strong><br><a href=/people/f/francisco-vargas/>Francisco Vargas</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--232><div class="card-body p-3 small">Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Their method takes pre-trained <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as input and attempts to isolate a <a href=https://en.wikipedia.org/wiki/Linear_subspace>linear subspace</a> that captures most of the <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a> in the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and analyze empirically whether the bias subspace is actually linear. Our analysis shows that <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> is in fact well captured by a <a href=https://en.wikipedia.org/wiki/Linear_subspace>linear subspace</a>, justifying the assumption of Bolukbasi et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.234.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938922 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.234/>Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models<span class=acl-fixed-case>D</span>irichlet Process Topic Models</a></strong><br><a href=/people/a/alexander-terenin/>Alexander Terenin</a>
|
<a href=/people/m/mans-magnusson/>Måns Magnusson</a>
|
<a href=/people/l/leif-jonsson/>Leif Jonsson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--234><div class="card-body p-3 small">To scale non-parametric extensions of probabilistic topic models such as <a href=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>Latent Dirichlet allocation</a> to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study <a href=https://en.wikipedia.org/wiki/Data_parallelism>data-parallel training</a> for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This <a href=https://en.wikipedia.org/wiki/Sampler_(musical_instrument)>sampler</a> utilizes all available sources of sparsity found in natural language-an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8 m documents and 768 m tokens, using a single multi-core machine in under four days.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.238.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939286 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.238" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.238/>Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction</a></strong><br><a href=/people/x/xu-zhao/>Xu Zhao</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/y/yong-zhang/>Yong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--238><div class="card-body p-3 small">Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised signal</a> and <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised alignment</a>. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> based on a cyclic or a parallel parameter feeding routine to update our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is a general framework that can incorporate any supervised and unsupervised BLI methods based on <a href=https://en.wikipedia.org/wiki/Optimal_transport>optimal transport</a>. Experimental results on MUSE and VecMap datasets show significant improvement of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938812 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.242" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.242/>BERT-EMD : Many-to-Many Layer Mapping for BERT Compression with Earth Mover’s Distance<span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>EMD</span>: Many-to-Many Layer Mapping for <span class=acl-fixed-case>BERT</span> Compression with Earth Mover’s Distance</a></strong><br><a href=/people/j/jianquan-li/>Jianquan Li</a>
|
<a href=/people/x/xiaokang-liu/>Xiaokang Liu</a>
|
<a href=/people/h/honghong-zhao/>Honghong Zhao</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/y/yaohong-jin/>Yaohong Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--242><div class="card-body p-3 small">Pre-trained language models (e.g., BERT) have achieved significant success in various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tasks</a>. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover&#8217;s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a> for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves competitive performance compared to strong competitors in terms of both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Mathematical_model>model compression</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938817 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.243/>Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking</a></strong><br><a href=/people/y/yexiang-wang/>Yexiang Wang</a>
|
<a href=/people/y/yi-guo/>Yi Guo</a>
|
<a href=/people/s/siqi-zhu/>Siqi Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--243><div class="card-body p-3 small">Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes : choosing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> without <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> or embedding <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> in <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>joint accuracy</a> of 54.52 % on MultiWOZ 2.0 and 54.86 % on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30 % <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> is used, VN can also contribute to our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.246" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.246/>Learning a Cost-Effective Annotation Policy for Question Answering<span class=acl-fixed-case>C</span>ost-<span class=acl-fixed-case>E</span>ffective <span class=acl-fixed-case>A</span>nnotation <span class=acl-fixed-case>P</span>olicy for <span class=acl-fixed-case>Q</span>uestion <span class=acl-fixed-case>A</span>nswering</a></strong><br><a href=/people/b/bernhard-kratzwald/>Bernhard Kratzwald</a>
|
<a href=/people/s/stefan-feuerriegel/>Stefan Feuerriegel</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--246><div class="card-body p-3 small">State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a> is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort : <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> leverages the underlying <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA system</a> to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> against traditional manual annotations in an extensive set of experiments. We find that our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> can reduce up to 21.1 % of the annotation cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--247 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.247/>Scene Restoring for Narrative Machine Reading Comprehension</a></strong><br><a href=/people/z/zhixing-tian/>Zhixing Tian</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a>
|
<a href=/people/z/zhicheng-sheng/>Zhicheng Sheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--247><div class="card-body p-3 small">This paper focuses on machine reading comprehension for <a href=https://en.wikipedia.org/wiki/Narrative>narrative passages</a>. Narrative passages usually describe a <a href=https://en.wikipedia.org/wiki/Chain_of_events>chain of events</a>. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939310 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.251/>Incorporating Behavioral Hypotheses for Query Generation</a></strong><br><a href=/people/r/ruey-cheng-chen/>Ruey-Cheng Chen</a>
|
<a href=/people/c/chia-jung-lee/>Chia-Jung Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--251><div class="card-body p-3 small">Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.252.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938729 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.252/>Conditional Causal Relationships between Emotions and Causes in Texts</a></strong><br><a href=/people/x/xinhong-chen/>Xinhong Chen</a>
|
<a href=/people/q/qing-li/>Qing Li</a>
|
<a href=/people/j/jianping-wang/>Jianping Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--252><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Causality>causal relationships</a> between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from <a href=https://en.wikipedia.org/wiki/Document>documents</a>. However, none of these works has considered the possibility that the <a href=https://en.wikipedia.org/wiki/Causality>causal relationships</a> among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related. To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> via manual annotation and negative sampling based on an existing benchmark dataset. Furthermore, we propose a prediction aggregation module with low <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a> to fine-tune the prediction results based on the characteristics of the input clauses. Experiments demonstrate the effectiveness and generality of our aggregation module.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939356 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.261" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.261/>Towards Interpreting BERT for Reading Comprehension Based QA<span class=acl-fixed-case>BERT</span> for Reading Comprehension Based <span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/s/sahana-ramnath/>Sahana Ramnath</a>
|
<a href=/people/p/preksha-nema/>Preksha Nema</a>
|
<a href=/people/d/deep-sahni/>Deep Sahni</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--261><div class="card-body p-3 small">BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer&#8217;s role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial <a href=https://en.wikipedia.org/wiki/Abstraction_layer>layers</a> focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much / how many), we notice that BERT focuses on <a href=https://en.wikipedia.org/wiki/Word_sense>confusing words</a> (i.e., on other numerical quantities in the passage) in the later <a href=https://en.wikipedia.org/wiki/Complexity>layers</a>, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.262.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938648 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.262" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.262/>How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking</a></strong><br><a href=/people/n/nicola-de-cao/>Nicola De Cao</a>
|
<a href=/people/m/michael-sejr-schlichtkrull/>Michael Sejr Schlichtkrull</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--262><div class="card-body p-3 small">Attribution methods assess the contribution of inputs to the <a href=https://en.wikipedia.org/wiki/Prediction>model prediction</a>. One way to do so is erasure : a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure&#8217;s objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the <a href=https://en.wikipedia.org/wiki/Hindsight_bias>hindsight bias</a> : the fact that an input can be dropped does not mean that the model &#8216;knows&#8217; it can be dropped. The resulting <a href=https://en.wikipedia.org/wiki/Pruning>pruning</a> is over-aggressive and does not reflect how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on intermediate hidden layers of the analyzed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. First, this makes the approach efficient because we predict rather than <a href=https://en.wikipedia.org/wiki/Search_algorithm>search</a>. Second, as with probing classifiers, this reveals what the network &#8216;knows&#8217; at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across <a href=https://en.wikipedia.org/wiki/Network_layer>network layers</a>. We use DiffMask to study BERT models on sentiment classification and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938813 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.263" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.263/>A Diagnostic Study of Explainability Techniques for Text Classification</a></strong><br><a href=/people/p/pepa-atanasova/>Pepa Atanasova</a>
|
<a href=/people/j/jakob-grue-simonsen/>Jakob Grue Simonsen</a>
|
<a href=/people/c/christina-lioma/>Christina Lioma</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--263><div class="card-body p-3 small">Recent developments in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> have introduced <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>&#8217; predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a <a href=https://en.wikipedia.org/wiki/Software_development_process>technique</a> given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such <a href=https://en.wikipedia.org/wiki/Software_development_process>technique</a>. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed <a href=https://en.wikipedia.org/wiki/List_(abstract_data_type)>list</a> to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model&#8217;s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--265 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938860 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.265" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.265/>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering</a></strong><br><a href=/people/z/zujie-liang/>Zujie Liang</a>
|
<a href=/people/w/weitao-jiang/>Weitao Jiang</a>
|
<a href=/people/h/haifeng-hu/>Haifeng Hu</a>
|
<a href=/people/j/jiaying-zhu/>Jiaying Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--265><div class="card-body p-3 small">In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model&#8217;s robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938674 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.266/>Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization<span class=acl-fixed-case>P</span>hysical <span class=acl-fixed-case>C</span>ommon <span class=acl-fixed-case>S</span>ense as <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>G</span>raph <span class=acl-fixed-case>C</span>ompletion via <span class=acl-fixed-case>BERT</span> <span class=acl-fixed-case>D</span>ata <span class=acl-fixed-case>A</span>ugmentation and <span class=acl-fixed-case>C</span>onstrained <span class=acl-fixed-case>T</span>ucker <span class=acl-fixed-case>F</span>actorization</a></strong><br><a href=/people/z/zhenjie-zhao/>Zhenjie Zhao</a>
|
<a href=/people/e/evangelos-papalexakis/>Evangelos Papalexakis</a>
|
<a href=/people/x/xiaojuan-ma/>Xiaojuan Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--266><div class="card-body p-3 small">Physical common sense plays an essential role in the cognition abilities of robots for <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot interaction</a>. Machine learning methods have shown promising results on physical commonsense learning in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent relationships</a> among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics : training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> with existing state-of-the-art knowledge graph embedding methods and show its superior performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939228 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.267/>A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses</a></strong><br><a href=/people/h/hisashi-kamezawa/>Hisashi Kamezawa</a>
|
<a href=/people/n/noriki-nishida/>Noriki Nishida</a>
|
<a href=/people/n/nobuyuki-shimizu/>Nobuyuki Shimizu</a>
|
<a href=/people/t/takashi-miyazaki/>Takashi Miyazaki</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--267><div class="card-body p-3 small">In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a>. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents&#8217; verbal and non-verbal responses. We present experimental results obtained using the proposed VFD dataset and recent <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> (e.g., BERT, ResNet). The results demonstrate that first-person vision helps <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural network models</a> correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--271 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938820 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.271/>Sub-Instruction Aware Vision-and-Language Navigation</a></strong><br><a href=/people/y/yicong-hong/>Yicong Hong</a>
|
<a href=/people/c/cristian-rodriguez/>Cristian Rodriguez</a>
|
<a href=/people/q/qi-wu/>Qi Wu</a>
|
<a href=/people/s/stephen-gould/>Stephen Gould</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--271><div class="card-body p-3 small">Vision-and-language navigation requires an agent to navigate through a real 3D environment following <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language instructions</a>. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>&#8217;s performance at following each part of the instruction can not be assessed during <a href=https://en.wikipedia.org/wiki/Navigation>navigation</a>. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>sub-instruction attention and shifting modules</a> that select and attend to a single <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>sub-instruction</a> at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--278 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938752 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.278/>Task-Completion Dialogue Policy Learning via <a href=https://en.wikipedia.org/wiki/Monte_Carlo_tree_search>Monte Carlo Tree Search</a> with Dueling Network<span class=acl-fixed-case>M</span>onte <span class=acl-fixed-case>C</span>arlo Tree Search with Dueling Network</a></strong><br><a href=/people/s/sihan-wang/>Sihan Wang</a>
|
<a href=/people/k/kaijie-zhou/>Kaijie Zhou</a>
|
<a href=/people/k/kunfeng-lai/>Kunfeng Lai</a>
|
<a href=/people/j/jianping-shen/>Jianping Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--278><div class="card-body p-3 small">We introduce a framework of <a href=https://en.wikipedia.org/wiki/Monte_Carlo_tree_search>Monte Carlo Tree Search</a> with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by <a href=https://en.wikipedia.org/wiki/Monte_Carlo_method>Monte Carlo simulations</a> and is robust to the simulation errors. Such idea arises naturally in <a href=https://en.wikipedia.org/wiki/Human_behavior>human behaviors</a>, e.g. predicting others&#8217; responses and then deciding our own actions. In the simulated movie-ticket booking task, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--289 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939311 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.289/>Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme</a></strong><br><a href=/people/c/chaofa-yuan/>Chaofa Yuan</a>
|
<a href=/people/c/chuang-fan/>Chuang Fan</a>
|
<a href=/people/j/jianzhu-bao/>Jianzhu Bao</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--289><div class="card-body p-3 small">The task of emotion-cause pair extraction deals with finding all <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with <a href=https://en.wikipedia.org/wiki/Time_complexity>linear time complexity</a>, leading to a speed up. Experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the best performance, outperforming the state-of-the-art <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> by 2.26 % (p0.001) in <a href=https://en.wikipedia.org/wiki/F-number>F1 measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--291 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938701 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.291/>Multi-modal Multi-label Emotion Detection with Modality and Label Dependence</a></strong><br><a href=/people/d/dong-zhang/>Dong Zhang</a>
|
<a href=/people/x/xincheng-ju/>Xincheng Ju</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/s/shoushan-li/>Shoushan Li</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--291><div class="card-body p-3 small">As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multi-modal scenario</a>. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--303 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.303.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938684 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.303/>Global-to-Local Neural Networks for Document-Level Relation Extraction</a></strong><br><a href=/people/d/difeng-wang/>Difeng Wang</a>
|
<a href=/people/w/wei-hu/>Wei Hu</a>
|
<a href=/people/e/ermei-cao/>Ermei Cao</a>
|
<a href=/people/w/weijian-sun/>Weijian Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--303><div class="card-body p-3 small">Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939355 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.304/>Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations</a></strong><br><a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/r/richong-zhang/>Richong Zhang</a>
|
<a href=/people/s/samuel-mensah/>Samuel Mensah</a>
|
<a href=/people/y/yongyi-mao/>Yongyi Mao</a>
|
<a href=/people/x/xudong-liu/>Xudong Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--304><div class="card-body p-3 small">The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from learning explicit interactions between the two tasks to improve the performance on the individual <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Empirical studies on two real-world datasets confirm the superiority of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.308.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938978 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.308/>Point to the Expression : Solving Algebraic Word Problems using the Expression-Pointer Transformer Model<span class=acl-fixed-case>P</span>oint to the <span class=acl-fixed-case>E</span>xpression: <span class=acl-fixed-case>S</span>olving <span class=acl-fixed-case>A</span>lgebraic <span class=acl-fixed-case>W</span>ord <span class=acl-fixed-case>P</span>roblems using the <span class=acl-fixed-case>E</span>xpression-<span class=acl-fixed-case>P</span>ointer <span class=acl-fixed-case>T</span>ransformer <span class=acl-fixed-case>M</span>odel</a></strong><br><a href=/people/b/bugeun-kim/>Bugeun Kim</a>
|
<a href=/people/k/kyung-seo-ki/>Kyung Seo Ki</a>
|
<a href=/people/d/donggeon-lee/>Donggeon Lee</a>
|
<a href=/people/g/gahgene-gweon/>Gahgene Gweon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--308><div class="card-body p-3 small">Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using &#8216;Op (operator / operand)&#8217; tokens as a unit of input / output. However, such a neural model suffered two issues : expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) &#8216;Expression&#8217; token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets : ALG514, DRAW-1 K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> ; 81.3 % on ALG514, 59.5 % on DRAW-1 K, and 84.5 % on MAWPS. The contribution of this paper is two-fold ; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--311 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939193 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.311/>Routing Enforced <a href=https://en.wikipedia.org/wiki/Generative_model>Generative Model</a> for Recipe Generation</a></strong><br><a href=/people/z/zhiwei-yu/>Zhiwei Yu</a>
|
<a href=/people/h/hongyu-zang/>Hongyu Zang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--311><div class="card-body p-3 small">One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, <a href=https://en.wikipedia.org/wiki/F-number>F1</a> and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--316 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939116 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.316/>DagoBERT : Generating Derivational Morphology with a Pretrained Language Model<span class=acl-fixed-case>D</span>ago<span class=acl-fixed-case>BERT</span>: <span class=acl-fixed-case>G</span>enerating Derivational Morphology with a Pretrained Language Model</a></strong><br><a href=/people/v/valentin-hofmann/>Valentin Hofmann</a>
|
<a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--316><div class="card-body p-3 small">Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT&#8217;s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT&#8217;s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--318 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938808 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.318/>A Joint Multiple Criteria Model in <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> for Cross-domain Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/k/kaiyu-huang/>Kaiyu Huang</a>
|
<a href=/people/d/degen-huang/>Degen Huang</a>
|
<a href=/people/z/zhuang-liu/>Zhuang Liu</a>
|
<a href=/people/f/fengran-mo/>Fengran Mo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--318><div class="card-body p-3 small">Word-level information is important in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>, especially for the <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese language</a> due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora. However, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words. In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks. Excessive amounts of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with saving different criteria will generate the explosive growth of the total parameters. To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Besides, we utilize a transfer learning method to improve the performance of <a href=https://en.wikipedia.org/wiki/Object-oriented_programming>OOV words</a>. Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010). Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves the state-of-the-art performances on all datasets. Importantly, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also shows a competitive practicability and generalization ability for the CWS task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--322 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.322" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.322/>Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--322><div class="card-body p-3 small">Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with <a href=https://en.wikipedia.org/wiki/Semantic_role>semantic roles</a>. Even though most semantic-role formalisms are built upon <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent syntax</a>, and only <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>syntactic constituents</a> can be labeled as arguments (e.g., <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituents</a>. The computation is done in 3 stages. First, initial node representations are produced by &#8216;composing&#8217; word representations of the first and last words in the <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent</a>. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are &#8216;decomposed&#8217; back into word representations, which are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939148 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.325/>Supervised Seeded Iterated Learning for Interactive Language Learning</a></strong><br><a href=/people/y/yuchen-lu/>Yuchen Lu</a>
|
<a href=/people/s/soumye-singhal/>Soumye Singhal</a>
|
<a href=/people/f/florian-strub/>Florian Strub</a>
|
<a href=/people/o/olivier-pietquin/>Olivier Pietquin</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--325><div class="card-body p-3 small">Language drift has been one of the major obstacles to train <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> through <a href=https://en.wikipedia.org/wiki/Interaction>interaction</a>. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. In recent literature, two general methods partially counter this phenomenon : Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, <a href=https://en.wikipedia.org/wiki/SIL_International>SIL</a> changes the training dynamics to prevent <a href=https://en.wikipedia.org/wiki/Language_drift>language drift</a> from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on <a href=https://en.wikipedia.org/wiki/Text_corpus>human corpus</a>. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of in the language-drift translation game.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--334 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939153 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.334" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.334/>Compositional Demographic Word Embeddings</a></strong><br><a href=/people/c/charles-welch/>Charles Welch</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--334><div class="card-body p-3 small">Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance and other language processing tasks, they can only be computed for people with a large amount of <a href=https://en.wikipedia.org/wiki/Panel_data>longitudinal data</a>, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for <a href=https://en.wikipedia.org/wiki/English_language>English</a> : <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939226 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.335/>Are Undocumented Workers the Same as Illegal Aliens? Disentangling Denotation and Connotation in Vector Spaces<span class=acl-fixed-case>D</span>isentangling Denotation and Connotation in Vector Spaces</a></strong><br><a href=/people/a/albert-webson/>Albert Webson</a>
|
<a href=/people/z/zhizhong-chen/>Zhizhong Chen</a>
|
<a href=/people/c/carsten-eickhoff/>Carsten Eickhoff</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--335><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> are frequently invented for partisan objectives. For example, <a href=https://en.wikipedia.org/wiki/Illegal_immigration_to_the_United_States>undocumented workers</a> and illegal aliens refer to the same group of people (i.e., they have the same denotation), but they carry clearly different <a href=https://en.wikipedia.org/wiki/Connotation>connotations</a>. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, however, popular pretrained models encode both <a href=https://en.wikipedia.org/wiki/Denotation>denotation</a> and <a href=https://en.wikipedia.org/wiki/Connotation>connotation</a> as one entangled representation. In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., immigrants vs. aliens, estate tax vs. death tax) move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval system</a> with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--341 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939204 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.341/>SLEDGE-Z : A Zero-Shot Baseline for COVID-19 Literature Search<span class=acl-fixed-case>SLEDGE-Z</span>: A Zero-Shot Baseline for <span class=acl-fixed-case>COVID</span>-19 Literature Search</a></strong><br><a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--341><div class="card-body p-3 small">With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a> down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939027 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.344" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.344/>Adversarial Semantic Collisions</a></strong><br><a href=/people/c/congzheng-song/>Congzheng Song</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a>
|
<a href=/people/v/vitaly-shmatikov/>Vitaly Shmatikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--344><div class="card-body p-3 small">We study semantic collisions : texts that are semantically unrelated but judged as similar by <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a>. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of textsincluding paraphrase identification, <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, response suggestion, and extractive summarizationare vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at.<i>semantic collisions</i>: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts&#8212;including paraphrase identification, document retrieval, response suggestion, and extractive summarization&#8212;are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at <url>https://github.com/csong27/collision-bert</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--345 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.345/>Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification</a></strong><br><a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/m/marina-danilevsky/>Marina Danilevsky</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/s/siddhartha-brahma/>Siddhartha Brahma</a>
|
<a href=/people/m/matthias-boehm/>Matthias Boehm</a>
|
<a href=/people/l/laura-chiticariu/>Laura Chiticariu</a>
|
<a href=/people/r/rajasekar-krishnamurthy/>Rajasekar Krishnamurthy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--345><div class="card-body p-3 small">Interpretability of <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic</a>, a <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a> extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> leads to human-machine co-creation of models with better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--346 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939188 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.346" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.346/>AutoPrompt : Eliciting Knowledge from Language Models with Automatically Generated Prompts<span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>P</span>rompt: <span class=acl-fixed-case>E</span>liciting <span class=acl-fixed-case>K</span>nowledge from <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odels with <span class=acl-fixed-case>A</span>utomatically <span class=acl-fixed-case>G</span>enerated <span class=acl-fixed-case>P</span>rompts</a></strong><br><a href=/people/t/taylor-shin/>Taylor Shin</a>
|
<a href=/people/y/yasaman-razeghi/>Yasaman Razeghi</a>
|
<a href=/people/r/robert-l-logan-iv/>Robert L. Logan IV</a>
|
<a href=/people/e/eric-wallace/>Eric Wallace</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--346><div class="card-body p-3 small">The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and natural language inference without additional parameters or <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939280 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.352/>Generating Dialogue Responses from a Semantic Latent Space</a></strong><br><a href=/people/w/wei-jen-ko/>Wei-Jen Ko</a>
|
<a href=/people/a/avik-ray/>Avik Ray</a>
|
<a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--352><div class="card-body p-3 small">Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>. We learn the pair relationship between the prompts and responses as a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a> on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> on a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous space</a> can generate responses that are both relevant and informative.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--355 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938646 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.355/>ALICE : <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a> with Contrastive Natural Language Explanations<span class=acl-fixed-case>ALICE</span>: Active Learning with Contrastive Natural Language Explanations</a></strong><br><a href=/people/w/weixin-liang/>Weixin Liang</a>
|
<a href=/people/j/james-zou/>James Zou</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--355><div class="card-body p-3 small">Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface : classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> extracts knowledge from these <a href=https://en.wikipedia.org/wiki/Explanation>explanations</a> using a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>. Finally, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> incorporates the extracted knowledge through dynamically changing the <a href=https://en.wikipedia.org/wiki/Machine_learning>learning model</a>&#8217;s structure. We applied ALICEin two visual recognition tasks, bird species classification and <a href=https://en.wikipedia.org/wiki/Social_relation>social relationship classification</a>. We found by incorporating contrastive explanations, our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> outperform baseline models that are trained with 40-100 % more training data. We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939191 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.366/>A Streaming Approach For Efficient Batched Beam Search</a></strong><br><a href=/people/k/kevin-yang/>Kevin Yang</a>
|
<a href=/people/v/violet-yao/>Violet Yao</a>
|
<a href=/people/j/john-denero/>John DeNero</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--366><div class="card-body p-3 small">We propose an efficient <a href=https://en.wikipedia.org/wiki/Batch_processing>batching strategy</a> for variable-length decoding on <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU architectures</a>. During decoding, when candidates terminate or are pruned according to <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>, our streaming approach periodically refills the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime</a> by up to 71 % compared to a fixed-width beam search baseline and 17 % compared to a variable-width baseline, while matching baselines&#8217; BLEU. Finally, experiments show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can speed up decoding in other domains, such as <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic and syntactic parsing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.375.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--375 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.375 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939210 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.375/>Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models</a></strong><br><a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/p/peng-qian/>Peng Qian</a>
|
<a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--375><div class="card-body p-3 small">Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models&#8217; syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation : the ability of a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence). We test four models trained on the same dataset : an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision. We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.378/>Optimus : Organizing Sentences via Pre-trained Modeling of a Latent Space</a></strong><br><a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiang-gao/>Xiang Gao</a>
|
<a href=/people/y/yuan-li/>Yuan Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--378><div class="card-body p-3 small">When trained effectively, the Variational Autoencoder (VAE) can be both a powerful <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> and an effective representation learning framework for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the <a href=https://en.wikipedia.org/wiki/Latent_vector>latent vectors</a>. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939106 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.381/>RussianSuperGLUE : A Russian Language Understanding Evaluation Benchmark<span class=acl-fixed-case>R</span>ussian<span class=acl-fixed-case>S</span>uper<span class=acl-fixed-case>GLUE</span>: A <span class=acl-fixed-case>R</span>ussian Language Understanding Evaluation Benchmark</a></strong><br><a href=/people/t/tatiana-shavrina/>Tatiana Shavrina</a>
|
<a href=/people/a/alena-fenogenova/>Alena Fenogenova</a>
|
<a href=/people/e/emelyanov-anton/>Emelyanov Anton</a>
|
<a href=/people/d/denis-shevelev/>Denis Shevelev</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a>
|
<a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/m/maria-tikhonova/>Maria Tikhonova</a>
|
<a href=/people/a/andrey-chertok/>Andrey Chertok</a>
|
<a href=/people/a/andrey-evlampiev/>Andrey Evlampiev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--381><div class="card-body p-3 small">In this paper, we introduce an advanced Russian general language understanding evaluation benchmark Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills-detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the <a href=https://en.wikipedia.org/wiki/Russian_language>Russian language</a>. We also provide baselines, human level evaluation, open-source framework for evaluating <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and an overall leaderboard of transformer models for the <a href=https://en.wikipedia.org/wiki/Russian_language>Russian language</a>. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> independently of language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.382.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--382 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.382 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939107 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.382" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.382/>An Empirical Study of Pre-trained Transformers for Arabic Information Extraction<span class=acl-fixed-case>A</span>rabic Information Extraction</a></strong><br><a href=/people/w/wuwei-lan/>Wuwei Lan</a>
|
<a href=/people/y/yang-chen/>Yang Chen</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--382><div class="card-body p-3 small">Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning. We study GigaBERT&#8217;s effectiveness on zero-short transfer across four IE tasks : <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at : https://github.com/lanwuwei/GigaBERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939136 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.383/>TNT : Text Normalization based Pre-training of Transformers for Content Moderation<span class=acl-fixed-case>TNT</span>: Text Normalization based Pre-training of Transformers for Content Moderation</a></strong><br><a href=/people/f/fei-tan/>Fei Tan</a>
|
<a href=/people/y/yifan-hu/>Yifan Hu</a>
|
<a href=/people/c/changwei-hu/>Changwei Hu</a>
|
<a href=/people/k/keqian-li/>Keqian Li</a>
|
<a href=/people/k/kevin-yen/>Kevin Yen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--383><div class="card-body p-3 small">In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation : substitution, transposition, <a href=https://en.wikipedia.org/wiki/Deletion_(linguistics)>deletion</a>, and insertion. Furthermore, the <a href=https://en.wikipedia.org/wiki/Data_normalization>normalization</a> involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that <a href=https://en.wikipedia.org/wiki/TNT>TNT</a> outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on the hate speech classification task. Additional <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a> experiments and case studies show that TNT is a new potential approach to misspelling correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939268 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.384/>Methods for Numeracy-Preserving Word Embeddings</a></strong><br><a href=/people/d/dhanasekar-sundararaman/>Dhanasekar Sundararaman</a>
|
<a href=/people/s/shijing-si/>Shijing Si</a>
|
<a href=/people/v/vivek-subramanian/>Vivek Subramanian</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--384><div class="card-body p-3 small">Word embedding models are typically able to capture the semantics of words via the <a href=https://en.wikipedia.org/wiki/Distributional_hypothesis>distributional hypothesis</a>, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with <a href=https://en.wikipedia.org/wiki/Numerical_reasoning>numerical reasoning</a> involving <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. We propose a new <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to assign and learn <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks : (i) evaluating the ability to capture numeration and magnitude ; and (ii) to perform list maximum, decoding, and <a href=https://en.wikipedia.org/wiki/Addition>addition</a>. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939346 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.385/>An Empirical Investigation of Contextualized Number Prediction</a></strong><br><a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a>
|
<a href=/people/d/daniel-spokoyny/>Daniel Spokoyny</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--385><div class="card-body p-3 small">We conduct a large scale empirical investigation of contextualized number prediction in <a href=https://en.wikipedia.org/wiki/Running_text>running text</a>. Specifically, we consider two tasks : (1)masked number prediction predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detectiondetecting an errorful numeric value within a sentence. We experiment with novel combinations of contextual encoders and output distributions over the <a href=https://en.wikipedia.org/wiki/Real_number_line>real number line</a>. Specifically, we introduce a suite of output distribution parameterizations that incorporate <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent and transformer-based encoder architectures. We evaluate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on two <a href=https://en.wikipedia.org/wiki/Data_set>numeric datasets</a> in the financial and scientific domain. Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical pre-diction and anomaly detection. We also show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938920 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.389/>Unsupervised Parsing via Constituency Tests</a></strong><br><a href=/people/s/steven-cao/>Steven Cao</a>
|
<a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--389><div class="card-body p-3 small">We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a> (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--391 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939256 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.391/>Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios</a></strong><br><a href=/people/r/ramy-eskander/>Ramy Eskander</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/m/michael-collins/>Michael Collins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--391><div class="card-body p-3 small">We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the <a href=https://en.wikipedia.org/wiki/Bible>Bible</a> as parallel data in our experiments : small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways : 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over previous work. In addition, we show that using multi-source information, either via <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a> or output combination, improves the performance for most target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.392.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--392 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.392 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939296 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.392/>Unsupervised Parsing with S-DIORA : Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DIORA</span>: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/s/subendhu-rongali/>Subendhu Rongali</a>
|
<a href=/people/y/yi-pei-chen/>Yi-Pei Chen</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--392><div class="card-body p-3 small">The deep inside-outside recursive autoencoder (DIORA ; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences * without access to labeled training data *. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and can not recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through * fine-tuning * a pre-trained DIORA with our new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, we improve the state of the art in * unsupervised * constituency parsing on the English WSJ Penn Treebank by 2.2-6 % F1, depending on the data used for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--393 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938914 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.393/>Utility is in the Eye of the User : A Critique of NLP Leaderboards<span class=acl-fixed-case>NLP</span> Leaderboards</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--393><div class="card-body p-3 small">Benchmarks such as GLUE have helped drive advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and <a href=https://en.wikipedia.org/wiki/Efficient_energy_use>energy efficiency</a>. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of <a href=https://en.wikipedia.org/wiki/Microeconomics>microeconomic theory</a>. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> as its utility to them. With this framing, we formalize how leaderboards in their current form can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a <a href=https://en.wikipedia.org/wiki/Glossary_of_economics>leaderboard</a>, since it is a cost that only the former must bear. To allow practitioners to better estimate a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., <a href=https://en.wikipedia.org/wiki/Mathematical_model>model size</a>, <a href=https://en.wikipedia.org/wiki/Efficient_energy_use>energy efficiency</a>, and inference latency).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--394 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938956 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.394" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.394/>An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training</a></strong><br><a href=/people/k/kristjan-arumae/>Kristjan Arumae</a>
|
<a href=/people/q/qing-sun/>Qing Sun</a>
|
<a href=/people/p/parminder-bhatia/>Parminder Bhatia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--394><div class="card-body p-3 small">Pre-training large language models has become a standard in the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing community</a>. Such <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate <a href=https://en.wikipedia.org/wiki/Cytomegalovirus>CF</a>. We find that elastic weight consolidation provides best overall scores yielding only a 0.33 % drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939083 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.402" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.402/>Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution<span class=acl-fixed-case>O</span>bjective <span class=acl-fixed-case>M</span>atter? <span class=acl-fixed-case>C</span>omparing <span class=acl-fixed-case>T</span>raining <span class=acl-fixed-case>O</span>bjectives for <span class=acl-fixed-case>P</span>ronoun <span class=acl-fixed-case>R</span>esolution</a></strong><br><a href=/people/y/yordan-yordanov/>Yordan Yordanov</a>
|
<a href=/people/o/oana-maria-camburu/>Oana-Maria Camburu</a>
|
<a href=/people/v/vid-kocijan/>Vid Kocijan</a>
|
<a href=/people/t/thomas-lukasiewicz/>Thomas Lukasiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--402><div class="card-body p-3 small">Hard cases of pronoun resolution have been used as a long-standing benchmark for <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training objective</a> is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and <a href=https://en.wikipedia.org/wiki/Pronoun>pronoun</a> performs the best out-of-domain. We also observe a seed-wise instability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using sequence ranking, which is not the case when the other objectives are used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--406 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939315 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.406/>Training for <a href=https://en.wikipedia.org/wiki/Gibbs_sampling>Gibbs Sampling</a> on <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a> with Neural Scoring Factors<span class=acl-fixed-case>G</span>ibbs Sampling on Conditional Random Fields with Neural Scoring Factors</a></strong><br><a href=/people/s/sida-gao/>Sida Gao</a>
|
<a href=/people/m/matthew-r-gormley/>Matthew R. Gormley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--406><div class="card-body p-3 small">Most recent improvements in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive <a href=https://en.wikipedia.org/wiki/Graphical_model>graphical models</a> are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of <a href=https://en.wikipedia.org/wiki/Graphical_model>graphical models</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> based on <a href=https://en.wikipedia.org/wiki/Gibbs_sampling>Gibbs sampling</a>. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on three languages from CoNLL-02/03. We obtain new state-of-the-art results on <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--412 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939326 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.412/>Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging</a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/w/wenhao-liu/>Wenhao Liu</a>
|
<a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--412><div class="card-body p-3 small">The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains-the act of request carries the same speaker intention whether it is for <a href=https://en.wikipedia.org/wiki/Table_reservation>restaurant reservation</a> or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model. Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--413 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938783 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.413/>Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing</a></strong><br><a href=/people/x/xilun-chen/>Xilun Chen</a>
|
<a href=/people/a/asish-ghoshal/>Asish Ghoshal</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--413><div class="card-body p-3 small">Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user&#8217;s intents (set reminder, play music, etc.). Recent advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018 ; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised neural model</a> at a 10-fold <a href=https://en.wikipedia.org/wiki/Data_reduction>data reduction</a>. In particular, we identify two fundamental factors for low-resource domain adaptation : better <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> and better <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training techniques</a>. Our <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938661 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.416/>Facilitating the Communication of Politeness through Fine-Grained Paraphrasing</a></strong><br><a href=/people/l/liye-fu/>Liye Fu</a>
|
<a href=/people/s/susan-fussell/>Susan Fussell</a>
|
<a href=/people/c/cristian-danescu-niculescu-mizil/>Cristian Danescu-Niculescu-Mizil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--416><div class="card-body p-3 small">Aided by <a href=https://en.wikipedia.org/wiki/Technology>technology</a>, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance. As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for suggesting paraphrases that achieve the intended level of <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a> under a given communication circumstance. We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker&#8217;s intentions and the listener&#8217;s perceptions in both cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.418.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939123 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.418/>Seq2Edits : Sequence Transduction Using Span-level Edit Operations<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>E</span>dits: Sequence Transduction Using Span-level Edit Operations</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/s/shankar-kumar/>Shankar Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--418><div class="card-body p-3 small">We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction</a>, our method speeds up <a href=https://en.wikipedia.org/wiki/Inference>inference</a> by up to 5.2x compared to full sequence models because <a href=https://en.wikipedia.org/wiki/Inference>inference time</a> depends on the number of edits rather than the number of target tokens. For <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>, sentence fusion, and <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction</a>, our approach improves explainability by associating each edit operation with a <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>human-readable tag</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939329 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.420/>Blank Language Models</a></strong><br><a href=/people/t/tianxiao-shen/>Tianxiao Shen</a>
|
<a href=/people/v/victor-quach/>Victor Quach</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/t/tommi-jaakkola/>Tommi Jaakkola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--420><div class="card-body p-3 small">We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>lower bound</a> of the <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginal data likelihood</a>. On the task of filling missing text snippets, BLM significantly outperforms all other <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> in terms of both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for a wide range of <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939341 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.421" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.421/>COD3S : Diverse Generation with Discrete Semantic Signatures<span class=acl-fixed-case>COD3S</span>: Diverse Generation with Discrete Semantic Signatures</a></strong><br><a href=/people/n/nathaniel-weir/>Nathaniel Weir</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--421><div class="card-body p-3 small">We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose <a href=https://en.wikipedia.org/wiki/Hamming_distance>Hamming distances</a> highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply to causal generation, the task of predicting a proposition&#8217;s plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> exhibit improved <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> without degrading task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--422 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939038 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.422" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.422/>Automatic Extraction of <a href=https://en.wikipedia.org/wiki/Rule_of_inference>Rules</a> Governing Morphological Agreement</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/z/zaid-sheikh/>Zaid Sheikh</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--422><div class="card-body p-3 small">Creating a <a href=https://en.wikipedia.org/wiki/Descriptive_grammar>descriptive grammar</a> of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement</a>, a morphosyntactic phenomenon at the core of the <a href=https://en.wikipedia.org/wiki/Grammar>grammars</a> of many of the world&#8217;s languages. We apply our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> produces, which have an average <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 78 %. We release an interface demonstrating the extracted <a href=https://en.wikipedia.org/wiki/Rule-based_programming>rules</a> at https://neulab.github.io/lase/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--425 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939176 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.425" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.425/>A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support</a></strong><br><a href=/people/a/ashish-sharma/>Ashish Sharma</a>
|
<a href=/people/a/adam-miner/>Adam Miner</a>
|
<a href=/people/d/david-atkins/>David Atkins</a>
|
<a href=/people/t/tim-althoff/>Tim Althoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--425><div class="card-body p-3 small">Empathy is critical to successful <a href=https://en.wikipedia.org/wiki/Mental_health_professional>mental health support</a>. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-based platforms</a> for <a href=https://en.wikipedia.org/wiki/Mental_health_professional>mental health support</a>, understanding <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> in these contexts is crucial. In this work, we present a computational approach to understanding how <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> over time, revealing opportunities for empathy training and feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--426 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939253 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.426" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.426/>Modeling Protagonist Emotions for Emotion-Aware Storytelling</a></strong><br><a href=/people/f/faeze-brahman/>Faeze Brahman</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--426><div class="card-body p-3 small">Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--428 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939316 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.428/>Quantifying Intimacy in Language</a></strong><br><a href=/people/j/jiaxin-pei/>Jiaxin Pei</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--428><div class="card-body p-3 small">Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> for accurately predicting the intimacy level of questions (Pearson r = 0.87). Through analyzing a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 80.5 M questions across <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, <a href=https://en.wikipedia.org/wiki/Book>books</a>, and <a href=https://en.wikipedia.org/wiki/Film>films</a>, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match <a href=https://en.wikipedia.org/wiki/Social_norm>social norms</a> around <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Social_distance>social distance</a>, and audience, each validating key findings from studies in <a href=https://en.wikipedia.org/wiki/Social_psychology>social psychology</a>. Our work demonstrates that <a href=https://en.wikipedia.org/wiki/Intimate_relationship>intimacy</a> is a pervasive and impactful social dimension of language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--430 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939132 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.430/>Weakly Supervised Subevent Knowledge Acquisition<span class=acl-fixed-case>S</span>upervised <span class=acl-fixed-case>S</span>ubevent <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>A</span>cquisition</a></strong><br><a href=/people/w/wenlin-yao/>Wenlin Yao</a>
|
<a href=/people/z/zeyu-dai/>Zeyu Dai</a>
|
<a href=/people/m/maitreyi-ramaswamy/>Maitreyi Ramaswamy</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--430><div class="card-body p-3 small">Subevents elaborate an event and widely exist in <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event descriptions</a>. Subevent knowledge is useful for <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a> and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239 K) are of high quality (90.1 % accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a> and identifying a range of event-event relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939154 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.431/>Biomedical Event Extraction as Sequence Labeling</a></strong><br><a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/r/rosario-lombardo/>Rosario Lombardo</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--431><div class="card-body p-3 small">We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best <a href=https://en.wikipedia.org/wiki/System>system</a> (Li et al., 2019) on the Genia 2011 benchmark by 1.57 % absolute <a href=https://en.wikipedia.org/wiki/Free_and_open-source_software>F1 score</a> reaching 60.22 % <a href=https://en.wikipedia.org/wiki/Free_and_open-source_software>F1</a>, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL&#8217;s speed and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> makes it a viable approach for large-scale real-world scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.432/>Annotating Temporal Dependency Graphs via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a><span class=acl-fixed-case>A</span>nnotating <span class=acl-fixed-case>T</span>emporal <span class=acl-fixed-case>D</span>ependency <span class=acl-fixed-case>G</span>raphs via <span class=acl-fixed-case>C</span>rowdsourcing</a></strong><br><a href=/people/j/jiarui-yao/>Jiarui Yao</a>
|
<a href=/people/h/haoling-qiu/>Haoling Qiu</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--432><div class="card-body p-3 small">We present the construction of a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good trade-off between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> by training a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> on this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>. The data set is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--434 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.434.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939312 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.434/>CHARM : Inferring Personal Attributes from Conversations<span class=acl-fixed-case>CHARM</span>: Inferring Personal Attributes from Conversations</a></strong><br><a href=/people/a/anna-tigunova/>Anna Tigunova</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/p/paramita-mirza/>Paramita Mirza</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--434><div class="card-body p-3 small">Personal knowledge about users&#8217; professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as <a href=https://en.wikipedia.org/wiki/Recommender_system>recommenders</a> or <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>. Conversations in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, such as <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM : a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> show the viability of CHARM for open-ended attributes, such as <a href=https://en.wikipedia.org/wiki/Profession>professions</a> and <a href=https://en.wikipedia.org/wiki/Hobby>hobbies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--437 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938651 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.437" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.437/>How Much Knowledge Can You Pack Into the Parameters of a <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a>?</a></strong><br><a href=/people/a/adam-roberts/>Adam Roberts</a>
|
<a href=/people/c/colin-raffel/>Colin Raffel</a>
|
<a href=/people/n/noam-shazeer/>Noam Shazeer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--437><div class="card-body p-3 small">It has recently been observed that neural language models trained on <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> can implicitly store and retrieve knowledge using <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language queries</a>. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--438 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938985 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.438" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.438/>EXAMS : A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering<span class=acl-fixed-case>EXAMS</span>: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering</a></strong><br><a href=/people/m/momchil-hardalov/>Momchil Hardalov</a>
|
<a href=/people/t/todor-mihaylov/>Todor Mihaylov</a>
|
<a href=/people/d/dimitrina-zlatkova/>Dimitrina Zlatkova</a>
|
<a href=/people/y/yoan-dinkov/>Yoan Dinkov</a>
|
<a href=/people/i/ivan-koychev/>Ivan Koychev</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--438><div class="card-body p-3 small">We propose EXAMS a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. EXAMS offers unique fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of the proposed models. We perform various experiments with existing top-performing multilingual pre-trained models and show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible by now. The data, code, pre-trained models, and evaluation are available at http://github.com/mhardalov/exams-qa.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--447 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.447.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938890 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.447/>Sequence-Level Mixed Sample Data Augmentation</a></strong><br><a href=/people/d/demi-guo/>Demi Guo</a>
|
<a href=/people/y/yoon-kim/>Yoon Kim</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--447><div class="card-body p-3 small">Despite their empirical success, <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input / output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation datasets</a> over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--448 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939066 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.448" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.448/>Consistency of a Recurrent Language Model With Respect to Incomplete Decoding</a></strong><br><a href=/people/s/sean-welleck/>Sean Welleck</a>
|
<a href=/people/i/ilia-kulikov/>Ilia Kulikov</a>
|
<a href=/people/j/jaedeok-kim/>Jaedeok Kim</a>
|
<a href=/people/r/richard-yuanzhe-pang/>Richard Yuanzhe Pang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--448><div class="card-body p-3 small">Despite strong performance on a variety of tasks, neural sequence models trained with <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood</a> have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can yield an infinite-length sequence that has zero probability under the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We prove that commonly used incomplete decoding algorithms <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a>, <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>, top-k sampling, and nucleus sampling are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency : consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that <a href=https://en.wikipedia.org/wiki/Inconsistency>inconsistency</a> occurs in practice, and that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> prevent <a href=https://en.wikipedia.org/wiki/Inconsistency>inconsistency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--451 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938882 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.451/>Inducing Target-Specific Latent Structures for Aspect Sentiment Classification</a></strong><br><a href=/people/c/chenhua-chen/>Chenhua Chen</a>
|
<a href=/people/z/zhiyang-teng/>Zhiyang Teng</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--451><div class="card-body p-3 small">Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsing accuracy of commercial product comments or <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> might be unsatisfactory. To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs. We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can complement supervised syntactic features with latent semantic dependencies. Experimental results on five <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> show the effectiveness of our proposed latent models, giving significantly better results than <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> without using latent graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--459 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938756 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.459" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.459/>Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph</a></strong><br><a href=/people/x/xin-lv/>Xin Lv</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/y/yichi-zhang/>Yichi Zhang</a>
|
<a href=/people/h/hao-kong/>Hao Kong</a>
|
<a href=/people/s/suhui-wu/>Suhui Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--459><div class="card-body p-3 small">Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning methods</a> are designed for dense KGs with enough paths between entities, but can not work well on those sparse KGs that only contain sparse paths for <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning</a>. On the one hand, sparse KGs contain less information, which makes it difficult for the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies : (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a>, <a href=https://en.wikipedia.org/wiki/NELL>NELL</a> and <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--460 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938826 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.460" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.460/>Knowledge Association with Hyperbolic Knowledge Graph Embeddings</a></strong><br><a href=/people/z/zequn-sun/>Zequn Sun</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/w/wei-hu/>Wei Hu</a>
|
<a href=/people/c/chengming-wang/>Chengming Wang</a>
|
<a href=/people/j/jian-dai/>Jian Dai</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--460><div class="card-body p-3 small">Capturing associations for knowledge graphs (KGs) through <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity alignment</a>, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity type inference</a> and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--461 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939236 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.461" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.461/>Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/y/yichao-zhou/>Yichao Zhou</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--461><div class="card-body p-3 small">Extracting event temporal relations is a critical task for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and plays an important role in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. Prior systems leverage <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to improve the performance of the task. However, these systems often suffer from two shortcomings : 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints ; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> with <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional constraints</a> constructed by probabilistic domain knowledge. We solve the constrained inference problem via <a href=https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field>Lagrangian Relaxation</a> and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance</a> on two widely used datasets in news and clinical domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.463.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938933 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.463" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.463/>Understanding the Difficulty of Training Transformers</a></strong><br><a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--463><div class="card-body p-3 small">Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train <a href=https://en.wikipedia.org/wiki/Transformers_(TV_series)>Transformers</a> effectively). Our objective here is to understand _ _ what complicates Transformer training _ _ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> substantiallyfor each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies <a href=https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)>small parameter perturbations</a> (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage&#8217;s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.464.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939147 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.464/>An Empirical Study of Generation Order for Machine Translation</a></strong><br><a href=/people/w/william-chan/>William Chan</a>
|
<a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/j/jamie-kiros/>Jamie Kiros</a>
|
<a href=/people/j/jakob-uszkoreit/>Jakob Uszkoreit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--464><div class="card-body p-3 small">In this work, we present an empirical study of generation order for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT&#8217;14 English German and WMT&#8217;18 English Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.<tex-math>\\to</tex-math> German and WMT&#8217;18 English <tex-math>\\to</tex-math> Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English <tex-math>\\to</tex-math> German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--469 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939385 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.469" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.469/>Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning</a></strong><br><a href=/people/y/yuncheng-hua/>Yuncheng Hua</a>
|
<a href=/people/y/yuan-fang-li/>Yuan-Fang Li</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/g/guilin-qi/>Guilin Qi</a>
|
<a href=/people/t/tongtong-wu/>Tongtong Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--469><div class="card-body p-3 small">Complex question-answering (CQA) involves answering complex <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural-language questions</a> on a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base (KB)</a>. However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1 % of the training set. We have released our code at https://github.com/DevinJake/MRL-CQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--477 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939085 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.477/>LAReQA : Language-Agnostic Answer Retrieval from a Multilingual Pool<span class=acl-fixed-case>LAR</span>e<span class=acl-fixed-case>QA</span>: Language-Agnostic Answer Retrieval from a Multilingual Pool</a></strong><br><a href=/people/u/uma-roy/>Uma Roy</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/r/rami-al-rfou/>Rami Al-Rfou</a>
|
<a href=/people/a/aditya-barua/>Aditya Barua</a>
|
<a href=/people/a/aaron-phillips/>Aaron Phillips</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--477><div class="card-body p-3 small">We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for strong cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> is important for the practical task of <a href=https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval>cross-lingual information retrieval</a>. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on zero-shot variants of our task that only target weak alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at.<i>cross</i>-language pairs to be closer in representation space than unrelated <i>same</i>-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target &#8220;weak&#8221; alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at <url>https://github.com/google-research-datasets/lareqa</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.478.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--478 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.478 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939129 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.478" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.478/>OCR Post Correction for Endangered Language Texts<span class=acl-fixed-case>OCR</span> <span class=acl-fixed-case>P</span>ost <span class=acl-fixed-case>C</span>orrection for <span class=acl-fixed-case>E</span>ndangered <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>T</span>exts</a></strong><br><a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--478><div class="card-body p-3 small">There is little to no data available to build <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language processing models</a> for most <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered languages</a>. However, <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>textual data</a> in these languages often exists in formats that are not machine-readable, such as <a href=https://en.wikipedia.org/wiki/Paperback>paper books</a> and <a href=https://en.wikipedia.org/wiki/Image_scanner>scanned images</a>. In this work, we address the task of extracting text from these <a href=https://en.wikipedia.org/wiki/Resource_(computing)>resources</a>. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34 % on average across the three languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.479.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--479 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.479 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939158 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.479/>X-FACTR : Multilingual Factual Knowledge Retrieval from Pretrained Language Models<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>FACTR</span>: Multilingual Factual Knowledge Retrieval from Pretrained Language Models</a></strong><br><a href=/people/z/zhengbao-jiang/>Zhengbao Jiang</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/j/jun-araki/>Jun Araki</a>
|
<a href=/people/h/haibo-ding/>Haibo Ding</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--479><div class="card-body p-3 small">Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as Punta Cana is located in _. However, while knowledge is both written and queried in many languages, studies on LMs&#8217; factual representation ability have almost invariably been performed on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--487 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.487.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939138 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.487" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.487/>Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks<span class=acl-fixed-case>C</span>ombinatory <span class=acl-fixed-case>C</span>ategorial <span class=acl-fixed-case>G</span>rammar with Attentive Graph Convolutional Networks</a></strong><br><a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--487><div class="card-body p-3 small">Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> from chunks (n-grams) extracted from a lexicon and apply <a href=https://en.wikipedia.org/wiki/Attention>attention</a> over the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.490.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--490 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.490 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938765 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.490/>Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots</a></strong><br><a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/h/hong-xu/>Hong Xu</a>
|
<a href=/people/s/sihong-liu/>Sihong Liu</a>
|
<a href=/people/f/fanyu-meng/>Fanyu Meng</a>
|
<a href=/people/m/min-hu/>Min Hu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--490><div class="card-body p-3 small">Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.492.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--492 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.492 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.492.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938925 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.492" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.492/>Structure Aware Negative Sampling in <a href=https://en.wikipedia.org/wiki/Knowledge_graph>Knowledge Graphs</a></a></strong><br><a href=/people/k/kian-ahrabian/>Kian Ahrabian</a>
|
<a href=/people/a/aarash-feizi/>Aarash Feizi</a>
|
<a href=/people/y/yasmin-salehi/>Yasmin Salehi</a>
|
<a href=/people/w/william-l-hamilton/>William L. Hamilton</a>
|
<a href=/people/a/avishek-joey-bose/>Avishek Joey Bose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--492><div class="card-body p-3 small">Learning low-dimensional representations for entities and relations in <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the <a href=https://en.wikipedia.org/wiki/Embedding>embedding model</a> to learn discriminative representations and find critical characteristics of observed data. While earlier <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> either employ too simple <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>corruption distributions</a>, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node&#8217;s k-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.<tex-math>k</tex-math>-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.493.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--493 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.493 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938671 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.493" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.493/>Neural Mask Generator : Learning to Generate Adaptive Word Maskings for Language Model Adaptation</a></strong><br><a href=/people/m/minki-kang/>Minki Kang</a>
|
<a href=/people/m/moonsu-han/>Moonsu Han</a>
|
<a href=/people/s/sung-ju-hwang/>Sung Ju Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--493><div class="card-body p-3 small">We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.498.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--498 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.498 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938695 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.498" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.498/>BAE : BERT-based Adversarial Examples for Text Classification<span class=acl-fixed-case>BAE</span>: <span class=acl-fixed-case>BERT</span>-based Adversarial Examples for Text Classification</a></strong><br><a href=/people/s/siddhant-garg/>Siddhant Garg</a>
|
<a href=/people/g/goutham-ramakrishnan/>Goutham Ramakrishnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--498><div class="card-body p-3 small">Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> and semantic coherence as compared to prior work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.499.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--499 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.499 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.499.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938706 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.499/>Adversarial Self-Supervised Data-Free Distillation for Text Classification<span class=acl-fixed-case>A</span>dversarial <span class=acl-fixed-case>S</span>elf-<span class=acl-fixed-case>S</span>upervised <span class=acl-fixed-case>D</span>ata-<span class=acl-fixed-case>F</span>ree <span class=acl-fixed-case>D</span>istillation for <span class=acl-fixed-case>T</span>ext <span class=acl-fixed-case>C</span>lassification</a></strong><br><a href=/people/x/xinyin-ma/>Xinyin Ma</a>
|
<a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/g/gongfan-fang/>Gongfan Fang</a>
|
<a href=/people/c/chen-chen/>Chen Chen</a>
|
<a href=/people/c/chenghao-jia/>Chenghao Jia</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--499><div class="card-body p-3 small">Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to a resource-efficient lightweight model. However, most KD algorithms, especially in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher&#8217;s hidden knowledge. Meanwhile, with a self-supervised module to quantify the student&#8217;s ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on several text classification datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--501 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938724 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.501/>The Thieves on Sesame Street are Polyglots-Extracting Multilingual Models from Monolingual APIs<span class=acl-fixed-case>API</span>s</a></strong><br><a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/b/bryan-mccann/>Bryan McCann</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--501><div class="card-body p-3 small">Pre-training in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim&#8217;s labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--502 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939044 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.502/>When Hearst Is not Enough : Improving Hypernymy Detection from Corpus with Distributional Models</a></strong><br><a href=/people/c/changlong-yu/>Changlong Yu</a>
|
<a href=/people/j/jialong-han/>Jialong Han</a>
|
<a href=/people/p/peifeng-wang/>Peifeng Wang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/w/wilfred-ng/>Wilfred Ng</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--502><div class="card-body p-3 small">We address hypernymy detection, i.e., whether an is-a relationship exists between words (x, y), with the help of large textual corpora. Most conventional approaches to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x, y) pairs relieved. However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional methods</a> are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> demonstrates improvements that are both competitive and explainable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--510 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939371 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.510" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.510/>Summarizing Text on Any Aspects : A Knowledge-Informed Weakly-Supervised Approach</a></strong><br><a href=/people/b/bowen-tan/>Bowen Tan</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--510><div class="card-body p-3 small">Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.<i>arbitrary</i> aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--514 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938977 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.514" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.514/>Coarse-to-Fine Pre-training for Named Entity Recognition<span class=acl-fixed-case>C</span>oarse-to-<span class=acl-fixed-case>F</span>ine <span class=acl-fixed-case>P</span>re-training for <span class=acl-fixed-case>N</span>amed <span class=acl-fixed-case>E</span>ntity <span class=acl-fixed-case>R</span>ecognition</a></strong><br><a href=/people/x/xue-mengge/>Xue Mengge</a>
|
<a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/z/zhenyu-zhang/>Zhenyu Zhang</a>
|
<a href=/people/t/tingwen-liu/>Tingwen Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--514><div class="card-body p-3 small">More recently, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> hasachieved great advances aided by pre-trainingapproaches such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models. Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we leverage thegazetteer-based distant supervision strategy totrain the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> extract coarse-grained typedentities. Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering. Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks. Besides, weshow that our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938987 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.515" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.515/>Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment</a></strong><br><a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--515><div class="card-body p-3 small">Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves significant improvements (5.10 % on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at.<url>https://github.com/thunlp/explore-and-evaluate</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--516 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939200 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.516/>Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning</a></strong><br><a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/a/arzoo-katiyar/>Arzoo Katiyar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--516><div class="card-body p-3 small">We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a>. Across several test domains, we show that a <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor classifier</a> in this <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature-space</a> is far more effective than the standard meta-learning approaches. We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6 % to 16 % absolute points over prior meta-learning based systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--517 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938677 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.517" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.517/>Learning Structured Representations of Entity Names using Active Learning and Weak Supervision<span class=acl-fixed-case>A</span>ctive <span class=acl-fixed-case>L</span>earning and Weak Supervision</a></strong><br><a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/p/poornima-chozhiyath-raman/>Poornima Chozhiyath Raman</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/lucian-popa/>Lucian Popa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--517><div class="card-body p-3 small">Structured representations of entity names are useful for many entity-related tasks such as <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity normalization</a> and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> and weak supervision to solve this problem. Our experimental evaluation show that this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> enables the learning of high-quality models from merely a dozen or so labeled examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--518 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939232 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.518/>Entity Enhanced BERT Pre-training for Chinese NER<span class=acl-fixed-case>BERT</span> Pre-training for <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/c/chen-jia/>Chen Jia</a>
|
<a href=/people/y/yuefeng-shi/>Yuefeng Shi</a>
|
<a href=/people/q/qinrong-yang/>Qinrong Yang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--518><div class="card-body p-3 small">Character-level BERT pre-trained in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> suffers a limitation of lacking lexicon information, which shows effectiveness for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese NER</a>. To integrate the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into <a href=https://en.wikipedia.org/wiki/Parameter>model parameters</a> in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--519 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939238 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.519" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.519/>Scalable Zero-shot Entity Linking with Dense Entity Retrieval</a></strong><br><a href=/people/l/ledell-wu/>Ledell Wu</a>
|
<a href=/people/f/fabio-petroni/>Fabio Petroni</a>
|
<a href=/people/m/martin-josifoski/>Martin Josifoski</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--519><div class="card-body p-3 small">This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>mention context</a> and the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity descriptions</a>. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor search</a> (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--520 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939254 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.520/>A Dataset for Tracking Entities in Open Domain Procedural Text</a></strong><br><a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/m/michal-guerquin/>Michal Guerquin</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--520><div class="card-body p-3 small">We present the first <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using <a href=https://en.wikipedia.org/wiki/Potato>potatoes</a>, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, we create OPENPI, a high-quality (91.5 % coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from <a href=https://en.wikipedia.org/wiki/WikiHow>WikiHow.com</a>. A current state-of-the-art generation model on this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> achieves 16.1 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> based on BLEU metric, leaving enough room for novel model architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--521 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939339 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.521/>Design Challenges in Low-resource Cross-lingual Entity Linking</a></strong><br><a href=/people/x/xingyu-fu/>Xingyu Fu</a>
|
<a href=/people/w/weijia-shi/>Weijia Shi</a>
|
<a href=/people/x/xiaodong-yu/>Xiaodong Yu</a>
|
<a href=/people/z/zian-zhao/>Zian Zhao</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--521><div class="card-body p-3 small">Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia&#8217;s interlanguage links and thus suffer when the foreign language&#8217;s Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25 % in gold candidate recall and of 13 % in end-to-end linking accuracy over state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--523 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938803 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.523" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.523/>LUKE : Deep Contextualized Entity Representations with Entity-aware Self-attention<span class=acl-fixed-case>LUKE</span>: Deep Contextualized Entity Representations with Entity-aware Self-attention</a></strong><br><a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/a/akari-asai/>Akari Asai</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/h/hideaki-takeda/>Hideaki Takeda</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--523><div class="card-body p-3 small">Entity representations are useful in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a> involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets : Open Entity (entity typing), TACRED (relation classification), <a href=https://en.wikipedia.org/wiki/Named_entity_recognition>CoNLL-2003 (named entity recognition)</a>, ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--527 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939152 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.527/>Template Guided Text Generation for Task-Oriented Dialogue</a></strong><br><a href=/people/m/mihir-kale/>Mihir Kale</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--527><div class="card-body p-3 small">Virtual assistants such as <a href=https://en.wikipedia.org/wiki/Google_Assistant>Google Assistant</a>, <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Amazon Alexa</a>, and <a href=https://en.wikipedia.org/wiki/Siri>Apple Siri</a> enable users to interact with a large number of services and APIs on the web using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a>. First, we propose a schema-guided approach which conditions the generation on a schema describing the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Our second method investigates the use of a small number of <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a>, growing linearly in number of slots, to convey the <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> of the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a>. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.528.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--528 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.528 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939212 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.528" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.528/>MOCHA : A Dataset for Training and Evaluating Generative Reading Comprehension Metrics<span class=acl-fixed-case>MOCHA</span>: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics</a></strong><br><a href=/people/a/anthony-chen/>Anthony Chen</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--528><div class="card-body p-3 small">Posing <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. To address this, we introduce a <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for training and evaluating generative reading comprehension metrics : MOdeling Correctness with Human Annotations. MOCHA contains 40 K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--529 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.529.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939248 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.529/>Plan ahead : Self-Supervised Text Planning for Paragraph Completion Task</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--529><div class="card-body p-3 small">Despite the recent success of contextualized language models on various NLP tasks, <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> itself can not capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a <a href=https://en.wikipedia.org/wiki/Planning>planning process</a>. Where can the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM ; predicting masked sentences in a paragraph. However, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of <a href=https://en.wikipedia.org/wiki/Index_term>content keywords</a> are provided, overall generation quality also increases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--531 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938660 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.531/>Towards Persona-Based Empathetic Conversational Models</a></strong><br><a href=/people/p/peixiang-zhong/>Peixiang Zhong</a>
|
<a href=/people/c/chen-zhang/>Chen Zhang</a>
|
<a href=/people/h/hao-wang/>Hao Wang</a>
|
<a href=/people/y/yong-liu/>Yong Liu</a>
|
<a href=/people/c/chunyan-miao/>Chunyan Miao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--531><div class="card-body p-3 small">Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In <a href=https://en.wikipedia.org/wiki/Psychology>Psychology</a>, <a href=https://en.wikipedia.org/wiki/Persona>persona</a> has been shown to be highly correlated to <a href=https://en.wikipedia.org/wiki/Personality>personality</a>, which in turn influences <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>. In addition, our empirical analysis also suggests that <a href=https://en.wikipedia.org/wiki/Persona>persona</a> plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of <a href=https://en.wikipedia.org/wiki/Persona>persona</a> on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of <a href=https://en.wikipedia.org/wiki/Persona>persona</a> on empathetic responding. Notably, our results show that <a href=https://en.wikipedia.org/wiki/Persona>persona</a> improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between <a href=https://en.wikipedia.org/wiki/Persona>persona</a> and <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a> in human conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--539 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938821 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.539" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.539/>Profile Consistency Identification for Open-domain Dialogue Agents</a></strong><br><a href=/people/h/haoyu-song/>Haoyu Song</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/z/zhengyu-zhao/>Zhengyu Zhao</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--539><div class="card-body p-3 small">Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attribute information</a> in the responses, but few efforts have been made to identify the <a href=https://en.wikipedia.org/wiki/Consistency_(database_systems)>consistency relations</a> between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110 K single-turn conversations and their key-value attribute profiles. Explicit relation between <a href=https://en.wikipedia.org/wiki/Information_retrieval>response</a> and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a> demonstrate that the profile consistency identification model is conducive for improving dialogue consistency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--540 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938872 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.540/>An Element-aware Multi-representation Model for Law Article Prediction</a></strong><br><a href=/people/h/huilin-zhong/>Huilin Zhong</a>
|
<a href=/people/j/junsheng-zhou/>Junsheng Zhou</a>
|
<a href=/people/w/weiguang-qu/>Weiguang Qu</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/y/yanhui-gu/>Yanhui Gu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--540><div class="card-body p-3 small">Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 5.84 %, the <a href=https://en.wikipedia.org/wiki/Macroeconomic_model>macro F1</a> of 6.42 %, and the micro F1 of 4.28 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--541 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938911 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.541/>Recurrent Event Network : Autoregressive Structure Inferenceover Temporal Knowledge Graphs</a></strong><br><a href=/people/w/woojeong-jin/>Woojeong Jin</a>
|
<a href=/people/m/meng-qu/>Meng Qu</a>
|
<a href=/people/x/xisen-jin/>Xisen Jin</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--541><div class="card-body p-3 small">Knowledge graph reasoning is a critical task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two <a href=https://en.wikipedia.org/wiki/Module_(mathematics)>modules</a>. We evaluate our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--543 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939245 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.543/>Less is More : <a href=https://en.wikipedia.org/wiki/Attentional_control>Attention Supervision</a> with Counterfactuals for Text Classification</a></strong><br><a href=/people/s/seungtaek-choi/>Seungtaek Choi</a>
|
<a href=/people/h/haeju-park/>Haeju Park</a>
|
<a href=/people/j/jinyoung-yeo/>Jinyoung Yeo</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--543><div class="card-body p-3 small">We aim to leverage <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human and machine intelligence</a> together for <a href=https://en.wikipedia.org/wiki/Attentional_control>attention supervision</a>. Specifically, we show that human annotation cost can be kept reasonably low, while its <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> can be enhanced by <a href=https://en.wikipedia.org/wiki/Machine_vision>machine self-supervision</a>. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and news categorization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--544 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939250 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.544/>MODE-LSTM : A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification<span class=acl-fixed-case>MODE</span>-<span class=acl-fixed-case>LSTM</span>: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification</a></strong><br><a href=/people/q/qianli-ma/>Qianli Ma</a>
|
<a href=/people/z/zhenxi-lin/>Zhenxi Lin</a>
|
<a href=/people/j/jiangyue-yan/>Jiangyue Yan</a>
|
<a href=/people/z/zipeng-chen/>Zipeng Chen</a>
|
<a href=/people/l/liuhong-yu/>Liuhong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--544><div class="card-body p-3 small">The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this <a href=https://en.wikipedia.org/wiki/Biomolecular_structure>structure</a> with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with BERT to further boost the generalization performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--545 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.545/>HSCNN : A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification<span class=acl-fixed-case>HSCNN</span>: A Hybrid-<span class=acl-fixed-case>S</span>iamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification</a></strong><br><a href=/people/w/wenshuo-yang/>Wenshuo Yang</a>
|
<a href=/people/j/jiyi-li/>Jiyi Li</a>
|
<a href=/people/f/fumiyo-fukumoto/>Fumiyo Fukumoto</a>
|
<a href=/people/y/yanming-ye/>Yanming Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--545><div class="card-body p-3 small">The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks ; a category-specific similarity in the Siamese structure ; a specific sampling method for training HSCNN. The results using two benchmark datasets and three <a href=https://en.wikipedia.org/wiki/Loss_function>loss objectives</a> show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--546 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939367 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.546/>Multi-Stage Pre-training for Automated Chinese Essay Scoring<span class=acl-fixed-case>C</span>hinese Essay Scoring</a></strong><br><a href=/people/w/wei-song/>Wei Song</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a>
|
<a href=/people/r/ruiji-fu/>Ruiji Fu</a>
|
<a href=/people/l/lizhen-liu/>Lizhen Liu</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/m/miaomiao-cheng/>Miaomiao Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--546><div class="card-body p-3 small">This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components : weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the <a href=https://en.wikipedia.org/wiki/Score_(sport)>scorer</a> is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of <a href=https://en.wikipedia.org/wiki/Effectiveness>effectiveness</a> and domain adaptation ability, while in-depth analysis also reveals its limitations..</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--547 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.547" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.547/>Multi-hop Inference for Question-driven Summarization</a></strong><br><a href=/people/y/yang-deng/>Yang Deng</a>
|
<a href=/people/w/wenxuan-zhang/>Wenxuan Zhang</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--547><div class="card-body p-3 small">Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely <a href=https://en.wikipedia.org/wiki/WikiHow>WikiHow</a> and PubMedQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--548 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938859 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.548" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.548/>Towards Interpretable Reasoning over Paragraph Effects in Situation</a></strong><br><a href=/people/m/mucheng-ren/>Mucheng Ren</a>
|
<a href=/people/x/xiubo-geng/>Xiubo Geng</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--548><div class="card-body p-3 small">We focus on the task of reasoning over paragraph effects in situation, which requires a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to understand the cause and effect described in a background paragraph, and apply the <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step black box model. Inspired by <a href=https://en.wikipedia.org/wiki/Cognition>human cognitive processes</a>, in this paper we propose a sequential approach for this task which explicitly models each step of the <a href=https://en.wikipedia.org/wiki/Reason>reasoning process</a> with <a href=https://en.wikipedia.org/wiki/Modular_programming>neural network modules</a>. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--549 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939022 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.549/>Question Directed Graph Attention Network for <a href=https://en.wikipedia.org/wiki/Numerical_analysis>Numerical Reasoning</a> over Text</a></strong><br><a href=/people/k/kunlong-chen/>Kunlong Chen</a>
|
<a href=/people/w/weidi-xu/>Weidi Xu</a>
|
<a href=/people/x/xingyi-cheng/>Xingyi Cheng</a>
|
<a href=/people/z/zou-xiaochuan/>Zou Xiaochuan</a>
|
<a href=/people/y/yuyu-zhang/>Yuyu Zhang</a>
|
<a href=/people/l/le-song/>Le Song</a>
|
<a href=/people/t/taifeng-wang/>Taifeng Wang</a>
|
<a href=/people/y/yuan-qi/>Yuan Qi</a>
|
<a href=/people/w/wei-chu/>Wei Chu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--549><div class="card-body p-3 small">Numerical reasoning over texts, such as <a href=https://en.wikipedia.org/wiki/Addition>addition</a>, <a href=https://en.wikipedia.org/wiki/Subtraction>subtraction</a>, sorting and counting, is a challenging machine reading comprehension task, since it requires both <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and <a href=https://en.wikipedia.org/wiki/Arithmetic>arithmetic computation</a>. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--550 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939151 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.550" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.550/>Dense Passage Retrieval for Open-Domain Question Answering</a></strong><br><a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/l/ledell-wu/>Ledell Wu</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--550><div class="card-body p-3 small">Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as <a href=https://en.wikipedia.org/wiki/TF-IDF>TF-IDF</a> or <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19 % absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--551 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938727 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.551/>Distilling Structured Knowledge for Text-Based Relational Reasoning</a></strong><br><a href=/people/j/jin-dong/>Jin Dong</a>
|
<a href=/people/m/marc-antoine-rondeau/>Marc-Antoine Rondeau</a>
|
<a href=/people/w/william-l-hamilton/>William L. Hamilton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--551><div class="card-body p-3 small">There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text. In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance. We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP model</a> (e.g., an LSTM) via knowledge distillation. To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN. We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.552.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--552 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.552 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938836 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.552/>Asking without Telling : Exploring Latent Ontologies in Contextual Representations</a></strong><br><a href=/people/j/julian-michael/>Julian Michael</a>
|
<a href=/people/j/jan-a-botha/>Jan A. Botha</a>
|
<a href=/people/i/ian-tenney/>Ian Tenney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--552><div class="card-body p-3 small">The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn : do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe&#8217;s inputs. Without access to fine-grained gold labels, LSL extracts <a href=https://en.wikipedia.org/wiki/Emergence>emergent structure</a> from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of <a href=https://en.wikipedia.org/wiki/Emergence>emergent structure</a> in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--553 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.553.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938879 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.553" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.553/>Pretrained Language Model <a href=https://en.wikipedia.org/wiki/Embryology>Embryology</a> : The Birth of ALBERT<span class=acl-fixed-case>P</span>retrained Language Model Embryology: <span class=acl-fixed-case>T</span>he Birth of <span class=acl-fixed-case>ALBERT</span></a></strong><br><a href=/people/c/cheng-han-chiang/>Cheng-Han Chiang</a>
|
<a href=/people/s/sung-feng-huang/>Sung-Feng Huang</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--553><div class="card-body p-3 small">While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the <a href=https://en.wikipedia.org/wiki/Embryology>embryology</a> of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> do not generally improve as pretraining proceeds, nor do downstream tasks&#8217; performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with more comprehensive knowledge. We provide <a href=https://en.wikipedia.org/wiki/Source_code>source codes</a> and pretrained models to reproduce our results at.<i>embryology</i> of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks&#8217; performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We provide source codes and pretrained models to reproduce our results at <url>https://github.com/d223302/albert-embryology</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--556 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.556/>You are grounded ! : Latent Name Artifacts in Pre-trained Language Models</a></strong><br><a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--556><div class="card-body p-3 small">Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a>, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for &#8216;Donald is a&#8217; substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional <a href=https://en.wikipedia.org/wiki/Training>pre-training</a> on different corpora may mitigate this bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--558 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.558.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938845 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.558/>Grounded Adaptation for Zero-shot Executable Semantic Parsing</a></strong><br><a href=/people/v/victor-zhong/>Victor Zhong</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/s/sida-i-wang/>Sida I. Wang</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--558><div class="card-body p-3 small">We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified through execution. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.561.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--561 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.561 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939001 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.561/>What Do You Mean by That? A Parser-Independent Interactive Approach for Enhancing Text-to-SQL<span class=acl-fixed-case>SQL</span></a></strong><br><a href=/people/y/yuntao-li/>Yuntao Li</a>
|
<a href=/people/b/bei-chen/>Bei Chen</a>
|
<a href=/people/q/qian-liu/>Qian Liu</a>
|
<a href=/people/y/yan-gao/>Yan Gao</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--561><div class="card-body p-3 small">In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users&#8217; natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--562 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938688 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.562/>DuSQL : A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset<span class=acl-fixed-case>D</span>u<span class=acl-fixed-case>SQL</span>: A Large-Scale and Pragmatic <span class=acl-fixed-case>C</span>hinese Text-to-<span class=acl-fixed-case>SQL</span> Dataset</a></strong><br><a href=/people/l/lijie-wang/>Lijie Wang</a>
|
<a href=/people/a/ao-zhang/>Ao Zhang</a>
|
<a href=/people/k/kun-wu/>Kun Wu</a>
|
<a href=/people/k/ke-sun/>Ke Sun</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--562><div class="card-body p-3 small">Due to the lack of <a href=https://en.wikipedia.org/wiki/Data>labeled data</a>, previous research on text-to-SQL parsing mainly focuses on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question / SQL pairs. Our new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer collaboration</a>. The basic idea is automatically generating <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a> based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and <a href=https://en.wikipedia.org/wiki/Data_analysis>data statistics</a> of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, including a simple yet effective extension to IRNet for handling calculation SQL queries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--563 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939352 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.563/>Mention Extraction and Linking for SQL Query Generation<span class=acl-fixed-case>SQL</span> Query Generation</a></strong><br><a href=/people/j/jianqiang-ma/>Jianqiang Ma</a>
|
<a href=/people/z/zeyu-yan/>Zeyu Yan</a>
|
<a href=/people/s/shuai-pang/>Shuai Pang</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/j/jianping-shen/>Jianping Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--563><div class="card-body p-3 small">On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves the first place on the WikiSQL benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.565.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--565 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.565 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.565.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938865 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.565" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.565/>A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis</a></strong><br><a href=/people/z/zehui-dai/>Zehui Dai</a>
|
<a href=/people/c/cheng-peng/>Cheng Peng</a>
|
<a href=/people/h/huajie-chen/>Huajie Chen</a>
|
<a href=/people/y/yadong-ding/>Yadong Ding</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--565><div class="card-body p-3 small">(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> feasible for <a href=https://en.wikipedia.org/wiki/Incremental_learning>incremental learning</a>, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., <a href=https://en.wikipedia.org/wiki/Category_(mathematics)>category name</a>, for task discrimination. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.566.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--566 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.566 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938884 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.566" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.566/>Train No Evil : Selective Masking for Task-Guided Pre-Training</a></strong><br><a href=/people/y/yuxian-gu/>Yuxian Gu</a>
|
<a href=/people/z/zhengyan-zhang/>Zhengyan Zhang</a>
|
<a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--566><div class="card-body p-3 small">Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models can not always well capture the domain-specific and task-specific patterns. In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns. Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens. Experimental results on two sentiment analysis tasks show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can achieve comparable or even better performance with less than 50 % of computation cost, which indicates our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is both effective and efficient. The source code of this paper can be obtained from.<url>https://github.com/thunlp/SelectiveMasking</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.569.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--569 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.569 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939058 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.569" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.569/>APE : Argument Pair Extraction from Peer Review and Rebuttal via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task Learning</a><span class=acl-fixed-case>APE</span>: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning</a></strong><br><a href=/people/l/liying-cheng/>Liying Cheng</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/q/qian-yu/>Qian Yu</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--569><div class="card-body p-3 small">Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(I)>them</a> simultaneously. In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them. We prepare a challenging <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task. To automatically detect argumentative propositions and extract argument pairs from this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, we cast it as the combination of a sequence labeling task and a text relation classification task. Thus, we propose a multitask learning framework based on hierarchical LSTM networks. Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as well as motivate future research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.576.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--576 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.576 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.576.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939173 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.576" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.576/>On the Ability and Limitations of Transformers to Recognize <a href=https://en.wikipedia.org/wiki/Formal_language>Formal Languages</a><span class=acl-fixed-case>A</span>bility and <span class=acl-fixed-case>L</span>imitations of <span class=acl-fixed-case>T</span>ransformers to <span class=acl-fixed-case>R</span>ecognize <span class=acl-fixed-case>F</span>ormal <span class=acl-fixed-case>L</span>anguages</a></strong><br><a href=/people/s/satwik-bhattamishra/>Satwik Bhattamishra</a>
|
<a href=/people/k/kabir-ahuja/>Kabir Ahuja</a>
|
<a href=/people/n/navin-goyal/>Navin Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--576><div class="card-body p-3 small">Transformers have supplanted <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent models</a> in a large number of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. However, the differences in their abilities to model different <a href=https://en.wikipedia.org/wiki/Syntax>syntactic properties</a> remain largely unknown. Past works suggest that LSTMs generalize very well on <a href=https://en.wikipedia.org/wiki/Regular_language>regular languages</a> and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such <a href=https://en.wikipedia.org/wiki/Programming_language>languages</a> as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this <a href=https://en.wikipedia.org/wiki/Class_(biology)>subclass</a>, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of <a href=https://en.wikipedia.org/wiki/Regular_language>regular languages</a> with degrading performance as we make languages more complex according to a well-known measure of <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.583.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--583 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.583 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938772 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.583/>Is <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>Graph Structure</a> Necessary for Multi-hop Question Answering?<span class=acl-fixed-case>G</span>raph <span class=acl-fixed-case>S</span>tructure <span class=acl-fixed-case>N</span>ecessary for <span class=acl-fixed-case>M</span>ulti-hop <span class=acl-fixed-case>Q</span>uestion <span class=acl-fixed-case>A</span>nswering?</a></strong><br><a href=/people/n/nan-shao/>Nan Shao</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--583><div class="card-body p-3 small">Recently, attempting to model texts as <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> and introducing graph neural networks to deal with it has become a trend in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP research areas</a>. In this paper, we investigate whether the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> may not be necessary for textual multi-hop reasoning. We point out that both <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> and <a href=https://en.wikipedia.org/wiki/Adjacency_matrix>adjacency matrix</a> are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments demonstrate that graph-attention or the entire <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> can be replaced by self-attention or Transformers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.588.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--588 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.588 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939295 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.588" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.588/>SLURP : A Spoken Language Understanding Resource Package<span class=acl-fixed-case>SLURP</span>: A Spoken Language Understanding Resource Package</a></strong><br><a href=/people/e/emanuele-bastianelli/>Emanuele Bastianelli</a>
|
<a href=/people/a/andrea-vanzo/>Andrea Vanzo</a>
|
<a href=/people/p/pawel-swietojanski/>Pawel Swietojanski</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--588><div class="card-body p-3 small">Spoken Language Understanding infers <a href=https://en.wikipedia.org/wiki/Semantics>semantic meaning</a> directly from audio data, and thus promises to reduce <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> and misunderstandings in <a href=https://en.wikipedia.org/wiki/End_user>end-user applications</a>. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following : (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets ; (2) Competitive baselines based on state-of-the-art NLU and ASR systems ; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.593.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--593 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.593 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938751 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.593" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.593/>DyERNIE : Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion<span class=acl-fixed-case>D</span>y<span class=acl-fixed-case>ERNIE</span>: <span class=acl-fixed-case>D</span>ynamic <span class=acl-fixed-case>E</span>volution of <span class=acl-fixed-case>R</span>iemannian <span class=acl-fixed-case>M</span>anifold <span class=acl-fixed-case>E</span>mbeddings for <span class=acl-fixed-case>T</span>emporal <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>G</span>raph <span class=acl-fixed-case>C</span>ompletion</a></strong><br><a href=/people/z/zhen-han/>Zhen Han</a>
|
<a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/y/yunpu-ma/>Yunpu Ma</a>
|
<a href=/people/v/volker-tresp/>Volker Tresp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--593><div class="card-body p-3 small">There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean space</a>, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity representations</a> evolve according to a <a href=https://en.wikipedia.org/wiki/Velocity_vector>velocity vector</a> defined in the <a href=https://en.wikipedia.org/wiki/Tangent_space>tangent space</a> at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on <a href=https://en.wikipedia.org/wiki/Riemannian_manifold>Riemannian manifolds</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.596.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--596 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.596 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.596.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939108 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.596" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.596/>Message Passing for Hyper-Relational Knowledge Graphs</a></strong><br><a href=/people/m/mikhail-galkin/>Mikhail Galkin</a>
|
<a href=/people/p/priyansh-trivedi/>Priyansh Trivedi</a>
|
<a href=/people/g/gaurav-maheshwari/>Gaurav Maheshwari</a>
|
<a href=/people/r/ricardo-usbeck/>Ricardo Usbeck</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--596><div class="card-body p-3 small">Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional <a href=https://en.wikipedia.org/wiki/Attribute&#8211;value_pair>key-value pairs</a> along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder-StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset-WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--602 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939042 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.602/>PowerTransformer : Unsupervised Controllable Revision for Biased Language Correction<span class=acl-fixed-case>P</span>ower<span class=acl-fixed-case>T</span>ransformer: Unsupervised Controllable Revision for Biased Language Correction</a></strong><br><a href=/people/x/xinyao-ma/>Xinyao Ma</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--602><div class="card-body p-3 small">Unconscious biases continue to be prevalent in modern text and media, calling for <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (_ She daydreams about being a doctor _) while a man is portrayed as more proactive and powerful (_ He pursues his dream of being a doctor _). We formulate * * Controllable Debiasing * *, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms <a href=https://en.wikipedia.org/wiki/Ablation>ablations</a> and existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> from related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--604 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939101 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.604/>Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments</a></strong><br><a href=/people/s/sungho-jeon/>Sungho Jeon</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--604><div class="card-body p-3 small">Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, <a href=https://en.wikipedia.org/wiki/Information_technology>they</a> do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> then incorporates this <a href=https://en.wikipedia.org/wiki/Structural_analysis>structural information</a> into a structure-aware transformer. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different <a href=https://en.wikipedia.org/wiki/Quality_(philosophy)>quality scores</a>. Finally, we investigate what our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns in terms of theoretical claims.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--608 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939146 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.608/>Which * BERT? A Survey Organizing Contextualized Encoders<span class=acl-fixed-case>BERT</span>? <span class=acl-fixed-case>A</span> Survey Organizing Contextualized Encoders</a></strong><br><a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--608><div class="card-body p-3 small">Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--609 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939235 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.609" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.609/>Fact or Fiction : Verifying Scientific Claims</a></strong><br><a href=/people/d/david-wadden/>David Wadden</a>
|
<a href=/people/s/shanchuan-lin/>Shanchuan Lin</a>
|
<a href=/people/k/kyle-lo/>Kyle Lo</a>
|
<a href=/people/l/lucy-lu-wang/>Lucy Lu Wang</a>
|
<a href=/people/m/madeleine-van-zuylen/>Madeleine van Zuylen</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--609><div class="card-body p-3 small">We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4 K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A <a href=https://en.wikipedia.org/wiki/Glossary_of_video_game_terms>leaderboard</a> and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--612 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939303 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.612/>Causal Inference of Script Knowledge</a></strong><br><a href=/people/n/noah-weber/>Noah Weber</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--612><div class="card-body p-3 small">When does a sequence of events define an everyday scenario and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> based on <a href=https://en.wikipedia.org/wiki/Causality>causal effects</a> better matches the intuition of what a script represents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--616 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939052 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.616" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.616/>Detecting Word Sense Disambiguation Biases in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for Model-Agnostic Adversarial Attacks</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--616><div class="card-body p-3 small">Word sense disambiguation is a well-known source of translation errors in <a href=https://en.wikipedia.org/wiki/NMT>NMT</a>. We posit that some of the incorrect disambiguation choices are due to models&#8217; over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on the same data are vulnerable to different attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.620.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--620 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.620.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939379 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.620" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.620/>Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in <a href=https://en.wikipedia.org/wiki/News_media>News Media</a></a></strong><br><a href=/people/s/shamik-roy/>Shamik Roy</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--620><div class="card-body p-3 small">In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in <a href=https://en.wikipedia.org/wiki/Ideology>political ideology</a> in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, <a href=https://en.wikipedia.org/wiki/Immigration>immigration</a>, <a href=https://en.wikipedia.org/wiki/Gun_politics_in_the_United_States>gun-control</a>, and <a href=https://en.wikipedia.org/wiki/Abortion>abortion</a>. We demonstrate the ability of the subframes to capture <a href=https://en.wikipedia.org/wiki/Ideology>ideological differences</a> and analyze <a href=https://en.wikipedia.org/wiki/Discourse_analysis>political discourse</a> in <a href=https://en.wikipedia.org/wiki/News_media>news media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--623 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939277 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.623" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.623/>Explainable Automated Fact-Checking for Public Health Claims</a></strong><br><a href=/people/n/neema-kotonya/>Neema Kotonya</a>
|
<a href=/people/f/francesca-toni/>Francesca Toni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--623><div class="card-body p-3 small">Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking studies</a> focus exclusively on <a href=https://en.wikipedia.org/wiki/Politics>political claims</a>. Very little research explores <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a> for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of <a href=https://en.wikipedia.org/wiki/Public_health>public health</a>. To support this case study we construct a new dataset PUBHEALTH of 11.8 K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> : veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three <a href=https://en.wikipedia.org/wiki/Coherence_(physics)>coherence properties</a> of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--627 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939144 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.627" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.627/>Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification<span class=acl-fixed-case>H</span>ierarchical <span class=acl-fixed-case>E</span>vidence <span class=acl-fixed-case>S</span>et <span class=acl-fixed-case>M</span>odeling for Automated Fact Extraction and Verification</a></strong><br><a href=/people/s/shyam-subramanian/>Shyam Subramanian</a>
|
<a href=/people/k/kyumin-lee/>Kyumin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--627><div class="card-body p-3 small">Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information ; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy. Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and <a href=https://en.wikipedia.org/wiki/Verification_and_validation>claim verification</a>. Our source code is available at https://github.com/ShyamSubramanian/HESM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--635 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939047 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.635" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.635/>Exploring and Predicting Transferability across NLP Tasks<span class=acl-fixed-case>NLP</span> Tasks</a></strong><br><a href=/people/t/tu-vu/>Tu Vu</a>
|
<a href=/people/t/tong-wang/>Tong Wang</a>
|
<a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/a/andrew-mattarella-micke/>Andrew Mattarella-Micke</a>
|
<a href=/people/s/subhransu-maji/>Subhransu Maji</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--635><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> other than <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and sequence labeling). Our results show that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.637.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--637 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938687 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.637" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.637/>Cold-start Active Learning through Self-supervised Language Modeling</a></strong><br><a href=/people/m/michelle-yuan/>Michelle Yuan</a>
|
<a href=/people/h/hsuan-tien-lin/>Hsuan-Tien Lin</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--637><div class="card-body p-3 small">Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a>. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is impractical because of model instability and data scarcity. Fortunately, modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> provides an additional source of information : pre-trained language models. The pre-training loss can find examples that surprise the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and should be labeled for efficient <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Compared to other baselines, our approach reaches higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> within less <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling iterations</a> and <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--641 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938831 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.641/>The importance of fillers for text representations of speech transcripts</a></strong><br><a href=/people/t/tanvi-dinkar/>Tanvi Dinkar</a>
|
<a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--641><div class="card-body p-3 small">While being an essential component of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>, fillers (e.g. um or uh) often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> and two downstream tasks predicting a speaker&#8217;s stance and expressed confidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.643.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--643 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939137 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.643/>VolTAGE : Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls<span class=acl-fixed-case>V</span>ol<span class=acl-fixed-case>TAGE</span>: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls</a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/p/piyush-khanna/>Piyush Khanna</a>
|
<a href=/people/a/arshiya-aggarwal/>Arshiya Aggarwal</a>
|
<a href=/people/t/taru-jain/>Taru Jain</a>
|
<a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--643><div class="card-body p-3 small">Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved <a href=https://en.wikipedia.org/wiki/Financial_forecasting>financial forecasting</a>. Transcripts of companies&#8217; earnings calls are well studied for <a href=https://en.wikipedia.org/wiki/Financial_risk_modeling>risk modeling</a>, offering unique investment insight into stock performance. However, <a href=https://en.wikipedia.org/wiki/Speech_recognition>vocal cues</a> in the speech of company executives present an underexplored rich source of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language data</a> for estimating <a href=https://en.wikipedia.org/wiki/Financial_risk>financial risk</a>. Additionally, most existing <a href=https://en.wikipedia.org/wiki/Portfolio_(finance)>approaches</a> ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> for volatility prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.644.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--644 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939224 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.644/>Effectively pretraining a speech translation decoder with Machine Translation data</a></strong><br><a href=/people/a/ashkan-alinejad/>Ashkan Alinejad</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--644><div class="card-body p-3 small">Directly translating from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> to text using an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approach</a> is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.646.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--646 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939209 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.646/>TESA : A Task in Entity Semantic Aggregation for Abstractive Summarization<span class=acl-fixed-case>TESA</span>: A <span class=acl-fixed-case>T</span>ask in <span class=acl-fixed-case>E</span>ntity <span class=acl-fixed-case>S</span>emantic <span class=acl-fixed-case>A</span>ggregation for Abstractive Summarization</a></strong><br><a href=/people/c/clement-jumel/>Clément Jumel</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--646><div class="card-body p-3 small">Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as &#8216;London&#8217; and &#8216;Paris&#8217; with different expressions : the major cities, the capital cities and two European cities. Yet generation, especially, abstractive summarization systems have so far focused heavily on <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> aimed at the semantic aggregation of entities. TESA contains a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 5.3 K crowd-sourced entity aggregations of Person, Organization, and Location named entities. The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the <a href=https://en.wikipedia.org/wiki/The_New_York_Times>New York Times</a>. We then build <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> for generating aggregations given a tuple of entities and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>document context</a>. We finetune on TESA an encoder-decoder language model and compare it with simpler <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification methods</a> based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.650.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--650 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-main.650.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938895 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.650/>Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness</a></strong><br><a href=/people/s/stefan-larson/>Stefan Larson</a>
|
<a href=/people/a/anthony-zheng/>Anthony Zheng</a>
|
<a href=/people/a/anish-mahendran/>Anish Mahendran</a>
|
<a href=/people/r/rishi-tekriwal/>Rishi Tekriwal</a>
|
<a href=/people/a/adrian-cheung/>Adrian Cheung</a>
|
<a href=/people/e/eric-guldan/>Eric Guldan</a>
|
<a href=/people/k/kevin-leach/>Kevin Leach</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--650><div class="card-body p-3 small">Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Finally, we show that our approach is complementary to recent work on improving <a href=https://en.wikipedia.org/wiki/Data>data diversity</a>, and training on <a href=https://en.wikipedia.org/wiki/Data>data</a> collected with our approach leads to more robust models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--658 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939009 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.658" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.658/>New Protocols and Negative Results for Textual Entailment Data Collection</a></strong><br><a href=/people/s/samuel-bowman/>Samuel R. Bowman</a>
|
<a href=/people/j/jennimaria-palomaki/>Jennimaria Palomaki</a>
|
<a href=/people/l/livio-baldini-soares/>Livio Baldini Soares</a>
|
<a href=/people/e/emily-pitler/>Emily Pitler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--658><div class="card-body p-3 small">Natural language inference (NLI) data has proven useful in <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarking</a> and, especially, as pretraining data for tasks requiring <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. However, the crowdsourcing protocol that was used to collect this <a href=https://en.wikipedia.org/wiki/Data>data</a> has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline protocol</a>, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline dataset</a> yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. In a small silver lining, we observe that all four new <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocols</a>, especially those where annotators edit * pre-filled * text boxes, reduce previously observed issues with annotation artifacts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.664.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--664 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.664 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939247 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.664" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.664/>Precise Task Formalization Matters in Winograd Schema Evaluations<span class=acl-fixed-case>W</span>inograd Schema Evaluations</a></strong><br><a href=/people/h/haokun-liu/>Haokun Liu</a>
|
<a href=/people/w/william-huang/>William Huang</a>
|
<a href=/people/d/dhara-mungra/>Dhara Mungra</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--664><div class="card-body p-3 small">Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89 % on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalizationthe combination of input specification, <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, and reuse of pretrained parametersby users of the dataset, rather than improvements in the pretrained model&#8217;s reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance dramatically and (ii)several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model&#8217;s extreme sensitivity to hyperparameters. We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.667.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--667 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.667 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.667/>Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction<span class=acl-fixed-case>E</span>valuating the <span class=acl-fixed-case>C</span>alibration of <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>G</span>raph <span class=acl-fixed-case>E</span>mbeddings for <span class=acl-fixed-case>T</span>rustworthy <span class=acl-fixed-case>L</span>ink <span class=acl-fixed-case>P</span>rediction</a></strong><br><a href=/people/t/tara-safavi/>Tara Safavi</a>
|
<a href=/people/d/danai-koutra/>Danai Koutra</a>
|
<a href=/people/e/edgar-meij/>Edgar Meij</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--667><div class="card-body p-3 small">Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence scores</a> that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of <a href=https://en.wikipedia.org/wiki/Calibration>calibration</a> for KGE from a practitioner&#8217;s perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.669.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--669 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.669 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.669" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.669/>CoDEx : A Comprehensive Knowledge Graph Completion Benchmark<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>DE</span>x: A <span class=acl-fixed-case>C</span>omprehensive <span class=acl-fixed-case>K</span>nowledge <span class=acl-fixed-case>G</span>raph <span class=acl-fixed-case>C</span>ompletion <span class=acl-fixed-case>B</span>enchmark</a></strong><br><a href=/people/t/tara-safavi/>Tara Safavi</a>
|
<a href=/people/d/danai-koutra/>Danai Koutra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--669><div class="card-body p-3 small">We present CoDEx, a set of knowledge graph completion datasets extracted from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.675.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--675 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.675 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939007 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.675/>Towards Modeling Revision Requirements in wikiHow Instructions<span class=acl-fixed-case>H</span>ow Instructions</a></strong><br><a href=/people/i/irshad-bhat/>Irshad Bhat</a>
|
<a href=/people/t/talita-anthonio/>Talita Anthonio</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--675><div class="card-body p-3 small">wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such <a href=https://en.wikipedia.org/wiki/Editing>edits</a> can be predicted automatically. For this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, we extend an existing <a href=https://en.wikipedia.org/wiki/Resource>resource</a> of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.677.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--677 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.677 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939257 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.677/>Natural Language Processing for Achieving <a href=https://en.wikipedia.org/wiki/Sustainable_development>Sustainable Development</a> : the Case of Neural Labelling to Enhance Community Profiling</a></strong><br><a href=/people/c/costanza-conforti/>Costanza Conforti</a>
|
<a href=/people/s/stephanie-hirmer/>Stephanie Hirmer</a>
|
<a href=/people/d/dai-morgan/>Dai Morgan</a>
|
<a href=/people/m/marco-basaldella/>Marco Basaldella</a>
|
<a href=/people/y/yau-ben-or/>Yau Ben Or</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--677><div class="card-body p-3 small">In recent years, there has been an increasing interest in the application of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a> and especially <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> to the field of Sustainable Development (SD). However, until now, <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> has not been systematically applied in this context. In this paper, we show the high potential of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> to enhance project sustainability. In particular, we focus on the case of community profiling in <a href=https://en.wikipedia.org/wiki/Developing_country>developing countries</a>, where, in contrast to the developed world, a notable data gap exists. Here, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> could help to address the cost and time barrier of structuring <a href=https://en.wikipedia.org/wiki/Qualitative_data>qualitative data</a> that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Sensory_processing>SD</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.678.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--678 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.678 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939368 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.678/>To Schedule or not to Schedule : Extracting Task Specific Temporal Entities and Associated Negation Constraints</a></strong><br><a href=/people/b/barun-patra/>Barun Patra</a>
|
<a href=/people/c/chala-fufa/>Chala Fufa</a>
|
<a href=/people/p/pamela-bhattacharya/>Pamela Bhattacharya</a>
|
<a href=/people/c/charles-c-lee/>Charles Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--678><div class="card-body p-3 small">State of the art research for date-time entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they do n&#8217;t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. Furthermore, some <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> require identifying negation constraints associated with the date-time entities to correctly reason over time. We showcase a novel <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for extracting task-specific date-time entities along with their negation constraints. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. Our method achieves an absolute gain of 19 % f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4 % improvement over baseline methods for detecting negation constraints over date-time entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.684.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--684 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.684 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938735 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.684" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.684/>Exploring Semantic Capacity of Terms</a></strong><br><a href=/people/j/jie-huang/>Jie Huang</a>
|
<a href=/people/z/zilong-wang/>Zilong Wang</a>
|
<a href=/people/k/kevin-chang/>Kevin Chang</a>
|
<a href=/people/w/wen-mei-hwu/>Wen-mei Hwu</a>
|
<a href=/people/j/jinjun-xiong/>JinJun Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--684><div class="card-body p-3 small">We introduce and study semantic capacity of terms. For example, the semantic capacity of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> is higher than that of <a href=https://en.wikipedia.org/wiki/Linear_regression>linear regression</a> since <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> compared with well-designed baselines and human-level evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.689.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--689 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.689 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938936 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.689" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.689/>Exploring Contextualized Neural Language Models for Temporal Dependency Parsing<span class=acl-fixed-case>E</span>xploring <span class=acl-fixed-case>C</span>ontextualized <span class=acl-fixed-case>N</span>eural <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odels for <span class=acl-fixed-case>T</span>emporal <span class=acl-fixed-case>D</span>ependency <span class=acl-fixed-case>P</span>arsing</a></strong><br><a href=/people/h/hayley-ross/>Hayley Ross</a>
|
<a href=/people/j/jonathon-cai/>Jonathon Cai</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--689><div class="card-body p-3 small">Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.692.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--692 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.692 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938992 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.692" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.692/>AxCell : Automatic Extraction of Results from Machine Learning Papers<span class=acl-fixed-case>AxCell</span>: Automatic Extraction of Results from Machine Learning Papers</a></strong><br><a href=/people/m/marcin-kardas/>Marcin Kardas</a>
|
<a href=/people/p/piotr-czapla/>Piotr Czapla</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/r/ross-taylor/>Ross Taylor</a>
|
<a href=/people/r/robert-stojnic/>Robert Stojnic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--692><div class="card-body p-3 small">Tracking progress in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> for results extraction, and a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for evaluating the performance of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.695.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--695 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.695 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939421 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.695/>Incremental Neural Coreference Resolution in Constant Memory</a></strong><br><a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--695><div class="card-body p-3 small">We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end algorithm</a> proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity&#8217;s representations before being forgotten ; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a high-performing model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3 % relative loss in F1 on OntoNotes 5.0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.697.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--697 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.697 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938913 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.697" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.697/>KGPT : Knowledge-Grounded Pre-Training for Data-to-Text Generation<span class=acl-fixed-case>KGPT</span>: Knowledge-Grounded Pre-Training for Data-to-Text Generation</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--697><div class="card-body p-3 small">Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. However, they rely on a significant amount of labeled data for each <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, which is costly to acquire and thus limits their application to new <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve remarkable gains over the known baselines. Under zero-shot setting, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.698.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--698 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.698 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938973 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.698" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.698/>POINTER : Constrained Progressive Text Generation via Insertion-based Generative Pre-training<span class=acl-fixed-case>POINTER</span>: Constrained Progressive Text Generation via Insertion-based Generative Pre-training</a></strong><br><a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--698><div class="card-body p-3 small">Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> can not be directly employed to generate text under specified <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical constraints</a>. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12 GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a <a href=https://en.wikipedia.org/wiki/Time_complexity>logarithmic time complexity</a> during <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a>. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-trained models</a> and the source code to facilitate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--706 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939207 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.706" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.706/>What is More Likely to Happen Next? Video-and-Language Future Event Prediction</a></strong><br><a href=/people/j/jie-lei/>Jie Lei</a>
|
<a href=/people/l/licheng-yu/>Licheng Yu</a>
|
<a href=/people/t/tamara-berg/>Tamara Berg</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--706><div class="card-body p-3 small">Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the <a href=https://en.wikipedia.org/wiki/Video>video</a> and dialogue, but also a significant amount of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. In this work, we explore whether <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI models</a> are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse <a href=https://en.wikipedia.org/wiki/Television_show>TV Show</a> and <a href=https://en.wikipedia.org/wiki/Vlog>YouTube Lifestyle Vlog video clips</a>. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> incorporating information from <a href=https://en.wikipedia.org/wiki/Video>video</a>, <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, and <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. Experiments show that each type of <a href=https://en.wikipedia.org/wiki/Information>information</a> is useful for this challenging task, and that compared to the high human performance on VLEP, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> provides a good starting point but leaves large room for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--708 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939350 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.708/>Towards Understanding Sample Variance in Visually Grounded Language Generation : Evaluations and Observations</a></strong><br><a href=/people/w/wanrong-zhu/>Wanrong Zhu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/p/pradyumna-narayana/>Pradyumna Narayana</a>
|
<a href=/people/k/kazoo-sone/>Kazoo Sone</a>
|
<a href=/people/s/sugato-basu/>Sugato Basu</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--708><div class="card-body p-3 small">A major challenge in visually grounded language generation is to build robust benchmark datasets and <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation : given that humans have different utilities and <a href=https://en.wikipedia.org/wiki/Attention>visual attention</a>, how will the <a href=https://en.wikipedia.org/wiki/Variance>sample variance</a> in multi-reference datasets affect the models&#8217; performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments ; that human-generated references could vary drastically in different datasets / tasks, revealing the nature of each task ; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--714 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938731 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.714" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.714/>SRLGRN : Semantic Role Labeling Graph Reasoning Network<span class=acl-fixed-case>SRLGRN</span>: Semantic Role Labeling Graph Reasoning Network</a></strong><br><a href=/people/c/chen-zheng/>Chen Zheng</a>
|
<a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--714><div class="card-body p-3 small">This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--723 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938819 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.723/>Named Entity Recognition Only from Word Embeddings</a></strong><br><a href=/people/y/ying-luo/>Ying Luo</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/j/junlang-zhan/>Junlang Zhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--723><div class="card-body p-3 small">Deep neural network models have helped <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> achieve amazing performance without handcrafting features. However, existing <a href=https://en.wikipedia.org/wiki/System>systems</a> require large amounts of human annotated training data. Efforts have been made to replace <a href=https://en.wikipedia.org/wiki/Annotation>human annotations</a> with external knowledge (e.g., NE dictionary, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a>), while it is another challenge to obtain such effective resources. In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings. We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--724 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938946 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.724" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.724/>Text Classification Using Label Names Only : A Language Model Self-Training Approach</a></strong><br><a href=/people/y/yu-meng/>Yu Meng</a>
|
<a href=/people/y/yunyi-zhang/>Yunyi Zhang</a>
|
<a href=/people/j/jiaxin-huang/>Jiaxin Huang</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/c/chao-zhang/>Chao Zhang</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--724><div class="card-body p-3 small">Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification models</a> on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for <a href=https://en.wikipedia.org/wiki/Categorization>category understanding</a> and as <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning models</a> for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to predict their implied categories, and (3) generalizes the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> via self-training. We show that our model achieves around 90 % accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--728 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939242 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.728/>PyMT5 : multi-mode translation of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> and <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python code</a> with transformers<span class=acl-fixed-case>P</span>y<span class=acl-fixed-case>MT</span>5: multi-mode translation of natural language and Python code with transformers</a></strong><br><a href=/people/c/colin-clement/>Colin Clement</a>
|
<a href=/people/d/dawn-drain/>Dawn Drain</a>
|
<a href=/people/j/jonathan-timcheck/>Jonathan Timcheck</a>
|
<a href=/people/a/alexey-svyatkovskiy/>Alexey Svyatkovskiy</a>
|
<a href=/people/n/neel-sundaresan/>Neel Sundaresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--728><div class="card-body p-3 small">Simultaneously modeling <a href=https://en.wikipedia.org/wiki/Source_code>source code</a> and <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations : a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python methods</a> and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1 % syntactically correct method bodies, achieved a <a href=https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>BLEU score</a> of 8.59 for method generation and 16.3 for <a href=https://en.wikipedia.org/wiki/Docstring>docstring generation (summarization)</a>, and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for <a href=https://en.wikipedia.org/wiki/Docstring>docstring generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.731.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--731 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.731 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939064 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.731" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.731/>COGS : A Compositional Generalization Challenge Based on Semantic Interpretation<span class=acl-fixed-case>COGS</span>: A Compositional Generalization Challenge Based on Semantic Interpretation</a></strong><br><a href=/people/n/najoung-kim/>Najoung Kim</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--731><div class="card-body p-3 small">Natural language is characterized by <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> : the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization ; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (9699 %), but generalization accuracy was substantially lower (1635 %) and showed high sensitivity to random seed (+ -68 %). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.732.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--732 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.732 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939113 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.732/>An Analysis of Natural Language Inference Benchmarks through the Lens of Negation</a></strong><br><a href=/people/m/md-mosharaf-hossain/>Md Mosharaf Hossain</a>
|
<a href=/people/v/venelin-kovatchev/>Venelin Kovatchev</a>
|
<a href=/people/p/pranoy-dutta/>Pranoy Dutta</a>
|
<a href=/people/t/tiffany-kao/>Tiffany Kao</a>
|
<a href=/people/e/elizabeth-wei/>Elizabeth Wei</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--732><div class="card-body p-3 small">Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> and still make the right inference judgments. In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> in which <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.733.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--733 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.733 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939378 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.733" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.733/>On the Sentence Embeddings from Pre-trained Language Models</a></strong><br><a href=/people/b/bohan-li/>Bohan Li</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--733><div class="card-body p-3 small">Pre-trained contextual representations like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> have achieved great success in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. However, the <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> from the pre-trained language models without <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised objective</a>. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at.<url>https://github.com/bohanli/BERT-flow</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.738.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--738 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.738 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939283 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.738" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.738/>Partially-Aligned Data-to-Text Generation with Distant Supervision</a></strong><br><a href=/people/z/zihao-fu/>Zihao Fu</a>
|
<a href=/people/b/bei-shi/>Bei Shi</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--738><div class="card-body p-3 small">The Data-to-Text task aims to generate <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable text</a> for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of <a href=https://en.wikipedia.org/wiki/Data>data</a> is much easier to obtain since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can be produced automatically. However, using this kind of <a href=https://en.wikipedia.org/wiki/Data>data</a> induces the over-generation problem posing difficulties for existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we propose a novel distant supervision generation framework. It firstly estimates the input data&#8217;s supportiveness for each target word with an <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and automatically extracting corresponding KB triples for each sentence from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.739.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--739 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.739 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938989 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.739" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.739/>Like hiking? You probably enjoy nature : Persona-grounded Dialog with Commonsense Expansions</a></strong><br><a href=/people/b/bodhisattwa-prasad-majumder/>Bodhisattwa Prasad Majumder</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--739><div class="card-body p-3 small">Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models can not infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on <a href=https://en.wikipedia.org/wiki/Persona>personas</a> by encouraging the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.741.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--741 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.741 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938681 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.741/>The World is Not Binary : Learning to Rank with <a href=https://en.wikipedia.org/wiki/Grayscale>Grayscale Data</a> for Dialogue Response Selection</a></strong><br><a href=/people/z/zibo-lin/>Zibo Lin</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/h/haitao-zheng/>Haitao Zheng</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--741><div class="card-body p-3 small">Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task : each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this <a href=https://en.wikipedia.org/wiki/Formal_system>formalization</a> can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating <a href=https://en.wikipedia.org/wiki/Grayscale>grayscale data</a> for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that <a href=https://en.wikipedia.org/wiki/Grayscale>grayscale data</a> can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed <a href=https://en.wikipedia.org/wiki/Grayscale>grayscale data</a>, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is simple, effective, and universal. Experiments on three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.742.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--742 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.742 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938945 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.742" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.742/>GRADE : Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems<span class=acl-fixed-case>GRADE</span>: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems</a></strong><br><a href=/people/l/lishan-huang/>Lishan Huang</a>
|
<a href=/people/z/zheng-ye/>Zheng Ye</a>
|
<a href=/people/j/jinghui-qin/>Jinghui Qin</a>
|
<a href=/people/l/liang-lin/>Liang Lin</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--742><div class="card-body p-3 small">Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, <a href=https://en.wikipedia.org/wiki/GRADE>GRADE</a> incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.748.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--748 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.748 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38938995 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.748/>On Extractive and Abstractive Neural Document Summarization with Transformer Language Models</a></strong><br><a href=/people/j/jonathan-pilault/>Jonathan Pilault</a>
|
<a href=/people/r/raymond-li/>Raymond Li</a>
|
<a href=/people/s/sandeep-subramanian/>Sandeep Subramanian</a>
|
<a href=/people/c/christopher-pal/>Chris Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--748><div class="card-body p-3 small">We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets : <a href=https://en.wikipedia.org/wiki/ArXiv>arXiv papers</a>, <a href=https://en.wikipedia.org/wiki/PubMed>PubMed papers</a>, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, but purely extractive methods score higher for informativeness and <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>. We hope that these <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> and experiments may serve as strong points of comparison for future work. Note : The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-main.751.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-main--751 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-main.751 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939364 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.751" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.emnlp-main.751/>Re-evaluating Evaluation in Text Summarization</a></strong><br><a href=/people/m/manik-bhandari/>Manik Bhandari</a>
|
<a href=/people/p/pranav-narayan-gour/>Pranav Narayan Gour</a>
|
<a href=/people/a/atabak-ashfaq/>Atabak Ashfaq</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-main--751><div class="card-body p-3 small">Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>. However, while the field has progressed, our standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> have not for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> : assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).</div></div></div><hr><div id=2020emnlp-demos><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.emnlp-demos/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.0/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.2/>BERTweet : A pre-trained language model for English Tweets<span class=acl-fixed-case>BERT</span>weet: A pre-trained language model for <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/a/anh-tuan-nguyen/>Anh Tuan Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--2><div class="card-body p-3 small">We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks : <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>Part-of-speech tagging</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named-entity recognition</a> and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.3/>NeuralQA : A Usable Library for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> (Contextual Query Expansion + BERT) on Large Datasets<span class=acl-fixed-case>N</span>eural<span class=acl-fixed-case>QA</span>: A Usable Library for Question Answering (Contextual Query Expansion + <span class=acl-fixed-case>BERT</span>) on Large Datasets</a></strong><br><a href=/people/v/victor-dibia/>Victor Dibia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--3><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Tool>tools</a> for Question Answering (QA) have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline (query expansion, retrieval, reading, and explanation / sensemaking). To help address these issues, we introduce NeuralQA-a usable library for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> on large datasets. NeuralQA integrates well with existing <a href=https://en.wikipedia.org/wiki/Infrastructure>infrastructure</a> (e.g., ElasticSearch instances and reader models trained with the HuggingFace Transformers API) and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion (CQE) using a masked language model (MLM) as well as relevant snippets (RelSnip)-a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> offers a flexible user interface to support <a href=https://en.wikipedia.org/wiki/Workflow>workflows</a> for <a href=https://en.wikipedia.org/wiki/Scientific_method>research explorations</a> (e.g., visualization of gradient-based explanations to support qualitative inspection of model behaviour) and <a href=https://en.wikipedia.org/wiki/Search_engine_technology>large scale search deployment</a>. Code and documentation for NeuralQA is available as open source on Github.<tex-math>RelSnip</tex-math>) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations (e.g., visualization of gradient-based explanations to support qualitative inspection of model behaviour) and large scale search deployment. Code and documentation for NeuralQA is available as open source on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.9/>DeezyMatch : A Flexible Deep Learning Approach to Fuzzy String Matching<span class=acl-fixed-case>D</span>eezy<span class=acl-fixed-case>M</span>atch: A Flexible Deep Learning Approach to Fuzzy String Matching</a></strong><br><a href=/people/k/kasra-hosseini/>Kasra Hosseini</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/m/mariona-coll-ardanuy/>Mariona Coll Ardanuy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--9><div class="card-body p-3 small">We present DeezyMatch, a free, open-source software library written in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a pretrained model, which paves the way for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> in fuzzy string matching. This approach is especially useful where only limited training examples are available. The learned DeezyMatch models can be used to generate rich vector representations from string inputs. The candidate ranker component in DeezyMatch uses these vector representations to find, for a given query, the best matching candidates in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. It uses an adaptive searching algorithm applicable to large knowledge bases and <a href=https://en.wikipedia.org/wiki/Query_language>query sets</a>. We describe DeezyMatch&#8217;s functionality, design and implementation, accompanied by a use case in toponym matching and <a href=https://en.wikipedia.org/wiki/Feasible_region>candidate ranking</a> in realistic noisy datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.11/>InVeRo : Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles<span class=acl-fixed-case>I</span>n<span class=acl-fixed-case>V</span>e<span class=acl-fixed-case>R</span>o: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles</a></strong><br><a href=/people/s/simone-conia/>Simone Conia</a>
|
<a href=/people/f/fabrizio-brignone/>Fabrizio Brignone</a>
|
<a href=/people/d/davide-zanfardino/>Davide Zanfardino</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--11><div class="card-body p-3 small">Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pretrained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use <a href=https://en.wikipedia.org/wiki/User_interface>Web interface</a> and <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>RESTful APIs</a> available at http://nlp.uniroma1.it/invero.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.12/>Youling : an AI-assisted Lyrics Creation System<span class=acl-fixed-case>AI</span>-assisted Lyrics Creation System</a></strong><br><a href=/people/r/rongsheng-zhang/>Rongsheng Zhang</a>
|
<a href=/people/x/xiaoxi-mao/>Xiaoxi Mao</a>
|
<a href=/people/l/le-li/>Le Li</a>
|
<a href=/people/l/lin-jiang/>Lin Jiang</a>
|
<a href=/people/l/lin-chen/>Lin Chen</a>
|
<a href=/people/z/zhiwei-hu/>Zhiwei Hu</a>
|
<a href=/people/y/yadong-xi/>Yadong Xi</a>
|
<a href=/people/c/changjie-fan/>Changjie Fan</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--12><div class="card-body p-3 small">Recently, a variety of <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural models</a> have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human intervention</a>. We believe that lyrics creation is a creative process with human intelligence centered. AI should play a role as an assistant in the lyrics creation process, where human interactions are crucial for high-quality creation. This paper demonstrates Youling, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, Youling supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The <a href=https://en.wikipedia.org/wiki/System>system</a> also provides a revision module which enables users to revise undesired sentences or words of <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics</a> repeatedly. Besides, Youling allows users to use multifaceted attributes to control the content and format of generated lyrics. The demo video of the <a href=https://en.wikipedia.org/wiki/System>system</a> is available at https://youtu.be/DFeNpHk0pm4.<i>Youling</i>, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, <i>Youling</i> supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The system also provides a revision module which enables users to revise undesired sentences or words of lyrics repeatedly. Besides, <i>Youling</i> allows users to use multifaceted attributes to control the content and format of generated lyrics. The demo video of the system is available at https://youtu.be/DFeNpHk0pm4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.emnlp-demos.13.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.13/>A Technical Question Answering System with Transfer Learning</a></strong><br><a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/y/yu-deng/>Yu Deng</a>
|
<a href=/people/r/ruchi-mahindru/>Ruchi Mahindru</a>
|
<a href=/people/q/qingkai-zeng/>Qingkai Zeng</a>
|
<a href=/people/s/sinem-guven/>Sinem Guven</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--13><div class="card-body p-3 small">In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving proper answers based on correctly answered similar questions in the past. TransTQA is built upon a siamese ALBERT network, which enables it to respond quickly and accurately. Furthermore, TransTQA adopts a standard deep transfer learning strategy to improve its capability of supporting multiple technical domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.15/>The Language Interpretability Tool : Extensible, Interactive Visualizations and Analysis for NLP Models<span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/i/ian-tenney/>Ian Tenney</a>
|
<a href=/people/j/james-wexler/>James Wexler</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/t/tolga-bolukbasi/>Tolga Bolukbasi</a>
|
<a href=/people/a/andy-coenen/>Andy Coenen</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/e/ellen-jiang/>Ellen Jiang</a>
|
<a href=/people/m/mahima-pushkarna/>Mahima Pushkarna</a>
|
<a href=/people/c/carey-radebaugh/>Carey Radebaugh</a>
|
<a href=/people/e/emily-reif/>Emily Reif</a>
|
<a href=/people/a/ann-yuan/>Ann Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--15><div class="card-body p-3 small">We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior : Why did my <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> make this prediction? When does <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, <a href=https://en.wikipedia.org/wiki/Aggregate_data>aggregate analysis</a>, and <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactual generation</a> into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, measuring <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in <a href=https://en.wikipedia.org/wiki/Coreference>coreference systems</a>, and exploring local behavior in text generation. LIT supports a wide range of modelsincluding <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, seq2seq, and structured predictionand is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.26/>A Data-Centric Framework for Composable NLP Workflows<span class=acl-fixed-case>NLP</span> Workflows</a></strong><br><a href=/people/z/zhengzhong-liu/>Zhengzhong Liu</a>
|
<a href=/people/g/guanxiong-ding/>Guanxiong Ding</a>
|
<a href=/people/a/avinash-bukkittu/>Avinash Bukkittu</a>
|
<a href=/people/m/mansi-gupta/>Mansi Gupta</a>
|
<a href=/people/p/pengzhi-gao/>Pengzhi Gao</a>
|
<a href=/people/a/atif-ahmed/>Atif Ahmed</a>
|
<a href=/people/s/shikun-zhang/>Shikun Zhang</a>
|
<a href=/people/x/xin-gao/>Xin Gao</a>
|
<a href=/people/s/swapnil-singhavi/>Swapnil Singhavi</a>
|
<a href=/people/l/linwei-li/>Linwei Li</a>
|
<a href=/people/w/wei-wei/>Wei Wei</a>
|
<a href=/people/z/zecong-hu/>Zecong Hu</a>
|
<a href=/people/h/haoran-shi/>Haoran Shi</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--26><div class="card-body p-3 small">Empirical natural language processing (NLP) systems in application domains (e.g., healthcare, finance, education) involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, <a href=https://en.wikipedia.org/wiki/Data_analysis>analysis</a>, generation, and <a href=https://en.wikipedia.org/wiki/Data_visualization>visualization</a>. We establish a unified open-source framework to support fast development of such sophisticated NLP workflows in a composable manner. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> introduces a uniform data representation to encode heterogeneous results by a wide range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. It offers a large repository of processors for NLP tasks, <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a>, and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, which can be easily assembled with full interoperability under the unified representation. The highly extensible framework allows plugging in custom processors from external off-the-shelf NLP and deep learning libraries. The whole framework is delivered through two modularized yet integratable open-source projects, namely Forte (for workflow infrastructure and NLP function processors) and Stave (for user interaction, visualization, and annotation).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-demos.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-demos--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-demos.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.emnlp-demos.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-demos.27/>CoRefi : A Crowd Sourcing Suite for Coreference Annotation<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>R</span>efi: A Crowd Sourcing Suite for Coreference Annotation</a></strong><br><a href=/people/a/ari-bornstein/>Ari Bornstein</a>
|
<a href=/people/a/arie-cattan/>Arie Cattan</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-demos--27><div class="card-body p-3 small">Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, we present CoRefi, a web-based coreference annotation suite, oriented for <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. Beyond the core coreference annotation tool, CoRefi provides guided onboarding for the task as well as a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for a reviewing phase. CoRefi is open source and directly embeds into any website, including popular <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing platforms</a>. CoRefi Demo : aka.ms/corefi Video Tour : aka.ms/corefivideo Github Repo : https://github.com/aribornstein/corefi</div></div></div><hr><div id=2020emnlp-tutorials><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.emnlp-tutorials/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-tutorials.0/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></strong><br><a href=/people/a/aline-villavicencio/>Aline Villavicencio</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.emnlp-tutorials.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--emnlp-tutorials--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.emnlp-tutorials.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.emnlp-tutorials.6/>Simultaneous Translation</a></strong><br><a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/n/naveen-arivazhagan/>Naveen Arivazhagan</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--emnlp-tutorials--6><div class="card-body p-3 small">Simultaneous translation, which performs <a href=https://en.wikipedia.org/wiki/Translation>translation</a> concurrently with the source speech, is widely useful in many scenarios such as international conferences, <a href=https://en.wikipedia.org/wiki/Negotiation>negotiations</a>, press releases, legal proceedings, and <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a>. This <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> has long been considered one of the hardest problems in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a> and one of its holy grails. Recently, with rapid improvements in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>speech synthesis</a>, there has been exciting progress towards <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a>. This tutorial will focus on the design and evaluation of <a href=https://en.wikipedia.org/wiki/Policy>policies</a> for <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a>, to leave attendees with a deep technical understanding of the history, the recent advances, and the remaining challenges in this field.</div></div></div><hr><div id=2020alw-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.alw-1/>Proceedings of the Fourth Workshop on Online Abuse and Harms</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.alw-1.0/>Proceedings of the Fourth Workshop on Online Abuse and Harms</a></strong><br><a href=/people/s/seyi-akiwowo/>Seyi Akiwowo</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939522 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.4/>Fine-tuning for multi-domain and multi-label uncivil language detection</a></strong><br><a href=/people/k/kadir-bulut-ozler/>Kadir Bulut Ozler</a>
|
<a href=/people/k/kate-kenski/>Kate Kenski</a>
|
<a href=/people/s/steve-rains/>Steve Rains</a>
|
<a href=/people/y/yotam-shmargad/>Yotam Shmargad</a>
|
<a href=/people/k/kevin-coe/>Kevin Coe</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--4><div class="card-body p-3 small">Incivility is a problem on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, and it comes in many forms (name-calling, <a href=https://en.wikipedia.org/wiki/Vulgarity>vulgarity</a>, <a href=https://en.wikipedia.org/wiki/Threat>threats</a>, etc.) and domains (microblog posts, <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news comments</a>, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia edits</a>, etc.). Training <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> to detect such <a href=https://en.wikipedia.org/wiki/Incivility>incivility</a> must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939530 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.5/>HurtBERT : Incorporating Lexical Features with BERT for the Detection of Abusive Language<span class=acl-fixed-case>H</span>urt<span class=acl-fixed-case>BERT</span>: Incorporating Lexical Features with <span class=acl-fixed-case>BERT</span> for the Detection of Abusive Language</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/e/endang-wahyu-pamungkas/>Endang Wahyu Pamungkas</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--5><div class="card-body p-3 small">The detection of abusive or offensive remarks in <a href=https://en.wikipedia.org/wiki/Social_text>social texts</a> has received significant attention in research. In several related <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared tasks</a>, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the <a href=https://en.wikipedia.org/wiki/Lexicon>lexical features</a> in the form of lexicon-based encodings at the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a> or embeddings at the <a href=https://en.wikipedia.org/wiki/Word_(linguistics)>word level</a>. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our in-domain and cross-domain experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939534 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.10/>Attending the Emotions to Detect Online Abusive Language</a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/a/afsheen-hatami/>Afsheen Hatami</a>
|
<a href=/people/m/mahsa-shafaei/>Mahsa Shafaei</a>
|
<a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--10><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Abuse>abusive behavior</a> has become a serious issue in <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>online social networks</a>. In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. We also develop <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> to incorporate <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> into <a href=https://en.wikipedia.org/wiki/Sensory_cue>textual cues</a> to improve <a href=https://en.wikipedia.org/wiki/Aggression>aggression identification</a>. We evaluate our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on a set of corpora related to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and show promising results with respect to abusive language detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.13.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939518 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.13/>Countering hate on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> : Large scale classification of hate and counter speech</a></strong><br><a href=/people/j/joshua-garland/>Joshua Garland</a>
|
<a href=/people/k/keyan-ghazi-zahedi/>Keyan Ghazi-Zahedi</a>
|
<a href=/people/j/jean-gabriel-young/>Jean-Gabriel Young</a>
|
<a href=/people/l/laurent-hebert-dufresne/>Laurent Hébert-Dufresne</a>
|
<a href=/people/m/mirta-galesic/>Mirta Galesic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--13><div class="card-body p-3 small">Hateful rhetoric is plaguing <a href=https://en.wikipedia.org/wiki/Online_discourse>online discourse</a>, fostering <a href=https://en.wikipedia.org/wiki/Extremism>extreme societal movements</a> and possibly giving rise to <a href=https://en.wikipedia.org/wiki/Violence>real-world violence</a>. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> to restore <a href=https://en.wikipedia.org/wiki/Civil_discourse>civil non-polarized discourse</a>. However, its actual effectiveness in curbing the spread of <a href=https://en.wikipedia.org/wiki/Hatred>hatred</a> is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training <a href=https://en.wikipedia.org/wiki/Statistical_classification>automated classifiers</a> to identify counter speech. Here we use a unique situation in <a href=https://en.wikipedia.org/wiki/Germany>Germany</a> where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939516 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.14/>Moderating Our (Dis)Content : Renewing the Regulatory Approach</a></strong><br><a href=/people/c/claire-pershan/>Claire Pershan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--14><div class="card-body p-3 small">As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of <a href=https://en.wikipedia.org/wiki/Content-control_software>content moderation</a> across a fragmented online landscape. This report urges regulators and legislators to consider a range of <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> and moderation approaches in the <a href=https://en.wikipedia.org/wiki/Regulation>regulation</a>. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.19.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939526 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.19/>Detecting East Asian Prejudice on Social Media<span class=acl-fixed-case>E</span>ast <span class=acl-fixed-case>A</span>sian Prejudice on Social Media</a></strong><br><a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a>
|
<a href=/people/e/ella-guest/>Ella Guest</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a>
|
<a href=/people/d/david-broniatowski/>David Broniatowski</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/a/austin-botelho/>Austin Botelho</a>
|
<a href=/people/m/matthew-hall/>Matthew Hall</a>
|
<a href=/people/r/rebekah-tromble/>Rebekah Tromble</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--19><div class="card-body p-3 small">During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> into four classes : Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> struggles with <a href=https://en.wikipedia.org/wiki/Edge_case>edge cases</a> and <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous content</a>. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.20.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939537 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.20/>On Cross-Dataset Generalization in Automatic Detection of Online Abuse</a></strong><br><a href=/people/i/isar-nejadgholi/>Isar Nejadgholi</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--20><div class="card-body p-3 small">NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and test datasets</a> are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics&#8217; keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.22.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939539 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.alw-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.22/>Investigating Annotator Bias with a Graph-Based Approach</a></strong><br><a href=/people/m/maximilian-wich/>Maximilian Wich</a>
|
<a href=/people/h/hala-al-kuwatly/>Hala Al Kuwatly</a>
|
<a href=/people/g/georg-groh/>Georg Groh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--22><div class="card-body p-3 small">A challenge that many online platforms face is <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or any other form of online abuse. To cope with this, hate speech detection systems are developed based on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to reduce manual work for monitoring these <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a>. Unfortunately, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> is vulnerable to unintended bias in training data, which could have severe consequences, such as a decrease in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification performance</a> or unfair behavior (e.g., discriminating minorities). In the scope of this study, we want to investigate annotator bias a form of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> that annotators cause due to different knowledge in regards to the task and their subjective perception. Our goal is to identify annotation bias based on similarities in the <a href=https://en.wikipedia.org/wiki/Annotation>annotation behavior</a> from annotators. To do so, we build a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> based on the annotations from the different annotators, apply a community detection algorithm to group the annotators, and train for each group classifiers whose performances we compare. By doing so, we are able to identify annotator bias within a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and collected insights can contribute to developing fairer and more reliable hate speech classification models.</div></div></div><hr><div id=2020blackboxnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.blackboxnlp-1/>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.0/>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></strong><br><a href=/people/a/afra-alishahi/>Afra Alishahi</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.6/>Leveraging Extracted Model Adversaries for Improved Black Box Attacks</a></strong><br><a href=/people/n/naveen-jafer-nizar/>Naveen Jafer Nizar</a>
|
<a href=/people/a/ari-kobren/>Ari Kobren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--6><div class="card-body p-3 small">We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our <a href=https://en.wikipedia.org/wiki/Stake_(Latter_Day_Saints)>approach</a> is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANYa white box attackperformed on the approximate model by 25 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>, and the ADDSENT attacka black box attackby 11 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939764 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.14/>The elephant in the interpretability room : Why use attention as explanation when we have <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>saliency methods</a>?</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--14><div class="card-body p-3 small">There is a recent surge of interest in using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> as explanation of model predictions, with mixed evidence on whether <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be used as such. While <a href=https://en.wikipedia.org/wiki/Attention>attention</a> conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.16.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.blackboxnlp-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.16/>Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation</a></strong><br><a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--16><div class="card-body p-3 small">We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods : the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on <a href=https://en.wikipedia.org/wiki/Logical_consequence>lexical entailment</a> and <a href=https://en.wikipedia.org/wiki/Negation>negation</a>. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing <a href=https://en.wikipedia.org/wiki/Negation>negation</a>, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the <a href=https://en.wikipedia.org/wiki/Causal_model>causal dynamics</a> of the model mirror the <a href=https://en.wikipedia.org/wiki/Causal_model>causal dynamics</a> of this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.19.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939765 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.19/>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</a></strong><br><a href=/people/r/rajiv-movva/>Rajiv Movva</a>
|
<a href=/people/j/jason-zhao/>Jason Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--19><div class="card-body p-3 small">Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for <a href=https://en.wikipedia.org/wiki/NMT>NMT</a> while maintaining <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, it is unclear how such pruning techniques affect a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more <a href=https://en.wikipedia.org/wiki/Code>encoding</a>. Attention mechanisms remain remarkably consistent as sparsity increases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939766 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.21/>BERTs of a feather do not generalize together : Large variability in generalization across models with similar test set performance<span class=acl-fixed-case>BERT</span>s of a feather do not generalize together: Large variability in generalization across models with similar test set performance</a></strong><br><a href=/people/r/r-thomas-mccoy/>R. Thomas McCoy</a>
|
<a href=/people/j/junghyun-min/>Junghyun Min</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--21><div class="card-body p-3 small">If the same neural network architecture is trained multiple times on the same <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ranging between 83.6 % and 84.8 %. In stark contrast, the same <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> varied widely in their <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> performance. For example, on the simple case of subject-object swap (e.g., determining that the doctor visited the lawyer does not entail the lawyer visited the doctor), <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ranged from 0.0 % to 66.2 %. Such variation is likely due to the presence of many <a href=https://en.wikipedia.org/wiki/Maxima_and_minima>local minima</a> in the loss surface that are equally attractive to a low-bias learner such as a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> ; decreasing the variability may therefore require models with stronger <a href=https://en.wikipedia.org/wiki/Inductive_reasoning>inductive biases</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.22.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.22/>Second-Order NLP Adversarial Examples<span class=acl-fixed-case>NLP</span> Adversarial Examples</a></strong><br><a href=/people/j/john-morris/>John Morris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--22><div class="card-body p-3 small">Adversarial example generation methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> rely on models like <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of a <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> to second-order adversarial examples. To generate this <a href=https://en.wikipedia.org/wiki/Curve>curve</a>, we design an <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial attack</a> to run directly on the semantic similarity models. We test on two <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.blackboxnlp-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.25/>Investigating Novel Verb Learning in BERT : Selectional Preference Classes and Alternation-Based Syntactic Generalization<span class=acl-fixed-case>BERT</span>: Selectional Preference Classes and Alternation-Based Syntactic Generalization</a></strong><br><a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--25><div class="card-body p-3 small">Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT&#8217;s few-shot learning capabilities for two aspects of English verbs : alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb / object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias : verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.29/>Defining Explanation in an AI Context<span class=acl-fixed-case>AI</span> Context</a></strong><br><a href=/people/t/tejaswani-verma/>Tejaswani Verma</a>
|
<a href=/people/c/christoph-lingenfelder/>Christoph Lingenfelder</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--29><div class="card-body p-3 small">With the increase in the use of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a>, a need for explanation systems arises. Building an <a href=https://en.wikipedia.org/wiki/Explanation>explanation system</a> requires a definition of <a href=https://en.wikipedia.org/wiki/Explanation>explanation</a>. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a>, <a href=https://en.wikipedia.org/wiki/Philosophy>philosophy</a>, and <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive sciences</a>. We study multiple perspectives and aspects of explainability of recommendations or predictions made by <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a>, and provide a generic definition of <a href=https://en.wikipedia.org/wiki/Explanation>explanation</a>. The proposed <a href=https://en.wikipedia.org/wiki/Definition>definition</a> is ambitious and challenging to apply. With the intention to bridge the gap between theory and application, we also propose a possible architecture of an automated explanation system based on our definition of explanation.</div></div></div><hr><div id=2020clinicalnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.clinicalnlp-1/>Proceedings of the 3rd Clinical Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.0/>Proceedings of the 3rd Clinical Natural Language Processing Workshop</a></strong><br><a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/t/tristan-naumann/>Tristan Naumann</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939817 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.1/>Various Approaches for Predicting Stroke Prognosis using Magnetic Resonance Imaging Text Records</a></strong><br><a href=/people/t/tak-sung-heo/>Tak-Sung Heo</a>
|
<a href=/people/c/chulho-kim/>Chulho Kim</a>
|
<a href=/people/j/jeong-myeong-choi/>Jeong-Myeong Choi</a>
|
<a href=/people/y/yeong-seok-jeong/>Yeong-Seok Jeong</a>
|
<a href=/people/y/yu-seop-kim/>Yu-Seop Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--1><div class="card-body p-3 small">Stroke is one of the leading causes of death and disability worldwide. Stroke is treatable, but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is prone to disability after treatment and must be prevented. To grasp the degree of disability caused by <a href=https://en.wikipedia.org/wiki/Stroke>stroke</a>, we use magnetic resonance imaging text records to predict <a href=https://en.wikipedia.org/wiki/Stroke>stroke</a> and measure the performance according to the document-level and sentence-level representation. As a result of the experiment, the document-level representation shows better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939836 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.3/>BERT-XML : Large Scale Automated ICD Coding Using BERT Pretraining<span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>XML</span>: Large Scale Automated <span class=acl-fixed-case>ICD</span> Coding Using <span class=acl-fixed-case>BERT</span> Pretraining</a></strong><br><a href=/people/z/zachariah-zhang/>Zachariah Zhang</a>
|
<a href=/people/j/jingshu-liu/>Jingshu Liu</a>
|
<a href=/people/n/narges-razavian/>Narges Razavian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--3><div class="card-body p-3 small">ICD coding is the task of classifying and cod-ing all diagnoses, symptoms and proceduresassociated with a patient&#8217;s visit. The process isoften manual, extremely time-consuming andexpensive for hospitals as clinical interactionsare usually recorded in free text medical notes. In this paper, we propose a machine learningmodel, BERT-XML, for large scale automatedICD coding of EHR notes, utilizing recentlydeveloped unsupervised pretraining that haveachieved state of the art performance on a va-riety of NLP tasks. We train a BERT modelfrom scratch on EHR notes, learning with vo-cabulary better suited for EHR tasks and thusoutperform off-the-shelf models. We furtheradapt the BERT architecture for ICD codingwith multi-label attention. We demonstratethe effectiveness of BERT-based models on thelarge scale ICD code classification task usingmillions of EHR notes to predict thousands ofunique codes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.4/>Incorporating Risk Factor Embeddings in Pre-trained Transformers Improves Sentiment Prediction in Psychiatric Discharge Summaries</a></strong><br><a href=/people/x/xiyu-ding/>Xiyu Ding</a>
|
<a href=/people/m/mei-hua-hall/>Mei-Hua Hall</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--4><div class="card-body p-3 small">Reducing rates of early hospital readmission has been recognized and identified as a key to improve quality of care and reduce costs. There are a number of <a href=https://en.wikipedia.org/wiki/Risk_factor>risk factors</a> that have been hypothesized to be important for understanding re-admission risk, including such factors as problems with substance abuse, ability to maintain work, relations with family. In this work, we develop Roberta-based models to predict the sentiment of sentences describing readmission risk factors in discharge summaries of patients with psychosis. We improve substantially on previous results by a scheme that shares information across <a href=https://en.wikipedia.org/wiki/Risk_factor>risk factors</a> while also allowing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn risk factor-specific information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939829 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.7/>BioBERTpt-A Portuguese Neural Language Model for Clinical Named Entity Recognition<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>BERT</span>pt - A <span class=acl-fixed-case>P</span>ortuguese Neural Language Model for Clinical Named Entity Recognition</a></strong><br><a href=/people/e/elisa-terumi-rubel-schneider/>Elisa Terumi Rubel Schneider</a>
|
<a href=/people/j/joao-vitor-andrioli-de-souza/>João Vitor Andrioli de Souza</a>
|
<a href=/people/j/julien-knafou/>Julien Knafou</a>
|
<a href=/people/l/lucas-emanuel-silva-e-oliveira/>Lucas Emanuel Silva e Oliveira</a>
|
<a href=/people/j/jenny-copara/>Jenny Copara</a>
|
<a href=/people/y/yohan-bonescki-gumiel/>Yohan Bonescki Gumiel</a>
|
<a href=/people/l/lucas-ferro-antunes-de-oliveira/>Lucas Ferro Antunes de Oliveira</a>
|
<a href=/people/e/emerson-cabrera-paraiso/>Emerson Cabrera Paraiso</a>
|
<a href=/people/d/douglas-teodoro/>Douglas Teodoro</a>
|
<a href=/people/c/claudia-maria-cabral-moro-barra/>Cláudia Maria Cabral Moro Barra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--7><div class="card-body p-3 small">With the growing number of <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health record data</a>, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity recognition (NER), in English corpus has recently improved by contextualised language models, less research is available for clinical texts in low resource languages. Our goal is to assess a deep contextual embedding model for <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, so called BioBERTpt, to support clinical and biomedical NER. We transfer learned information encoded in a multilingual-BERT model to a corpora of clinical narratives and biomedical-scientific papers in <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a>. To evaluate the performance of BioBERTpt, we ran NER experiments on two annotated corpora containing clinical narratives and compared the results with existing BERT models. Our in-domain model outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a> in <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> by 2.72 %, achieving higher performance in 11 out of 13 assessed entities. We demonstrate that enriching contextual embedding models with domain literature can play an important role in improving performance for specific NLP tasks. The transfer learning process enhanced the Portuguese biomedical NER model by reducing the necessity of labeled data and the demand for retraining a whole new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939814 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.13/>How You Ask Matters : The Effect of Paraphrastic Questions to BERT Performance on a Clinical SQuAD Dataset<span class=acl-fixed-case>BERT</span> Performance on a Clinical <span class=acl-fixed-case>SQ</span>u<span class=acl-fixed-case>AD</span> Dataset</a></strong><br><a href=/people/s/sungrim-riea-moon/>Sungrim (Riea) Moon</a>
|
<a href=/people/j/jungwei-fan/>Jungwei Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--13><div class="card-body p-3 small">Reading comprehension style question-answering (QA) based on patient-specific documents represents a growing area in clinical NLP with plentiful applications. Bidirectional Encoder Representations from Transformers (BERT) and its derivatives lead the state-of-the-art accuracy on the task, but most evaluation has treated the data as a pre-mixture without systematically looking into the potential effect of imperfect train / test questions. The current study seeks to address this gap by experimenting with full versus partial train / test data consisting of paraphrastic questions. Our key findings include 1) training with all pooled question variants yielded best <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, 2) the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> varied widely, from 0.74 to 0.80, when trained with each single question variant, and 3) questions of similar lexical / syntactic structure tended to induce identical answers. The results suggest that how you ask questions matters in BERT-based QA, especially at the training stage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.clinicalnlp-1.15.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939819 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.clinicalnlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.15/>MeDAL : Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>DAL</span>: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining</a></strong><br><a href=/people/z/zhi-wen/>Zhi Wen</a>
|
<a href=/people/x/xing-han-lu/>Xing Han Lu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--15><div class="card-body p-3 small">One of the biggest challenges that prohibit the use of many current <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP methods</a> in clinical settings is the availability of <a href=https://en.wikipedia.org/wiki/Data_set>public datasets</a>. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence speed</a> when <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on downstream medical tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939827 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.21/>Extracting Relations between Radiotherapy Treatment Details</a></strong><br><a href=/people/d/danielle-bitterman/>Danielle Bitterman</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/d/david-harris/>David Harris</a>
|
<a href=/people/c/chen-lin/>Chen Lin</a>
|
<a href=/people/s/sean-finan/>Sean Finan</a>
|
<a href=/people/j/jeremy-warner/>Jeremy Warner</a>
|
<a href=/people/r/raymond-mak/>Raymond Mak</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--21><div class="card-body p-3 small">We present work on extraction of radiotherapy treatment information from the clinical narrative in the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical records</a>. Radiotherapy is a central component of the treatment of most solid cancers. Its details are described in non-standardized fashions using jargon not found in other medical specialties, complicating the already difficult task of manual data extraction. We examine the performance of several state-of-the-art neural methods for relation extraction of radiotherapy treatment details, with a goal of automating detailed information extraction. The <a href=https://en.wikipedia.org/wiki/Nervous_system>neural systems</a> perform at 0.82-0.88 macro-average F1, which approximates or in some cases exceeds the <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a>. To the best of our knowledge, this is the first effort to develop models for radiotherapy relation extraction and one of the few efforts for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> to describe <a href=https://en.wikipedia.org/wiki/Treatment_of_cancer>cancer treatment</a> in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939830 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.22/>Cancer Registry Information Extraction via <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a></a></strong><br><a href=/people/y/yan-jie-lin/>Yan-Jie Lin</a>
|
<a href=/people/h/hong-jie-dai/>Hong-Jie Dai</a>
|
<a href=/people/y/you-chen-zhang/>You-Chen Zhang</a>
|
<a href=/people/c/chung-yang-wu/>Chung-Yang Wu</a>
|
<a href=/people/y/yu-cheng-chang/>Yu-Cheng Chang</a>
|
<a href=/people/p/pin-jou-lu/>Pin-Jou Lu</a>
|
<a href=/people/c/chih-jen-huang/>Chih-Jen Huang</a>
|
<a href=/people/y/yu-tsang-wang/>Yu-Tsang Wang</a>
|
<a href=/people/h/hui-min-hsieh/>Hui-Min Hsieh</a>
|
<a href=/people/k/kun-san-chao/>Kun-San Chao</a>
|
<a href=/people/t/tsang-wu-liu/>Tsang-Wu Liu</a>
|
<a href=/people/i/i-shou-chang/>I-Shou Chang</a>
|
<a href=/people/y/yi-hsin-connie-yang/>Yi-Hsin Connie Yang</a>
|
<a href=/people/t/ti-hao-wang/>Ti-Hao Wang</a>
|
<a href=/people/k/ko-jiunn-liu/>Ko-Jiunn Liu</a>
|
<a href=/people/l/li-tzong-chen/>Li-Tzong Chen</a>
|
<a href=/people/s/sheau-fang-yang/>Sheau-Fang Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--22><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Cancer_registry>cancer registry</a> is a critical and massive database for which various types of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> are needed and whose maintenance requires labor-intensive <a href=https://en.wikipedia.org/wiki/Data_curation>data curation</a>. In order to facilitate the curation process for building a high-quality and integrated cancer registry database, we compiled a cross-hospital corpus and applied neural network methods to develop a natural language processing system for extracting cancer registry variables buried in unstructured pathology reports. The performance of the developed networks was compared with various baselines using standard <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>micro-precision</a>, recall and <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>. Furthermore, we conducted experiments to study the feasibility of applying <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to rapidly develop a well-performing <a href=https://en.wikipedia.org/wiki/System>system</a> for processing reports from different sources that might be presented in different writing styles and formats. The results demonstrate that the transfer learning method enables us to develop a satisfactory <a href=https://en.wikipedia.org/wiki/System>system</a> for a new hospital with only a few annotations and suggest more opportunities to reduce the burden of <a href=https://en.wikipedia.org/wiki/Cancer_registry>cancer registry curation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939833 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.24/>Where’s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data</a></strong><br><a href=/people/g/george-michalopoulos/>George Michalopoulos</a>
|
<a href=/people/h/helen-chen/>Helen Chen</a>
|
<a href=/people/a/alexander-wong/>Alexander Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--24><div class="card-body p-3 small">In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via expert-review, where clinicians can have a dialogue with a domain expert (reviewers) and ask them questions about data entry rules. Automatically identifying real questions in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting. In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect an answer (information or help) about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence (e.g., can you clarify this?), which we will refer as c-questions. We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939816 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.28/>An Ensemble Approach for Automatic Structuring of Radiology Reports</a></strong><br><a href=/people/m/morteza-pourreza-shahri/>Morteza Pourreza Shahri</a>
|
<a href=/people/a/amir-tahmasebi/>Amir Tahmasebi</a>
|
<a href=/people/b/bingyang-ye/>Bingyang Ye</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/j/javed-aslam/>Javed Aslam</a>
|
<a href=/people/t/timothy-ferris/>Timothy Ferris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--28><div class="card-body p-3 small">Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department / institute specific templates. Moreover, radiologists&#8217; reporting style varies from one to another as sentences are written in a telegraphic format and do not follow general English grammar rules. In this work, we present an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are : 1) Focus Sentence model, capturing context of the target sentence ; 2) Surrounding Context model, capturing the neighboring context of the target sentence ; and finally, 3) Formatting / Layout model, aimed at learning report formatting cues. We utilize Bi-directional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that incorporate the structure of reports. We compare our proposed approach against multiple baselines and state-of-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> significantly outperforms other <a href=https://en.wikipedia.org/wiki/Scientific_method>approaches</a> by achieving 97.1 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939834 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.30/>Advancing Seq2seq with Joint Paraphrase Learning</a></strong><br><a href=/people/s/so-yeon-min/>So Yeon Min</a>
|
<a href=/people/p/preethi-raghavan/>Preethi Raghavan</a>
|
<a href=/people/p/peter-szolovits/>Peter Szolovits</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--30><div class="card-body p-3 small">We address the problem of model generalization for sequence to sequence (seq2seq) architectures. We propose going beyond <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> via paraphrase-optimized multi-task learning and observe that it is useful in correctly handling unseen sentential paraphrases as inputs. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> greatly outperform SOTA seq2seq models for <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic parsing</a> on diverse domains (Overnight-up to 3.2 % and emrQA-7 %) and <a href=https://en.wikipedia.org/wiki/Nematus>Nematus</a>, the winning solution for WMT 2017, for <a href=https://en.wikipedia.org/wiki/Czech_language>Czech to English translation</a> (CzENG 1.6-1.5 BLEU).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939812 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.31/>On the diminishing return of labeling clinical reports</a></strong><br><a href=/people/j/jean-baptiste-lamare/>Jean-Baptiste Lamare</a>
|
<a href=/people/o/oloruntobiloba-olatunji/>Oloruntobiloba Olatunji</a>
|
<a href=/people/l/li-yao/>Li Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--31><div class="card-body p-3 small">Ample evidence suggests that better machine learning models may be steadily obtained by training on increasingly larger datasets on natural language processing (NLP) problems from non-medical domains. Whether the same holds true for medical NLP has by far not been thoroughly investigated. This work shows that this is indeed not always the case. We reveal the somehow counter-intuitive observation that performant medical NLP models may be obtained with small amount of labeled data, quite the opposite to the common belief, most likely due to the domain specificity of the problem. We show quantitatively the effect of training data size on a fixed test set composed of two of the largest public chest x-ray radiology report datasets on the task of abnormality classification. The trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> not only make use of the training data efficiently, but also outperform the current state-of-the-art <a href=https://en.wikipedia.org/wiki/Rule-based_system>rule-based systems</a> by a significant margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939828 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.32/>The Chilean Waiting List Corpus : a new resource for clinical Named Entity Recognition in Spanish<span class=acl-fixed-case>C</span>hilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in <span class=acl-fixed-case>S</span>panish</a></strong><br><a href=/people/p/pablo-baez/>Pablo Báez</a>
|
<a href=/people/f/fabian-villena/>Fabián Villena</a>
|
<a href=/people/m/matias-rojas/>Matías Rojas</a>
|
<a href=/people/m/manuel-duran/>Manuel Durán</a>
|
<a href=/people/j/jocelyn-dunstan/>Jocelyn Dunstan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--32><div class="card-body p-3 small">In this work we describe the Waiting List Corpus consisting of de-identified referrals for several specialty consultations from the waiting list in Chilean public hospitals. A subset of 900 referrals was manually annotated with 9,029 entities, 385 attributes, and 284 pairs of relations with clinical relevance. A trained medical doctor annotated these referrals, and then together with other three researchers, consolidated each of the annotations. The annotated corpus has nested entities, with 32.2 % of entities embedded in other entities. We use this annotated corpus to obtain preliminary results for Named Entity Recognition (NER). The best results were achieved by using a biLSTM-CRF architecture using word embeddings trained over <a href=https://en.wikipedia.org/wiki/Spanish_Wikipedia>Spanish Wikipedia</a> together with clinical embeddings computed by the group. NER models applied to this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> can leverage statistics of diseases and pending procedures within this waiting list. This work constitutes the first <a href=https://en.wikipedia.org/wiki/Text_corpus>annotated corpus</a> using clinical narratives from <a href=https://en.wikipedia.org/wiki/Chile>Chile</a>, and one of the few for the <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish language</a>. The annotated corpus, the clinical word embeddings, and the annotation guidelines are freely released to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.clinicalnlp-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--clinicalnlp-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.clinicalnlp-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939838 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.clinicalnlp-1.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.clinicalnlp-1.33/>Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/j/john-chen/>John Chen</a>
|
<a href=/people/i/ian-berlot-attwell/>Ian Berlot-Attwell</a>
|
<a href=/people/x/xindi-wang/>Xindi Wang</a>
|
<a href=/people/s/safwan-hossain/>Safwan Hossain</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--clinicalnlp-1--33><div class="card-body p-3 small">Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured forms</a> such as free text. We propose a novel task of exploring fairness on a multimodal clinical dataset, adopting equalized odds for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm-equalized odds post processing-and compare it to a text-specific fairness algorithm : debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a> may simultaneously achieve a good balance of performance classical notions of <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a>. Our work opens the door for future work at the critical intersection of clinical NLP and <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a>.<i>fairness</i> on a multimodal clinical dataset, adopting <i>equalized odds</i> for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.</div></div></div><hr><div id=2020cmcl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cmcl-1/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmcl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cmcl-1.0/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra Jacobs</a>
|
<a href=/people/y/yohei-oseki/>Yohei Oseki</a>
|
<a href=/people/l/laurent-prevot/>Laurent Prévot</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmcl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cmcl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cmcl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939683 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.cmcl-1.2/>Images and Imagination : Automated Analysis of Priming Effects Related to Autism Spectrum Disorder and Developmental Language Disorder</a></strong><br><a href=/people/m/michaela-regneri/>Michaela Regneri</a>
|
<a href=/people/d/diane-king/>Diane King</a>
|
<a href=/people/f/fahreen-walji/>Fahreen Walji</a>
|
<a href=/people/o/olympia-palikara/>Olympia Palikara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cmcl-1--2><div class="card-body p-3 small">Different aspects of <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a> have been shown to be sensitive to <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>priming</a> but the findings of studies examining priming effects in adolescents with Autism Spectrum Disorder (ASD) and Developmental Language Disorder (DLD) have been inconclusive. We present a study analysing visual and implicit semantic priming in adolescents with ASD and DLD. Based on a dataset of fictional and script-like narratives, we evaluate how often and how extensively, content of two different priming sources is used by the participants. The first <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>priming source</a> was <a href=https://en.wikipedia.org/wiki/Visual_system>visual</a>, consisting of images shown to the participants to assist them with their storytelling. The second priming source originated from <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>, using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourced data</a> containing prototypical script elements. Our results show that individuals with <a href=https://en.wikipedia.org/wiki/Autism_spectrum>ASD</a> are less sensitive to both types of <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>priming</a>, but show typical usage of primed cues when they use them at all. In contrast, children with DLD show mostly average priming sensitivity, but exhibit an over-proportional use of the priming cues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cmcl-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cmcl-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cmcl-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cmcl-1.3/>Production-based Cognitive Models as a Test Suite for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning Algorithms</a></a></strong><br><a href=/people/a/adrian-brasoveanu/>Adrian Brasoveanu</a>
|
<a href=/people/j/jakub-dotlacil/>Jakub Dotlacil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cmcl-1--3><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in which production-rule based computational cognitive modeling and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> can systematically interact and inform each other. We focus on linguistic applications because the sophisticated rule-based cognitive models needed to capture linguistic behavioral data promise to provide a stringent test suite for RL algorithms, connecting RL algorithms to both accuracy and reaction-time experimental data. Thus, we open a path towards assembling an experimentally rigorous and cognitively realistic benchmark for RL algorithms. We extend our previous work on lexical decision tasks and tabular RL algorithms (Brasoveanu and Dotlail, 2020b) with a discussion of neural-network based approaches, and a discussion of how <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> can be formalized as an RL problem.</div></div></div><hr><div id=2020codi-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.codi-1/>Proceedings of the First Workshop on Computational Approaches to Discourse</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.codi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.codi-1.0/>Proceedings of the First Workshop on Computational Approaches to Discourse</a></strong><br><a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.codi-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--codi-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.codi-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939691 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.codi-1.6/>Exploring Coreference Features in <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>Heterogeneous Data</a></a></strong><br><a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/k/kerstin-kunz/>Kerstin Kunz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--codi-1--6><div class="card-body p-3 small">The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communicative situation of language production. We define coreference features that also include reference to <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract entities</a> and <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>. These features are inspired through several sources cognitive parameters, <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatic factors</a> and <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typological status</a>. We pay attention to the distributions of these features in a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> containing English and German texts of spoken and written discourse mode, which can be classified into seven different registers. We apply text classification and <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> to find out how these variational dimensions (language, mode and register) impact on coreference features. Knowledge on the variation under analysis is valuable for <a href=https://en.wikipedia.org/wiki/Contrastive_linguistics>contrastive linguistics</a>, <a href=https://en.wikipedia.org/wiki/Translation_studies>translation studies</a> and multilingual natural language processing (NLP), e.g. machine translation or cross-lingual coreference resolution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.codi-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--codi-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.codi-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939693 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.codi-1.8/>DSNDM : Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking<span class=acl-fixed-case>DSNDM</span>: Deep <span class=acl-fixed-case>S</span>iamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking</a></strong><br><a href=/people/a/alexander-chernyavskiy/>Alexander Chernyavskiy</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--codi-1--8><div class="card-body p-3 small">In this paper, the utility and advantages of the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a> for text pairs categorization and <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> are investigated. We consider two tasks in which discourse structure seems useful and important : automatic verification of political statements, and <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> in <a href=https://en.wikipedia.org/wiki/Question_answering>question answering systems</a>. We propose a neural network based approach to learn the match between pairs of discourse tree structures. To this end, the neural TreeLSTM model is modified to effectively encode discourse trees and DSNDM model based on it is suggested to analyze pairs of texts. In addition, the integration of the attention mechanism in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is proposed. Moreover, different ranking approaches are investigated for the second <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In the paper, the comparison with <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art methods</a> is given. Experiments illustrate that combination of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and discourse structure in DSNDM is effective since it reaches top results in the assigned <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The evaluation also demonstrates that <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a> improves <a href=https://en.wikipedia.org/wiki/Quality_(philosophy)>quality</a> for the processing of longer texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.codi-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--codi-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.codi-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939698 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.codi-1.10/>Joint Modeling of Arguments for Event Understanding</a></strong><br><a href=/people/y/yunmo-chen/>Yunmo Chen</a>
|
<a href=/people/t/tongfei-chen/>Tongfei Chen</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--codi-1--10><div class="card-body p-3 small">We recognize the task of event argument linking in documents as similar to that of intent slot resolution in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.codi-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--codi-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.codi-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939702 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.codi-1.14/>Extending Implicit Discourse Relation Recognition to the PDTB-3<span class=acl-fixed-case>PDTB</span>-3</a></strong><br><a href=/people/l/li-liang/>Li Liang</a>
|
<a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--codi-1--14><div class="card-body p-3 small">The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future state-of-the-art recognizers for implicit discourse relations.</div></div></div><hr><div id=2020deelio-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.deelio-1/>Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.deelio-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.deelio-1.0/>Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a></strong><br><a href=/people/e/eneko-agirre/>Eneko Agirre</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.deelio-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--deelio-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.deelio-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939726 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.deelio-1.3/>Generalization to Mitigate Synonym Substitution Attacks</a></strong><br><a href=/people/b/basemah-alshemali/>Basemah Alshemali</a>
|
<a href=/people/j/jugal-kalita/>Jugal Kalita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--deelio-1--3><div class="card-body p-3 small">Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples perturbed inputs that cause DNN-based models to produce incorrect results. One robust adversarial attack in the NLP domain is the synonym substitution. In attacks of this variety, the adversary substitutes words with <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a>. Since synonym substitution perturbations aim to satisfy all lexical, grammatical, and semantic constraints, they are difficult to detect with automatic syntax check as well as by humans. In this paper, we propose a structure-free defensive method that is capable of improving the performance of DNN-based models with both clean and adversarial data. Our findings show that replacing the embeddings of the important words in the input samples with the average of their synonyms&#8217; embeddings can significantly improve the generalization of DNN-based classifiers. By doing so, we reduce <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>model sensitivity</a> to particular words in the input samples. Our results indicate that the proposed defense is not only capable of defending against adversarial attacks, but is also capable of improving the performance of DNN-based models when tested on benign data. On average, the proposed defense improved the classification accuracy of the CNN and Bi-LSTM models by 41.30 % and 55.66 %, respectively, when tested under adversarial attacks. Extended investigation shows that our defensive method can improve the robustness of nonneural models, achieving an average of 17.62 % and 22.93 % classification accuracy increase on the SVM and XGBoost models, respectively. The proposed defensive method has also shown an average of 26.60 % <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> improvement when tested with the infamous BERT model.</div></div></div><hr><div id=2020eval4nlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.eval4nlp-1/>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.0/>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.eval4nlp-1.3.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939718 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.3/>Item Response Theory for Efficient Human Evaluation of Chatbots</a></strong><br><a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--3><div class="card-body p-3 small">Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance</a>. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, using a paired comparison in which annotators judge which system responds better to the next turn of a conversation. IRT is widely used in educational testing for simultaneously assessing the ability of test takers and the quality of test questions. It is similarly well suited for chatbot evaluation since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> allows the assessment of both <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and the prompts used to evaluate them. We use IRT to efficiently assess <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>, and show that different examples from the evaluation set are better suited for comparing high-quality (nearer to human performance) than low-quality systems. Finally, we use IRT to reduce the number of evaluation examples assessed by human annotators while retaining discriminative power.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939719 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eval4nlp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.4/>ViLBERTScore : Evaluating Image Caption Using Vision-and-Language BERT<span class=acl-fixed-case>V</span>i<span class=acl-fixed-case>LBERTS</span>core: Evaluating Image Caption Using Vision-and-Language <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/h/hwanhee-lee/>Hwanhee Lee</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--4><div class="card-body p-3 small">In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic representations. The proposed method generates image-conditioned embeddings for each token using ViLBERT from both generated and reference texts. Then, these contextual embeddings from each of the two sentence-pair are compared to compute the <a href=https://en.wikipedia.org/wiki/Similarity_score>similarity score</a>. Experimental results on three benchmark datasets show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> correlates significantly better with human judgments than all existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939709 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.5/>BLEU Neighbors : A Reference-less Approach to Automatic Evaluation<span class=acl-fixed-case>BLEU</span> Neighbors: A Reference-less Approach to Automatic Evaluation</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/dorsa-sadigh/>Dorsa Sadigh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--5><div class="card-body p-3 small">Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although <a href=https://en.wikipedia.org/wiki/Language>language diversity</a> can be estimated using statistical measures such as <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a>, measuring language quality requires <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a>. However, because human evaluation at scale is slow and expensive, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is used sparingly ; <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can not be used to rapidly iterate on NLG models, in the way <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is used for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a <a href=https://en.wikipedia.org/wiki/Positive-definite_kernel>kernel function</a>. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that on average the quality estimation from a BLEU Neighbors model has a lower <a href=https://en.wikipedia.org/wiki/Mean_squared_error>mean squared error</a> and higher <a href=https://en.wikipedia.org/wiki/Spearman_correlation>Spearman correlation</a> with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on automatically grading essays, including <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have access to a gold-standard reference essay.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939707 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.8/>Artemis : A Novel Annotation Methodology for Indicative Single Document Summarization</a></strong><br><a href=/people/r/rahul-jha/>Rahul Jha</a>
|
<a href=/people/k/keping-bi/>Keping Bi</a>
|
<a href=/people/y/yang-li/>Yang Li</a>
|
<a href=/people/m/mahdi-pakdaman/>Mahdi Pakdaman</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/i/ivan-zhiboedov/>Ivan Zhiboedov</a>
|
<a href=/people/k/kieran-mcdonald/>Kieran McDonald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--8><div class="card-body p-3 small">We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found, such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in <a href=https://en.wikipedia.org/wiki/Document_management_system>document management</a> and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval systems</a>, which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid, Artemis is more tractable because judges do n&#8217;t need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939710 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.9/>Probabilistic Extension of Precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, and <a href=https://en.wikipedia.org/wiki/F-number>F1 Score</a> for More Thorough Evaluation of Classification Models</a></strong><br><a href=/people/r/reda-yacouby/>Reda Yacouby</a>
|
<a href=/people/d/dustin-axman/>Dustin Axman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--9><div class="card-body p-3 small">In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Accuracy</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Precision</a>, and Recall are often insufficient as they fail to give a complete picture of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s behavior. We present a probabilistic extension of Precision, Recall, and F1 score, which we refer to as confidence-Precision (cPrecision), confidence-Recall (cRecall), and confidence-F1 (cF1) respectively. The proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> address some of the challenges faced when evaluating large-scale NLP systems, specifically when the model&#8217;s confidence score assignments have an impact on the system&#8217;s behavior. We describe four key benefits of our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> as compared to their threshold-based counterparts. Two of these benefits, which we refer to as robustness to missing values and sensitivity to model confidence score assignments are self-evident from the metrics&#8217; definitions ; the remaining benefits, generalization, and functional consistency are demonstrated empirically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939720 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.14/>On Aligning OpenIE Extractions with Knowledge Bases : A Case Study<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>IE</span> Extractions with Knowledge Bases: A Case Study</a></strong><br><a href=/people/k/kiril-gashteovski/>Kiril Gashteovski</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/b/bhushan-kotnis/>Bhushan Kotnis</a>
|
<a href=/people/s/sven-hertling/>Sven Hertling</a>
|
<a href=/people/c/christian-meilicke/>Christian Meilicke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--14><div class="card-body p-3 small">Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>ques- tion answering</a> and automatic knowledge base (KB) construction. Many of these downstream tasks rely on aligning OIE triples with refer- ence KBs. Such alignments are usually eval- uated w.r.t. a specific downstream task and, to date, no direct manual evaluation of such alignments has been performed. In this paper, we directly evaluate how OIE triples from the OPIEC corpus are related to the DBpedia KB w.r.t. information content. First, we investigate OPIEC triples and <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia facts</a> having the same arguments by comparing the information on the OIE surface relation with the KB rela- tion. Second, we evaluate the expressibility of general OPIEC triples in <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a>. We in- vestigate whetherand, if so, howa given OIE triple can be mapped to a single KB fact. We found that such mappings are not always possible because the information in the OIE triples tends to be more specific. Our evalua- tion suggests, however, that significant part of OIE triples can be expressed by means of KB formulas instead of individual facts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eval4nlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.15/>ClusterDataSplit : Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation<span class=acl-fixed-case>C</span>luster<span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>S</span>plit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation</a></strong><br><a href=/people/h/hanna-wecker/>Hanna Wecker</a>
|
<a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--15><div class="card-body p-3 small">This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> to different <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>data distributions</a>, we propose a clustering-based data splitting algorithm. It creates development (or test) sets which are lexically different from the training data while ensuring similar label distributions. Hence, we are able to create challenging cross-validation evaluation setups while abstracting away from performance differences resulting from label distribution shifts between training and test data. In addition, we present a <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python-based tool</a> for analyzing and visualizing data split characteristics and model performance. We illustrate the workings and results of our approach using a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and a patent classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939713 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.16/>Best Practices for Crowd-based Evaluation of German Summarization : Comparing Crowd, Expert and Automatic Evaluation<span class=acl-fixed-case>G</span>erman Summarization: Comparing Crowd, Expert and Automatic Evaluation</a></strong><br><a href=/people/n/neslihan-iskender/>Neslihan Iskender</a>
|
<a href=/people/t/tim-polzehl/>Tim Polzehl</a>
|
<a href=/people/s/sebastian-moller/>Sebastian Möller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--16><div class="card-body p-3 small">One of the main challenges in the development of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization tools</a> is <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization quality evaluation</a>. On the one hand, the human assessment of summarization quality conducted by <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic experts</a> is slow, expensive, and still not a standardized procedure. On the other hand, the automatic assessment metrics are reported not to correlate high enough with <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human quality ratings</a>. As a solution, we propose <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> as a fast, scalable, and cost-effective alternative to expert evaluations to assess the intrinsic and extrinsic quality of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> by comparing crowd ratings with expert ratings and automatic metrics such as ROUGE, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, or BertScore on a German summarization data set. Our results provide a basis for best practices for crowd-based summarization evaluation regarding major influential factors such as the best annotation aggregation method, the influence of readability and reading effort on summarization evaluation, and the optimal number of crowd workers to achieve comparable results to experts, especially when determining factors such as overall quality, grammaticality, referential clarity, focus, structure & coherence, summary usefulness, and summary informativeness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eval4nlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eval4nlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eval4nlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939712 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.eval4nlp-1.17/>Evaluating Word Embeddings on Low-Resource Languages</a></strong><br><a href=/people/n/nathan-stringham/>Nathan Stringham</a>
|
<a href=/people/m/mike-izbicki/>Mike Izbicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eval4nlp-1--17><div class="card-body p-3 small">The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reasons : (1) it requires that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> be trained on large amounts of text, and (2) analogies may not be well-defined in some low-resource settings. We solve these problems by introducing the OddOneOut and Topk tasks, which are specifically designed for <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> in the low-resource setting. We use these metrics to successfully tune hyperparameters for a low-resource emoji embedding task and word embeddings on 16 extinct languages. The largest of these languages (Ancient Hebrew) has a 41 million token dataset, and the smallest (Old Gujarati) has only a 1813 token dataset.</div></div></div><hr><div id=2020findings-emnlp><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.findings-emnlp/>Findings of the Association for Computational Linguistics: EMNLP 2020</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.0/>Findings of the Association for Computational Linguistics: EMNLP 2020</a></strong><br><a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.1/>Fully Quantized Transformer for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/g/gabriele-prato/>Gabriele Prato</a>
|
<a href=/people/e/ella-charlaix/>Ella Charlaix</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--1><div class="card-body p-3 small">State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing <a href=https://en.wikipedia.org/wiki/Computational_cost>computational costs</a> of such <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT : an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to <a href=https://en.wikipedia.org/wiki/Significant_figures>full-precision</a>, our 8-bit models score greater or equal <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on most tasks. Comparing ourselves to all previously proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, we achieve state-of-the-art <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantization</a> results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.2/>Summarizing Chinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention<span class=acl-fixed-case>C</span>hinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention</a></strong><br><a href=/people/n/ningyu-zhang/>Ningyu Zhang</a>
|
<a href=/people/s/shumin-deng/>Shumin Deng</a>
|
<a href=/people/j/juan-li/>Juan Li</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/h/huajun-chen/>Huajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--2><div class="card-body p-3 small">Online search engines are a popular source of medical information for users, where users can enter questions and obtain relevant answers. It is desirable to generate answer summaries for <a href=https://en.wikipedia.org/wiki/Web_search_engine>online search engines</a>, particularly summaries that can reveal direct answers to questions. Moreover, answer summaries are expected to reveal the most relevant information in response to questions ; hence, the summaries should be generated with a focus on the question, which is a challenging topic-focused summarization task. In this paper, we propose an approach that utilizes graph convolution networks and question-focused dual attention for Chinese medical answer summarization. We first organize the original long answer text into a medical concept graph with graph convolution networks to better understand the internal structure of the text and the correlation between medical concepts. Then, we introduce a question-focused dual attention mechanism to generate summaries relevant to questions. Experimental results demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more coherent and informative summaries compared with baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.7/>Reducing Sentiment Bias in <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> via Counterfactual Evaluation</a></strong><br><a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/h/huan-zhang/>Huan Zhang</a>
|
<a href=/people/r/ray-jiang/>Ray Jiang</a>
|
<a href=/people/r/robert-stanforth/>Robert Stanforth</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/j/jack-rae/>Jack Rae</a>
|
<a href=/people/v/vishal-maini/>Vishal Maini</a>
|
<a href=/people/d/dani-yogatama/>Dani Yogatama</a>
|
<a href=/people/p/pushmeet-kohli/>Pushmeet Kohli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--7><div class="card-body p-3 small">Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> capable of generating coherent texts, it also prompts <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> exhibited by <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> : bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model&#8217;s latent representations. The <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizations</a> improve fairness metrics while retaining comparable levels of <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.11/>Difference-aware Knowledge Selection for Knowledge-grounded Conversation Generation</a></strong><br><a href=/people/c/chujie-zheng/>Chujie Zheng</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--11><div class="card-body p-3 small">In a multi-turn knowledge-grounded dialog, the difference between the knowledge selected at different turns usually provides potential clues to knowledge selection, which has been largely neglected in previous research. In this paper, we propose a difference-aware knowledge selection method. It first computes the difference between the candidate knowledge sentences provided at the current turn and those chosen in the previous turns. Then, the differential information is fused with or disentangled from the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> to facilitate final knowledge selection. Automatic, human observational, and interactive evaluation shows that our method is able to select knowledge more accurately and generate more informative responses, significantly outperforming the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.21.OptionalSupplementaryMaterial.bbl data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.21/>Semantic Matching for Sequence-to-Sequence Learning</a></strong><br><a href=/people/r/ruiyi-zhang/>Ruiyi Zhang</a>
|
<a href=/people/c/changyou-chen/>Changyou Chen</a>
|
<a href=/people/x/xinyuan-zhang/>Xinyuan Zhang</a>
|
<a href=/people/k/ke-bai/>Ke Bai</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--21><div class="card-body p-3 small">In sequence-to-sequence models, classical optimal transport (OT) can be applied to semantically match generated sentences with target sentences. However, in non-parallel settings, target sentences are usually unavailable. To tackle this issue without losing the benefits of classical OT, we present a semantic matching scheme based on the Optimal Partial Transport (OPT). Specifically, our approach partially matches semantically meaningful words between source and partial target sequences. To overcome the difficulty of detecting active regions in OPT (corresponding to the words needed to be matched), we further exploit prior knowledge to perform partial matching. Extensive experiments are conducted to evaluate the proposed approach, showing consistent improvements over sequence-to-sequence tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.24.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.24/>Gradient-based Analysis of NLP Models is Manipulable<span class=acl-fixed-case>NLP</span> Models is Manipulable</a></strong><br><a href=/people/j/junlin-wang/>Junlin Wang</a>
|
<a href=/people/j/jens-tuyls/>Jens Tuyls</a>
|
<a href=/people/e/eric-wallace/>Eric Wallace</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--24><div class="card-body p-3 small">Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, the fact that they directly reflect the model internals. In this paper, however, we demonstrate that the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a Facade Model that overwhelms the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> without affecting the predictions. This Facade Model can be trained to have <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> that are misleading and irrelevant to the task, such as focusing only on the <a href=https://en.wikipedia.org/wiki/Stop_words>stop words</a> in the input. On a variety of NLP tasks (sentiment analysis, NLI, and QA), we show that the merged model effectively fools different analysis tools : saliency maps differ significantly from the original model&#8217;s, input reduction keeps more irrelevant input tokens, and adversarial perturbations identify unimportant tokens as being highly important.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.25/>Pretrain-KGE : Learning <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>Knowledge Representation</a> from Pretrained Language Models<span class=acl-fixed-case>KGE</span>: Learning Knowledge Representation from Pretrained Language Models</a></strong><br><a href=/people/z/zhiyuan-zhang/>Zhiyuan Zhang</a>
|
<a href=/people/x/xiaoqian-liu/>Xiaoqian Liu</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a>
|
<a href=/people/q/qi-su/>Qi Su</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/b/bin-he/>Bin He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--25><div class="card-body p-3 small">Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> via pretrained language models by leveraging <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> from pretrained models. Specifically, we present a universal training framework named Pretrain-KGE consisting of three phases : semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed Pretrain-KGE can improve results over KGE models, especially on solving the low-resource problem.<i>Pretrain-KGE</i> consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed Pretrain-KGE can improve results over KGE models, especially on solving the low-resource problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.33/>Control, Generate, Augment : A Scalable Framework for Multi-Attribute Text Generation</a></strong><br><a href=/people/g/giuseppe-russo/>Giuseppe Russo</a>
|
<a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/c/claudiu-cristian-musat/>Claudiu Cristian Musat</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--33><div class="card-body p-3 small">We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> with a context-aware loss and a cyclical word dropout routine. We demonstrate the value of the individual model components in an ablation study. The <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a> of our approach is ensured through a single <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a>, independently of the number of attributes. We show high quality, <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>diversity</a> and attribute control in the generated sentences through a series of automatic and human assessments. As the main application of our work, we test the potential of this new NLG model in a data augmentation scenario. In a downstream NLP task, the sentences generated by our CGA model show significant improvements over a strong baseline, and a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance often comparable to adding same amount of additional real data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.35.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.35/>Dual Low-Rank Multimodal Fusion</a></strong><br><a href=/people/t/tao-jin/>Tao Jin</a>
|
<a href=/people/s/siyu-huang/>Siyu Huang</a>
|
<a href=/people/y/yingming-li/>Yingming Li</a>
|
<a href=/people/z/zhongfei-zhang/>Zhongfei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--35><div class="card-body p-3 small">Tensor-based fusion methods have been proven effective in multimodal fusion tasks. However, existing tensor-based methods make a poor use of the fine-grained temporal dynamics of multimodal sequential features. Motivated by this observation, this paper proposes a novel multimodal fusion method called Fine-Grained Temporal Low-Rank Multimodal Fusion (FT-LMF). FT-LMF correlates the features of individual time steps between multiple modalities, while it involves multiplications of high-order tensors in its calculation. This paper further proposes Dual Low-Rank Multimodal Fusion (Dual-LMF) to reduce the computational complexity of FT-LMF through low-rank tensor approximation along dual dimensions of input features. Dual-LMF is conceptually simple and practically effective and efficient. Empirical studies on benchmark multimodal analysis tasks show that our proposed methods outperform the state-of-the-art tensor-based fusion methods with a similar <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.38.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.38/>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels</a></strong><br><a href=/people/y/youxuan-jiang/>Youxuan Jiang</a>
|
<a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--38><div class="card-body p-3 small">Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing methods</a> have either had low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> or required substantial <a href=https://en.wikipedia.org/wiki/Annotation>expert annotation</a>. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for predicate labels and 93 % for argument labels, which is comparable to expert agreement. Compared to prior work on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> for SRL, we decrease expert effort by 4x, from 56 % to 14 % of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.39/>KorNLI and KorSTS : New Benchmark Datasets for Korean Natural Language Understanding<span class=acl-fixed-case>K</span>or<span class=acl-fixed-case>NLI</span> and <span class=acl-fixed-case>K</span>or<span class=acl-fixed-case>STS</span>: New Benchmark Datasets for <span class=acl-fixed-case>K</span>orean Natural Language Understanding</a></strong><br><a href=/people/j/jiyeon-ham/>Jiyeon Ham</a>
|
<a href=/people/y/yo-joong-choe/>Yo Joong Choe</a>
|
<a href=/people/k/kyubyong-park/>Kyubyong Park</a>
|
<a href=/people/i/ilji-choi/>Ilji Choi</a>
|
<a href=/people/h/hyungjoon-soh/>Hyungjoon Soh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--39><div class="card-body p-3 small">Natural language inference (NLI) and semantic textual similarity (STS) are key tasks in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding (NLU)</a>. Although several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> for those tasks have been released in English and a few other languages, there are no publicly available NLI or STS datasets in the <a href=https://en.wikipedia.org/wiki/Korean_language>Korean language</a>. Motivated by this, we construct and release new datasets for Korean NLI and STS, dubbed KorNLI and KorSTS, respectively. Following previous approaches, we machine-translate existing English training sets and manually translate development and test sets into <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. To accelerate research on Korean NLU, we also establish <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on KorNLI and KorSTS. Our datasets are publicly available at https://github.com/kakaobrain/KorNLUDatasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.43/>Claim Check-Worthiness Detection as Positive Unlabelled Learning</a></strong><br><a href=/people/d/dustin-wright/>Dustin Wright</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--43><div class="card-body p-3 small">As the first step of <a href=https://en.wikipedia.org/wiki/Fact-checking>automatic fact checking</a>, claim check-worthiness detection is a critical component of fact checking systems. There are multiple lines of research which study this problem : check-worthiness ranking from political speeches and debates, rumour detection on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, and citation needed detection from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. To date, there has been no structured comparison of these various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> to understand their relatedness, and no investigation into whether or not a unified approach to all of them is achievable. In this work, we illuminate a central challenge in claim check-worthiness detection underlying all of these tasks, being that they hinge upon detecting both how factual a sentence is, as well as how likely a sentence is to be believed without verification. As such, annotators only mark those instances they judge to be clear-cut check-worthy. Our best performing method is a unified approach which automatically corrects for this using a variant of positive unlabelled learning that finds instances which were incorrectly labelled as not check-worthy. In applying this, we out-perform the state of the art in two of the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> studied for claim check-worthiness detection in <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.44" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.44/>ConceptBert : Concept-Aware Representation for Visual Question Answering<span class=acl-fixed-case>C</span>oncept<span class=acl-fixed-case>B</span>ert: Concept-Aware Representation for Visual Question Answering</a></strong><br><a href=/people/f/francois-garderes/>François Gardères</a>
|
<a href=/people/m/maryam-ziaeefard/>Maryam Ziaeefard</a>
|
<a href=/people/b/baptiste-abeloos/>Baptiste Abeloos</a>
|
<a href=/people/f/freddy-lecue/>Freddy Lecue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--44><div class="card-body p-3 small">Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> and the natural language processing communities. A VQA model combines visual and textual features in order to answer questions grounded in an <a href=https://en.wikipedia.org/wiki/Image>image</a>. Current works in VQA focus on questions which are answerable by direct analysis of the question and image alone. We present a concept-aware algorithm, ConceptBert, for questions which require <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>, or basic factual knowledge from external structured content. Given an <a href=https://en.wikipedia.org/wiki/Image>image</a> and a question in natural language, ConceptBert requires visual elements of the <a href=https://en.wikipedia.org/wiki/Image>image</a> and a Knowledge Graph (KG) to infer the correct answer. We introduce a multi-modal representation which learns a joint Concept-Vision-Language embedding inspired by the popular BERT architecture. We exploit ConceptNet KG for encoding the common sense knowledge and evaluate our methodology on the Outside Knowledge-VQA (OK-VQA) and VQA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940115 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.53" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.53/>PBoS : Probabilistic Bag-of-Subwords for Generalizing Word Embedding<span class=acl-fixed-case>PB</span>o<span class=acl-fixed-case>S</span>: Probabilistic Bag-of-Subwords for Generalizing Word Embedding</a></strong><br><a href=/people/z/zhao-jinman/>Zhao Jinman</a>
|
<a href=/people/s/shawn-zhong/>Shawn Zhong</a>
|
<a href=/people/x/xiaomin-zhang/>Xiaomin Zhang</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--53><div class="card-body p-3 small">We look into the task of generalizing word embeddings : given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.54/>Interpretable Entity Representations through Large-Scale Typing</a></strong><br><a href=/people/y/yasumasa-onoe/>Yasumasa Onoe</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--54><div class="card-body p-3 small">In standard methodology for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, entities in text are typically embedded in dense vector spaces with pre-trained models. The <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> produced this way are effective when fed into downstream models, but they require end-task fine-tuning and are fundamentally difficult to interpret. In this paper, we present an approach to creating entity representations that are human readable and achieve high performance on entity-related tasks out of the box. Our representations are vectors whose values correspond to <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probabilities</a> over fine-grained entity types, indicating the confidence of a typing model&#8217;s decision that the entity belongs to the corresponding type. We obtain these representations using a fine-grained entity typing model, trained either on supervised ultra-fine entity typing data (Choi et al. 2018) or distantly-supervised examples from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. On entity probing tasks involving recognizing entity identity, our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> used in parameter-free downstream models achieve competitive performance with ELMo- and BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and improve performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.55/>Empirical Studies of Institutional Federated Learning For <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/x/xinghua-zhu/>Xinghua Zhu</a>
|
<a href=/people/j/jianzong-wang/>Jianzong Wang</a>
|
<a href=/people/z/zhenhou-hong/>Zhenhou Hong</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--55><div class="card-body p-3 small">Federated learning has sparkled new interests in the deep learning society to make use of isolated data sources from independent institutes. With the development of novel training tools, we have successfully deployed federated natural language processing networks on GPU-enabled server clusters. This paper demonstrates federated training of a popular NLP model, TextCNN, with applications in sentence intent classification. Furthermore, <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> is introduced to protect participants in the training process, in a manageable manner. Distinguished from previous client-level privacy protection schemes, the proposed differentially private federated learning procedure is defined in the dataset sample level, inherent with the applications among institutions instead of individual users. Optimal settings of hyper-parameters for the federated TextCNN model are studied through comprehensive experiments. We also evaluated the performance of federated TextCNN model under imbalanced data load configuration. Experiments show that, the sampling ratio has a large impact on the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_model>FL models</a>, causing up to 38.4 % decrease in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>test accuracy</a>, while they are robust to different noise multiplier levels, with less than 3 % variance in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>test accuracy</a>. It is also found that the FL models are sensitive to <a href=https://en.wikipedia.org/wiki/Load_balancing_(computing)>data load balancedness</a> among client datasets. When the <a href=https://en.wikipedia.org/wiki/Load_(computing)>data load</a> is imbalanced, model performance dropped by up to 10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.56/>NeuReduce : Reducing Mixed Boolean-Arithmetic Expressions by <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Network</a><span class=acl-fixed-case>N</span>eu<span class=acl-fixed-case>R</span>educe: Reducing Mixed <span class=acl-fixed-case>B</span>oolean-Arithmetic Expressions by Recurrent Neural Network</a></strong><br><a href=/people/w/weijie-feng/>Weijie Feng</a>
|
<a href=/people/b/binbin-liu/>Binbin Liu</a>
|
<a href=/people/d/dongpeng-xu/>Dongpeng Xu</a>
|
<a href=/people/q/qilong-zheng/>Qilong Zheng</a>
|
<a href=/people/y/yun-xu/>Yun Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--56><div class="card-body p-3 small">Mixed Boolean-Arithmetic (MBA) expressions involve both <a href=https://en.wikipedia.org/wiki/Arithmetic>arithmetic calculation</a> (e.g.,plus, minus, multiply) and <a href=https://en.wikipedia.org/wiki/Bitwise_operation>bitwise computation</a> (e.g., and, or, negate, xor). MBA expressions have been widely applied in <a href=https://en.wikipedia.org/wiki/Obfuscation_(software)>software obfuscation</a>, transforming <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> from a simple form to a complex form. MBA expressions are challenging to be simplified, because the interleaving bitwise and arithmetic operations causing mathematical reduction laws to be ineffective. Our goal is to recover the original, simple form from an obfuscated MBA expression. In this paper, we first propose NeuReduce, a string to string method based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to automatically learn and reduce complex MBA expressions. We develop a comprehensive MBA dataset, including one million diversified MBA expression samples and corresponding simplified forms. After training on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, NeuReduce can reduce MBA rules to homelier but mathematically equivalent forms. By comparing with three state-of-the-art MBA reduction methods, our evaluation result shows that NeuReduce outperforms all other tools in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Time_complexity>solving time</a>, and <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>performance overhead</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.57/>From Language to Language-ish : How Brain-Like is an LSTM’s Representation of Nonsensical Language Stimuli?<span class=acl-fixed-case>LSTM</span>’s Representation of Nonsensical Language Stimuli?</a></strong><br><a href=/people/m/maryam-hashemzadeh/>Maryam Hashemzadeh</a>
|
<a href=/people/g/greta-kaufeld/>Greta Kaufeld</a>
|
<a href=/people/m/martha-white/>Martha White</a>
|
<a href=/people/a/andrea-e-martin/>Andrea E. Martin</a>
|
<a href=/people/a/alona-fyshe/>Alona Fyshe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--57><div class="card-body p-3 small">The representations generated by many models of language (word embeddings, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain&#8217;s reaction to syntactically and semantically sound language stimuli. In this study, we asked : how does an LSTM (long short term memory) language model, trained (by and large) on semantically and syntactically intact language, represent a language sample with degraded semantic or syntactic information? Does the LSTM representation still resemble the brain&#8217;s reaction? We found that, even for some kinds of nonsensical language, there is a statistically significant relationship between the brain&#8217;s activity and the representations of an LSTM. This indicates that, at least in some instances, LSTMs and the human brain handle nonsensical data similarly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.59/>Cascaded Semantic and Positional Self-Attention Network for Document Classification</a></strong><br><a href=/people/j/juyong-jiang/>Juyong Jiang</a>
|
<a href=/people/j/jie-zhang/>Jie Zhang</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--59><div class="card-body p-3 small">Transformers have shown great success in learning representations for <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a>. However, an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (word orders). In this work, we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network (CSPAN) in the context of <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. The <a href=https://en.wikipedia.org/wiki/CSPAN>CSPAN</a> uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner, and then adaptively combine them together through a residue connection. Compared with commonly used positional encoding schemes, CSPAN can exploit the interaction between <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and word positions in a more interpretable and adaptive manner, and the classification performance can be notably improved while simultaneously preserving a compact model size and high <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence rate</a>. We evaluate the CSPAN model on several benchmark data sets for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> with careful ablation studies, and demonstrate the encouraging results compared with state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.60/>Toward Recognizing More Entity Types in NER : An Efficient Implementation using Only Entity Lexicons<span class=acl-fixed-case>NER</span>: An Efficient Implementation using Only Entity Lexicons</a></strong><br><a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/r/ruotian-ma/>Ruotian Ma</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/l/lujun-zhao/>Lujun Zhao</a>
|
<a href=/people/m/mengxi-wei/>Mengxi Wei</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--60><div class="card-body p-3 small">In this work, we explore the way to quickly adjust an existing named entity recognition (NER) system to make it capable of recognizing entity types not defined in the <a href=https://en.wikipedia.org/wiki/System>system</a>. As an illustrative example, consider the case that a NER system has been built to recognize person and organization names, and now it requires to additionally recognize job titles. Such a situation is common in the industrial areas, where the entity types required to recognize vary a lot in different products and keep changing. To avoid laborious data labeling and achieve fast adaptation, we propose to adjust the existing NER system using the previously labeled data and entity lexicons of the newly introduced entity types. We formulate such a <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> as a partially supervised learning problem and accordingly propose an effective <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. Comprehensive experimental studies on several public NER datasets validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940112 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.64" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.64/>Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior</a></strong><br><a href=/people/z/zi-lin/>Zi Lin</a>
|
<a href=/people/j/jeremiah-liu/>Jeremiah Liu</a>
|
<a href=/people/z/zi-yang/>Zi Yang</a>
|
<a href=/people/n/nan-hua/>Nan Hua</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--64><div class="card-body p-3 small">Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresholding operator on the function norm, and is applicable to any structured module including a single attention head, an entire attention blocks, or a feed-forward subnetwork. Furthermore, we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0 % on average at 50 % compression ratio.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.65/>Rethinking Self-Attention : Towards Interpretability in Neural Parsing</a></strong><br><a href=/people/k/khalil-mrini/>Khalil Mrini</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/w/walter-chang/>Walter Chang</a>
|
<a href=/people/n/ndapandula-nakashole/>Ndapa Nakashole</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--65><div class="card-body p-3 small">Attention mechanisms have improved the performance of NLP tasks while allowing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to remain explainable. Self-attention is currently widely used, however interpretability is difficult due to the numerous attention distributions. Recent work has shown that model representations can benefit from label-specific information, while facilitating <a href=https://en.wikipedia.org/wiki/Prediction>interpretation of predictions</a>. We introduce the Label Attention Layer : a new form of self-attention where attention heads represent labels. We test our novel layer by running constituency and dependency parsing experiments and show our new model obtains new state-of-the-art results for both tasks on both the Penn Treebank (PTB) and Chinese Treebank. Additionally, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> requires fewer self-attention layers compared to existing work. Finally, we find that the Label Attention heads learn relations between <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic categories</a> and show pathways to analyze errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.69.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.69/>Syntactic and Semantic-driven Learning for Open Information Extraction</a></strong><br><a href=/people/j/jialong-tang/>Jialong Tang</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--69><div class="card-body p-3 small">One of the biggest bottlenecks in building accurate, high coverage neural open IE systems is the need for large labelled corpora. The diversity of <a href=https://en.wikipedia.org/wiki/Open_domain>open domain corpora</a> and the variety of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language expressions</a> further exacerbate this problem. In this paper, we propose a syntactic and semantic-driven learning approach, which can learn neural open IE models without any human-labelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervision. Specifically, we first employ syntactic patterns as data labelling functions and pretrain a base model using the generated labels. Then we propose a syntactic and semantic-driven reinforcement learning algorithm, which can effectively generalize the base model to open situations with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Experimental results show that our approach significantly outperforms the supervised counterparts, and can even achieve competitive performance to supervised state-of-the-art (SoA) model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.75/>Actor-Double-Critic : Incorporating Model-Based Critic for Task-Oriented Dialogue Systems</a></strong><br><a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--75><div class="card-body p-3 small">In order to improve the sample-efficiency of deep reinforcement learning (DRL), we implemented imagination augmented agent (I2A) in spoken dialogue systems (SDS). Although I2A achieves a higher success rate than <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by augmenting predicted future into a policy network, its complicated architecture introduces unwanted instability. In this work, we propose actor-double-critic (ADC) to improve the stability and overall performance of I2A. ADC simplifies the architecture of I2A to reduce excessive parameters and hyper-parameters. More importantly, a separate model-based critic shares parameters between actions and makes <a href=https://en.wikipedia.org/wiki/Backpropagation>back-propagation</a> explicit. In our experiments on Cambridge Restaurant Booking task, ADC enhances success rates considerably and shows robustness to imperfect environment models. In addition, ADC exhibits the stability and sample-efficiency as significantly reducing the baseline standard deviation of success rates and reaching the 80 % success rate with half training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.77.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.77/>Sequential Span Classification with Neural Semi-Markov CRFs for Biomedical Abstracts<span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>CRF</span>s for Biomedical Abstracts</a></strong><br><a href=/people/k/kosuke-yamada/>Kosuke Yamada</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--77><div class="card-body p-3 small">Dividing biomedical abstracts into several segments with rhetorical roles is essential for supporting researchers&#8217; information access in the biomedical domain. Conventional methods have regarded the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as a sequence labeling task based on sequential sentence classification, i.e., they assign a rhetorical label to each sentence by considering the context in the abstract. However, these methods have a critical problem : they are prone to mislabel longer continuous sentences with the same rhetorical label. To tackle the problem, we propose sequential span classification that assigns a rhetorical label, not to a single sentence but to a span that consists of continuous sentences. Accordingly, we introduce Neural Semi-Markov Conditional Random Fields to assign the labels to such spans by considering all possible spans of various lengths. Experimental results obtained from PubMed 20k RCT and NICTA-PIBOSO datasets demonstrate that our proposed method achieved the best micro sentence-F1 score as well as the best micro span-F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.79.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.79" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.79/>AirConcierge : Generating Task-Oriented Dialogue via Efficient Large-Scale Knowledge Retrieval<span class=acl-fixed-case>A</span>ir<span class=acl-fixed-case>C</span>oncierge: Generating Task-Oriented Dialogue via Efficient Large-Scale Knowledge Retrieval</a></strong><br><a href=/people/c/chieh-yang-chen/>Chieh-Yang Chen</a>
|
<a href=/people/p/pei-hsin-wang/>Pei-Hsin Wang</a>
|
<a href=/people/s/shih-chieh-chang/>Shih-Chieh Chang</a>
|
<a href=/people/d/da-cheng-juan/>Da-Cheng Juan</a>
|
<a href=/people/w/wei-wei/>Wei Wei</a>
|
<a href=/people/j/jia-yu-pan/>Jia-Yu Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--79><div class="card-body p-3 small">Despite recent success in neural task-oriented dialogue systems, developing such a real-world system involves accessing large-scale knowledge bases (KBs), which can not be simply encoded by neural approaches, such as memory network mechanisms. To alleviate the above problem, we propose, an end-to-end trainable text-to-SQL guided framework to learn a <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>neural agent</a> that interacts with KBs using the generated SQL queries. Specifically, the neural agent first learns to ask and confirm the customer&#8217;s intent during the multi-turn interactions, then dynamically determining when to ground the user constraints into executable SQL queries so as to fetch relevant information from KBs. With the help of our method, the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> can use less but more accurate fetched results to generate useful responses efficiently, instead of incorporating the entire KBs. We evaluate the proposed method on the AirDialogue dataset, a large corpus released by Google, containing the conversations of customers booking flight tickets from the agent. The experimental results show that significantly improves over previous work in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and the BLEU score, which demonstrates not only the ability to achieve the given <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> but also the good quality of the generated dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940629 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.83/>Cross-lingual Alignment Methods for Multilingual BERT : A Comparative Study<span class=acl-fixed-case>BERT</span>: A Comparative Study</a></strong><br><a href=/people/s/saurabh-kulshreshtha/>Saurabh Kulshreshtha</a>
|
<a href=/people/j/jose-luis-redondo-garcia/>Jose Luis Redondo Garcia</a>
|
<a href=/people/c/ching-yun-chang/>Ching-Yun Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--83><div class="card-body p-3 small">Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks : Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment methods</a> to the type of task and proximity to the transfer language. We also find that <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> from <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> is generally superior to dictionary alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.86" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.86/>Persian Ezafe Recognition Using Transformers and Its Role in Part-Of-Speech Tagging<span class=acl-fixed-case>P</span>ersian Ezafe Recognition Using Transformers and Its Role in Part-Of-Speech Tagging</a></strong><br><a href=/people/e/ehsan-doostmohammadi/>Ehsan Doostmohammadi</a>
|
<a href=/people/m/minoo-nassajian/>Minoo Nassajian</a>
|
<a href=/people/a/adel-rahimi/>Adel Rahimi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--86><div class="card-body p-3 small">Ezafe is a <a href=https://en.wikipedia.org/wiki/Grammatical_particle>grammatical particle</a> in some <a href=https://en.wikipedia.org/wiki/Iranian_languages>Iranian languages</a> that links two words together. Regardless of the important information it conveys, it is almost always not indicated in <a href=https://en.wikipedia.org/wiki/Persian_alphabet>Persian script</a>, resulting in mistakes in reading complex sentences and errors in natural language processing tasks. In this paper, we experiment with different machine learning methods to achieve state-of-the-art results in the task of ezafe recognition. Transformer-based methods, BERT and XLMRoBERTa, achieve the best results, the latter achieving 2.68 % F1-score more than the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. We, moreover, use ezafe information to improve Persian part-of-speech tagging results and show that such information will not be useful to transformer-based methods and explain why that might be the case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.90.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940138 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.90/>Generative Data Augmentation for Commonsense Reasoning</a></strong><br><a href=/people/y/yiben-yang/>Yiben Yang</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/j/jared-fernandez/>Jared Fernandez</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/j/ji-ping-wang/>Ji-Ping Wang</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--90><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUGC, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. On experiments with multiple commonsense reasoning benchmarks, G-DAUGC consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, <a href=https://en.wikipedia.org/wiki/CODAH>CODAH</a>, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUGC produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.98/>Semi-Supervised Learning for Video Captioning</a></strong><br><a href=/people/k/ke-lin/>Ke Lin</a>
|
<a href=/people/z/zhuoxin-gan/>Zhuoxin Gan</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--98><div class="card-body p-3 small">Deep neural networks have made great success on video captioning in supervised learning setting. However, annotating videos with descriptions is very expensive and time-consuming. If the video captioning algorithm can benefit from a large number of unlabeled videos, the cost of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> can be reduced. In the proposed study, we make the first attempt to train the video captioning model on labeled data and unlabeled data jointly, in a semi-supervised learning manner. For labeled data, we train them with the traditional cross-entropy loss. For unlabeled data, we leverage a self-critical policy gradient method with the difference between the scores obtained by <a href=https://en.wikipedia.org/wiki/Monte_Carlo_method>Monte-Carlo sampling</a> and greedy decoding as the reward function, while the scores are the negative K-L divergence between output distributions of original video data and augmented video data. The final <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> is the <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted sum of losses</a> obtained by labeled data and unlabeled data. Experiments conducted on VATEX, MSR-VTT and MSVD dataset demonstrate that the introduction of unlabeled data can improve the performance of the video captioning model. The proposed semi-supervised learning algorithm also outperforms several state-of-the-art semi-supervised learning approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.99.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.99" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.99/>Multi2OIE : Multilingual Open Information Extraction Based on Multi-Head Attention with BERT<span class=acl-fixed-case>M</span>ultiˆ2<span class=acl-fixed-case>OIE</span>: Multilingual Open Information Extraction Based on Multi-Head Attention with <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/y/youngbin-ro/>Youngbin Ro</a>
|
<a href=/people/y/yukyung-lee/>Yukyung Lee</a>
|
<a href=/people/p/pilsung-kang/>Pilsung Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--99><div class="card-body p-3 small">In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is a <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence-labeling system</a> with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi^2OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.<tex-math>^2</tex-math>OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi<tex-math>^2</tex-math>OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.101/>Using the Past Knowledge to Improve Sentiment Classification</a></strong><br><a href=/people/q/qi-qin/>Qi Qin</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--101><div class="card-body p-3 small">This paper studies sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks. It proposes a new lifelong learning model (called L2PG) that can retain and selectively transfer the knowledge learned in the past to help learn the new task. A key innovation of this proposed model is a novel parameter-gate (p-gate) mechanism that regulates the flow or transfer of the previously learned knowledge to the new task. Specifically, it can selectively use the network parameters (which represent the retained knowledge gained from the previous tasks) to assist the learning of the new task t. Knowledge distillation is also employed in the process to preserve the past knowledge by approximating the network output at the state when task t-1 was learned. Experimental results show that L2PG outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, including even multiple task learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.102.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.102/>High-order Semantic Role Labeling</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kevin-parnow/>Kevin Parnow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--102><div class="card-body p-3 small">Semantic role labeling is primarily used to identify <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>, <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>arguments</a>, and their <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relationships</a>. Due to the limitations of modeling methods and the conditions of pre-identified predicates, previous work has focused on the relationships between predicates and arguments and the correlations between arguments at most, while the correlations between predicates have been neglected for a long time. High-order features and structure learning were very common in modeling such <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlations</a> before the neural network era. In this paper, we introduce a high-order graph structure for the neural semantic role labeling model, which enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to explicitly consider not only the isolated predicate-argument pairs but also the interaction between the predicate-argument pairs. Experimental results on 7 languages of the CoNLL-2009 benchmark show that the high-order structural learning techniques are beneficial to the strong performing SRL models and further boost our baseline to achieve new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.108.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940168 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.108/>Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection</a></strong><br><a href=/people/h/hoang-nguyen/>Hoang Nguyen</a>
|
<a href=/people/c/chenwei-zhang/>Chenwei Zhang</a>
|
<a href=/people/c/congying-xia/>Congying Xia</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--108><div class="card-body p-3 small">Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture <a href=https://en.wikipedia.org/wiki/High-_and_low-level>high-level information</a>, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a> on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Our code and data are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.112/>What Can We Do to Improve <a href=https://en.wikipedia.org/wiki/Peer_review>Peer Review</a> in NLP?<span class=acl-fixed-case>NLP</span>?</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--112><div class="card-body p-3 small">Peer review is our best tool for judging the quality of conference submissions, but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.116/>Examining the Ordering of Rhetorical Strategies in Persuasive Requests</a></strong><br><a href=/people/o/omar-shaikh/>Omar Shaikh</a>
|
<a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/j/jon-saad-falcon/>Jon Saad-Falcon</a>
|
<a href=/people/p/polo-chau/>Polo Chau</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--116><div class="card-body p-3 small">Interpreting how persuasive language influences audiences has implications across many domains like <a href=https://en.wikipedia.org/wiki/Advertising>advertising</a>, <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a>, and <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. Persuasion relies on more than a message&#8217;s content. Arranging the order of the message itself (i.e., ordering specific rhetorical strategies) also plays an important role. To examine how strategy orderings contribute to persuasiveness, we first utilize a Variational Autoencoder model to disentangle content and rhetorical strategies in textual requests from a large-scale loan request corpus. We then visualize interplay between content and strategy through an attentional LSTM that predicts the success of textual requests. We find that specific (orderings of) strategies interact uniquely with a request&#8217;s content to impact success rate, and thus the persuasiveness of a request.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.122/>A Compare Aggregate Transformer for Understanding Document-grounded Dialogue</a></strong><br><a href=/people/l/longxuan-ma/>Longxuan Ma</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/r/runxin-sun/>Runxin Sun</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--122><div class="card-body p-3 small">Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However, dialogue history that is not related to the current dialogue may introduce <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a> in the KS processing. In this paper, we propose a Compare Aggregate Transformer (CAT) to jointly denoise the dialogue context and aggregate the document information for response generation. We designed two different comparison mechanisms to reduce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> (before and during decoding). In addition, we propose two <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for evaluating document utilization efficiency based on word overlap. Experimental results on the CMU_DoG dataset show that the proposed CAT model outperforms the state-of-the-art approach and strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.124/>Modeling Intra and Inter-modality Incongruity for Multi-Modal Sarcasm Detection</a></strong><br><a href=/people/h/hongliang-pan/>Hongliang Pan</a>
|
<a href=/people/z/zheng-lin/>Zheng Lin</a>
|
<a href=/people/p/peng-fu/>Peng Fu</a>
|
<a href=/people/y/yatao-qi/>Yatao Qi</a>
|
<a href=/people/w/weiping-wang/>Weiping Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--124><div class="card-body p-3 small">Sarcasm is a pervasive phenomenon in today&#8217;s <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. These platforms allow users to create multi-modal messages, including <a href=https://en.wikipedia.org/wiki/Text_messaging>texts</a>, <a href=https://en.wikipedia.org/wiki/Image_sharing>images</a>, and <a href=https://en.wikipedia.org/wiki/Online_video_platform>videos</a>. Existing multi-modal sarcasm detection methods either simply concatenate the features from multi modalities or fuse the multi modalities information in a designed manner. However, they ignore the incongruity character in sarcastic utterance, which is often manifested between modalities or within modalities. Inspired by this, we propose a BERT architecture-based model, which concentrates on both intra and inter-modality incongruity for multi-modal sarcasm detection. To be specific, we are inspired by the idea of self-attention mechanism and design inter-modality attention to capturing inter-modality incongruity. In addition, the co-attention mechanism is applied to model the contradiction within the text. The incongruity information is then used for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. The experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on a public multi-modal sarcasm detection dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.131.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.131/>Conditional Neural Generation using Sub-Aspect Functions for Extractive News Summarization</a></strong><br><a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/k/ke-shi/>Ke Shi</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--131><div class="card-body p-3 small">Much progress has been made in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inverted pyramid writing style. In addition, there is an unmet need to generate a variety of summaries for different users. In this paper, we propose a neural framework that can flexibly control summary generation by introducing a set of sub-aspect functions (i.e. importance, <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>diversity</a>, position). These sub-aspect functions are regulated by a set of <a href=https://en.wikipedia.org/wiki/Control_code>control codes</a> to decide which sub-aspect to focus on during summary generation. We demonstrate that extracted summaries with minimal position bias is comparable with those generated by standard models that take advantage of position preference. We also show that news summaries generated with a focus on <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> can be more preferred by human raters. These results suggest that a more flexible neural summarization framework providing more control options could be desirable in tailoring to different user preferences, which is useful since it is often impractical to articulate such preferences for different applications a priori.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.134.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940121 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.134" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.134/>Inexpensive Domain Adaptation of Pretrained Language Models : Case Studies on Biomedical NER and Covid-19 QA<span class=acl-fixed-case>NER</span> and Covid-19 <span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/n/nina-poerner/>Nina Poerner</a>
|
<a href=/people/u/ulli-waltinger/>Ulli Waltinger</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--134><div class="card-body p-3 small">Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of <a href=https://en.wikipedia.org/wiki/Computer_hardware>hardware</a>, <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime</a> and <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO 2 emissions</a>. Here, we propose a cheaper alternative : We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60 % of the BioBERT-BERT F1 delta, at 5 % of BioBERT&#8217;s CO 2 footprint and 2 % of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain : the Covid-19 pandemic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940063 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.135" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.135/>Semantically Driven Sentence Fusion : Modeling and Evaluation</a></strong><br><a href=/people/e/eyal-ben-david/>Eyal Ben-David</a>
|
<a href=/people/o/orgad-keller/>Orgad Keller</a>
|
<a href=/people/e/eric-malmi/>Eric Malmi</a>
|
<a href=/people/i/idan-szpektor/>Idan Szpektor</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--135><div class="card-body p-3 small">Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> from robustly capturing the semantic relationship between input sentences. To alleviate this, we present an approach in which ground-truth solutions are automatically expanded into multiple references via curated equivalence classes of connective phrases. We apply this method to a large-scale dataset and use the augmented dataset for both model training and evaluation. To improve the learning of semantic representation using multiple references, we enrich the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with auxiliary discourse classification tasks under a multi-tasking framework. Our experiments highlight the improvements of our approach over <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.140/>StyleDGPT : Stylized Response Generation with Pre-trained Language Models<span class=acl-fixed-case>S</span>tyle<span class=acl-fixed-case>DGPT</span>: Stylized Response Generation with Pre-trained Language Models</a></strong><br><a href=/people/z/ze-yang/>Ze Yang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/x/xinnian-liang/>Xinnian Liang</a>
|
<a href=/people/j/jiaqi-bai/>Jiaqi Bai</a>
|
<a href=/people/l/liran-wang/>Liran Wang</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--140><div class="card-body p-3 small">Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> that have brought breakthrough to various <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a>. To this end, we introduce a KL loss and a style classifier to the fine-tuning step in order to steer response generation towards the target style in both a word-level and a sentence-level. Comprehensive empirical studies with two public datasets indicate that our model can significantly outperform state-of-the-art methods in terms of both style consistency and contextual coherence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.141/>Enhancing Automated Essay Scoring Performance via Fine-tuning Pre-trained Language Models with Combination of <a href=https://en.wikipedia.org/wiki/Regression_analysis>Regression</a> and Ranking</a></strong><br><a href=/people/r/ruosong-yang/>Ruosong Yang</a>
|
<a href=/people/j/jiannong-cao/>Jiannong Cao</a>
|
<a href=/people/z/zhiyuan-wen/>Zhiyuan Wen</a>
|
<a href=/people/y/youzheng-wu/>Youzheng Wu</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--141><div class="card-body p-3 small">Automated Essay Scoring (AES) is a critical text regression task that automatically assigns scores to essays based on their writing quality. Recently, the performance of sentence prediction tasks has been largely improved by using Pre-trained Language Models via fusing representations from different layers, constructing an auxiliary sentence, using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, etc. However, to solve the AES task, previous works utilize shallow neural networks to learn essay representations and constrain calculated scores with regression loss or ranking loss, respectively. Since shallow neural networks trained on limited samples show poor performance to capture deep semantic of texts. And without an accurate scoring function, ranking loss and regression loss measures two different aspects of the calculated scores. To improve <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES</a>&#8217;s performance, we find a new way to fine-tune pre-trained language models with multiple losses of the same task. In this paper, we propose to utilize a pre-trained language model to learn text representations first. With scores calculated from the representations, <a href=https://en.wikipedia.org/wiki/Mean_square_error>mean square error loss</a> and the batch-wise ListNet loss with dynamic weights constrain the scores simultaneously. We utilize Quadratic Weighted Kappa to evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the Automated Student Assessment Prize dataset. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms not only state-of-the-art neural models near 3 percent but also the latest statistic model. Especially on the two narrative prompts, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs much better than all other state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.143/>Inferring about fraudulent collusion risk on Brazilian public works contracts in official texts using a Bi-LSTM approach<span class=acl-fixed-case>B</span>razilian public works contracts in official texts using a <span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span> approach</a></strong><br><a href=/people/m/marcos-lima/>Marcos Lima</a>
|
<a href=/people/r/roberta-silva/>Roberta Silva</a>
|
<a href=/people/f/felipe-lopes-de-souza-mendes/>Felipe Lopes de Souza Mendes</a>
|
<a href=/people/l/leonardo-r-de-carvalho/>Leonardo R. de Carvalho</a>
|
<a href=/people/a/aleteia-araujo/>Aleteia Araujo</a>
|
<a href=/people/f/flavio-de-barros-vidal/>Flavio de Barros Vidal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--143><div class="card-body p-3 small">Public works procurements move US$ 10 billion yearly in Brazil and are a preferred field for <a href=https://en.wikipedia.org/wiki/Collusion>collusion</a> and <a href=https://en.wikipedia.org/wiki/Fraud>fraud</a>. Federal Police and audit agencies investigate collusion (bid-rigging), over-pricing, and delivery fraud in this field and efforts have been employed to early detect fraud and <a href=https://en.wikipedia.org/wiki/Collusion>collusion</a> on public works procurements. The current automatic methods of <a href=https://en.wikipedia.org/wiki/Fraud_detection>fraud detection</a> use <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> and usually do not involve annotated data. The use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for this kind of <a href=https://en.wikipedia.org/wiki/Application_software>application</a> is rare. Our work introduces a new dataset formed by public procurement calls available on Brazilian official journal (Dirio Oficial da Unio), using by 15,132,968 textual entries of which 1,907 are annotated risky entries. Both bottleneck deep neural network and BiLSTM shown competitive compared with classical <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> and achieved better precision (93.0 % and 92.4 %, respectively), which signs improvements in a <a href=https://en.wikipedia.org/wiki/Criminal_investigation>criminal fraud investigation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.144" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.144/>Data-to-Text Generation with Style Imitation</a></strong><br><a href=/people/s/shuai-lin/>Shuai Lin</a>
|
<a href=/people/w/wentao-wang/>Wentao Wang</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a>
|
<a href=/people/f/frank-f-xu/>Frank F. Xu</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--144><div class="card-body p-3 small">Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., sentence structures, word choices). More traditional <a href=https://en.wikipedia.org/wiki/System>systems</a> use <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> to determine the realization of text. Yet manual or automatic construction of high-quality templates is difficult, and a <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>template</a> acting as hard constraints could harm content fidelity when it does not match the record perfectly. We study a new way of <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic control</a> by using existing sentences as soft templates. That is, a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns to imitate the <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a> of any given exemplar sentence, with automatic adaptions to faithfully describe the record. The <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is challenging due to the lack of parallel data. We develop a neural approach that includes a hybrid attention-copy mechanism, learns with weak supervisions, and is enhanced with a new content coverage constraint. We conduct experiments in <a href=https://en.wikipedia.org/wiki/Restaurant>restaurants</a> and <a href=https://en.wikipedia.org/wiki/Sport>sports domains</a>. Results show our approach achieves stronger performance than a range of comparison methods. Our approach balances well between content fidelity and style control given exemplars that match the records to varying degrees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.145/>Teaching Machine Comprehension with Compositional Explanations</a></strong><br><a href=/people/q/qinyuan-ye/>Qinyuan Ye</a>
|
<a href=/people/x/xiao-huang/>Xiao Huang</a>
|
<a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--145><div class="card-body p-3 small">Advances in machine reading comprehension (MRC) rely heavily on the collection of large scale human-annotated examples in the form of (question, paragraph, answer) triples. In contrast, humans are typically able to generalize with only a few examples, relying on deeper underlying <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, linguistic sophistication, and/or simply superior <a href=https://en.wikipedia.org/wiki/Deductive_reasoning>deductive powers</a>. In this paper, we focus on teaching machines reading comprehension, using a small number of semi-structured explanations that explicitly inform machines why answer spans are correct. We extract structured variables and rules from explanations and compose neural module teachers that annotate instances for training downstream MRC models. We use learnable neural modules and soft logic to handle linguistic variation and overcome sparse coverage ; the modules are jointly optimized with the MRC model to improve final performance. On the SQuAD dataset, our proposed method achieves 70.14 % <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> with supervision from 26 explanations, comparable to plain <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> using 1,100 labeled instances, yielding a 12x speed up.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940631 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.147/>SimAlign : High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>A</span>lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings</a></strong><br><a href=/people/m/masoud-jalili-sabet/>Masoud Jalili Sabet</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--147><div class="card-body p-3 small">Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and <a href=https://en.wikipedia.org/wiki/Data_quality>quality</a> decreases as less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available. We propose word alignment methods that require no <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. The key idea is to leverage multilingual word embeddings both static and contextualized for <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>. Our multilingual embeddings are created from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only without relying on any parallel data or <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a>. We find that alignments created from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners even with abundant parallel data ; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.148/>TweetEval : Unified Benchmark and Comparative Evaluation for Tweet Classification<span class=acl-fixed-case>T</span>weet<span class=acl-fixed-case>E</span>val: Unified Benchmark and Comparative Evaluation for Tweet Classification</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/l/leonardo-neves/>Leonardo Neves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--148><div class="card-body p-3 small">The experimental landscape in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.151/>Cost-effective Selection of Pretraining Data : A Case Study of Pretraining BERT on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>BERT</span> on Social Media</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--151><div class="card-body p-3 small">Recent studies on domain-specific BERT models show that effectiveness on downstream tasks can be improved when <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are pretrained on in-domain data. Often, the pretraining data used in these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are selected based on their subject matter, e.g., <a href=https://en.wikipedia.org/wiki/Biology>biology</a> or <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>. Given the range of applications using social media text, and its unique language variety, we pretrain two models on <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and <a href=https://en.wikipedia.org/wiki/Internet_forum>forum text</a> respectively, and empirically demonstrate the effectiveness of these two resources. In addition, we investigate how <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> can be used to nominate in-domain pretraining data. We publicly release our pretrained models at https://bit.ly/35RpTf0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940122 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.152/>TopicBERT for Energy Efficient Document Classification<span class=acl-fixed-case>T</span>opic<span class=acl-fixed-case>BERT</span> for Energy Efficient Document Classification</a></strong><br><a href=/people/y/yatin-chaudhary/>Yatin Chaudhary</a>
|
<a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/k/khushbu-saxena/>Khushbu Saxena</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/t/thomas-runkler/>Thomas Runkler</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--152><div class="card-body p-3 small">Prior research notes that BERT&#8217;s computational cost grows quadratically with sequence length thus leading to longer <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training times</a>, higher GPU memory constraints and <a href=https://en.wikipedia.org/wiki/Greenhouse_gas>carbon emissions</a>. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> especially for long sequence tasks like <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. Our work thus focuses on optimizing the <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations a main performance bottleneck. Consequently, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a 1.4x (40 %) <a href=https://en.wikipedia.org/wiki/Speedup>speedup</a> with 40 % reduction in <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO2 emission</a> while retaining 99.9 % performance over 5 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.162.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.162/>Active Learning Approaches to Enhancing Neural Machine Translation</a></strong><br><a href=/people/y/yuekai-zhao/>Yuekai Zhao</a>
|
<a href=/people/h/haoran-zhang/>Haoran Zhang</a>
|
<a href=/people/s/shuchang-zhou/>Shuchang Zhou</a>
|
<a href=/people/z/zhihua-zhang/>Zhihua Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--162><div class="card-body p-3 small">Active learning is an efficient approach for mitigating <a href=https://en.wikipedia.org/wiki/Data_dependency>data dependency</a> when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> into various techniques such as <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and iterative back-translation (IBT) under a limited human translation budget. We design a word frequency based acquisition function and combine it with a strong uncertainty based method. The combined <a href=https://en.wikipedia.org/wiki/Methodology>method</a> steadily outperforms all other acquisition functions in various scenarios. As far as we know, we are the first to do a large-scale study on actively training Transformer for <a href=https://en.wikipedia.org/wiki/Neurotransmission>NMT</a>. Specifically, with a human translation budget of only 20 % of the original <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>, we manage to surpass Transformer trained on the entire <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> in three language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.164.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940170 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.164/>Continual Learning Long Short Term Memory</a></strong><br><a href=/people/x/xin-guo/>Xin Guo</a>
|
<a href=/people/y/yu-tian/>Yu Tian</a>
|
<a href=/people/q/qinghan-xue/>Qinghan Xue</a>
|
<a href=/people/p/panos-lampropoulos/>Panos Lampropoulos</a>
|
<a href=/people/s/steven-eliuk/>Steven Eliuk</a>
|
<a href=/people/k/kenneth-barner/>Kenneth Barner</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--164><div class="card-body p-3 small">Catastrophic forgetting in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> indicates the performance decreasing of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task&#8217;s output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of CL-LSTM over state-of-the-art approaches on spoken language understanding (SLU) tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.166.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940105 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.166" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.166/>Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers</a></strong><br><a href=/people/b/brian-lester/>Brian Lester</a>
|
<a href=/people/d/daniel-pressel/>Daniel Pressel</a>
|
<a href=/people/a/amy-hemmeter/>Amy Hemmeter</a>
|
<a href=/people/s/sagnik-ray-choudhury/>Sagnik Ray Choudhury</a>
|
<a href=/people/s/srinivas-bangalore/>Srinivas Bangalore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--166><div class="card-body p-3 small">Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. Current work eschews prior knowledge of how the span encoding scheme works and relies on the CRF learning which transitions are illegal and which are not to facilitate global coherence. We find that by constraining the output to suppress illegal transitions we can train a tagger with a cross-entropy loss twice as fast as a CRF with differences in <a href=https://en.wikipedia.org/wiki/F-number>F1</a> that are statistically insignificant, effectively eliminating the need for a CRF. We analyze the dynamics of tag co-occurrence to explain when these constraints are most effective and provide open source implementations of our tagger in both <a href=https://en.wikipedia.org/wiki/PyTorch>PyTorch</a> and <a href=https://en.wikipedia.org/wiki/TensorFlow>TensorFlow</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.168.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.168/>TED : A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising<span class=acl-fixed-case>TED</span>: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising</a></strong><br><a href=/people/z/ziyi-yang/>Ziyi Yang</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/r/robert-gmyr/>Robert Gmyr</a>
|
<a href=/people/m/michael-zeng/>Michael Zeng</a>
|
<a href=/people/x/xuedong-huang/>Xuedong Huang</a>
|
<a href=/people/e/eric-darve/>Eric Darve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--168><div class="card-body p-3 small">Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently proposed transformer exhibits much more capability. Moreover, most of previous summarization models ignore abundant unlabeled corpora resources available for pretraining. In order to address these issues, we propose TED, a transformer-based unsupervised abstractive summarization system with pretraining on large-scale data. We first leverage the lead bias in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> to pretrain the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on millions of unlabeled corpora. Next, we finetune TED on target domains through theme modeling and a denoising autoencoder to enhance the quality of generated summaries. Notably, TED outperforms all unsupervised abstractive baselines on NYT, CNN / DM and English Gigaword datasets with various document styles. Further analysis shows that the summaries generated by <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED</a> are highly abstractive, and each component in the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> of <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED</a> is highly effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.169/>Improving End-to-End Bangla Speech Recognition with <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-supervised Training</a><span class=acl-fixed-case>B</span>angla Speech Recognition with Semi-supervised Training</a></strong><br><a href=/people/n/nafis-sadeq/>Nafis Sadeq</a>
|
<a href=/people/n/nafis-tahmid-chowdhury/>Nafis Tahmid Chowdhury</a>
|
<a href=/people/f/farhan-tanvir-utshaw/>Farhan Tanvir Utshaw</a>
|
<a href=/people/s/shafayat-ahmed/>Shafayat Ahmed</a>
|
<a href=/people/m/muhammad-abdullah-adnan/>Muhammad Abdullah Adnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--169><div class="card-body p-3 small">Automatic speech recognition systems usually require large annotated speech corpus for training. The manual annotation of a large corpus is very difficult. It can be very helpful to use unsupervised and semi-supervised learning methods in addition to <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. In this work, we focus on using a semi-supervised training approach for Bangla Speech Recognition that can exploit large unpaired audio and text data. We encode speech and text data in an intermediate domain and propose a novel <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> based on the global encoding distance between encoded data to guide the <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised training</a>. Our proposed method reduces the Word Error Rate (WER) of the <a href=https://en.wikipedia.org/wiki/System>system</a> from 37 % to 31.9 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.171.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.171/>UNIFIEDQA : Crossing Format Boundaries with a Single QA System<span class=acl-fixed-case>UNIFIEDQA</span>: Crossing Format Boundaries with a Single <span class=acl-fixed-case>QA</span> System</a></strong><br><a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--171><div class="card-body p-3 small">Question answering (QA) tasks have been posed using a variety of <a href=https://en.wikipedia.org/wiki/File_format>formats</a>, such as extractive span selection, <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice</a>, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.173/>Pragmatic Issue-Sensitive Image Captioning</a></strong><br><a href=/people/a/allen-nie/>Allen Nie</a>
|
<a href=/people/r/reuben-cohn-gordon/>Reuben Cohn-Gordon</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--173><div class="card-body p-3 small">Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper article</a> about a <a href=https://en.wikipedia.org/wiki/Sport>sports event</a>, a caption that not only identifies the player in a picture but also comments on their ethnicity could create unwanted reader reactions. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, the captioner is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. For the sports article, we could construct a <a href=https://en.wikipedia.org/wiki/Partition_of_a_set>partition</a> that places images into <a href=https://en.wikipedia.org/wiki/Equivalence_class>equivalence classes</a> based on player position. To model this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we use an extension of the Rational Speech Acts model. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly uses image partitions to control caption generation. In both automatic and human evaluations, we show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> generate captions that are descriptive and issue-sensitive. Finally, we show how ISIC can complement and enrich the related task of Visual Question Answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.175" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.175/>Adversarial Subword Regularization for Robust Neural Machine Translation</a></strong><br><a href=/people/j/jungsoo-park/>Jungsoo Park</a>
|
<a href=/people/m/mujeen-sung/>Mujeen Sung</a>
|
<a href=/people/j/jinhyuk-lee/>Jinhyuk Lee</a>
|
<a href=/people/j/jaewoo-kang/>Jaewoo Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--175><div class="card-body p-3 small">Exposing diverse subword segmentations to neural machine translation (NMT) models often improves the robustness of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> as NMT models can experience various subword candidates. However, the diversification of subword segmentations mostly relies on the pre-trained subword language models from which erroneous segmentations of unseen words are less likely to be sampled. In this paper, we present adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations. We experimentally show that our model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of NMT models in low-resource and out-domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940178 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.176/>Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays<span class=acl-fixed-case>X</span>-rays</a></strong><br><a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/c/chun-nan-hsu/>Chun-Nan Hsu</a>
|
<a href=/people/a/amilcare-gentili/>Amilcare Gentili</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--176><div class="card-body p-3 small">Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists&#8217; workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are affected by <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>data bias</a> (e.g. label imbalance) and face common issues inherent in text generation models (e.g. repetition). In this work, we focus on reporting abnormal findings on radiology images ; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.177/>SynET : Synonym Expansion using Transitivity<span class=acl-fixed-case>S</span>yn<span class=acl-fixed-case>ET</span>: Synonym Expansion using Transitivity</a></strong><br><a href=/people/j/jiale-yu/>Jiale Yu</a>
|
<a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/x/xinyin-ma/>Xinyin Ma</a>
|
<a href=/people/c/chenghao-jia/>Chenghao Jia</a>
|
<a href=/people/c/chen-chen/>Chen Chen</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--177><div class="card-body p-3 small">In this paper, we study a new task of synonym expansion using transitivity, and propose a novel approach named SynET, which considers both the contexts of two given synonym pairs. It introduces an auxiliary task to reduce the impact of noisy sentences, and proposes a Multi-Perspective Entity Matching Network to match entities from multiple perspectives. Extensive experiments on a real-world dataset show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.178/>Scheduled DropHead : A Regularization Method for Transformer Models<span class=acl-fixed-case>D</span>rop<span class=acl-fixed-case>H</span>ead: A Regularization Method for Transformer Models</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--178><div class="card-body p-3 small">We introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism which is a key component of <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a>. In contrast to the conventional dropout mechanism which randomly drops units or connections, DropHead drops entire <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a> during training to prevent the multi-head attention model from being dominated by a small portion of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a>. It can help reduce the risk of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and allow the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to better benefit from the multi-head attention. Given the interaction between <a href=https://en.wikipedia.org/wiki/Multi-headedness>multi-headedness</a> and training dynamics, we further propose a novel dropout rate scheduler to adjust the dropout rate of DropHead throughout training, which results in a better regularization effect. Experimental results demonstrate that our proposed approach can improve transformer models by 0.9 BLEU score on WMT14 En-De translation task and around 1.0 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for various text classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940033 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.180/>Automatically Identifying Gender Issues in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> using Perturbations</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--180><div class="card-body p-3 small">The successful application of neural methods to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>evaluation benchmark</a> spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.181/>Ruler : Data Programming by Demonstration for Document Labeling</a></strong><br><a href=/people/s/sara-evensen/>Sara Evensen</a>
|
<a href=/people/c/chang-ge/>Chang Ge</a>
|
<a href=/people/c/cagatay-demiralp/>Cagatay Demiralp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--181><div class="card-body p-3 small">Data programming aims to reduce the cost of curating training data by encoding <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> as labeling functions over source data. As such it not only requires <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain expertise</a> but also <a href=https://en.wikipedia.org/wiki/Computer_programming>programming experience</a>, a skill that many subject matter experts lack. Additionally, generating functions by enumerating rules is not only time consuming but also inherently difficult, even for people with programming experience. In this paper we introduce Ruler, an interactive system that synthesizes labeling rules using span-level interactive demonstrations over document examples. Ruler is a first-of-a-kind implementation of data programming by demonstration (DPBD). This new framework aims to relieve users from the burden of writing labeling functions, enabling them to focus on higher-level semantic analysis, such as identifying relevant signals for the labeling task. We compare Ruler with conventional data programming through a user study conducted with 10 data scientists who were asked to create labeling functions for sentiment and spam classification tasks. Results show <a href=https://en.wikipedia.org/wiki/Ruler>Ruler</a> is easier to learn and to use, and that it offers higher overall user-satisfaction while providing model performances comparable to those achieved by conventional data programming.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.182/>Dual Reconstruction : a Unifying Objective for Semi-Supervised Neural Machine Translation</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--182><div class="card-body p-3 small">While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.185/>Multi-pretraining for Large-scale Text Classification</a></strong><br><a href=/people/k/kang-min-kim/>Kang-Min Kim</a>
|
<a href=/people/b/bumsu-hyeon/>Bumsu Hyeon</a>
|
<a href=/people/y/yeachan-kim/>Yeachan Kim</a>
|
<a href=/people/j/jun-hyung-park/>Jun-Hyung Park</a>
|
<a href=/people/s/sangkeun-lee/>SangKeun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--185><div class="card-body p-3 small">Deep neural network-based pretraining methods have achieved impressive results in many natural language processing tasks including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. However, their applicability to large-scale text classification with numerous categories (e.g., several thousands) is yet to be well-studied, where the training data is insufficient and skewed in terms of categories. In addition, existing pretraining methods usually involve excessive computation and <a href=https://en.wikipedia.org/wiki/Memory_management>memory overheads</a>. In this paper, we develop a novel multi-pretraining framework for large-scale text classification. This multi-pretraining framework includes both a self-supervised pretraining and a weakly supervised pretraining. We newly introduce an out-of-context words detection task on the unlabeled data as the self-supervised pretraining. It captures the topic-consistency of words used in sentences, which is proven to be useful for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. In addition, we propose a weakly supervised pretraining, where labels for text classification are obtained automatically from an existing approach. Experimental results clearly show that both pretraining approaches are effective for large-scale text classification task. The proposed scheme exhibits significant improvements as much as 3.8 % in terms of macro-averaging F1-score over strong pretraining methods, while being computationally efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.189.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.189/>Extracting Chemical-Protein Interactions via Calibrated Deep Neural Network and Self-training</a></strong><br><a href=/people/d/dongha-choi/>Dongha Choi</a>
|
<a href=/people/h/hyunju-lee/>Hyunju Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--189><div class="card-body p-3 small">The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical research</a> such as <a href=https://en.wikipedia.org/wiki/Drug_development>drug development</a> and prediction of drug side effects. Several <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing methods</a>, including deep neural network (DNN) models, have been applied to address this problem. However, these methods were trained with hard-labeled data, which tend to become over-confident, leading to degradation of the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>model reliability</a>. To estimate the <a href=https://en.wikipedia.org/wiki/Uncertainty>data uncertainty</a> and improve the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a>, <a href=https://en.wikipedia.org/wiki/Calibration>calibration techniques</a> have been applied to <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. In this study, to extract chemicalprotein interactions, we propose a DNN-based approach incorporating uncertainty information and calibration techniques. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> first encodes the input sequence using a pre-trained language-understanding model, following which it is trained using two calibration methods : mixup training and addition of a <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence penalty loss</a>. Finally, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is re-trained with <a href=https://en.wikipedia.org/wiki/Augmented_reality>augmented data</a> that are extracted using the estimated uncertainties. Our approach has achieved state-of-the-art performance with regard to the Biocreative VI ChemProt task, while preserving higher calibration abilities than those of previous approaches. Furthermore, our approach also presents the possibilities of using uncertainty estimation for performance improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.191.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940723 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.191/>MedICaT : A Dataset of Medical Images, Captions, and Textual References<span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>IC</span>a<span class=acl-fixed-case>T</span>: A Dataset of Medical Images, Captions, and Textual References</a></strong><br><a href=/people/s/sanjay-subramanian/>Sanjay Subramanian</a>
|
<a href=/people/l/lucy-lu-wang/>Lucy Lu Wang</a>
|
<a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/s/sachin-mehta/>Sachin Mehta</a>
|
<a href=/people/m/madeleine-van-zuylen/>Madeleine van Zuylen</a>
|
<a href=/people/s/sravanthi-parasa/>Sravanthi Parasa</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--191><div class="card-body p-3 small">Understanding the relationship between figures and text is key to <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific document understanding</a>. Medical figures in particular are quite complex, often consisting of several subfigures (75 % of figures in our dataset), with detailed text describing their content. Previous work studying figures in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a> focused on classifying figure content rather than understanding how images relate to the text. To address challenges in figure retrieval and figure-to-text alignment, we introduce MedICaT, a dataset of <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical images</a> in context. MedICaT consists of 217 K images from 131 K open access biomedical papers, and includes captions, inline references for 74 % of figures, and manually annotated subfigures and subcaptions for a subset of figures. Using MedICaT, we introduce the task of subfigure to subcaption alignment in compound figures and demonstrate the utility of inline references in image-text matching. Our data and code can be accessed at https://github.com/allenai/medicat.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.198" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.198/>Zero-Shot Rationalization by Multi-Task Transfer Learning from Question Answering</a></strong><br><a href=/people/p/po-nien-kung/>Po-Nien Kung</a>
|
<a href=/people/t/tse-hsuan-yang/>Tse-Hsuan Yang</a>
|
<a href=/people/y/yi-cheng-chen/>Yi-Cheng Chen</a>
|
<a href=/people/s/sheng-siang-yin/>Sheng-Siang Yin</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--198><div class="card-body p-3 small">Extracting rationales can help human understand which information the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> utilizes and how it makes the prediction towards better <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. However, annotating rationales requires much effort and only few datasets contain such labeled rationales, making <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Rationalization_(sociology)>rationalization</a> difficult. In this paper, we propose a novel approach that leverages the benefits of both <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for generating rationales through <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> in a zero-shot fashion. For two benchmark rationalization datasets, the proposed method achieves comparable or even better performance of rationalization without any supervised signal, demonstrating the great potential of zero-shot rationalization for better <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.201.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.201/>Service-oriented Text-to-SQL Parsing<span class=acl-fixed-case>SQL</span> Parsing</a></strong><br><a href=/people/w/wangsu-hu/>Wangsu Hu</a>
|
<a href=/people/j/jilei-tian/>Jilei Tian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--201><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Relational_database>relational database</a> requires professionals who has an understanding of structural query language such as <a href=https://en.wikipedia.org/wiki/SQL>SQL</a>. TEXT2SQL models apply <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> to enable user interacting the database via <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language utterance</a>. Current TEXT2SQL models normally focus on generating complex SQL query in a precise and complete fashion while certain features of real-world application in the production environment is not fully addressed. This paper is aimed to develop a service-oriented Text-to-SQL parser that translates natural language utterance to structural and executable SQL query. We introduce a algorithmic framework named Semantic-Enriched SQL generator (SE-SQL) that enables flexibly access database than rigid API in the application while keeping the performance quality for the most commonly used cases. The qualitative result shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 88.3 % execution accuracy on WikiSQL task, outperforming <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 13 % error reduction. Moreover, the framework considers several service-oriented needs including low-complexity inference, out-of-table rejection, and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.203/>Reducing Quantity Hallucinations in Abstractive Summarization</a></strong><br><a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--203><div class="card-body p-3 small">It is well-known that abstractive summaries are subject to hallucinationincluding material that is not supported by the original text. While summaries can be made hallucination-free by limiting them to general phrases, such summaries would fail to be very informative. Alternatively, one can try to avoid <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> by verifying that any specific entities in the summary appear in the original text in a similar context. This is the approach taken by our <a href=https://en.wikipedia.org/wiki/System>system</a>, Herman. The <a href=https://en.wikipedia.org/wiki/System>system</a> learns to recognize and verify quantity entities (dates, numbers, sums of money, etc.) in a beam-worth of abstractive summaries produced by state-of-the-art models, in order to up-rank those summaries whose quantity terms are supported by the original text. Experimental results demonstrate that the ROUGE scores of such up-ranked summaries have a higher <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision</a> than summaries that have not been up-ranked, without a comparable loss in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, resulting in higher F1. Preliminary human evaluation of up-ranked vs. original summaries shows people&#8217;s preference for the former.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.205/>Sparse and Decorrelated Representations for Stable Zero-shot NMT<span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/b/bokyung-son/>Bokyung Son</a>
|
<a href=/people/s/sungwon-lyu/>Sungwon Lyu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--205><div class="card-body p-3 small">Using a single encoder and decoder for all directions and training with English-centric data is a popular scheme for multilingual NMT. However, zero-shot translation under this scheme is vulnerable to changes in training conditions, as the model degenerates by decoding non-English texts into English regardless of the target specifier token. We present that enforcing both sparsity and decorrelation on encoder intermediate representations with the SLNI regularizer (Aljundi et al., 2019) efficiently mitigates this problem, without performance loss in supervised directions. Notably, effects of SLNI turns out to be irrelevant to promoting language-invariance in encoder representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.207/>BERT-MK : Integrating Graph Contextualized Knowledge into Pre-trained Language Models<span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>MK</span>: Integrating Graph Contextualized Knowledge into Pre-trained Language Models</a></strong><br><a href=/people/b/bin-he/>Bin He</a>
|
<a href=/people/d/di-zhou/>Di Zhou</a>
|
<a href=/people/j/jinghui-xiao/>Jinghui Xiao</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/n/nicholas-jing-yuan/>Nicholas Jing Yuan</a>
|
<a href=/people/t/tong-xu/>Tong Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--207><div class="card-body p-3 small">Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraphs</a> in a medical KG. Then, the learned <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> is integrated with a pre-trained language model to do the <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge generalization</a>. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.208.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.208/>Recursive Top-Down Production for Sentence Generation with Latent Trees</a></strong><br><a href=/people/s/shawn-tan/>Shawn Tan</a>
|
<a href=/people/y/yikang-shen/>Yikang Shen</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a>
|
<a href=/people/t/timothy-odonnell/>Timothy J. O’Donnell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--208><div class="card-body p-3 small">We model the recursive production property of <a href=https://en.wikipedia.org/wiki/Context-free_grammar>context-free grammars</a> for natural and synthetic languages. To this end, we present a <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming algorithm</a> that marginalises over latent binary tree structures with N leaves, allowing us to compute the likelihood of a sequence of N tokens under a latent tree model, which we maximise to train a recursive neural function. We demonstrate performance on two synthetic tasks : <a href=https://en.wikipedia.org/wiki/Boolean_satisfiability_problem>SCAN</a>, where it outperforms previous models on the <a href=https://en.wikipedia.org/wiki/Boolean_satisfiability_problem>LENGTH split</a>, and English question formation, where it performs comparably to decoders with the ground-truth tree structure. We also present experimental results on German-English translation on the Multi30k dataset, and qualitatively analyse the induced tree structures our model learns for the SCAN tasks and the German-English translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.213/>Differentially Private Representation for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> : Formal Guarantee and An Empirical Study on Privacy and Fairness<span class=acl-fixed-case>NLP</span>: Formal Guarantee and An Empirical Study on Privacy and Fairness</a></strong><br><a href=/people/l/lingjuan-lyu/>Lingjuan Lyu</a>
|
<a href=/people/x/xuanli-he/>Xuanli He</a>
|
<a href=/people/y/yitong-li/>Yitong Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--213><div class="card-body p-3 small">It has been demonstrated that hidden representation learned by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep model</a> can encode private information of the input, hence can be exploited to recover such <a href=https://en.wikipedia.org/wiki/Information>information</a> with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide formal privacy guarantee. Further, we show that masking words via dropout can further enhance <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a>. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.217/>ProphetNet : Predicting Future N-gram for Sequence-to-SequencePre-training<span class=acl-fixed-case>P</span>rophet<span class=acl-fixed-case>N</span>et: Predicting Future N-gram for Sequence-to-<span class=acl-fixed-case>S</span>equence<span class=acl-fixed-case>P</span>re-training</a></strong><br><a href=/people/w/weizhen-qi/>Weizhen Qi</a>
|
<a href=/people/y/yu-yan/>Yu Yan</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/j/jiusheng-chen/>Jiusheng Chen</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--217><div class="card-body p-3 small">This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to plan for the future tokens and prevent <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> on strong local correlations. We pre-train ProphetNet using a base scale dataset (16 GB) and a large-scale dataset (160 GB), respectively. Then we conduct experiments on CNN / DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> compared to the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using the same scale pre-training corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.218/>DivGAN : Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network<span class=acl-fixed-case>D</span>iv<span class=acl-fixed-case>GAN</span>: Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network</a></strong><br><a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--218><div class="card-body p-3 small">Paraphrases refer to texts that convey the same meaning with different expression forms. Traditional seq2seq-based models on <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> mainly focus on the <a href=https://en.wikipedia.org/wiki/Fidelity>fidelity</a> while ignoring the diversity of outputs. In this paper, we propose a deep generative model to generate diverse paraphrases. We build our model based on the conditional generative adversarial network, and propose to incorporate a simple yet effective diversity loss term into the model in order to improve the diversity of outputs. The proposed diversity loss maximizes the ratio of pairwise distance between the generated texts and their corresponding <a href=https://en.wikipedia.org/wiki/Latent_variable>latent codes</a>, forcing the generator to focus more on the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent codes</a> and produce diverse samples. Experimental results on benchmarks of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more diverse paraphrases compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.225/>Improving Compositional Generalization in Semantic Parsing</a></strong><br><a href=/people/i/inbar-oren/>Inbar Oren</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/n/nitish-gupta/>Nitish Gupta</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--225><div class="card-body p-3 small">Generalization of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the <a href=https://en.wikipedia.org/wiki/Attention>attention module</a> of the <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>, aiming to improve compositional generalization. We find that the following factors improve compositional generalization : (a) using contextual representations, such as ELMo and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.226/>Answer Span Correction in Machine Reading Comprehension</a></strong><br><a href=/people/r/revanth-gangi-reddy/>Revanth Gangi Reddy</a>
|
<a href=/people/m/md-arafat-sultan/>Md Arafat Sultan</a>
|
<a href=/people/e/efsun-sarioglu-kayi/>Efsun Sarioglu Kayi</a>
|
<a href=/people/r/rong-zhang/>Rong Zhang</a>
|
<a href=/people/v/vittorio-castelli/>Vittorio Castelli</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--226><div class="card-body p-3 small">Answer validation in machine reading comprehension (MRC) consists of verifying an extracted answer against an input context and question pair. Previous work has looked at re-assessing the answerability of the question given the extracted answer. Here we address a different problem : the tendency of existing MRC systems to produce partially correct answers when presented with answerable questions. We explore the nature of such errors and propose a post-processing correction method that yields statistically significant performance improvements over state-of-the-art MRC systems in both monolingual and multilingual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.229/>How Does <a href=https://en.wikipedia.org/wiki/Context_(computing)>Context</a> Matter? On the Robustness of Event Detection with Context-Selective Mask Generalization</a></strong><br><a href=/people/j/jian-liu/>Jian Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a>
|
<a href=/people/z/zhicheng-sheng/>Zhicheng Sheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--229><div class="card-body p-3 small">Event detection (ED) aims to identify and classify event triggers in texts, which is a crucial subtask of event extraction (EE). Despite many advances in ED, the existing studies are typically centered on improving the overall performance of an ED model, which rarely consider the robustness of an ED model. This paper aims to fill this research gap by stressing the importance of robustness modeling in ED models. We first pinpoint three stark cases demonstrating the brittleness of the existing ED models. After analyzing the underlying reason, we propose a new training mechanism, called context-selective mask generalization for ED, which can effectively mine context-specific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>full context</a> for <a href=https://en.wikipedia.org/wiki/Feature_learning>feature learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.230/>Adaptive Feature Selection for End-to-End Speech Translation</a></strong><br><a href=/people/b/biao-zhang/>Biao Zhang</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--230><div class="card-body p-3 small">Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an <a href=https://en.wikipedia.org/wiki/Speech_recognition>ASR encoder</a> and apply AFS to dynamically estimate the importance of each encoded speech feature to <a href=https://en.wikipedia.org/wiki/Speech_recognition>ASR</a>. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take L0DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech EnFr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ~84 % temporal features, yielding an average translation gain of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a <a href=https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>BLEU score</a> of 18.56 (without data augmentation).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.231/>Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization</a></strong><br><a href=/people/h/hanqi-jin/>Hanqi Jin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--231><div class="card-body p-3 small">Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for single-document and multi-document summarizations by fully sharing the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and decoder and utilizing a decoding controller to aggregate the <a href=https://en.wikipedia.org/wiki/Code>decoder</a>&#8217;s outputs for multiple input documents. We evaluate our model on two multi-document summarization datasets : Multi-News and DUC-04. Experimental results show the efficacy of our approach, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can substantially outperform several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. We also verify the helpfulness of single-document summarization to abstractive multi-document summarization task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.234/>Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction</a></strong><br><a href=/people/z/zhen-wu/>Zhen Wu</a>
|
<a href=/people/c/chengcan-ying/>Chengcan Ying</a>
|
<a href=/people/f/fei-zhao/>Fei Zhao</a>
|
<a href=/people/z/zhifang-fan/>Zhifang Fan</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--234><div class="card-body p-3 small">Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting aspect terms and opinion terms from review in the form of opinion pairs or additionally extracting sentiment polarity of aspect term to form opinion triplet. Because of containing several opinion factors, the complete AFOE task is usually divided into multiple subtasks and achieved in the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>. However, pipeline approaches easily suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> and inconvenience in real-world scenarios. To this end, we propose a novel tagging scheme, Grid Tagging Scheme (GTS), to address the AFOE task in an end-to-end fashion only with one unified grid tagging task. Additionally, we design an effective inference strategy on GTS to exploit mutual indication between different opinion factors for more accurate extractions. To validate the feasibility and compatibility of GTS, we implement three different GTS models respectively based on CNN, BiLSTM, and BERT, and conduct experiments on the aspect-oriented opinion pair extraction and opinion triplet extraction datasets. Extensive experimental results indicate that GTS models outperform strong baselines significantly and achieve state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.235" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.235/>Learning Numeral Embedding</a></strong><br><a href=/people/c/chengyue-jiang/>Chengyue Jiang</a>
|
<a href=/people/z/zhonglin-nian/>Zhonglin Nian</a>
|
<a href=/people/k/kaihao-guo/>Kaihao Guo</a>
|
<a href=/people/s/shanbo-chu/>Shanbo Chu</a>
|
<a href=/people/y/yinggong-zhao/>Yinggong Zhao</a>
|
<a href=/people/l/libin-shen/>Libin Shen</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--235><div class="card-body p-3 small">Word embedding is an essential building block for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Although <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> has been extensively studied over the years, the problem of how to effectively embed <a href=https://en.wikipedia.org/wiki/Numeral_system>numerals</a>, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for <a href=https://en.wikipedia.org/wiki/Numerical_digit>numerals</a>. We first induce a finite set of prototype numerals using either a <a href=https://en.wikipedia.org/wiki/Self-organizing_map>self-organizing map</a> or a <a href=https://en.wikipedia.org/wiki/Mixture_model>Gaussian mixture model</a>. We then represent the embedding of a <a href=https://en.wikipedia.org/wiki/Numeral_system>numeral</a> as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks : word similarity, embedding numeracy, numeral prediction, and <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.237.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.237/>Fast End-to-end Coreference Resolution for Korean<span class=acl-fixed-case>K</span>orean</a></strong><br><a href=/people/c/cheoneum-park/>Cheoneum Park</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/s/sungjoon-park/>Sungjoon Park</a>
|
<a href=/people/j/joonho-lim/>Joonho Lim</a>
|
<a href=/people/c/changki-lee/>Changki Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--237><div class="card-body p-3 small">Recently, end-to-end neural network-based approaches have shown significant improvements over traditional pipeline-based models in English coreference resolution. However, such advancements came at a cost of <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> and recent works have not focused on tackling this problem. Hence, in this paper, to cope with this issue, we propose BERT-SRU-based Pointer Networks that leverages the linguistic property of head-final languages. Applying this model to the Korean coreference resolution, we significantly reduce the coreference linking search space. Combining this with Ensemble Knowledge Distillation, we maintain state-of-the-art performance 66.9 % of CoNLL F1 on ETRI test set while achieving 2x speedup (30 doc / sec) in document processing time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940065 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.238/>Toward Stance-based Personas for Opinionated Dialogues</a></strong><br><a href=/people/t/thomas-scialom/>Thomas Scialom</a>
|
<a href=/people/s/serra-sinem-tekiroglu/>Serra Sinem Tekiroğlu</a>
|
<a href=/people/j/jacopo-staiano/>Jacopo Staiano</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--238><div class="card-body p-3 small">In the context of chit-chat dialogues it has been shown that endowing <a href=https://en.wikipedia.org/wiki/System>systems</a> with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such <a href=https://en.wikipedia.org/wiki/Persona>personas</a> has thus far been limited to a fact-based representation (e.g. I have two cats.). We argue that these <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> remain superficial w.r.t. the complexity of <a href=https://en.wikipedia.org/wiki/Personality_psychology>human personality</a>. In this work, we propose to make a step forward and investigate stance-based persona, trying to grasp more profound characteristics, such as opinions, <a href=https://en.wikipedia.org/wiki/Value_(ethics)>values</a>, and beliefs to drive <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a>. To this end, we introduce a novel dataset allowing to explore different stance-based persona representations and their impact on claim generation, showing that they are able to grasp abstract and profound aspects of the author persona.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.239.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.239/>Hierarchical Pre-training for Sequence Labelling in Spoken Dialog</a></strong><br><a href=/people/e/emile-chapuis/>Emile Chapuis</a>
|
<a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/m/matteo-manica/>Matteo Manica</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--239><div class="card-body p-3 small">Sequence labelling tasks like Dialog Act and Emotion / Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> we call Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE benchmark (SILICONE). SILICONE is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pre-training objectives. Pre-training is performed on OpenSubtitles : a large corpus of <a href=https://en.wikipedia.org/wiki/Dialogue>spoken dialog</a> containing over 2.3 billion of tokens. We demonstrate how hierarchical encoders achieve competitive results with consistently fewer parameters compared to state-of-the-art models and we show their importance for both pre-training and fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940171 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.241/>Out-of-Sample Representation Learning for Knowledge Graphs</a></strong><br><a href=/people/m/marjan-albooyeh/>Marjan Albooyeh</a>
|
<a href=/people/r/rishab-goel/>Rishab Goel</a>
|
<a href=/people/s/seyed-mehran-kazemi/>Seyed Mehran Kazemi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--241><div class="card-body p-3 small">Many important problems can be formulated as reasoning in <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. Representation learning has proved extremely effective for <a href=https://en.wikipedia.org/wiki/Transductive_reasoning>transductive reasoning</a>, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers <a href=https://en.wikipedia.org/wiki/Attributed_graph>attributed graph</a>. However, this <a href=https://en.wikipedia.org/wiki/Graph_theory>problem</a> is surprisingly under-explored for <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>non-attributed graphs</a>. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and baselines, and provide empirical analyses and comparisons of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.242" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.242/>Fine-Grained Grounding for Multimodal Speech Recognition</a></strong><br><a href=/people/t/tejas-srinivasan/>Tejas Srinivasan</a>
|
<a href=/people/r/ramon-sanabria/>Ramon Sanabria</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--242><div class="card-body p-3 small">Multimodal automatic speech recognition systems integrate information from images to improve <a href=https://en.wikipedia.org/wiki/Speech>speech recognition quality</a>, by grounding the <a href=https://en.wikipedia.org/wiki/Speech>speech</a> in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the <a href=https://en.wikipedia.org/wiki/Sound>audio</a>, these models should be capable of recovering a broader range of word types. Existing systems rely on global visual features that represent the entire image, but localizing the relevant regions of the image will make it possible to recover a larger set of words, such as <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that uses finer-grained visual information from different parts of the <a href=https://en.wikipedia.org/wiki/Image>image</a>, using automatic object proposals. In experiments on the Flickr8 K Audio Captions Corpus, we find that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> improves over approaches that use global visual features, that the proposals enable the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to recover entities and other related words, such as adjectives, and that improvements are due to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s ability to localize the correct proposals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.243.OptionalSupplementaryMaterial.txt data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.243/>Unsupervised Expressive Rules Provide Explainability and Assist Human Experts Grasping New Domains</a></strong><br><a href=/people/e/eyal-shnarch/>Eyal Shnarch</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/g/guy-moshkowich/>Guy Moshkowich</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--243><div class="card-body p-3 small">Approaching new data can be quite deterrent ; you do not know how your categories of interest are realized in it, commonly, there is no labeled data at hand, and the performance of domain adaptation methods is unsatisfactory. Aiming to assist domain experts in their first steps into a new task over a new corpus, we present an unsupervised approach to reveal complex rules which cluster the unexplored corpus by its prominent categories (or facets). These <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> are human-readable, thus providing an important ingredient which has become in short supply lately-explainability. Each <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rule</a> provides an explanation for the commonality of all the texts it clusters together. The experts can then identify which <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> best capture texts of their categories of interest, and utilize them to deepen their understanding of these categories. These rules can also bootstrap the process of data labeling by pointing at a subset of the corpus which is enriched with texts demonstrating the target categories. We present an extensive evaluation of the usefulness of these rules in identifying target categories, as well as a user study which assesses their interpretability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.254.OptionalSupplementaryMaterial.txt data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.254" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.254/>Corpora Evaluation and System Bias Detection in Multi-document Summarization</a></strong><br><a href=/people/a/alvin-dey/>Alvin Dey</a>
|
<a href=/people/t/tanya-chowdhury/>Tanya Chowdhury</a>
|
<a href=/people/y/yash-kumar/>Yash Kumar</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--254><div class="card-body p-3 small">Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, <a href=https://en.wikipedia.org/wiki/Review>product reviews</a>, etc. from various sources. Owing to no standard definition of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes <a href=https://en.wikipedia.org/wiki/Summary_(law)>summary information</a> in <a href=https://en.wikipedia.org/wiki/Summary_(law)>MDS</a>. Adding to the challenge is the fact that new systems report results on a set of chosen datasets, which might not correlate with their performance on the other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In this paper, we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next, we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>system metrics</a> are influenced, and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/LCS2-IIITD/summarization_bias.git</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.255.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.255" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.255/>Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem</a></strong><br><a href=/people/s/shucheng-li/>Shucheng Li</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/s/shiwei-feng/>Shiwei Feng</a>
|
<a href=/people/f/fangli-xu/>Fangli Xu</a>
|
<a href=/people/f/fengyuan-xu/>Fengyuan Xu</a>
|
<a href=/people/s/sheng-zhong/>Sheng Zhong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--255><div class="card-body p-3 small">The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, and math word problem solving. However, these models either only consider input objects as <a href=https://en.wikipedia.org/wiki/Sequence>sequences</a> while ignoring the important structural information for <a href=https://en.wikipedia.org/wiki/Code>encoding</a>, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.256/>Target Conditioning for One-to-Many Generation</a></strong><br><a href=/people/m/marie-anne-lachaux/>Marie-Anne Lachaux</a>
|
<a href=/people/a/armand-joulin/>Armand Joulin</a>
|
<a href=/people/g/guillaume-lample/>Guillaume Lample</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--256><div class="card-body p-3 small">Neural Machine Translation (NMT) models often lack diversity in their generated translations, even when paired with <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a>, like <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. A challenge is that the diversity in translations are caused by the variability in the target language, and can not be inferred from the source sentence alone. In this paper, we propose to explicitly model this one-to-many mapping by conditioning the decoder of a NMT model on a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> that represents the domain of target sentences. The domain is a <a href=https://en.wikipedia.org/wiki/Discrete_and_continuous_variables>discrete variable</a> generated by a target encoder that is jointly trained with the NMT model. The predicted domain of target sentences are given as input to the decoder during training. At <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, we can generate diverse translations by decoding with different domains. Unlike our strongest baseline (Shen et al., 2019), our method can scale to any number of domains without affecting the performance or the training time. We assess the quality and diversity of translations generated by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with several <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, on three different datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.258/>FENAS : Flexible and Expressive Neural Architecture Search<span class=acl-fixed-case>FENAS</span>: Flexible and Expressive Neural Architecture Search</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--258><div class="card-body p-3 small">Architecture search is the automatic process of designing the model or cell structure that is optimal for the given dataset or task. Recently, this approach has shown good improvements in terms of performance (tested on language modeling and image classification) with reasonable training speed using a weight sharing-based approach called Efficient Neural Architecture Search (ENAS). In this work, we propose a novel architecture search algorithm called Flexible and Expressible Neural Architecture Search (FENAS), with more flexible and expressible search space than ENAS, in terms of more activation functions, input edges, and atomic operations. Also, our FENAS approach is able to reproduce the well-known LSTM and GRU architectures (unlike ENAS), and is also able to initialize with them for finding <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> more efficiently. We explore this extended search space via evolutionary search and show that FENAS performs significantly better on several popular text classification tasks and performs similar to <a href=https://en.wikipedia.org/wiki/ENAS>ENAS</a> on standard language model benchmark. Further, we present ablations and analyses on our FENAS approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.259" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.259/>Inferring symmetry in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a></a></strong><br><a href=/people/c/chelsea-tanchip/>Chelsea Tanchip</a>
|
<a href=/people/l/lei-yu/>Lei Yu</a>
|
<a href=/people/a/aotao-xu/>Aotao Xu</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--259><div class="card-body p-3 small">We present a methodological framework for inferring symmetry of verb predicates in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Empirical work on predicate symmetry has taken two main approaches. The feature-based approach focuses on <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> pertaining to <a href=https://en.wikipedia.org/wiki/Symmetry>symmetry</a>. The context-based approach denies the existence of absolute symmetry but instead argues that such <a href=https://en.wikipedia.org/wiki/Inference>inference</a> is context dependent. We develop methods that formalize these approaches and evaluate them against a novel symmetry inference sentence (SIS) dataset comprised of 400 naturalistic usages of literature-informed verbs spanning the spectrum of symmetry-asymmetry. Our results show that a hybrid transfer learning model that integrates <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> with contextualized language models most faithfully predicts the <a href=https://en.wikipedia.org/wiki/Empirical_evidence>empirical data</a>. Our work integrates existing approaches to symmetry in natural language and suggests how symmetry inference can improve systematicity in state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--260 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.260" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.260/>A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder<span class=acl-fixed-case>C</span>hinese Word Segmentation with Transformer Encoder</a></strong><br><a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/h/hengzhi-pei/>Hengzhi Pei</a>
|
<a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--260><div class="card-body p-3 small">Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the relations among the multiple heterogeneous segmentation criteria and further improve the performance of each single criterion. Previous work usually regards MCCWS as different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, which are learned together under the multi-task learning framework. In this paper, we propose a concise but effective unified model for MCCWS, which is fully-shared for all the criteria. By leveraging the powerful ability of the Transformer encoder, the proposed unified model can segment <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese text</a> according to a unique criterion-token indicating the output criterion. Besides, the proposed <a href=https://en.wikipedia.org/wiki/Unified_Model>unified model</a> can segment both <a href=https://en.wikipedia.org/wiki/Simplified_Chinese_characters>simplified and traditional Chinese</a> and has an excellent transfer capability. Experiments on eight datasets with different criteria show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.262.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.262/>Enhancing Content Planning for Table-to-Text Generation with Data Understanding and Verification</a></strong><br><a href=/people/h/heng-gong/>Heng Gong</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--262><div class="card-body p-3 small">Neural table-to-text models, which select and order salient data, as well as verbalizing them fluently via surface realization, have achieved promising progress. Based on results from previous work, the performance bottleneck of current models lies in the stage of content planing (selecting and ordering salient content from the input). That is, performance drops drastically when an <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle content plan</a> is replaced by a model-inferred one during surface realization. In this paper, we propose to enhance neural content planning by (1) understanding data values with contextual numerical value representations that bring the sense of value comparison into content planning ; (2) verifying the importance and ordering of the selected sequence of records with policy gradient. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on ROTOWIRE and MLB, two datasets on this task, and results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing systems with respect to content planning metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940104 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.264/>DiPair : Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling<span class=acl-fixed-case>D</span>i<span class=acl-fixed-case>P</span>air: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling</a></strong><br><a href=/people/j/jiecao-chen/>Jiecao Chen</a>
|
<a href=/people/l/liu-yang/>Liu Yang</a>
|
<a href=/people/k/karthik-raman/>Karthik Raman</a>
|
<a href=/people/m/michael-bendersky/>Michael Bendersky</a>
|
<a href=/people/j/jung-jung-yeh/>Jung-Jung Yeh</a>
|
<a href=/people/y/yun-zhou/>Yun Zhou</a>
|
<a href=/people/m/marc-najork/>Marc Najork</a>
|
<a href=/people/d/danyang-cai/>Danyang Cai</a>
|
<a href=/people/e/ehsan-emadzadeh/>Ehsan Emadzadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--264><div class="card-body p-3 small">Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. However, deploying these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However as we show here existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>they</a> are either not scalable or demonstrate subpar performance. In this work, we propose DiPair a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940651 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.270/>An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text<span class=acl-fixed-case>A</span>n <span class=acl-fixed-case>I</span>nstance <span class=acl-fixed-case>L</span>evel <span class=acl-fixed-case>A</span>pproach for <span class=acl-fixed-case>S</span>hallow <span class=acl-fixed-case>S</span>emantic <span class=acl-fixed-case>P</span>arsing in <span class=acl-fixed-case>S</span>cientific <span class=acl-fixed-case>P</span>rocedural <span class=acl-fixed-case>T</span>ext</a></strong><br><a href=/people/d/daivik-swarup/>Daivik Swarup</a>
|
<a href=/people/a/ahsaas-bajaj/>Ahsaas Bajaj</a>
|
<a href=/people/s/sheshera-mysore/>Sheshera Mysore</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--270><div class="card-body p-3 small">In specific domains, such as procedural scientific text, human labeled data for <a href=https://en.wikipedia.org/wiki/Shallow_semantic_parsing>shallow semantic parsing</a> is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus, through an appropriately rich similarity metric. In light of this opportunity, this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing, in which semantic labels from structurally similar sentences in the training set are copied to test sentences. Candidate similar sentences are retrieved using SciBERT embeddings. For labels where it is possible to copy from a similar sentence we employ an instance level copy network, when this is not possible, a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--271 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940109 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.271/>General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference</a></strong><br><a href=/people/j/jingfei-du/>Jingfei Du</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/x/xing-zhou/>Xing Zhou</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--271><div class="card-body p-3 small">The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> can be amortized over the different predictions (tasks) using a shared text encoder. We compare approaches for training such an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and show that encoders pre-trained over multiple tasks generalize well to unseen tasks. We also compare ways of extracting fixed- and limited-size representations from this <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, including pooling features extracted from multiple layers or positions. Our best approach compares favorably to knowledge distillation, achieving higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and lower <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> once the <a href=https://en.wikipedia.org/wiki/System>system</a> is handling around 7 tasks. Further, we show that through binary quantization, we can reduce the size of the extracted representations by a factor of 16 to store them for later use. The resulting method offers a compelling solution for using large-scale pre-trained models at a fraction of the <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> when multiple tasks are performed on the same text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--277 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.277/>Knowing What You Know : Calibrating Dialogue Belief State Distributions via Ensembles</a></strong><br><a href=/people/c/carel-van-niekerk/>Carel van Niekerk</a>
|
<a href=/people/m/michael-heck/>Michael Heck</a>
|
<a href=/people/c/christian-geishauser/>Christian Geishauser</a>
|
<a href=/people/h/hsien-chin-lin/>Hsien-chin Lin</a>
|
<a href=/people/n/nurul-lubis/>Nurul Lubis</a>
|
<a href=/people/m/marco-moresi/>Marco Moresi</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--277><div class="card-body p-3 small">The ability to accurately track what happens during a conversation is essential for the performance of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Current state-of-the-art multi-domain dialogue state trackers achieve just over 55 % accuracy on the current go-to benchmark, which means that in almost every second dialogue turn they place full confidence in an incorrect dialogue state. Belief trackers, on the other hand, maintain a distribution over possible dialogue states. However, they lack in performance compared to dialogue state trackers, and do not produce well calibrated distributions. In this work we present state-of-the-art performance in <a href=https://en.wikipedia.org/wiki/Calibration>calibration</a> for multi-domain dialogue belief trackers using a calibrated ensemble of models. Our resulting dialogue belief tracker also outperforms previous dialogue belief tracking models in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.281/>Towards Domain-Independent Text Structuring Trainable on Large Discourse Treebanks</a></strong><br><a href=/people/g/grigorii-guz/>Grigorii Guz</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--281><div class="card-body p-3 small">Text structuring is a fundamental step in NLG, especially when generating multi-sentential text. With the goal of fostering more general and data-driven approaches to text structuring, we propose the new and domain-independent NLG task of structuring and ordering a (possibly large) set of EDUs. We then present a solution for this task that combines neural dependency tree induction with pointer networks, and can be trained on large discourse treebanks that have only recently become available. Further, we propose a new evaluation metric that is arguably more suitable for our new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> compared to existing content ordering metrics. Finally, we empirically show that our approach outperforms competitive alternatives on the proposed measure and is equivalent in performance with respect to previously established measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.283/>A Multilingual View of Unsupervised Machine Translation</a></strong><br><a href=/people/x/xavier-garcia/>Xavier Garcia</a>
|
<a href=/people/p/pierre-foret/>Pierre Foret</a>
|
<a href=/people/t/thibault-sellam/>Thibault Sellam</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--283><div class="card-body p-3 small">We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a> on the WMT&#8217;14 English-French, WMT&#8217;16 English-German, and WMT&#8217;16 English-Romanian datasets in most directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940035 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.287" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.287/>KoBE : Knowledge-Based Machine Translation Evaluation<span class=acl-fixed-case>K</span>o<span class=acl-fixed-case>BE</span>: Knowledge-Based Machine Translation Evaluation</a></strong><br><a href=/people/z/zorik-gekhman/>Zorik Gekhman</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/g/genady-beryozkin/>Genady Beryozkin</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/w/wolfgang-macherey/>Wolfgang Macherey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--287><div class="card-body p-3 small">We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. To foster further research, we release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--290 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.290" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.290/>Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer</a></strong><br><a href=/people/x/xuelu-chen/>Xuelu Chen</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/c/changjun-fan/>Changjun Fan</a>
|
<a href=/people/a/ankith-uppunda/>Ankith Uppunda</a>
|
<a href=/people/y/yizhou-sun/>Yizhou Sun</a>
|
<a href=/people/c/carlo-zaniolo/>Carlo Zaniolo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--290><div class="card-body p-3 small">Predicting missing facts in a knowledge graph(KG) is a crucial task in knowledge base construction and reasoning, and it has been the subject of much research in recent works us-ing KG embeddings. While existing KG embedding approaches mainly learn and predict facts within a single KG, a more plausible solution would benefit from the knowledge in multiple language-specific KGs, considering that different KGs have their own strengths and limitations on <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> and coverage. This is quite challenging since the transfer of knowledge among multiple independently maintained KGs is often hindered by the insufficiency of alignment information and inconsistency of described facts. In this paper, we propose kens, a novel framework for embedding learning and ensemble knowledge transfer across a number of language-specific KGs. KEnS embeds all KGs in a shared embedding space, where the association of entities is captured based on self-learning. Then, KEnS performs ensemble inference to com-bine prediction results from multiple language-specific embeddings, for which multiple en-semble techniques are investigated. Experiments on the basis of five real-world language-specific KGs show that, by effectively identifying and leveraging complementary knowledge, KEnS consistently improves state-of-the-art methods on KG completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--291 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.291" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.291/>Towards Controllable Biases in Language Generation<span class=acl-fixed-case>C</span>ontrollable <span class=acl-fixed-case>B</span>iases in <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>G</span>eneration</a></strong><br><a href=/people/e/emily-sheng/>Emily Sheng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/p/prem-natarajan/>Prem Natarajan</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--291><div class="card-body p-3 small">We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups. We then analyze two scenarios : 1) inducing negative biases for one demographic and positive biases for another demographic, and 2) equalizing biases between demographics. The former <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a> enables us to detect the types of <a href=https://en.wikipedia.org/wiki/Bias>biases</a> present in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Specifically, we show the effectiveness of our approach at facilitating bias analysis by finding topics that correspond to demographic inequalities in generated text and comparing the relative effectiveness of inducing biases for different demographics. The second <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a> is useful for mitigating biases in downstream applications such as dialogue generation. In our experiments, the mitigation technique proves to be effective at equalizing the amount of biases across demographics while simultaneously generating less negatively biased text overall.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--294 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.294.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940653 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.294" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.294/>Graph-to-Graph Transformer for Transition-based Dependency Parsing</a></strong><br><a href=/people/a/alireza-mohammadshahi/>Alireza Mohammadshahi</a>
|
<a href=/people/j/james-henderson/>James Henderson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--294><div class="card-body p-3 small">We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dependency parsing as strong baselines, we show that adding the proposed mechanisms for conditioning on and predicting graphs of Graph2Graph Transformer results in significant improvements, both with and without BERT pre-training. The novel baselines and their integration with Graph2Graph Transformer significantly outperform the state-of-the-art in traditional transition-based dependency parsing on both English Penn Treebank, and 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer can be integrated with many previous structured prediction methods, making it easy to apply to a wide range of NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.297/>A Novel Challenge Set for Hebrew Morphological Disambiguation and Diacritics Restoration<span class=acl-fixed-case>H</span>ebrew Morphological Disambiguation and Diacritics Restoration</a></strong><br><a href=/people/a/avi-shmidman/>Avi Shmidman</a>
|
<a href=/people/j/joshua-guedalia/>Joshua Guedalia</a>
|
<a href=/people/s/shaltiel-shmidman/>Shaltiel Shmidman</a>
|
<a href=/people/m/moshe-koppel/>Moshe Koppel</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--297><div class="card-body p-3 small">One of the primary tasks of <a href=https://en.wikipedia.org/wiki/Parsing>morphological parsers</a> is the disambiguation of homographs. Particularly difficult are cases of unbalanced ambiguity, where one of the possible analyses is far more frequent than the others. In such cases, there may not exist sufficient examples of the minority analyses in order to properly evaluate performance, nor to train effective <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. In this paper we address the issue of unbalanced morphological ambiguities in <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>. We offer a challenge set for Hebrew homographs the first of its kind containing substantial attestation of each analysis of 21 Hebrew homographs. We show that the current SOTA of Hebrew disambiguation performs poorly on cases of unbalanced ambiguity. Leveraging our new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we achieve a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for all 21 words, improving the overall average F1 score from 0.67 to 0.95. Our resulting annotated datasets are made publicly available for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.299/>A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge Graph</a></strong><br><a href=/people/l/lin-qiao/>Lin Qiao</a>
|
<a href=/people/j/jianhao-yan/>Jianhao Yan</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/z/zhendong-yang/>Zhendong Yang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--299><div class="card-body p-3 small">Generating a vivid, novel, and diverse essay with only several given topic words is a promising task of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Previous work in this task exists two challenging problems : neglect of sentiment beneath the text and insufficient utilization of topic-related knowledge. Therefore, we propose a novel Sentiment Controllable topic-to- essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational auto-encoder (CVAE) framework. We firstly inject the sentiment information into the generator for controlling sentiment for each sentence, which leads to various generated essays. Then we design a Topic Knowledge Graph enhanced decoder. Unlike existing models that use knowledge entities separately, our model treats knowledge graph as a whole and encodes more structured, connected semantic information in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> to generate a more relevant essay. Experimental results show that our SCTKG can generate sentiment controllable essays and outperform the state-of-the-art approach in terms of topic relevance, fluency, and diversity on both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.301/>RealToxicityPrompts : Evaluating Neural Toxic Degeneration in Language Models<span class=acl-fixed-case>R</span>eal<span class=acl-fixed-case>T</span>oxicity<span class=acl-fixed-case>P</span>rompts: Evaluating Neural Toxic Degeneration in Language Models</a></strong><br><a href=/people/s/samuel-gehman/>Samuel Gehman</a>
|
<a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--301><div class="card-body p-3 small">Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100 K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from <a href=https://en.wikipedia.org/wiki/Toxicity_(disambiguation)>toxicity</a> than simpler solutions (e.g., banning bad words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2 ; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.305/>On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks<span class=acl-fixed-case>O</span>n the <span class=acl-fixed-case>I</span>mportance of <span class=acl-fixed-case>A</span>daptive <span class=acl-fixed-case>D</span>ata <span class=acl-fixed-case>C</span>ollection for <span class=acl-fixed-case>E</span>xtremely <span class=acl-fixed-case>I</span>mbalanced <span class=acl-fixed-case>P</span>airwise <span class=acl-fixed-case>T</span>asks</a></strong><br><a href=/people/s/stephen-mussmann/>Stephen Mussmann</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--305><div class="card-body p-3 small">Many pairwise classification tasks, such as <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> and <a href=https://en.wikipedia.org/wiki/Open-domain_question_answering>open-domain question answering</a>, naturally have extreme label imbalance (e.g., 99.99 % of examples are negatives). In contrast, many recent <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> heuristically choose examples to ensure label balance. We show that these heuristics lead to trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that generalize poorly : State-of-the art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on <a href=https://en.wikipedia.org/wiki/QQP>QQP</a> and WikiQA each have only 2.4 % average precision when evaluated on realistically imbalanced test data. We instead collect training data with <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>, using a BERT-based embedding model to efficiently retrieve uncertain points from a very large pool of unlabeled utterance pairs. By creating balanced training data with more informative negative examples, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> greatly improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>average precision</a> to 32.5 % on <a href=https://en.wikipedia.org/wiki/QQP>QQP</a> and 20.1 % on WikiQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--312 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.312.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.312/>A Semantics-based Approach to Disclosure Classification in <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Online Content</a></a></strong><br><a href=/people/c/chandan-akiti/>Chandan Akiti</a>
|
<a href=/people/a/anna-squicciarini/>Anna Squicciarini</a>
|
<a href=/people/s/sarah-rajtmajer/>Sarah Rajtmajer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--312><div class="card-body p-3 small">As users engage in <a href=https://en.wikipedia.org/wiki/Public_sphere>public discourse</a>, the rate of voluntarily disclosed personal information has seen a steep increase. So-called <a href=https://en.wikipedia.org/wiki/Self-disclosure>self-disclosure</a> can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal information they share across online forums, commentaries, and <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>, as well as the power of modern AI to synthesize and gain insights from this <a href=https://en.wikipedia.org/wiki/Data>data</a>. This paper presents an approach to detect emotional and informational self-disclosure in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We hypothesize that identifying <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>frame semantics</a> can meaningfully support this task. Specifically, we use <a href=https://en.wikipedia.org/wiki/Semantic_Role_Labeling>Semantic Role Labeling</a> to identify the lexical units and their semantic roles that signal <a href=https://en.wikipedia.org/wiki/Self-disclosure>self-disclosure</a>. Experimental results on Reddit data show the performance gain of our method when compared to standard text classification methods based on BiLSTM, and BERT. In addition to improved performance, our approach provides insights into the drivers of disclosure behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--313 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.313.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.313/>Mining Knowledge for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a> from Wikipedia Categories<span class=acl-fixed-case>W</span>ikipedia Categories</a></strong><br><a href=/people/m/mingda-chen/>Mingda Chen</a>
|
<a href=/people/z/zewei-chu/>Zewei Chu</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--313><div class="card-body p-3 small">Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce WikiNLI : a resource for improving model performance on NLI and LE tasks. It contains 428,899 pairs of phrases constructed from naturally annotated category hierarchies in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We show that we can improve strong baselines such as BERT and RoBERTa by pretraining them on WikiNLI and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> to find that pretraining on WikiNLI gives the best performance. In addition, we construct WikiNLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.314/>OCNLI : Original Chinese Natural Language Inference<span class=acl-fixed-case>OCNLI</span>: <span class=acl-fixed-case>O</span>riginal <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>N</span>atural <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>I</span>nference</a></strong><br><a href=/people/h/hai-hu/>Hai Hu</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/l/liang-xu/>Liang Xu</a>
|
<a href=/people/l/lu-li/>Lu Li</a>
|
<a href=/people/s/sandra-kubler/>Sandra Kübler</a>
|
<a href=/people/l/lawrence-s-moss/>Lawrence Moss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--314><div class="card-body p-3 small">Despite the tremendous recent progress on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference (NLI)</a>, driven largely by large-scale investment in new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (e.g.,SNLI, MNLI) and advances in <a href=https://en.wikipedia.org/wiki/Mathematical_model>modeling</a>, most progress has been limited to <a href=https://en.wikipedia.org/wiki/English_language>English</a> due to a lack of reliable datasets for most of the world&#8217;s languages. In this paper, we present the first large-scale NLI dataset (consisting of ~56,000 annotated sentence pairs) for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> called the Original <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Natural Language Inference dataset (OCNLI)</a>. Unlike recent attempts at extending NLI to other languages, our dataset does not rely on any <a href=https://en.wikipedia.org/wiki/Automatic_translation>automatic translation</a> or non-expert annotation. Instead, we elicit <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from native speakers specializing in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>. We follow closely the annotation protocol used for MNLI, but create new strategies for eliciting diverse hypotheses. We establish several baseline results on our dataset using state-of-the-art pre-trained models for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and find even the best performing models to be far outpaced by human performance (~12 % absolute performance gap), making it a challenging new resource that we hope will help to accelerate progress in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese NLU</a>. To the best of our knowledge, this is the first human-elicited MNLI-style corpus for a non-English language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--315 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.315/>Margin-aware Unsupervised Domain Adaptation for Cross-lingual Text Labeling</a></strong><br><a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/f/feng-nan/>Feng Nan</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--315><div class="card-body p-3 small">Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in a target domain where labels are unavailable. In this paper, we improve upon a recent theoretical work (Zhang et al., 2019b) and adopt the Margin Disparity Discrepancy (MDD) unsupervised domain adaptation algorithm to solve the cross-lingual text labeling problems. Experiments on cross-lingual document classification and NER demonstrate the proposed domain adaptation approach advances the state-of-the-art results by a large margin. Specifically, we improve MDD by efficiently optimizing the margin loss on the source domain via Virtual Adversarial Training (VAT). This bridges the gap between theory and the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> used in the original work Zhang et al. (2019b), and thereby significantly boosts the performance. Our numerical results also indicate that VAT can remarkably improve the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> performance of both domains for various domain adaptation approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--323 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940174 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.323" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.323/>Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher</a></strong><br><a href=/people/g/giannis-karamanolakis/>Giannis Karamanolakis</a>
|
<a href=/people/d/daniel-hsu/>Daniel Hsu</a>
|
<a href=/people/l/luis-gravano/>Luis Gravano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--323><div class="card-body p-3 small">Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while less expensive cross-lingual representation learning approaches train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> without target labeled documents. In this work, we propose a cross-lingual teacher-student method, CLTS, that generates weak supervision in the target language using minimal cross-lingual resources, in the form of a small number of word translations. Given a limited translation budget, CLTS extracts and transfers only the most important task-specific seed words across languages and initializes a teacher classifier based on the translated seed words. Then, CLTS iteratively trains a more powerful student that also exploits the context of the seed words in unlabeled target documents and outperforms the teacher. CLTS is simple and surprisingly effective in 18 diverse languages : by transferring just 20 seed words, even a bag-of-words logistic regression student outperforms state-of-the-art cross-lingual methods (e.g., based on multilingual BERT). Moreover, CLTS can accommodate any type of student classifier : leveraging a monolingual BERT student leads to further improvements and outperforms even more expensive approaches by up to 12 % in accuracy. Finally, CLTS addresses emerging tasks in low-resource languages using just a small number of <a href=https://en.wikipedia.org/wiki/Translations>word translations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--328 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.328" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.328/>Using Visual Feature Space as a Pivot Across Languages</a></strong><br><a href=/people/z/ziyan-yang/>Ziyan Yang</a>
|
<a href=/people/l/leticia-pinto-alva/>Leticia Pinto-Alva</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--328><div class="card-body p-3 small">Our work aims to leverage visual feature space to pass information across languages. We show that models trained to generate textual captions in more than one language conditioned on an input image can leverage their jointly trained <a href=https://en.wikipedia.org/wiki/Feature_space>feature space</a> during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> to pivot across languages. We particularly demonstrate improved quality on a <a href=https://en.wikipedia.org/wiki/Closed_captioning>caption</a> generated from an input <a href=https://en.wikipedia.org/wiki/Image>image</a>, by leveraging a caption in a second language. More importantly, we demonstrate that even without conditioning on any <a href=https://en.wikipedia.org/wiki/Visual_system>visual input</a>, the model demonstrates to have learned implicitly to perform to some extent <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> from one language to another in their shared visual feature space. We show results in German-English, and Japanese-English language pairs that pave the way for using the visual world to learn a common representation for language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.332/>Document Classification for COVID-19 Literature<span class=acl-fixed-case>COVID</span>-19 Literature</a></strong><br><a href=/people/b/bernal-jimenez-gutierrez/>Bernal Jimenez Gutierrez</a>
|
<a href=/people/j/jucheng-zeng/>Jucheng Zeng</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/p/ping-zhang/>Ping Zhang</a>
|
<a href=/people/y/yu-su/>Yu Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--332><div class="card-body p-3 small">The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86 % and 75 % respectively on the test set. We evaluate the <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a> and generalizability of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> as essential features of any system prepared to deal with an urgent situation like the current <a href=https://en.wikipedia.org/wiki/Health_crisis>health crisis</a>. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles ; both of which are important issues to address in future work. Both data and code are available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--333 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940111 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.333/>Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--333><div class="card-body p-3 small">Reading comprehension models often overfit to nuances of training datasets and fail at <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial evaluation</a>. Training with adversarially augmented dataset improves <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> against those adversarial attacks but hurts <a href=https://en.wikipedia.org/wiki/Generalization>generalization of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a></a>. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.335.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940182 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.335/>Dr. Summarize : Global Summarization of Medical Dialogue by Exploiting Local Structures.</a></strong><br><a href=/people/a/anirudh-joshi/>Anirudh Joshi</a>
|
<a href=/people/n/namit-katariya/>Namit Katariya</a>
|
<a href=/people/x/xavier-amatriain/>Xavier Amatriain</a>
|
<a href=/people/a/anitha-kannan/>Anitha Kannan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--335><div class="card-body p-3 small">Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice : capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient&#8217;s medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80 % of the conversations making it a realistic alternative to costly manual summarization by medical experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--347 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940709 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.347/>Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations</a></strong><br><a href=/people/p/praveen-kumar-bodigutla/>Praveen Kumar Bodigutla</a>
|
<a href=/people/a/aditya-tiwari/>Aditya Tiwari</a>
|
<a href=/people/s/spyros-matsoukas/>Spyros Matsoukas</a>
|
<a href=/people/j/josep-valls-vargas/>Josep Valls-Vargas</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--347><div class="card-body p-3 small">Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which reduce the generalizability of the trained models. We propose a novel user satisfaction estimation approach which minimizes an adaptive multi-task loss function in order to jointly predict turn-level Response Quality labels provided by experts and explicit dialogue-level ratings provided by end users. The proposed BiLSTM based deep neural net model automatically weighs each turn&#8217;s contribution towards the estimated dialogue-level rating, implicitly encodes temporal dependencies, and removes the need to hand-craft features. On dialogues sampled from 28 <a href=https://en.wikipedia.org/wiki/Alexa_Internet>Alexa domains</a>, two dialogue systems and three user groups, the joint dialogue-level satisfaction estimation model achieved up to an absolute 27 % (0.43-0.70) and 7 % (0.63-0.70) improvement in <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>linear correlation</a> performance over baseline deep neural net and benchmark Gradient boosting regression models, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.349/>Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training</a></strong><br><a href=/people/r/renjie-zheng/>Renjie Zheng</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/b/baigong-zheng/>Baigong Zheng</a>
|
<a href=/people/k/kaibo-liu/>Kaibo Liu</a>
|
<a href=/people/j/jiahong-yuan/>Jiahong Yuan</a>
|
<a href=/people/k/kenneth-church/>Kenneth Church</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--349><div class="card-body p-3 small">Simultaneous speech-to-speech translation is an extremely challenging but widely useful scenario that aims to generate target-language speech only a few seconds behind the source-language speech. In addition, we have to continuously translate a speech of multiple sentences, but all recent solutions merely focus on the single-sentence scenario. As a result, current approaches will accumulate more and more latencies in later sentences when the speaker talks faster and introduce unnatural pauses into translated speech when the speaker talks slower. To overcome these issues, we propose Self-Adaptive Translation which flexibly adjusts the length of translations to accommodate different source speech rates. At similar levels of translation quality (as measured by BLEU), our method generates more fluent target speech latency than the baseline, in both Zh-En directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.351.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--351 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.351 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.351.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.351/>MCMH : Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning<span class=acl-fixed-case>MCMH</span>: Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning</a></strong><br><a href=/people/l/lu-zhang/>Lu Zhang</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/t/tian-gao/>Tian Gao</a>
|
<a href=/people/y/yue-yu/>Yue Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--351><div class="card-body p-3 small">Multi-hop reasoning approaches over <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation chains</a>. To learn such generalized rules efficiently, we propose a two-step approach that first selects a small set of relation chains as a rule and then evaluates the confidence of the target relationship by jointly scoring the selected chains. A game-theoretical framework is proposed to this end to simultaneously optimize the rule selection and prediction steps. Empirical results show that our multi-chain multi-hop (MCMH) rules result in superior results compared to the standard single-chain approaches, justifying both our formulation of generalized rules and the effectiveness of the proposed learning framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.352.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.352" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.352/>Finding the Optimal Vocabulary Size for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/t/thamme-gowda/>Thamme Gowda</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--352><div class="card-body p-3 small">We cast neural machine translation (NMT) as a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> in an autoregressive setting and analyze the limitations of both <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary sizes</a> on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> performance on multiple languages with many data sizes, and reveal an explanation for why certain <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary sizes</a> are better than others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--355 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.355/>Generalizable and Explainable Dialogue Generation via Explicit Action Learning</a></strong><br><a href=/people/x/xinting-huang/>Xinting Huang</a>
|
<a href=/people/j/jianzhong-qi/>Jianzhong Qi</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--355><div class="card-body p-3 small">Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time : task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two <a href=https://en.wikipedia.org/wiki/Goal>objectives</a>. Such an <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> relies on system action annotations which are expensive to obtain. To alleviate the need of action annotations, latent action learning is introduced to map each utterance to a latent representation. However, this approach is prone to over-dependence on the training data, and the generalization capability is thus restricted. To address this issue, we propose to learn <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language actions</a> that represent utterances as a span of words. This explicit action representation promotes <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> via the compositional structure of language. It also enables an explainable generation process. Our proposed <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset.<i>natural language actions</i> that represent utterances as a span of words. This explicit action representation promotes generalization via the compositional structure of language. It also enables an explainable generation process. Our proposed unsupervised approach learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.356/>More <a href=https://en.wikipedia.org/wiki/Embedding>Embeddings</a>, Better Sequence Labelers?</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--356><div class="card-body p-3 small">Recent work proposes a family of contextual embeddings that significantly improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> in various settings. In this paper, we conduct extensive experiments on 3 tasks over 18 datasets and 8 languages to study the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labeling with various embedding concatenations and make three observations : (1) concatenating more embedding variants leads to better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in rich-resource and cross-domain settings and some conditions of low-resource settings ; (2) concatenating contextual sub-word embeddings with contextual character embeddings hurts the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in extremely low-resource settings ; (3) based on the conclusion of (1), concatenating additional similar contextual embeddings can not lead to further improvements. We hope these conclusions can help people build stronger sequence labelers in various settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--363 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.363" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.363/>Temporal Reasoning in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a></a></strong><br><a href=/people/s/siddharth-vashishtha/>Siddharth Vashishtha</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/y/yash-kumar-lal/>Yash Kumar Lal</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--363><div class="card-body p-3 small">We introduce five new natural language inference (NLI) datasets focused on temporal reasoning. We recast four existing datasets annotated for event durationhow long an event lastsand event orderinghow events are temporally arrangedinto more than one million NLI examples. We use these datasets to investigate how well neural models trained on a popular NLI corpus capture these forms of temporal reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.366/>An Empirical Methodology for Detecting and Prioritizing Needs during Crisis Events</a></strong><br><a href=/people/m/m-janina-sarol/>M. Janina Sarol</a>
|
<a href=/people/l/ly-dinh/>Ly Dinh</a>
|
<a href=/people/r/rezvaneh-rezapour/>Rezvaneh Rezapour</a>
|
<a href=/people/c/chieh-li-chin/>Chieh-Li Chin</a>
|
<a href=/people/p/pingjing-yang/>Pingjing Yang</a>
|
<a href=/people/j/jana-diesner/>Jana Diesner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--366><div class="card-body p-3 small">In times of crisis, identifying essential needs is crucial to providing appropriate resources and services to affected entities. Social media platforms such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> contain a vast amount of information about the general public&#8217;s needs. However, the sparsity of information and the amount of noisy content present a challenge for practitioners to effectively identify relevant information on these <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a>. This study proposes two novel methods for two needs detection tasks : 1) extracting a list of needed resources, such as <a href=https://en.wikipedia.org/wiki/Mask>masks</a> and <a href=https://en.wikipedia.org/wiki/Ventilation_(architecture)>ventilators</a>, and 2) detecting sentences that specify who-needs-what resources (e.g., we need testing). We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on a set of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> about the COVID-19 crisis. For extracting a list of needs, we compare our results against two official lists of resources, achieving 0.64 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. For detecting who-needs-what sentences, we compared our results against a set of 1,000 <a href=https://en.wikipedia.org/wiki/Annotation>annotated tweets</a> and achieved a 0.68 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--367 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940131 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.367" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.367/>SupMMD : A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy<span class=acl-fixed-case>S</span>up<span class=acl-fixed-case>MMD</span>: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy</a></strong><br><a href=/people/u/umanga-bista/>Umanga Bista</a>
|
<a href=/people/a/alexander-mathews/>Alexander Mathews</a>
|
<a href=/people/a/aditya-menon/>Aditya Menon</a>
|
<a href=/people/l/lexing-xie/>Lexing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--367><div class="card-body p-3 small">Most work on <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salience</a> and <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a> for coverage and diversity. Further, we adapt <a href=https://en.wikipedia.org/wiki/Multiple_kernel_learning>multiple kernel learning</a> to make use of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> across multiple information sources (e.g., <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>text features</a> and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.372.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--372 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.372 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.372.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.372" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.372/>TinyBERT : Distilling BERT for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a><span class=acl-fixed-case>T</span>iny<span class=acl-fixed-case>BERT</span>: Distilling <span class=acl-fixed-case>BERT</span> for Natural Language Understanding</a></strong><br><a href=/people/x/xiaoqi-jiao/>Xiaoqi Jiao</a>
|
<a href=/people/y/yichun-yin/>Yichun Yin</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a>
|
<a href=/people/f/fang-wang/>Fang Wang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--372><div class="card-body p-3 small">Language model pre-training, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8 % the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28 % parameters and ~31 % inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--373 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940808 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.373" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.373/>Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder</a></strong><br><a href=/people/a/alvin-chan/>Alvin Chan</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/y/yew-soon-ong/>Yew-Soon Ong</a>
|
<a href=/people/a/aston-zhang/>Aston Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--373><div class="card-body p-3 small">This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a &#8216;backdoor poisoning&#8217; attack on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a>. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1 % poisoned data, our experiments show that a victim BERT finetuned classifier&#8217;s predictions can be steered to the poison target class with success rates of > 80 % when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.<tex-math>>80\\%</tex-math> when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.384.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.384/>How Can Self-Attention Networks Recognize Dyck-n Languages?<span class=acl-fixed-case>D</span>yck-n Languages?</a></strong><br><a href=/people/j/javid-ebrahimi/>Javid Ebrahimi</a>
|
<a href=/people/d/dhruv-gelda/>Dhruv Gelda</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--384><div class="card-body p-3 small">We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these <a href=https://en.wikipedia.org/wiki/Neural_network>networks</a>. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one without (SA-). Our results show that SA+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that <a href=https://en.wikipedia.org/wiki/S-number>SA-</a> completely breaks down on long sequences whereas the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/S-number>SA+</a> is 58.82 %. We find attention maps learned by SA+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchies</a> without <a href=https://en.wikipedia.org/wiki/Recursion>recursion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.385.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.385/>Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation</a></strong><br><a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--385><div class="card-body p-3 small">The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to train a flexible depth model that can adapt to different depth configurations during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training methodLayerDrop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.389" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.389/>What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models<span class=acl-fixed-case>BERT</span>’s layers? A closer look at the <span class=acl-fixed-case>NLP</span> pipeline in monolingual and multilingual models</a></strong><br><a href=/people/w/wietse-de-vries/>Wietse de Vries</a>
|
<a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--389><div class="card-body p-3 small">Peeking into the inner workings of BERT has shown that its <a href=https://en.wikipedia.org/wiki/Abstraction_layer>layers</a> resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper analysis of <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, we show that also within a given task, information is spread over different parts of the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> and the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--390 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.390" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.390/>Leakage-Adjusted Simulatability : Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?</a></strong><br><a href=/people/p/peter-hase/>Peter Hase</a>
|
<a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/h/harry-xie/>Harry Xie</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--390><div class="card-body p-3 small">Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>evaluating explanations</a> from the the model simulatability perspective. Our contributions are as follows : (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model&#8217;s output, while controlling for how <a href=https://en.wikipedia.org/wiki/Explanation>explanations</a> can directly leak the output. We use a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as a proxy for a <a href=https://en.wikipedia.org/wiki/Human_subject_research>human observer</a>, and validate this choice with two <a href=https://en.wikipedia.org/wiki/Human_subject_research>human subject experiments</a>. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches ; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.398.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.398" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.398/>Parsing All : Syntax and Semantics, Dependencies and Spans</a></strong><br><a href=/people/j/junru-zhou/>Junru Zhou</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--398><div class="card-body p-3 small">Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> help syntactic parsing. As linguistic representation formalisms, both <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and semantics may be represented in either span (constituent / phrase) or <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency</a>, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and benefits from two representation formalisms in a uniform way. The experiments show that <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--400 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.400.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.400/>Improving Limited Labeled Dialogue State Tracking with Self-Supervision</a></strong><br><a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--400><div class="card-body p-3 small">Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical <a href=https://en.wikipedia.org/wiki/Discrete-time_stochastic_process>DST problem</a> that is rarely discussed, i.e., learning efficiently with limited labeled data. We present and investigate two self-supervised objectives : preserving latent consistency and modeling conversational behavior. We encourage a DST model to have consistent latent distributions given a perturbed input, making it more robust to an unseen scenario. We also add an auxiliary utterance generation task, modeling a potential correlation between conversational behavior and dialogue states. The experimental results show that our proposed self-supervised signals can improve <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>joint goal accuracy</a> by 8.95 % when only 1 % labeled data is used on the MultiWOZ dataset. We can achieve an additional 1.76 % improvement if some unlabeled data is jointly trained as <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a>. We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--408 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.408.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940092 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.408" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.408/>Decoding Language Spatial Relations to 2D Spatial Arrangements<span class=acl-fixed-case>D</span>ecoding Language Spatial Relations to 2<span class=acl-fixed-case>D</span> Spatial Arrangements</a></strong><br><a href=/people/g/gorjan-radevski/>Gorjan Radevski</a>
|
<a href=/people/g/guillem-collell/>Guillem Collell</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/t/tinne-tuytelaars/>Tinne Tuytelaars</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--408><div class="card-body p-3 small">We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as arranging a scene of clip-arts given a textual description. We propose a simple and effective model architecture Spatial-Reasoning Bert (SR-Bert), trained to decode text to 2D spatial arrangements in a non-autoregressive manner. SR-Bert can decode both explicit and implicit language to 2D spatial arrangements, generalizes to out-of-sample data to a reasonable extent and can generate complete abstract scenes if paired with a clip-arts predictor. Finally, we qualitatively evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with a <a href=https://en.wikipedia.org/wiki/User_study>user study</a>, validating that our generated spatial arrangements align with <a href=https://en.wikipedia.org/wiki/Expectation_(epistemic)>human expectation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.409/>The Dots Have Their Values : Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction</a></strong><br><a href=/people/h/hieu-minh-tran/>Hieu Minh Tran</a>
|
<a href=/people/m/minh-trung-nguyen/>Minh Trung Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--409><div class="card-body p-3 small">The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. However, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not capture the representations for the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, thus preventing it from effectively encoding the specific and relevant information of the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> for DRE. To address this issue, we propose to explicitly compute the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in the graph-based edge-oriented model for <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>DRE</a>. These node representations allow us to introduce two novel representation regularization mechanisms to improve the <a href=https://en.wikipedia.org/wiki/Representation_theory>representation vectors</a> for <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>DRE</a>. The experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--412 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.412" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.412/>Long Document Ranking with Query-Directed Sparse Transformer</a></strong><br><a href=/people/j/jyun-yu-jiang/>Jyun-Yu Jiang</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/c/chia-jung-lee/>Chia-Jung Lee</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--412><div class="card-body p-3 small">The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper, we design Query-Directed Sparse attention that induces IR-axiomatic structures in transformer self-attention. Our model, QDS-Transformer, enforces the principle properties desired in ranking : local contextualization, hierarchical representation, and query-oriented proximity matching, while it also enjoys efficiency from sparsity. Experiments on four fully supervised and few-shot TREC document ranking benchmarks demonstrate the consistent and robust advantage of QDS-Transformer over previous approaches, as they either retrofit long documents into BERT or use sparse attention without emphasizing IR principles. We further quantify the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computing complexity</a> and demonstrates that our sparse attention with TVM implementation is twice more efficient that the fully-connected self-attention. All source codes, trained model, and predictions of this work are available at https://github.com/hallogameboy/QDS-Transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--413 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.413.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.413/>Visuo-Linguistic Question Answering (VLQA) Challenge<span class=acl-fixed-case>VLQA</span>) Challenge</a></strong><br><a href=/people/s/shailaja-keyur-sampat/>Shailaja Keyur Sampat</a>
|
<a href=/people/y/yezhou-yang/>Yezhou Yang</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--413><div class="card-body p-3 small">Understanding <a href=https://en.wikipedia.org/wiki/Image>images</a> and text together is an important aspect of <a href=https://en.wikipedia.org/wiki/Cognition>cognition</a> and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an <a href=https://en.wikipedia.org/wiki/Image>image</a> and a reading passage, where questions are designed to combine both <a href=https://en.wikipedia.org/wiki/Visual_system>visual and textual information</a> i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a <a href=https://en.wikipedia.org/wiki/Modular_programming>modular method</a> with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, code and leaderboard is available at https://shailaja183.github.io/vlqa/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--415 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.415" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.415/>Exploring BERT’s Sensitivity to Lexical Cues using Tests from Semantic Priming<span class=acl-fixed-case>BERT</span>’s Sensitivity to Lexical Cues using Tests from Semantic Priming</a></strong><br><a href=/people/k/kanishka-misra/>Kanishka Misra</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/j/julia-rayz/>Julia Rayz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--415><div class="card-body p-3 small">Models trained to estimate word probabilities in context have become ubiquitous in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. How do these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> use <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical cues</a> in context to inform their <a href=https://en.wikipedia.org/wiki/Lexical_analysis>word probabilities</a>? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by <a href=https://en.wikipedia.org/wiki/Semantic_priming>semantic priming</a>. Using English lexical stimuli that show <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>priming</a> in humans, we find that BERT too shows <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>priming</a>, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a> in these models, and highlight possible parallels with human processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940120 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.416/>Multi-hop Question Generation with Graph Convolutional Network</a></strong><br><a href=/people/d/dan-su/>Dan Su</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/w/wenliang-dai/>Wenliang Dai</a>
|
<a href=/people/z/ziwei-ji/>Ziwei Ji</a>
|
<a href=/people/t/tiezheng-yu/>Tiezheng Yu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--416><div class="card-body p-3 small">Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the sentence containing the answer or nearby sentences in the same paragraph without complex reasoning. To address the additional challenges in multi-hop QG, we propose Multi-Hop Encoding Fusion Network for Question Generation (MulQG), which does context encoding in multiple hops with Graph Convolutional Network and encoding fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence-level information</a>. Empirical results on HotpotQA dataset demonstrate the effectiveness of our method, in comparison with baselines on automatic evaluation metrics. Moreover, from the human evaluation, our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate fluent questions with high completeness and outperforms the strongest <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> by 20.8 % in the multi-hop evaluation. on. The code is publicly availableat https://github.com/HLTCHKU</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940700 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.418/>Thinking Like a Skeptic : Defeasible Inference in Natural Language</a></strong><br><a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--418><div class="card-body p-3 small">Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> and <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. We introduce Defeasible NLI (abbreviated -NLI), a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for defeasible inference in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning : <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>, natural language inference, and <a href=https://en.wikipedia.org/wiki/Social_norm>social norms</a>. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> are capable of writing sentences that weaken or strengthen a specified <a href=https://en.wikipedia.org/wiki/Inference>inference</a> up to 68 % of the time.<tex-math>\\delta</tex-math>-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940091 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.420/>Language-Conditioned Feature Pyramids for Visual Selection Tasks<span class=acl-fixed-case>C</span>onditioned <span class=acl-fixed-case>F</span>eature <span class=acl-fixed-case>P</span>yramids for <span class=acl-fixed-case>V</span>isual <span class=acl-fixed-case>S</span>election <span class=acl-fixed-case>T</span>asks</a></strong><br><a href=/people/t/taichi-iki/>Taichi Iki</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--420><div class="card-body p-3 small">Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that fuse <a href=https://en.wikipedia.org/wiki/Visual_system>visual and linguistic features</a> have been proposed. However, few models consider the fusion of linguistic features with multiple <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a> with different sizes of <a href=https://en.wikipedia.org/wiki/Receptive_field>receptive fields</a>, though the proper size of the receptive field of <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a> intuitively varies depending on expressions. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> that modulates visual features with varying sizes of <a href=https://en.wikipedia.org/wiki/Receptive_field>receptive field</a> by <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. Source code is available at https://github.com/Alab-NII/lcfp.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.421.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.421/>Learning to Classify Events from Human Needs Category Descriptions</a></strong><br><a href=/people/h/haibo-ding/>Haibo Ding</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--421><div class="card-body p-3 small">We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to : (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LeaPI, a zero-shot learning method that first automatically generate weak labels by instantiating high-level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high-quality concepts for instantiation. Experimental results on the human needs categorization task show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms baseline methods, producing substantially better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.431/>Understanding User Resistance Strategies in Persuasive Conversations</a></strong><br><a href=/people/y/youzhi-tian/>Youzhi Tian</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--431><div class="card-body p-3 small">Persuasive dialog systems have various usages, such as donation persuasion and physical exercise persuasion. Previous persuasive dialog systems research mostly focused on analyzing the persuader&#8217;s strategies and paid little attention to the persuadee (user). However, understanding and addressing users&#8217; resistance strategies is an essential job of a persuasive dialog system. So, we adopt a preliminary framework on persuasion resistance in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> and design a fine-grained resistance strategy annotation scheme. We annotate the PersuasionForGood dataset with the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a>. With the enriched annotations, we build a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to predict the resistance strategies. Furthermore, we analyze the relationships between <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> and persuasion resistance strategies. Our work lays the ground for developing a persuasive dialogue system that can understand and address user resistance strategy appropriately. The code and data will be released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--433 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940118 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.433/>Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation</a></strong><br><a href=/people/i/insoo-chung/>Insoo Chung</a>
|
<a href=/people/b/byeongwook-kim/>Byeongwook Kim</a>
|
<a href=/people/y/yoonjung-choi/>Yoonjung Choi</a>
|
<a href=/people/s/se-jung-kwon/>Se Jung Kwon</a>
|
<a href=/people/y/yongkweon-jeon/>Yongkweon Jeon</a>
|
<a href=/people/b/baeseong-park/>Baeseong Park</a>
|
<a href=/people/s/sangha-kim/>Sangha Kim</a>
|
<a href=/people/d/dongsoo-lee/>Dongsoo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--433><div class="card-body p-3 small">The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, especially when the target device is limited in <a href=https://en.wikipedia.org/wiki/Computational_resource>computational resources</a> such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantization bits</a>, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>mixed precision quantization strategy</a> to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8 smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3 reduction in run-time memory footprints and 3.5 speed up (Galaxy N10 +) such that our proposed compression strategy enables efficient implementation for on-device NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--445 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.445/>IndicNLPSuite : Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian Languages</a><span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>NLPS</span>uite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for <span class=acl-fixed-case>I</span>ndian Languages</a></strong><br><a href=/people/d/divyanshu-kakwani/>Divyanshu Kakwani</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/s/satish-golla/>Satish Golla</a>
|
<a href=/people/g/gokul-n-c/>Gokul N.C.</a>
|
<a href=/people/a/avik-bhattacharyya/>Avik Bhattacharyya</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/p/pratyush-kumar/>Pratyush Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--445><div class="card-body p-3 small">In this paper, we introduce NLP resources for 11 major <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a> from two major language families. These resources include : (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple NLU evaluation datasets (IndicGLUE benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and <a href=https://en.wikipedia.org/wiki/Indian_English>Indian English</a>, primarily sourced from <a href=https://en.wikipedia.org/wiki/Web_crawler>news crawls</a>. The <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are based on <a href=https://en.wikipedia.org/wiki/FastText>FastText</a>, hence suitable for handling morphological complexity of <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a>. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (IndicGLUE benchmark for Indian language NLU. To this end, we create datasets for the following tasks : Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd NLI and COPA. We also include publicly available datasets for some Indic languages for tasks like <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a>, Cross-lingual Sentence Retrieval, <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>Paraphrase detection</a>, etc. Our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> will accelerate Indic NLP research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> over a more diverse pool of languages. The data and models are available at.<i>IndicGLUE</i> benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and Indian English, primarily sourced from news crawls. The word embeddings are based on <i>FastText</i>, hence suitable for handling morphological complexity of Indian languages. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (<i>IndicGLUE</i> benchmark for Indian language NLU. To this end, we create datasets for the following tasks: Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd NLI and COPA. We also include publicly available datasets for some Indic languages for tasks like Named Entity Recognition, Cross-lingual Sentence Retrieval, Paraphrase detection, <i>etc.</i> Our embeddings are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the dataset will accelerate Indic NLP research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in NLP over a more diverse pool of languages. The data and models are available at <url>https://indicnlp.ai4bharat.org</url>.</div></div></div><hr><div id=2020insights-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.insights-1/>Proceedings of the First Workshop on Insights from Negative Results in NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.insights-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.insights-1.0/>Proceedings of the First Workshop on Insights from Negative Results in NLP</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.insights-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--insights-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.insights-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.insights-1.5.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940792 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.insights-1.5/>Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models</a></strong><br><a href=/people/s/silvia-terragni/>Silvia Terragni</a>
|
<a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/e/elisabetta-fersini/>Elisabetta Fersini</a>
|
<a href=/people/m/messina-enza/>Messina Enza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--insights-1--5><div class="card-body p-3 small">Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of <a href=https://en.wikipedia.org/wiki/Relational_model>relational information</a>, i.e. document relationships and concept relationships. While exploiting the document network significantly improves <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>topic coherence</a>, the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.insights-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--insights-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.insights-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.insights-1.6.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940793 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.insights-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.insights-1.6/>On Task-Level Dialogue Composition of Generative Transformer Model</a></strong><br><a href=/people/p/prasanna-parthasarathi/>Prasanna Parthasarathi</a>
|
<a href=/people/s/sharan-narang/>Sharan Narang</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--insights-1--6><div class="card-body p-3 small">Task-oriented dialogue systems help users accomplish <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as booking a movie ticket and ordering food via <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a>. Generative models parameterized by a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> are widely used for next turn response generation in such systems. It is natural for users of the <a href=https://en.wikipedia.org/wiki/System>system</a> to want to accomplish multiple tasks within the same conversation, but the ability of <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions : (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.insights-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--insights-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.insights-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940795 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.insights-1.8/>Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification</a></strong><br><a href=/people/a/ashwin-geet-dsa/>Ashwin Geet D’Sa</a>
|
<a href=/people/i/irina-illina/>Irina Illina</a>
|
<a href=/people/d/dominique-fohr/>Dominique Fohr</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--insights-1--8><div class="card-body p-3 small">Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>. Semi-supervised learning takes advantage of a small amount of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> and a large amount of <a href=https://en.wikipedia.org/wiki/Labeled_data>unlabeled data</a>. In this paper, label propagation-based semi-supervised learning is explored for the task of hate speech classification. The quality of labeling the unlabeled set depends on the input representations. In this work, we show that pre-trained representations are label agnostic, and when used with label propagation yield poor results. Neural network-based fine-tuning can be adopted to learn task-specific representations using a small amount of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. We show that fully fine-tuned representations may not always be the best representations for the label propagation and intermediate representations may perform better in a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised setup</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.insights-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--insights-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.insights-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940796 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.insights-1.9/>Layout-Aware Text Representations Harm Clustering Documents by Type</a></strong><br><a href=/people/c/catherine-finegan-dollak/>Catherine Finegan-Dollak</a>
|
<a href=/people/a/ashish-verma/>Ashish Verma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--insights-1--9><div class="card-body p-3 small">Clustering documents by typegrouping invoices with <a href=https://en.wikipedia.org/wiki/Invoice>invoices</a> and articles with articlesis a desirable first step for organizing large collections of document scans. Humans approaching this task use both the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the text and the <a href=https://en.wikipedia.org/wiki/Page_layout>document layout</a> to assist in grouping like documents. LayoutLM (Xu et al., 2019), a layout-aware transformer built on top of BERT with state-of-the-art performance on document-type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document-type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p 0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.</div></div></div><hr><div id=2020intexsempar-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.intexsempar-1/>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.intexsempar-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.intexsempar-1.0/>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/x/xi-victoria-lin/>Victoria Lin</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/a/alane-suhr/>Alane Suhr</a>
|
<a href=/people/p/panupong/>Panupong</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/v/victor-zhong/>Victor Zhong</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.intexsempar-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--intexsempar-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.intexsempar-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939455 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.intexsempar-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.intexsempar-1.3/>Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog</a></strong><br><a href=/people/c/chaoting-xuan/>Chaoting Xuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--intexsempar-1--3><div class="card-body p-3 small">Task Oriented Parsing (TOP) attempts to map utterances to compositional requests, including multiple intents and their slots. Previous work focus on a tree-based hierarchical meaning representation, and applying constituency parsing techniques to address TOP. In this paper, we propose a new format of meaning representation that is more compact and amenable to sequence-to-sequence (seq-to-seq) models. A simple copy-augmented seq-to-seq parser is built and evaluated over a <a href=https://en.wikipedia.org/wiki/TOP500>public TOP dataset</a>, resulting in 3.44 % improvement over prior best seq-to-seq parser (exact match accuracy), which is also comparable to constituency parsers&#8217; performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.intexsempar-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--intexsempar-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.intexsempar-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939457 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.intexsempar-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.intexsempar-1.5/>ColloQL : Robust Text-to-SQL Over Search Queries<span class=acl-fixed-case>C</span>ollo<span class=acl-fixed-case>QL</span>: Robust Text-to-<span class=acl-fixed-case>SQL</span> Over Search Queries</a></strong><br><a href=/people/k/karthik-radhakrishnan/>Karthik Radhakrishnan</a>
|
<a href=/people/a/arvind-srikantan/>Arvind Srikantan</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--intexsempar-1--5><div class="card-body p-3 small">Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in <a href=https://en.wikipedia.org/wiki/Relational_database>relational databases</a> accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual input</a> that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a>. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL&#8217;s superior performance extends to well-formed text, achieving an 84.9 % (logical) and 90.7 % (execution) <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding.</div></div></div><hr><div id=2020louhi-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.louhi-1/>Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.louhi-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.louhi-1.0/>Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.louhi-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--louhi-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.louhi-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940048 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.louhi-1.2/>Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text</a></strong><br><a href=/people/m/maciej-wiatrak/>Maciej Wiatrak</a>
|
<a href=/people/j/juha-iso-sipila/>Juha Iso-Sipila</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--louhi-1--2><div class="card-body p-3 small">Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined, often domain-specific features. The features try to capture the properties of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> and complex multi-step architectures to detect, and subsequently link entity mentions. We propose a significant simplification to the biomedical entity linking setup that does not rely on any heuristic methods. The <a href=https://en.wikipedia.org/wiki/System>system</a> performs all the steps of the entity linking task jointly in either single or two stages. We explore the use of hierarchical multi-task learning, using mention recognition and entity typing tasks as auxiliary tasks. We show that hierarchical multi-task models consistently outperform single-task models when trained tasks are homogeneous. We evaluate the performance of our models on the biomedical entity linking benchmarks using MedMentions and BC5CDR datasets. We achieve state-of-theart results on the challenging MedMentions dataset, and comparable results on BC5CDR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.louhi-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--louhi-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.louhi-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940042 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.louhi-1.7/>Evaluation of Machine Translation Methods applied to <a href=https://en.wikipedia.org/wiki/Medical_terminology>Medical Terminologies</a></a></strong><br><a href=/people/k/konstantinos-skianis/>Konstantinos Skianis</a>
|
<a href=/people/y/yann-briand/>Yann Briand</a>
|
<a href=/people/f/florent-desgrippes/>Florent Desgrippes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--louhi-1--7><div class="card-body p-3 small">Medical terminologies resources and standards play vital roles in clinical data exchanges, enabling significantly the services&#8217; interoperability within healthcare national information networks. Health and medical science are constantly evolving causing requirements to advance the terminologies editions. In this paper, we present our evaluation work of the latest machine translation techniques addressing <a href=https://en.wikipedia.org/wiki/Medical_terminology>medical terminologies</a>. Experiments have been conducted leveraging selected statistical and neural machine translation methods. The devised procedure is tested on a validated sample of ICD-11 and ICF terminologies from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a> with promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.louhi-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--louhi-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.louhi-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940047 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.louhi-1.12/>Defining and Learning Refined Temporal Relations in the Clinical Narrative</a></strong><br><a href=/people/k/kristin-wright-bettner/>Kristin Wright-Bettner</a>
|
<a href=/people/c/chen-lin/>Chen Lin</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/j/james-h-martin/>James H. Martin</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--louhi-1--12><div class="card-body p-3 small">We present refinements over existing temporal relation annotations in the Electronic Medical Record clinical narrative. We refined the THYME corpus annotations to more faithfully represent nuanced temporality and nuanced temporal-coreferential relations. The main contributions are in re-defining CONTAINS and OVERLAP relations into CONTAINS, CONTAINS-SUBEVENT, OVERLAP and NOTED-ON. We demonstrate that these refinements lead to substantial gains in learnability for state-of-the-art transformer models as compared to previously reported results on the original THYME corpus. We thus establish a baseline for the automatic extraction of these refined temporal relations. Although our study is done on clinical narrative, we believe it addresses far-reaching challenges that are corpus- and domain- agnostic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.louhi-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--louhi-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.louhi-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940053 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.louhi-1.13/>Context-Aware Automatic Text Simplification of Health Materials in Low-Resource Domains</a></strong><br><a href=/people/t/tarek-sakakini/>Tarek Sakakini</a>
|
<a href=/people/j/jong-yoon-lee/>Jong Yoon Lee</a>
|
<a href=/people/a/aditya-duri/>Aditya Duri</a>
|
<a href=/people/r/renato-f-l-azevedo/>Renato F.L. Azevedo</a>
|
<a href=/people/v/victor-sadauskas/>Victor Sadauskas</a>
|
<a href=/people/k/kuangxiao-gu/>Kuangxiao Gu</a>
|
<a href=/people/s/suma-bhat/>Suma Bhat</a>
|
<a href=/people/d/dan-morrow/>Dan Morrow</a>
|
<a href=/people/j/james-graumlich/>James Graumlich</a>
|
<a href=/people/s/saqib-walayat/>Saqib Walayat</a>
|
<a href=/people/m/mark-hasegawa-johnson/>Mark Hasegawa-Johnson</a>
|
<a href=/people/t/thomas-huang/>Thomas Huang</a>
|
<a href=/people/a/ann-willemsen-dunlap/>Ann Willemsen-Dunlap</a>
|
<a href=/people/d/donald-halpin/>Donald Halpin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--louhi-1--13><div class="card-body p-3 small">Healthcare systems have increased patients&#8217; exposure to their own health materials to enhance patients&#8217; health levels, but this has been impeded by patients&#8217; lack of understanding of their health material. We address potential barriers to their comprehension by developing a context-aware text simplification system for health material. Given the scarcity of annotated parallel corpora in healthcare domains, we design our system to be independent of a parallel corpus, complementing the availability of data-driven neural methods when such <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> are available. Our system compensates for the lack of direct supervision using a biomedical lexical database : Unified Medical Language System (UMLS). Compared to a competitive prior approach that uses a tool for identifying biomedical concepts and a consumer-directed vocabulary list, we empirically show the enhanced accuracy of our system due to improved handling of ambiguous terms. We also show the enhanced <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of our <a href=https://en.wikipedia.org/wiki/System>system</a> over directly-supervised neural methods in this low-resource setting. Finally, we show the direct impact of our system on laypeople&#8217;s comprehension of health material via a human subjects&#8217; study (n=160).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.louhi-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--louhi-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.louhi-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940051 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.louhi-1.14/>Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a Language Model and Its Updating</a></strong><br><a href=/people/m/minghao-zhu/>Minghao Zhu</a>
|
<a href=/people/y/youzhe-song/>Youzhe Song</a>
|
<a href=/people/g/ge-jin/>Ge Jin</a>
|
<a href=/people/k/keyuan-jiang/>Keyuan Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--louhi-1--14><div class="card-body p-3 small">Post-market surveillance, the practice of monitoring the safe use of <a href=https://en.wikipedia.org/wiki/Medication>pharmaceutical drugs</a> is an important part of <a href=https://en.wikipedia.org/wiki/Pharmacovigilance>pharmacovigilance</a>. Being able to collect personal experience related to pharmaceutical product use could help us gain insight into how the human body reacts to different medications. Twitter, a popular social media service, is being considered as an important alternative data source for collecting personal experience information with medications. Identifying personal experience tweets is a challenging classification task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this study, we utilized three methods based on <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>&#8217;s Robustly Optimized BERT Pretraining Approach (RoBERTa) to predict personal experience tweets related to medication use : the first one combines the pre-trained RoBERTa model with a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>, the second combines the updated pre-trained RoBERTa model using a corpus of unlabeled tweets with a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>, and the third combines the RoBERTa model that was trained with our unlabeled tweets from scratch with the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> too. Our results show that all of these approaches outperform the published methods (Word Embedding + LSTM) in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance (p 0.05), and updating the pre-trained language model with tweets related to medications could even improve the performance further.</div></div></div><hr><div id=2020nlpbt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.nlpbt-1/>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpbt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlpbt-1.0/>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</a></strong><br><a href=/people/g/giuseppe-castellucci/>Giuseppe Castellucci</a>
|
<a href=/people/s/simone-filice/>Simone Filice</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpbt-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpbt-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpbt-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939781 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlpbt-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpbt-1.7/>MAST : Multimodal Abstractive Summarization with Trimodal Hierarchical Attention<span class=acl-fixed-case>MAST</span>: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</a></strong><br><a href=/people/a/aman-khullar/>Aman Khullar</a>
|
<a href=/people/u/udit-arora/>Udit Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpbt-1--7><div class="card-body p-3 small">This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities text, audio and video in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the <a href=https://en.wikipedia.org/wiki/Audio_signal>audio modality</a> and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpbt-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpbt-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpbt-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939783 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpbt-1.9/>Reasoning Over History : Context Aware Visual Dialog</a></strong><br><a href=/people/m/muhammad-shah/>Muhammad Shah</a>
|
<a href=/people/s/shikib-mehri/>Shikib Mehri</a>
|
<a href=/people/t/tejas-srinivasan/>Tejas Srinivasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpbt-1--9><div class="card-body p-3 small">While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the <a href=https://en.wikipedia.org/wiki/Medium_access_control>MAC network</a>, which decomposes a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> into a series of attention-based reasoning steps. However, since the <a href=https://en.wikipedia.org/wiki/Medium_access_control>MAC network</a> is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that require reasoning over the dialog history, particularly <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25 % accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30 % (absolute). Our <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> indicates that with <a href=https://en.wikipedia.org/wiki/Computer-aided_manufacturing>CAM</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance particularly improved on questions that required <a href=https://en.wikipedia.org/wiki/Coreference>coreference resolution</a>.</div></div></div><hr><div id=2020nlpcovid19-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.nlpcovid19-2/>Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcovid19-2.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlpcovid19-2.0/>Proceedings of the 1st Workshop on <span class=acl-fixed-case>NLP</span> for <span class=acl-fixed-case>COVID</span>-19 (Part 2) at <span class=acl-fixed-case>EMNLP</span> 2020</a></strong><br><a href=/people/k/karin-verspoor/>Karin Verspoor</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/m/michael-conway/>Michael Conway</a>
|
<a href=/people/b/berry-de-bruijn/>Berry de Bruijn</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p></div><hr><div id=2020nlpcss-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.nlpcss-1/>Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.0/>Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science</a></strong><br><a href=/people/d/david-bamman/>David Bamman</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/b/brendan-o-connor/>Brendan O'Connor</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcss-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcss-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940616 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.3/>Swimming with the Tide? Positional Claim Detection across Political Text Types</a></strong><br><a href=/people/n/nico-blokker/>Nico Blokker</a>
|
<a href=/people/e/erenay-dayanik/>Erenay Dayanik</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcss-1--3><div class="card-body p-3 small">Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper articles</a>, to understand the <a href=https://en.wikipedia.org/wiki/Party_platform>party positions</a> on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross-text type setting : we train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcss-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcss-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940602 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.9/>Identifying Worry in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> : Beyond Emotion Analysis<span class=acl-fixed-case>T</span>witter: Beyond Emotion Analysis</a></strong><br><a href=/people/r/reyha-verma/>Reyha Verma</a>
|
<a href=/people/c/christian-von-der-weth/>Christian von der Weth</a>
|
<a href=/people/j/jithin-vachery/>Jithin Vachery</a>
|
<a href=/people/m/mohan-kankanhalli/>Mohan Kankanhalli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcss-1--9><div class="card-body p-3 small">Identifying the worries of individuals and societies plays a crucial role in providing <a href=https://en.wikipedia.org/wiki/Social_support>social support</a> and enhancing policy decision-making. Due to the popularity of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, users share worries about personal issues (e.g., health, finances, relationships) and broader issues (e.g., changes in society, environmental concerns, terrorism) freely. In this paper, we explore and evaluate a wide range of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> to predict <a href=https://en.wikipedia.org/wiki/Worry>worry</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. While this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> has been closely associated with emotion prediction, we argue and show that identifying worry needs to be addressed as a separate <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> given the unique challenges associated with it. We conduct a user study to provide evidence that social media posts express two basic kinds of worry normative and pathological as stated in psychology literature. In addition, we show that existing <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection techniques</a> underperform, especially while capturing normative worry. Finally, we discuss the current limitations of our approach and propose future applications of the worry identification system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcss-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcss-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.nlpcss-1.14.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.14/>Recalibrating classifiers for interpretable abusive content detection</a></strong><br><a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a>
|
<a href=/people/s/sam-staton/>Sam Staton</a>
|
<a href=/people/t/tom-melham/>Tom Melham</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a>
|
<a href=/people/o/ohad-kammar/>Ohad Kammar</a>
|
<a href=/people/m/marcin-szymczak/>Marcin Szymczak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcss-1--14><div class="card-body p-3 small">We investigate the use of <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> for detecting online abuse in empirical research. We show that uncalibrated classifiers (i.e. where the &#8216;raw&#8217; scores are used) align poorly with human evaluations. This limits their use for understanding the dynamics, patterns and prevalence of <a href=https://en.wikipedia.org/wiki/Online_abuse>online abuse</a>. We examine two widely used <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> (created by Perspective and Davidson et al.) on a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> directed against candidates in the UK&#8217;s 2017 general election. A <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian approach</a> is presented to recalibrate the raw scores from the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, using <a href=https://en.wikipedia.org/wiki/Probabilistic_programming>probabilistic programming</a> and newly <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a>. We argue that interpretability evaluation and recalibration is integral to the application of abusive content classifiers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcss-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcss-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940613 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.17/>Mapping Local News Coverage : Precise location extraction in textual news content using fine-tuned BERT based language model<span class=acl-fixed-case>BERT</span> based language model</a></strong><br><a href=/people/s/sarang-gupta/>Sarang Gupta</a>
|
<a href=/people/k/kumari-nishu/>Kumari Nishu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcss-1--17><div class="card-body p-3 small">Mapping local news coverage from <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>textual content</a> is a challenging problem that requires extracting precise location mentions from <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they can not recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcss-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcss-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940614 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlpcss-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.18/>Foreigner-directed speech is simpler than native-directed : Evidence from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a></a></strong><br><a href=/people/a/aleksandrs-berdicevskis/>Aleksandrs Berdicevskis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcss-1--18><div class="card-body p-3 small">I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies : first, that non-native production is simpler than native ; second, that <a href=https://en.wikipedia.org/wiki/Language_production>production</a> addressed to non-native speakers is simpler than that addressed to natives. The second hypothesis is particularly important for theories about contact-induced simplification, since the accommodation to non-natives may explain how the simplification can spread from <a href=https://en.wikipedia.org/wiki/Adult_learner>adult learners</a> to the whole community. To test the hypotheses, I create a very large corpus of native and non-native written speech in four languages (English, French, Italian, Spanish), extracting data from an <a href=https://en.wikipedia.org/wiki/Internet_forum>internet forum</a> where native languages of the participants are known and the structure of the interactions can be inferred. The corpus data yield inconsistent evidence with respect to the first hypothesis, but largely support the second one, suggesting that foreigner-directed speech is indeed simpler than native-directed. Importantly, when testing the first hypothesis, I contrast production of different speakers, which can introduce confounds and is a likely reason for the inconsistencies. When testing the second hypothesis, the comparison is always within the production of the same speaker (but with different addressees), which makes it more reliable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcss-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcss-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940607 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlpcss-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.21/>Understanding Weekly COVID-19 Concerns through Dynamic Content-Specific LDA Topic Modeling<span class=acl-fixed-case>COVID</span>-19 Concerns through Dynamic Content-Specific <span class=acl-fixed-case>LDA</span> Topic Modeling</a></strong><br><a href=/people/m/mohammadzaman-zamani/>Mohammadzaman Zamani</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a>
|
<a href=/people/j/johannes-eichstaedt/>Johannes Eichstaedt</a>
|
<a href=/people/s/sharath-chandra-guntuku/>Sharath Chandra Guntuku</a>
|
<a href=/people/a/adithya-virinchipuram-ganesan/>Adithya Virinchipuram Ganesan</a>
|
<a href=/people/s/sean-clouston/>Sean Clouston</a>
|
<a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcss-1--21><div class="card-body p-3 small">The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We propose a dynamic content-specific LDA topic modeling technique that can help to identify different domains of COVID-specific discourse that can be used to track societal shifts in concerns or views. Our experiments show that these model-derived topics are more coherent than standard LDA topics, and also provide new features that are more helpful in prediction of COVID-19 related outcomes including <a href=https://en.wikipedia.org/wiki/Social_mobility>social mobility</a> and <a href=https://en.wikipedia.org/wiki/Unemployment>unemployment rate</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlpcss-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlpcss-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlpcss-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940622 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlpcss-1.22/>Emoji and Self-Identity in Twitter Bios<span class=acl-fixed-case>T</span>witter Bios</a></strong><br><a href=/people/j/jinhang-li/>Jinhang Li</a>
|
<a href=/people/g/giorgos-longinos/>Giorgos Longinos</a>
|
<a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlpcss-1--22><div class="card-body p-3 small">Emoji are widely used to express emotions and concepts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, and prior work has shown that users&#8217; choice of <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> reflects the way that they wish to present themselves to the world. Emoji usage is typically studied in the context of posts made by users, and this view has provided important insights into phenomena such as <a href=https://en.wikipedia.org/wiki/Emotion>emotional expression</a> and <a href=https://en.wikipedia.org/wiki/Self-representation>self-representation</a>. In addition to making posts, however, <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> allow for users to provide a short bio, which is an opportunity to briefly describe their account as a whole. In this work, we focus on the use of <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> in these bio statements. We explore the ways in which users include <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> in these self-descriptions, finding different patterns than those observed around <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> usage in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. We examine the relationships between <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> used in <a href=https://en.wikipedia.org/wiki/Biography>bios</a> and the content of users&#8217; tweets, showing that the topics and even the average sentiment of tweets varies for users with different <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> in their bios. Lastly, we confirm that homophily effects exist with respect to the types of <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> that are included in bios of users and their followers.</div></div></div><hr><div id=2020nlposs-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.nlposs-1/>Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlposs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlposs-1.0/>Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)</a></strong><br><a href=/people/e/eunjeong-l-park/>Eunjeong L. Park</a>
|
<a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/d/dmitrijs-milajevs/>Dmitrijs Milajevs</a>
|
<a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/g/geeticka-chauhan/>Geeticka Chauhan</a>
|
<a href=/people/l/liling-tan/>Liling Tan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlposs-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlposs-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlposs-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939741 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlposs-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlposs-1.4/>End-to-end NLP Pipelines in Rust<span class=acl-fixed-case>NLP</span> Pipelines in Rust</a></strong><br><a href=/people/g/guillaume-becquin/>Guillaume Becquin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlposs-1--4><div class="card-body p-3 small">The recent progress in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> research has been supported by the development of a rich open source ecosystem in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a>. Libraries allowing NLP practitioners but also non-specialists to leverage state-of-the-art models have been instrumental in the democratization of this technology. The maturity of the open-source NLP ecosystem however varies between languages. This work proposes a new <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source library</a> aimed at bringing state-of-the-art <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> to <a href=https://en.wikipedia.org/wiki/Rust_(programming_language)>Rust</a>. Rust is a <a href=https://en.wikipedia.org/wiki/Systems_programming>systems programming language</a> for which the foundations required to build machine learning applications are available but still lacks ready-to-use, end-to-end NLP libraries. The proposed <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a>, rust-bert, implements modern language models and ready-to-use pipelines (for example <a href=https://en.wikipedia.org/wiki/Translation>translation</a> or summarization). This allows further development by the Rust community from both NLP experts and non-specialists. It is hoped that this <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> will accelerate the development of the NLP ecosystem in Rust. The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> is under active development and available at https://github.com/guillaume-be/rust-bert.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlposs-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlposs-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlposs-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939751 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlposs-1.12/>Open Korean Corpora : A Practical Report<span class=acl-fixed-case>K</span>orean Corpora: A Practical Report</a></strong><br><a href=/people/w/won-ik-cho/>Won Ik Cho</a>
|
<a href=/people/s/sangwhan-moon/>Sangwhan Moon</a>
|
<a href=/people/y/youngsook-song/>Youngsook Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlposs-1--12><div class="card-body p-3 small">Korean is often referred to as a low-resource language in the research community. While this claim is partially true, it is also because the availability of resources is inadequately advertised and curated. This work curates and reviews a list of Korean corpora, first describing institution-level resource development, then further iterate through a list of current open datasets for different types of tasks. We then propose a direction on how open-source dataset construction and releases should be done for less-resourced languages to promote research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlposs-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlposs-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlposs-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.nlposs-1.15.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939754 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlposs-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlposs-1.15/>PySBD : Pragmatic Sentence Boundary Disambiguation<span class=acl-fixed-case>P</span>y<span class=acl-fixed-case>SBD</span>: Pragmatic Sentence Boundary Disambiguation</a></strong><br><a href=/people/n/nipun-sadvilkar/>Nipun Sadvilkar</a>
|
<a href=/people/m/mark-neumann/>Mark Neumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlposs-1--15><div class="card-body p-3 small">We present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>logical sentences</a> even when the format and domain of the input text is unknown. In our work, we adapt the Golden Rules Set (a language specific set of sentence boundary exemplars) originally implemented as a ruby gem pragmatic segmenter which we ported to <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> with additional improvements and functionality. PySBD passes 97.92 % of the Golden Rule Set examplars for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, an improvement of 25 % over the next best open source Python tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlposs-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlposs-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlposs-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.nlposs-1.17.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939755 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.nlposs-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.nlposs-1.17/>SacreROUGE : An Open-Source Library for Using and Developing Summarization Evaluation Metrics<span class=acl-fixed-case>S</span>acre<span class=acl-fixed-case>ROUGE</span>: An Open-Source Library for Using and Developing Summarization Evaluation Metrics</a></strong><br><a href=/people/d/daniel-deutsch/>Daniel Deutsch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlposs-1--17><div class="card-body p-3 small">We present SacreROUGE, an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source library</a> for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics : (1) The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface ; (2) it provides functionality to evaluate how well any metric implemented in the <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric ; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a>, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.</div></div></div><hr><div id=2020privatenlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.privatenlp-1/>Proceedings of the Second Workshop on Privacy in NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.privatenlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.privatenlp-1.0/>Proceedings of the Second Workshop on Privacy in NLP</a></strong><br><a href=/people/o/oluwaseyi-feyisetan/>Oluwaseyi Feyisetan</a>
|
<a href=/people/s/sepideh-ghanavati/>Sepideh Ghanavati</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/p/patricia-thaine/>Patricia Thaine</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.privatenlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--privatenlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.privatenlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939773 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.privatenlp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.privatenlp-1.4/>Surfacing Privacy Settings Using Semantic Matching</a></strong><br><a href=/people/r/rishabh-khandelwal/>Rishabh Khandelwal</a>
|
<a href=/people/a/asmit-nayak/>Asmit Nayak</a>
|
<a href=/people/y/yao-yao-uwisc/>Yao Yao</a>
|
<a href=/people/k/kassem-fawaz/>Kassem Fawaz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--privatenlp-1--4><div class="card-body p-3 small">Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-centric encoders and leverage them to create an interface that allows users to search for privacy settings using free-form queries. In order to achieve this goal, we create a custom Semantic Similarity dataset, which consists of real user queries covering various privacy settings. We then use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to fine-tune a state of the art <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Using this fine-tuned encoder, we perform <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a> between the <a href=https://en.wikipedia.org/wiki/Information_retrieval>user queries</a> and the privacy settings to retrieve the most relevant setting. Finally, we also use the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to generate embeddings of privacy settings from the top 100 websites and perform unsupervised clustering to learn about the online privacy settings types. We find that the most common type of privacy settings are &#8216;Personalization&#8217; and &#8216;Notifications&#8217;, with coverage of 35.8 % and 34.4 %, respectively, in our dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.privatenlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--privatenlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.privatenlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939774 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.privatenlp-1.5/>Differentially Private Language Models Benefit from Public Pre-training</a></strong><br><a href=/people/g/gavin-kerrigan/>Gavin Kerrigan</a>
|
<a href=/people/d/dylan-slack/>Dylan Slack</a>
|
<a href=/people/j/jens-tuyls/>Jens Tuyls</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--privatenlp-1--5><div class="card-body p-3 small">Language modeling is a keystone task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. When training a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> on <a href=https://en.wikipedia.org/wiki/Information_sensitivity>sensitive information</a>, differential privacy (DP) allows us to quantify the degree to which our <a href=https://en.wikipedia.org/wiki/Personal_data>private data</a> is protected. However, training algorithms which enforce <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> often lead to degradation in model quality. We study the feasibility of learning a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> possible.</div></div></div><hr><div id=2020scai-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.scai-1/>Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.scai-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.scai-1.0/>Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI)</a></strong><br><a href=/people/j/jeff-dalton/>Jeff Dalton</a>
|
<a href=/people/a/aleksandr-chuklin/>Aleksandr Chuklin</a>
|
<a href=/people/j/julia-kiseleva/>Julia Kiseleva</a>
|
<a href=/people/m/mikhail-burtsev/>Mikhail Burtsev</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.scai-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--scai-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.scai-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940062 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.scai-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.scai-1.2/>A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering</a></strong><br><a href=/people/s/svitlana-vakulenko/>Svitlana Vakulenko</a>
|
<a href=/people/s/shayne-longpre/>Shayne Longpre</a>
|
<a href=/people/z/zhucheng-tu/>Zhucheng Tu</a>
|
<a href=/people/r/raviteja-anantha/>Raviteja Anantha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--scai-1--2><div class="card-body p-3 small">The dependency between an adequate question formulation and correct answer selection is a very intriguing but still underexplored area. In this paper, we show that question rewriting (QR) of the conversational context allows to shed more light on this phenomenon and also use it to evaluate <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of different answer selection approaches. We introduce a simple framework that enables an automated analysis of the conversational question answering (QA) performance using question rewrites, and present the results of this analysis on the TREC CAsT and QuAC (CANARD) datasets. Our experiments uncover sensitivity to question formulation of the popular state-of-the-art question answering approaches. Our results demonstrate that the reading comprehension model is insensitive to question formulation, while the passage ranking changes dramatically with a little variation in the input question. The benefit of <a href=https://en.wikipedia.org/wiki/QR_code>QR</a> is that it allows us to pinpoint and group such cases automatically. We show how to use this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to verify whether QA models are really learning the task or just finding shortcuts in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, and better understand the frequent types of error they make.</div></div></div><hr><div id=2020sdp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.sdp-1/>Proceedings of the First Workshop on Scholarly Document Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.0/>Proceedings of the First Workshop on Scholarly Document Processing</a></strong><br><a href=/people/m/muthu-kumar-chandrasekaran/>Muthu Kumar Chandrasekaran</a>
|
<a href=/people/a/anita-de-waard/>Anita de Waard</a>
|
<a href=/people/g/guy-feigenblat/>Guy Feigenblat</a>
|
<a href=/people/d/dayne-freitag/>Dayne Freitag</a>
|
<a href=/people/t/tirthankar-ghosal/>Tirthankar Ghosal</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/p/petr-knoth/>Petr Knoth</a>
|
<a href=/people/d/david-konopnicki/>David Konopnicki</a>
|
<a href=/people/p/philipp-mayr/>Philipp Mayr</a>
|
<a href=/people/r/robert-m-patton/>Robert M. Patton</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.2/>The future of <a href=https://en.wikipedia.org/wiki/ArXiv>arXiv</a> and <a href=https://en.wikipedia.org/wiki/Discovery_(observation)>knowledge discovery</a> in open science<span class=acl-fixed-case>X</span>iv and knowledge discovery in open science</a></strong><br><a href=/people/s/steinn-sigurdsson/>Steinn Sigurdsson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--2><div class="card-body p-3 small">arXiv, the preprint server for the physical and mathematical sciences, is in its third decade of operation. As the flow of new, open access research increases inexorably, the challenges to keep up with and discover research content also become greater. I will discuss the status and future of <a href=https://en.wikipedia.org/wiki/ArXiv>arXiv</a>, and possibilities and plans to make more effective use of the research database to enhance ongoing research efforts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940712 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sdp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sdp-1.3/>Acknowledgement Entity Recognition in CORD-19 Papers<span class=acl-fixed-case>CORD</span>-19 Papers</a></strong><br><a href=/people/j/jian-wu/>Jian Wu</a>
|
<a href=/people/p/pei-wang/>Pei Wang</a>
|
<a href=/people/x/xin-wei/>Xin Wei</a>
|
<a href=/people/s/sarah-rajtmajer/>Sarah Rajtmajer</a>
|
<a href=/people/c/c-lee-giles/>C. Lee Giles</a>
|
<a href=/people/c/christopher-griffin/>Christopher Griffin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--3><div class="card-body p-3 small">Acknowledgements are ubiquitous in <a href=https://en.wikipedia.org/wiki/Academic_publishing>scholarly papers</a>. Existing acknowledgement entity recognition methods assume all named entities are acknowledged. Here, we examine the nuances between acknowledged and named entities by analyzing <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a>. We develop an acknowledgement extraction system, AckExtract based on open-source text mining software and evaluate our method using manually labeled data. AckExtract uses the PDF of a scholarly paper as input and outputs <a href=https://en.wikipedia.org/wiki/Acknowledgement_(data_networks)>acknowledgement entities</a>. Results show an overall performance of F_1=0.92. We built a supplementary database by linking CORD-19 papers with acknowledgement entities extracted by AckExtract including persons and organizations and find that only up to 5060 % of named entities are actually acknowledged. We further analyze chronological trends of acknowledgement entities in CORD-19 papers. All codes and labeled data are publicly available at https://github.com/lamps-lab/ackextract.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940716 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sdp-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sdp-1.7/>Effective <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> for academic expert search</a></strong><br><a href=/people/m/mark-berger/>Mark Berger</a>
|
<a href=/people/j/jakub-zavrel/>Jakub Zavrel</a>
|
<a href=/people/p/paul-groth/>Paul Groth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--7><div class="card-body p-3 small">Expert search aims to find and rank experts based on a user&#8217;s query. In <a href=https://en.wikipedia.org/wiki/Academy>academia</a>, retrieving experts is an efficient way to navigate through a large amount of <a href=https://en.wikipedia.org/wiki/Outline_of_academic_disciplines>academic knowledge</a>. Here, we study how different distributed representations of academic papers (i.e. embeddings) impact academic expert retrieval. We use the Microsoft Academic Graph dataset and experiment with different configurations of a document-centric voting model for retrieval. In particular, we explore the impact of the use of contextualized embeddings on <a href=https://en.wikipedia.org/wiki/Web_search_engine>search</a> performance. We also present results for paper embeddings that incorporate <a href=https://en.wikipedia.org/wiki/Citation>citation information</a> through <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a>. Additionally, experiments are conducted using different <a href=https://en.wikipedia.org/wiki/Scientific_technique>techniques</a> for assigning author weights based on <a href=https://en.wikipedia.org/wiki/Author_order>author order</a>. We observe that using contextual embeddings produced by a transformer model trained for sentence similarity tasks produces the most effective paper representations for document-centric expert retrieval. However, retrofitting the paper embeddings and using elaborate author contribution weighting strategies did not improve retrieval performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940727 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sdp-1.14/>Multi-task Peer-Review Score Prediction</a></strong><br><a href=/people/j/jiyi-li/>Jiyi Li</a>
|
<a href=/people/a/ayaka-sato/>Ayaka Sato</a>
|
<a href=/people/k/kazuya-shimura/>Kazuya Shimura</a>
|
<a href=/people/f/fumiyo-fukumoto/>Fumiyo Fukumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--14><div class="card-body p-3 small">Automatic prediction on the peer-review aspect scores of <a href=https://en.wikipedia.org/wiki/Academic_publishing>academic papers</a> can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures. We propose a multi-task shared structure encoding approach which automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naive multi-task methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940725 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sdp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sdp-1.15/>ERLKG : Entity Representation Learning and Knowledge Graph based association analysis of COVID-19 through mining of unstructured biomedical corpora<span class=acl-fixed-case>ERLKG</span>: Entity Representation Learning and Knowledge Graph based association analysis of <span class=acl-fixed-case>COVID</span>-19 through mining of unstructured biomedical corpora</a></strong><br><a href=/people/s/sayantan-basu/>Sayantan Basu</a>
|
<a href=/people/s/sinchani-chakraborty/>Sinchani Chakraborty</a>
|
<a href=/people/a/atif-hassan/>Atif Hassan</a>
|
<a href=/people/s/sana-siddique/>Sana Siddique</a>
|
<a href=/people/a/ashish-anand/>Ashish Anand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--15><div class="card-body p-3 small">We introduce a generic, human-out-of-the-loop pipeline, ERLKG, to perform rapid association analysis of any biomedical entity with other existing entities from a corpora of the same domain. Our pipeline consists of a Knowledge Graph (KG) created from the Open Source CORD-19 dataset by fully automating the procedure of <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> using SciBERT. The best latent entity representations are then found by benchnmarking different KG embedding techniques on the task of link prediction using a Graph Convolution Network Auto Encoder (GCN-AE). We demonstrate the utility of ERLKG with respect to COVID-19 through multiple qualitative evaluations. Due to the lack of a gold standard, we propose a relatively large intrinsic evaluation dataset for COVID-19 and use it for validating the top two performing KG embedding techniques. We find TransD to be the best performing KG embedding technique with Pearson and Spearman correlation scores of 0.4348 and 0.4570 respectively. We demonstrate that a considerable number of ERLKG&#8217;s top protein, chemical and disease predictions are currently in consideration for COVID-19 related research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sdp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.21/>Scaling Systematic Literature Reviews with Machine Learning Pipelines</a></strong><br><a href=/people/s/seraphina-goldfarb-tarrant/>Seraphina Goldfarb-Tarrant</a>
|
<a href=/people/a/alexander-robertson/>Alexander Robertson</a>
|
<a href=/people/j/jasmina-lazic/>Jasmina Lazic</a>
|
<a href=/people/t/theodora-tsouloufi/>Theodora Tsouloufi</a>
|
<a href=/people/l/louise-donnison/>Louise Donnison</a>
|
<a href=/people/k/karen-smyth/>Karen Smyth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--21><div class="card-body p-3 small">Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a <a href=https://en.wikipedia.org/wiki/Systematic_review>systematic review</a> are easily done automatically : searching for documents can be done via <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a> and scrapers, selection of relevant documents can be done via <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a>, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We construct a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of <a href=https://en.wikipedia.org/wiki/Data_extraction>data extraction</a> with varying difficulty in <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, and five different neural architectures to do the <a href=https://en.wikipedia.org/wiki/Data_extraction>extraction</a>. We find that we can get surprising <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and generalisability of the whole <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> with only 2 weeks of human-expert annotation, which is only 15 % of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sdp-1.22.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940724 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sdp-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sdp-1.22/>Document-Level Definition Detection in Scholarly Documents : Existing Models, Error Analyses, and Future Directions</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/a/andrew-head/>Andrew Head</a>
|
<a href=/people/r/risham-sidhu/>Risham Sidhu</a>
|
<a href=/people/k/kyle-lo/>Kyle Lo</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a>
|
<a href=/people/m/marti-a-hearst/>Marti A. Hearst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--22><div class="card-body p-3 small">The task of definition detection is important for <a href=https://en.wikipedia.org/wiki/Academic_publishing>scholarly papers</a>, because papers often make use of <a href=https://en.wikipedia.org/wiki/Jargon>technical terminology</a> that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic filters</a>, and evaluate it on a standard sentence-level benchmark. Because current <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> evaluate <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>randomly sampled sentences</a>, we propose an alternative <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> that assesses every sentence within a document. This allows for evaluating <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> in addition to <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as <a href=https://en.wikipedia.org/wiki/Software_feature>features</a>. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.23/>A New Neural Search and Insights Platform for Navigating and Organizing AI Research<span class=acl-fixed-case>AI</span> Research</a></strong><br><a href=/people/m/marzieh-fadaee/>Marzieh Fadaee</a>
|
<a href=/people/o/olga-gureenkova/>Olga Gureenkova</a>
|
<a href=/people/f/fernando-rejon-barrera/>Fernando Rejon Barrera</a>
|
<a href=/people/c/carsten-schnober/>Carsten Schnober</a>
|
<a href=/people/w/wouter-weerkamp/>Wouter Weerkamp</a>
|
<a href=/people/j/jakub-zavrel/>Jakub Zavrel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--23><div class="card-body p-3 small">To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical <a href=https://en.wikipedia.org/wiki/Keyword_search>keyword search</a> with neural retrieval to discover and organize relevant literature. The system provides <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a> at multiple levels of textual granularity, from sentences to aggregations across documents, both in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> and through navigation in a domain specific Knowledge Graph. We give an overview of the overall architecture of the system and of the components for document analysis, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a>, <a href=https://en.wikipedia.org/wiki/Analytics>analytics</a>, expert search, and recommendations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.24/>Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020 : CL-SciSumm, LaySumm and LongSumm<span class=acl-fixed-case>CL</span>-<span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>S</span>umm, <span class=acl-fixed-case>L</span>ay<span class=acl-fixed-case>S</span>umm and <span class=acl-fixed-case>L</span>ong<span class=acl-fixed-case>S</span>umm</a></strong><br><a href=/people/m/muthu-kumar-chandrasekaran/>Muthu Kumar Chandrasekaran</a>
|
<a href=/people/g/guy-feigenblat/>Guy Feigenblat</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a>
|
<a href=/people/a/anita-de-waard/>Anita de Waard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--24><div class="card-body p-3 small">We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020 : CL-SciSumm, LaySumm and LongSumm. We report on each of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, which received 18 submissions in total, with some submissions addressing two or three of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.32/>Team MLU@CL-SciSumm20 : Methods for Computational Linguistics Scientific Citation Linkage<span class=acl-fixed-case>MLU</span>@<span class=acl-fixed-case>CL</span>-<span class=acl-fixed-case>S</span>ci<span class=acl-fixed-case>S</span>umm20: Methods for Computational Linguistics Scientific Citation Linkage</a></strong><br><a href=/people/r/rong-huang/>Rong Huang</a>
|
<a href=/people/k/kseniia-krylova/>Kseniia Krylova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--32><div class="card-body p-3 small">This paper describes our approach to the CL-SciSumm 2020 shared task toward the problem of identifying reference span of the citing article in the referred article. In Task 1a, we apply and compare different methods in combination with similarity scores to identify spans of the reference text for the given citance. In Task 1b, we use a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> to classifying the discourse facets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.36/>ARTU / TU Wien and Artificial Researcher@ LongSumm 20<span class=acl-fixed-case>ARTU</span> / <span class=acl-fixed-case>TU</span> <span class=acl-fixed-case>W</span>ien and Artificial Researcher@ <span class=acl-fixed-case>L</span>ong<span class=acl-fixed-case>S</span>umm 20</a></strong><br><a href=/people/a/alaa-el-ebshihy/>Alaa El-Ebshihy</a>
|
<a href=/people/a/annisa-maulida-ningtyas/>Annisa Maulida Ningtyas</a>
|
<a href=/people/l/linda-andersson/>Linda Andersson</a>
|
<a href=/people/f/florina-piroi/>Florina Piroi</a>
|
<a href=/people/a/andreas-rauber/>Andreas Rauber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--36><div class="card-body p-3 small">In this paper, we present our approach to solve the LongSumm 2020 Shared Task, at the 1st Workshop on Scholarly Document Processing. The objective of the long summaries task is to generate long summaries that cover salient information in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific articles</a>. The task is to generate abstractive and extractive summaries of a given <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific article</a>. In the proposed approach, we are inspired by the concept of Argumentative Zoning (AZ) that de- fines the main rhetorical structure in scientific articles. We define two aspects that should be covered in scientific paper summary, namely Claim / Method and Conclusion / Result aspects. We use Solr index to expand the sentences of the paper abstract. We formulate each abstract sentence in a given publication as query to retrieve similar sentences from the text body of the document itself. We utilize a sentence selection algorithm described in previous literature to select sentences for the final summary that covers the two aforementioned aspects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sdp-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sdp-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sdp-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sdp-1.40.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sdp-1.40" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.sdp-1.40/>Divide and Conquer : From <a href=https://en.wikipedia.org/wiki/Complexity>Complexity</a> to <a href=https://en.wikipedia.org/wiki/Simplicity>Simplicity</a> for Lay Summarization</a></strong><br><a href=/people/r/rochana-chaturvedi/>Rochana Chaturvedi</a>
|
<a href=/people/s/saachi/>Saachi .</a>
|
<a href=/people/j/jaspreet-singh-dhani/>Jaspreet Singh Dhani</a>
|
<a href=/people/a/anurag-joshi/>Anurag Joshi</a>
|
<a href=/people/a/ankush-khanna/>Ankush Khanna</a>
|
<a href=/people/n/neha-tomar/>Neha Tomar</a>
|
<a href=/people/s/swagata-duari/>Swagata Duari</a>
|
<a href=/people/a/alka-khurana/>Alka Khurana</a>
|
<a href=/people/v/vasudha-bhatnagar/>Vasudha Bhatnagar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sdp-1--40><div class="card-body p-3 small">We describe our approach for the 1st Computational Linguistics Lay Summary Shared Task CL-LaySumm20. The task is to produce non-technical summaries of scholarly documents. The summary should be within easy grasp of a layman who may not be well versed with the domain of the research article. We propose a two step divide-and-conquer approach. First, we judiciously select segments of the documents that are not overly pedantic and are likely to be of interest to the laity, and over-extract sentences from each segment using an unsupervised network based method. Next, we perform abstractive summarization on these <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>extractions</a> and systematically merge the <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>abstractions</a>. We run ablation studies to establish that each step in our pipeline is critical for improvement in the quality of lay summary. Our approach leverages state-of-the-art pre-trained deep neural network based models as zero-shot learners to achieve high scores on the task.</div></div></div><hr><div id=2020sigtyp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.sigtyp-1/>Proceedings of the Second Workshop on Computational Research in Linguistic Typology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigtyp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sigtyp-1.0/>Proceedings of the Second Workshop on Computational Research in Linguistic Typology</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/e/edoardo-m-ponti/>Edoardo M. Ponti</a>
|
<a href=/people/e/eitan-grossman/>Eitan Grossman</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigtyp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigtyp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigtyp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939790 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigtyp-1.1/>SIGTYP 2020 Shared Task : Prediction of Typological Features<span class=acl-fixed-case>SIGTYP</span> 2020 Shared Task: Prediction of Typological Features</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/g/giuseppe-g-a-celano/>Giuseppe G. A. Celano</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigtyp-1--1><div class="card-body p-3 small">Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world&#8217;s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>, and skewed, in that few <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> attracted 8 submissions from 5 teams, out of which the most successful <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> make use of such feature correlations. However, our <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigtyp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigtyp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigtyp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939792 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sigtyp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigtyp-1.4/>Predicting Typological Features in WALS using Language Embeddings and Conditional Probabilities : FAL Submission to the SIGTYP 2020 Shared Task<span class=acl-fixed-case>WALS</span> using Language Embeddings and Conditional Probabilities: <span class=acl-fixed-case>ÚFAL</span> Submission to the <span class=acl-fixed-case>SIGTYP</span> 2020 Shared Task</a></strong><br><a href=/people/m/martin-vastl/>Martin Vastl</a>
|
<a href=/people/d/daniel-zeman/>Daniel Zeman</a>
|
<a href=/people/r/rudolf-rosa/>Rudolf Rosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigtyp-1--4><div class="card-body p-3 small">We present our submission to the SIGTYP 2020 Shared Task on the prediction of typological features. We submit a constrained system, predicting typological features only based on the WALS database. We investigate two approaches. The simpler of the two is a system based on estimating correlation of feature values within languages by computing <a href=https://en.wikipedia.org/wiki/Conditional_probability>conditional probabilities</a> and <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>. The second approach is to train a neural predictor operating on precomputed language embeddings based on WALS features. Our submitted <a href=https://en.wikipedia.org/wiki/System>system</a> combines the two approaches based on their self-estimated confidence scores. We reach the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 70.7 % on the test data and rank first in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sigtyp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sigtyp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sigtyp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939794 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sigtyp-1.6/>NUIG : Multitasking Self-attention based approach to SigTyp 2020 Shared Task<span class=acl-fixed-case>NUIG</span>: Multitasking Self-attention based approach to <span class=acl-fixed-case>S</span>ig<span class=acl-fixed-case>T</span>yp 2020 Shared Task</a></strong><br><a href=/people/c/chinmay-choudhary/>Chinmay Choudhary</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sigtyp-1--6><div class="card-body p-3 small">The paper describes the Multitasking Self-attention based approach to constrained sub-task within Sigtyp 2020 Shared task. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is simple <a href=https://en.wikipedia.org/wiki/Neural_network>neural network based architecture</a> inspired by Transformers (CITATION) model. The model uses <a href=https://en.wikipedia.org/wiki/Computer_multitasking>Multitasking</a> to compute values of all WALS features for a given input language simultaneously. Results show that our approach performs at par with the baseline approaches, even though our proposed approach requires only phylogenetic and geographical attributes namely <a href=https://en.wikipedia.org/wiki/Longitude>Longitude</a>, <a href=https://en.wikipedia.org/wiki/Latitude>Latitude</a>, Genus-index, Family-index and Country-index and do not use any of the known WALS features of the respective input language, to compute its missing WALS features.<i>Multitasking Self-attention based approach</i> to constrained sub-task within Sigtyp 2020 Shared task. Our model is simple neural network based architecture inspired by Transformers (CITATION) model. The model uses Multitasking to compute values of all WALS features for a given input language simultaneously.\n\nResults show that our approach performs at par with the baseline approaches, even though our proposed approach requires only phylogenetic and geographical attributes namely <i>Longitude</i>, <i>Latitude</i>, <i>Genus-index</i>, <i>Family-index</i> and <i>Country-index</i> and do not use any of the known WALS features of the respective input language, to compute its missing WALS features.</div></div></div><hr><div id=2020splu-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.splu-1/>Proceedings of the Third International Workshop on Spatial Language Understanding</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.splu-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.splu-1.0/>Proceedings of the Third International Workshop on Spatial Language Understanding</a></strong><br><a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a>
|
<a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.splu-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--splu-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.splu-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940081 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.splu-1.1/>An Element-wise Visual-enhanced BiLSTM-CRF Model for Location Name Recognition<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Model for Location Name Recognition</a></strong><br><a href=/people/t/takuya-komada/>Takuya Komada</a>
|
<a href=/people/t/takashi-inui/>Takashi Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--splu-1--1><div class="card-body p-3 small">In recent years, previous studies have used <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> in <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition (NER)</a> for <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a> with attached images. However, these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can only be applied to documents with attached images. In this paper, we propose a NER method that can use element-wise visual information for any documents by using image data corresponding to each word in the document. The proposed method obtains element-wise image data using an <a href=https://en.wikipedia.org/wiki/Image_retrieval>image retrieval engine</a>, to be used as extra features in the neural NER model. Experimental results on the standard Japanese NER dataset show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves a higher F1 value (89.67 %) than a baseline method, demonstrating the effectiveness of using element-wise visual information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.splu-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--splu-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.splu-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940078 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.splu-1.2/>BERT-based Spatial Information Extraction<span class=acl-fixed-case>BERT</span>-based Spatial Information Extraction</a></strong><br><a href=/people/h/hyeong-jin-shin/>Hyeong Jin Shin</a>
|
<a href=/people/j/jeong-yeon-park/>Jeong Yeon Park</a>
|
<a href=/people/d/dae-bum-yuk/>Dae Bum Yuk</a>
|
<a href=/people/j/jae-sung-lee/>Jae Sung Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--splu-1--2><div class="card-body p-3 small">Spatial information extraction is essential to understand <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geographical information</a> in text. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is largely divided to two subtasks : spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018), which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT (Wu and He, 2019) for spatial relation extraction. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was evaluated with the SemEval 2015 dataset. The result showed a 15.4 % point increase in spatial element extraction and an 8.2 % point increase in spatial relation extraction in comparison to the baseline model (Nichols and Botros, 2015).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.splu-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--splu-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.splu-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940076 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.splu-1.4/>They Are Not All Alike : Answering Different Spatial Questions Requires Different Grounding Strategies</a></strong><br><a href=/people/a/alberto-testoni/>Alberto Testoni</a>
|
<a href=/people/c/claudio-greco/>Claudio Greco</a>
|
<a href=/people/t/tobias-bianchi/>Tobias Bianchi</a>
|
<a href=/people/m/mauricio-mazuecos/>Mauricio Mazuecos</a>
|
<a href=/people/a/agata-marcante/>Agata Marcante</a>
|
<a href=/people/l/luciana-benotti/>Luciana Benotti</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--splu-1--4><div class="card-body p-3 small">In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat? ! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by a large extent (9.70 % on spatial questions and 6.27 % overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.</div></div></div><hr><div id=2020spnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.spnlp-1/>Proceedings of the Fourth Workshop on Structured Prediction for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.0/>Proceedings of the Fourth Workshop on Structured Prediction for NLP</a></strong><br><a href=/people/p/priyanka-agrawal/>Priyanka Agrawal</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/g/gerasimos-lampouras/>Gerasimos Lampouras</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--spnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.spnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.spnlp-1.2.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940142 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.2/>CopyNext : Explicit Span Copying and Alignment in Sequence to Sequence Models<span class=acl-fixed-case>C</span>opy<span class=acl-fixed-case>N</span>ext: Explicit Span Copying and Alignment in Sequence to Sequence Models</a></strong><br><a href=/people/a/abhinav-singh/>Abhinav Singh</a>
|
<a href=/people/p/patrick-xia/>Patrick Xia</a>
|
<a href=/people/g/guanghui-qin/>Guanghui Qin</a>
|
<a href=/people/m/mahsa-yarmohammadi/>Mahsa Yarmohammadi</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--spnlp-1--2><div class="card-body p-3 small">Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with an explicit token-level copy operation and extend it to copying entire spans. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with an order of magnitude increase in decoding speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--spnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.spnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940154 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.5/>Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking</a></strong><br><a href=/people/a/anh-duong-trinh/>Anh Duong Trinh</a>
|
<a href=/people/r/robert-j-ross/>Robert J. Ross</a>
|
<a href=/people/j/john-kelleher/>John D. Kelleher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--spnlp-1--5><div class="card-body p-3 small">Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that : (i) modelling variable dependencies yields better results ; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--spnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.spnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.spnlp-1.7.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940156 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.7/>Layer-wise Guided Training for BERT : Learning Incrementally Refined Document Representations<span class=acl-fixed-case>BERT</span>: Learning Incrementally Refined Document Representations</a></strong><br><a href=/people/n/nikolaos-manginas/>Nikolaos Manginas</a>
|
<a href=/people/i/ilias-chalkidis/>Ilias Chalkidis</a>
|
<a href=/people/p/prodromos-malakasiotis/>Prodromos Malakasiotis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--spnlp-1--7><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor>BERT</a> is widely used by the <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP community</a>, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT&#8217;s over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune BERT in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results but also leads to better parameter utilization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--spnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.spnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.spnlp-1.10.OptionalSupplementaryMaterial.tex data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940144 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.spnlp-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.10/>On the Discrepancy between <a href=https://en.wikipedia.org/wiki/Density_estimation>Density Estimation</a> and Sequence Generation</a></strong><br><a href=/people/j/jason-lee/>Jason Lee</a>
|
<a href=/people/d/dustin-tran/>Dustin Tran</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--spnlp-1--10><div class="card-body p-3 small">Many sequence-to-sequence generation tasks, including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech</a>, can be posed as estimating the density of the output y given the input x : p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y * : R(y, y * | x). While we hope that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that excels in <a href=https://en.wikipedia.org/wiki/Density_estimation>density estimation</a> also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on <a href=https://en.wikipedia.org/wiki/Likelihood_function>log-likelihood</a> and BLEU varies significantly depending on the range of the model families being compared. First, <a href=https://en.wikipedia.org/wiki/Likelihood_function>log-likelihood</a> is highly correlated with <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> when we consider <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> within the same family (e.g. autoregressive models, or <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable models</a> with the same parameterization of the prior).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.spnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--spnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.spnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940152 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.spnlp-1.12/>Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering : A Practitioner’s Perspective</a></strong><br><a href=/people/a/arindam-mitra/>Arindam Mitra</a>
|
<a href=/people/s/sanjay-narayana/>Sanjay Narayana</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--spnlp-1--12><div class="card-body p-3 small">Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality. This has severely limited the application of knowledge representation and reasoning (KR) in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and also prevented a proper evaluation of KR based NLU systems. Our goal is to build KR based systems for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> without relying on a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>. Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, so the model can be trained end-to-end. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel). Our system achieves same <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as that of the state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system. The results show that the bias introduced by a KR solution does not prevent it from doing a better job at the end task. Moreover, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is interpretable due to the bias introduced by the KR approach.</div></div></div><hr><div id=2020sustainlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.sustainlp-1/>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.0/>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939419 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.1/>Knowing Right from Wrong : Should We Use More Complex <a href=https://en.wikipedia.org/wiki/Mathematical_model>Models</a> for Automatic Short-Answer Scoring in Bahasa Indonesia?<span class=acl-fixed-case>B</span>ahasa <span class=acl-fixed-case>I</span>ndonesia?</a></strong><br><a href=/people/a/ali-akbar-septiandri/>Ali Akbar Septiandri</a>
|
<a href=/people/y/yosef-ardhito-winatmoko/>Yosef Ardhito Winatmoko</a>
|
<a href=/people/i/ilham-firdausi-putra/>Ilham Firdausi Putra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--1><div class="card-body p-3 small">We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring : single classical, <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classical</a>, and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble models</a> and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2 % difference in <a href=https://en.wikipedia.org/wiki/F-number>F1</a> compared to the deep learning approach with 1/18 time for model training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939422 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.3/>Learning Informative Representations of Biomedical Relations with Latent Variable Models</a></strong><br><a href=/people/h/harshil-shah/>Harshil Shah</a>
|
<a href=/people/j/julien-fauqueur/>Julien Fauqueur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--3><div class="card-body p-3 small">Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a <a href=https://en.wikipedia.org/wiki/Point_estimation>point estimate</a> to represent the <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> ; this is then used as the input to a relation classifier. However, the <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> expressed in text between a pair of biomedical entities is often more complex than can be captured by a <a href=https://en.wikipedia.org/wiki/Point_estimate>point estimate</a>. To address this issue, we propose a <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable model</a> with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.4.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939423 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.4/>End to End Binarized Neural Networks for Text Classification</a></strong><br><a href=/people/k/kumar-shridhar/>Kumar Shridhar</a>
|
<a href=/people/h/harshil-jain/>Harshil Jain</a>
|
<a href=/people/a/akshat-agarwal/>Akshat Agarwal</a>
|
<a href=/people/d/denis-kleyko/>Denis Kleyko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--4><div class="card-body p-3 small">Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a> pose high requirements for <a href=https://en.wikipedia.org/wiki/Computer_hardware>computing hardware</a> and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is one way of addressing the issue of the increasing <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a>. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and text classification with a larger dataset. On the considered <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, the proposed <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> achieves comparable to the state-of-the-art results while utilizing 20-40 % lesser <a href=https://en.wikipedia.org/wiki/Computer_memory>memory</a> and training time compared to the benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939426 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.5/>Exploring the Boundaries of Low-Resource BERT Distillation<span class=acl-fixed-case>BERT</span> Distillation</a></strong><br><a href=/people/m/moshe-wasserblat/>Moshe Wasserblat</a>
|
<a href=/people/o/oren-pereg/>Oren Pereg</a>
|
<a href=/people/p/peter-izsak/>Peter Izsak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--5><div class="card-body p-3 small">In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on devices with limited resources is challenging due to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>&#8217; large <a href=https://en.wikipedia.org/wiki/Computation>computational consumption</a> and <a href=https://en.wikipedia.org/wiki/Computer_memory>memory requirements</a>. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, <a href=https://en.wikipedia.org/wiki/Load_(computing)>computational load</a> and <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a>. In this paper we test the boundaries of BERT model distillation in terms of <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a>, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.6.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939427 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.6/>Efficient Estimation of Influence of a Training Instance</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--6><div class="card-body p-3 small">Understanding the influence of a training instance on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> leads to improving <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. However, it is difficult and inefficient to evaluate the influence, which shows how a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s prediction would be changed if a training instance were not used. In this paper, we propose an efficient <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for estimating the influence. Our method is inspired by dropout, which zero-masks a <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> and prevents the <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939429 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.7/>Efficient Inference For Neural Machine Translation</a></strong><br><a href=/people/y/yi-te-hsu/>Yi-Te Hsu</a>
|
<a href=/people/s/sarthak-garg/>Sarthak Garg</a>
|
<a href=/people/y/yi-hsiu-liao/>Yi-Hsiu Liao</a>
|
<a href=/people/i/ilya-chatsviorkin/>Ilya Chatsviorkin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--7><div class="card-body p-3 small">Large Transformer models have achieved state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speed</a> without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109 % and 84 % speedup on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a> and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> respectively and reduce the number of parameters by 25 % while maintaining the same translation quality in terms of BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.8.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939430 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.8/>Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm</a></strong><br><a href=/people/a/alicia-tsai/>Alicia Tsai</a>
|
<a href=/people/l/laurent-el-ghaoui/>Laurent El Ghaoui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--8><div class="card-body p-3 small">We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated <a href=https://en.wikipedia.org/wiki/Frank-Wolfe_algorithm>Frank-Wolfe algorithm</a>. To generate a summary with k sentences, the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and works especially well when combined with <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for highly paraphrased summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939432 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.10/>A Two-stage Model for Slot Filling in Low-resource Settings : Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings</a></strong><br><a href=/people/c/cennet-oguz/>Cennet Oguz</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--10><div class="card-body p-3 small">Learning-based slot filling-a key component of spoken language understanding systems-typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as <a href=https://en.wikipedia.org/wiki/ELMO>ELMO</a> and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939433 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.11/>Early Exiting BERT for Efficient Document Ranking<span class=acl-fixed-case>BERT</span> for Efficient Document Ranking</a></strong><br><a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/r/rodrigo-nogueira/>Rodrigo Nogueira</a>
|
<a href=/people/y/yaoliang-yu/>Yaoliang Yu</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--11><div class="card-body p-3 small">Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for <a href=https://en.wikipedia.org/wiki/Document_ranking>document ranking</a>. With a slight modification, BERT becomes a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with multiple output paths, and each inference sample can exit early from these <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>paths</a>. In this way, <a href=https://en.wikipedia.org/wiki/Computation>computation</a> can be effectively allocated among samples, and overall <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>system latency</a> is significantly reduced while the original <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speedup</a> with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939436 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.14/>A Little Bit Is Worse Than None : Ranking with Limited Training Data</a></strong><br><a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--14><div class="card-body p-3 small">Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from <a href=https://en.wikipedia.org/wiki/Keyword_search>keyword search</a>. In this work, we tackle the challenge of fine-tuning these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using corpus-specific labeled data from sources such as TREC. We first answer the question : How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that some labeled in-domain data can be worse than none at all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939438 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.16/>Load What You Need : Smaller Versions of Mutililingual BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/amine-abdaoui/>Amine Abdaoui</a>
|
<a href=/people/c/camille-pradel/>Camille Pradel</a>
|
<a href=/people/g/gregoire-sigel/>Grégoire Sigel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--16><div class="card-body p-3 small">Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> is often a drawback for their deployment in <a href=https://en.wikipedia.org/wiki/Real-time_computing>real production applications</a>. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that keep comparable results, while reducing up to 45 % of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> induced a 1.7 % to 6 % drop in the overall accuracy on the XNLI data set. The presented <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and code are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939441 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.19/>Towards Accurate and Reliable Energy Measurement of NLP Models<span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/q/qingqing-cao/>Qingqing Cao</a>
|
<a href=/people/a/aruna-balasubramanian/>Aruna Balasubramanian</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--19><div class="card-body p-3 small">Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. We conduct energy measurement experiments with four different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for a <a href=https://en.wikipedia.org/wiki/Question_answering>question answering task</a>. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. We release the code and data at https://github.com/csarron/sustainlp2020-energy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.24/>Overview of the SustaiNLP 2020 Shared Task<span class=acl-fixed-case>S</span>ustai<span class=acl-fixed-case>NLP</span> 2020 Shared Task</a></strong><br><a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--24><div class="card-body p-3 small">We describe the SustaiNLP 2020 shared task : efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> as well as energy consumed in making predictions on the test sets. We describe the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, its organization, and the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a>. Across the six submissions to the shared task, participants achieved efficiency gains of 20 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.</div></div></div><hr><div id=2020wmt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wmt-1/>Proceedings of the Fifth Conference on Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wmt-1.0/>Proceedings of the Fifth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939545 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.12/>Tohoku-AIP-NTT at WMT 2020 News Translation Task<span class=acl-fixed-case>AIP</span>-<span class=acl-fixed-case>NTT</span> at <span class=acl-fixed-case>WMT</span> 2020 News Translation Task</a></strong><br><a href=/people/s/shun-kiyono/>Shun Kiyono</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/r/ryuto-konno/>Ryuto Konno</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--12><div class="card-body p-3 small">In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT&#8217;20 news translation task. We participated in this task in two language pairs and four language directions : <a href=https://en.wikipedia.org/wiki/German_language>English German</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>English Japanese</a>. Our system consists of techniques such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, which are already widely adopted in translation tasks. We attempted to develop new <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for both synthetic data filtering and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>. However, the <a href=https://en.wikipedia.org/wiki/Scientific_method>methods</a> turned out to be ineffective, and they provided us with no significant improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>. We analyze these negative results to provide insights for future studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939639 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.13/>NRC Systems for the 2020 Inuktitut-English News Translation Task<span class=acl-fixed-case>NRC</span> Systems for the 2020 <span class=acl-fixed-case>I</span>nuktitut-<span class=acl-fixed-case>E</span>nglish News Translation Task</a></strong><br><a href=/people/r/rebecca-knowles/>Rebecca Knowles</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--13><div class="card-body p-3 small">We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetuned transformer models, trained using the Nunavut Hansard and news data and, in the case of Inuktitut-English, backtranslated news and parliamentary data. In this work we explore challenges related to the relatively small amount of parallel data, morphological complexity, and domain shifts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939666 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.14/>CUNI Submission for the Inuktitut Language in WMT News 2020<span class=acl-fixed-case>CUNI</span> Submission for the <span class=acl-fixed-case>I</span>nuktitut Language in <span class=acl-fixed-case>WMT</span> News 2020</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--14><div class="card-body p-3 small">This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario InuktitutEnglish in both translation directions. Our system combines <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a CzechEnglish high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939661 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.17/>Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model : the UEDIN-CUNI Submission to the WMT 2020 News Translation Task<span class=acl-fixed-case>UEDIN</span>-<span class=acl-fixed-case>CUNI</span> Submission to the <span class=acl-fixed-case>WMT</span> 2020 News Translation Task</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/r/radina-dobreva/>Radina Dobreva</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--17><div class="card-body p-3 small">We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech / English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference efficiency</a>. On the WMT 2020 Czech English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single <a href=https://en.wikipedia.org/wiki/Thread_(computing)>CPU thread</a>, thus making neural translation feasible on consumer hardware without a <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939663 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.18/>The University of Edinburgh’s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s submission to the <span class=acl-fixed-case>G</span>erman-to-<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>E</span>nglish-to-<span class=acl-fixed-case>G</span>erman Tracks in the <span class=acl-fixed-case>WMT</span> 2020 News Translation and Zero-shot Translation Robustness Tasks</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--18><div class="card-body p-3 small">This paper describes the University of Edinburgh&#8217;s submission of German-English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939657 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.22/>SJTU-NICT’s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task<span class=acl-fixed-case>SJTU</span>-<span class=acl-fixed-case>NICT</span>’s Supervised and Unsupervised Neural Machine Translation Systems for the <span class=acl-fixed-case>WMT</span>20 News Translation Task</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--22><div class="card-body p-3 small">In this paper, we introduced our joint team SJTU-NICT &#8216;s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs : English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques : document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>. In our submissions, the primary systems won the first place on <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and German to Upper Sorbian translation directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939668 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.28/>CUNI English-Czech and English-Polish Systems in WMT20 : Robust Document-Level Training<span class=acl-fixed-case>CUNI</span> <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>zech and <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>P</span>olish Systems in <span class=acl-fixed-case>WMT</span>20: Robust Document-Level Training</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--28><div class="card-body p-3 small">We describe our two NMT systems submitted to the WMT 2020 shared task in English-Czech and English-Polish news translation. One <a href=https://en.wikipedia.org/wiki/System>system</a> is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>multi-sentence sequences</a> up to 3000 characters long.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939558 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.30/>OPPO’s Machine Translation Systems for WMT20<span class=acl-fixed-case>OPPO</span>’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/t/tingxun-shi/>Tingxun Shi</a>
|
<a href=/people/s/shiyu-zhao/>Shiyu Zhao</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/x/xiaoxue-wang/>Xiaoxue Wang</a>
|
<a href=/people/q/qian-zhang/>Qian Zhang</a>
|
<a href=/people/d/di-ai/>Di Ai</a>
|
<a href=/people/d/dawei-dang/>Dawei Dang</a>
|
<a href=/people/x/xue-zhengshan/>Xue Zhengshan</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--30><div class="card-body p-3 small">In this paper we demonstrate our (OPPO&#8217;s) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts : the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English Czech, English Russian, French German and Tamil English), third in 2 directions (English German, English Japanese), and fourth in 2 directions (English Pashto and and English Tamil).<tex-math>\\leftrightarrow</tex-math> Czech, English <tex-math>\\leftrightarrow</tex-math> Russian, French <tex-math>\\rightarrow</tex-math> German and Tamil <tex-math>\\rightarrow</tex-math> English), third in 2 directions (English <tex-math>\\rightarrow</tex-math> German, English <tex-math>\\rightarrow</tex-math> Japanese), and fourth in 2 directions (English <tex-math>\\rightarrow</tex-math> Pashto and and English <tex-math>\\rightarrow</tex-math> Tamil).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939573 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.31/>HW-TSC’s Participation in the WMT 2020 News Translation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2020 News Translation Shared Task</a></strong><br><a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a>
|
<a href=/people/s/shiliang-sun/>Shiliang Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--31><div class="card-body p-3 small">This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh / En, Km / En, and Ps / En and in both directions under the constrained condition. We use the standard Transformer-Big model as the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939581 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.33/>The Volctrans Machine Translation System for WMT20<span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/l/liwei-wu/>Liwei Wu</a>
|
<a href=/people/x/xiao-pan/>Xiao Pan</a>
|
<a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/y/yaoming-zhu/>Yaoming Zhu</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--33><div class="card-body p-3 small">This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer (CITATION), into which we also employed new architectures (bigger or deeper Transformers, dynamic convolution). The final systems include text pre-process, subword(a.k.a. BPE(CITATION)), baseline model training, iterative back-translation, model ensemble, knowledge distillation and multilingual pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939572 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.37/>The NiuTrans Machine Translation Systems for WMT20<span class=acl-fixed-case>N</span>iu<span class=acl-fixed-case>T</span>rans Machine Translation Systems for <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/z/ziyang-wang/>Ziyang Wang</a>
|
<a href=/people/r/runzhe-cao/>Runzhe Cao</a>
|
<a href=/people/b/binghao-wei/>Binghao Wei</a>
|
<a href=/people/w/weiqiao-shan/>Weiqiao Shan</a>
|
<a href=/people/s/shuhan-zhou/>Shuhan Zhou</a>
|
<a href=/people/a/abudurexiti-reheman/>Abudurexiti Reheman</a>
|
<a href=/people/t/tao-zhou/>Tao Zhou</a>
|
<a href=/people/x/xin-zeng/>Xin Zeng</a>
|
<a href=/people/l/laohu-wang/>Laohu Wang</a>
|
<a href=/people/y/yongyu-mu/>Yongyu Mu</a>
|
<a href=/people/j/jingnan-zhang/>Jingnan Zhang</a>
|
<a href=/people/x/xiaoqian-liu/>Xiaoqian Liu</a>
|
<a href=/people/x/xuanjun-zhou/>Xuanjun Zhou</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--37><div class="card-body p-3 small">This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese-English, English-Chinese, Inuktitut-English and Tamil-English total five tasks and rank first in Japanese-English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut-English and Tamil-English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939659 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.39/>Gender Coreference and Bias Evaluation at WMT 2020<span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--39><div class="card-body p-3 small">Gender bias in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to languages with <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender</a>. We extend WinoMT to handle two new <a href=https://en.wikipedia.org/wiki/Language>languages</a> tested in WMT : <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We find that all <a href=https://en.wikipedia.org/wiki/System>systems</a> consistently use spurious correlations in the data rather than meaningful <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.42/>Translating Similar Languages : Role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>Mutual Intelligibility</a> in Multilingual Transformers</a></strong><br><a href=/people/i/ife-adebara/>Ife Adebara</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--42><div class="card-body p-3 small">In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and used <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. We also investigate the role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> in <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939595 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.47/>The IPN-CIC team system submission for the WMT 2020 similar language task<span class=acl-fixed-case>IPN</span>-<span class=acl-fixed-case>CIC</span> team system submission for the <span class=acl-fixed-case>WMT</span> 2020 similar language task</a></strong><br><a href=/people/l/luis-a-menendez-salazar/>Luis A. Menéndez-Salazar</a>
|
<a href=/people/g/grigori-sidorov/>Grigori Sidorov</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-Jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--47><div class="card-body p-3 small">This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> for the Spanish-Portuguese language pair (in both directions). The three submitted systems are based on the Transformer architecture and used fine tuning for <a href=https://en.wikipedia.org/wiki/Domain_Adaptation>domain Adaptation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939638 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.49/>NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020<span class=acl-fixed-case>NUIG</span>-Panlingua-<span class=acl-fixed-case>KMI</span> <span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>M</span>arathi <span class=acl-fixed-case>MT</span> Systems for Similar Language Translation Task @ <span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a>
|
<a href=/people/a/akanksha-bansal/>Akanksha Bansal</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--49><div class="card-body p-3 small">NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for HindiMarathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for HindiMarathi each and 1 NMT systems were developed for <a href=https://en.wikipedia.org/wiki/Marathi_language>HindiMarathi</a> using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939608 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.53/>Document Level NMT of Low-Resource Languages with Backtranslation<span class=acl-fixed-case>NMT</span> of Low-Resource Languages with Backtranslation</a></strong><br><a href=/people/s/sami-ul-haq/>Sami Ul Haq</a>
|
<a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/a/arsalan-shaukat/>Arsalan Shaukat</a>
|
<a href=/people/a/abdullah-saeed/>Abdullah Saeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--53><div class="card-body p-3 small">This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair MarathiHindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> with <a href=https://en.wikipedia.org/wiki/Back_translation>back translation</a> to train our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.56/>The University of Maryland’s Submissions to the WMT20 Chat Translation Task : Searching for More Data to Adapt Discourse-Aware Neural Machine Translation<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>aryland’s Submissions to the <span class=acl-fixed-case>WMT</span>20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation</a></strong><br><a href=/people/c/calvin-bao/>Calvin Bao</a>
|
<a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/c/chujun-song/>Chujun Song</a>
|
<a href=/people/j/jie-li/>Jie Li</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--56><div class="card-body p-3 small">This paper describes the University of Maryland&#8217;s submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformer</a>, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on the agent side (ende), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939588 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.62/>Fast Interleaved Bidirectional Sequence Generation</a></strong><br><a href=/people/b/biao-zhang/>Biao Zhang</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--62><div class="card-body p-3 small">Independence assumptions during sequence generation can speed up <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ~2x compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independence assumptions</a> in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4x11x across different <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> at the cost of 1 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or 0.5 ROUGE (on average)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939559 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.70" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.70/>Towards Multimodal Simultaneous Neural Machine Translation</a></strong><br><a href=/people/a/aizhan-imankulova/>Aizhan Imankulova</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/t/tosho-hirasawa/>Tosho Hirasawa</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--70><div class="card-body p-3 small">Simultaneous translation involves translating a sentence before the speaker&#8217;s utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wmt-1.74.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939560 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.74" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.74/>Document-aligned Japanese-English Conversation Parallel Corpus<span class=acl-fixed-case>J</span>apanese-<span class=acl-fixed-case>E</span>nglish Conversation Parallel Corpus</a></strong><br><a href=/people/m/matiss-rikters/>Matīss Rikters</a>
|
<a href=/people/r/ryokan-ri/>Ryokan Ri</a>
|
<a href=/people/t/tong-li/>Tong Li</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--74><div class="card-body p-3 small">Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data ; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to demonstrate how using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> leads to improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939672 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.75/>Findings of the WMT 2020 Shared Task on Automatic Post-Editing<span class=acl-fixed-case>WMT</span> 2020 Shared Task on Automatic Post-Editing</a></strong><br><a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--75><div class="card-body p-3 small">We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a black-box machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source / domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year&#8217;s results are not directly comparable with last year&#8217;s round. However, on both language directions, participants&#8217; submissions show considerable improvements over the baseline results. On <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, the top ranked system improves over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by -11.35 TER and +16.68 BLEU points, while on <a href=https://en.wikipedia.org/wiki/Chinese_language>EnglishChinese</a> the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year&#8217;s round.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wmt-1.77/>Results of the WMT20 Metrics Shared Task<span class=acl-fixed-case>WMT</span>20 Metrics Shared Task</a></strong><br><a href=/people/n/nitika-mathur/>Nitika Mathur</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--77><div class="card-body p-3 small">This paper presents the results of the WMT20 Metrics Shared Task. Participants were asked to score the outputs of the translation systems competing in the WMT20 News Translation Task with <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. Ten research groups submitted 27 metrics, four of which are <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>reference-less metrics</a>. In addition, we computed five baseline metrics, including sentBLEU, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, TER and using the SacreBLEU scorer. All <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> were evaluated on how well they correlate at the system-, document- and segment-level with the WMT20 official human scores. We present an extensive analysis on influence of different reference translations on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric reliability</a>, how well automatic metrics score human translations, and we also flag major discrepancies between <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> and human scores when evaluating MT systems. Finally, we investigate whether we can use <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> to flag incorrect human ratings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939547 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.81/>Cross-Lingual Transformers for Neural Automatic Post-Editing</a></strong><br><a href=/people/d/dongjun-lee/>Dongjun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--81><div class="card-body p-3 small">In this paper, we describe the Bering Lab&#8217;s submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (MT) sentence as an input to generate the post-edited (PE) output. For further improvement, we mask incorrect or missing words in the PE output based on word-level quality estimation and then predict the actual word for each mask based on the fine-tuned cross-lingual language model (XLM-RoBERTa). Finally, to address the over-correction problem, we select the final output among the PE outputs and the original MT sentence based on a sentence-level quality estimation. When evaluated on the WMT 2020 English-German APE test dataset, our <a href=https://en.wikipedia.org/wiki/System>system</a> improves the NMT output by -3.95 and +4.50 in terms of <a href=https://en.wikipedia.org/wiki/Terminology_of_the_Low_Countries>TER</a> and BLEU, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939561 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.82/>POSTECH-ETRI’s Submission to the WMT2020 APE Shared Task : Automatic Post-Editing with Cross-lingual Language Model<span class=acl-fixed-case>POSTECH</span>-<span class=acl-fixed-case>ETRI</span>’s Submission to the <span class=acl-fixed-case>WMT</span>2020 <span class=acl-fixed-case>APE</span> Shared Task: Automatic Post-Editing with Cross-lingual Language Model</a></strong><br><a href=/people/j/jihyung-lee/>Jihyung Lee</a>
|
<a href=/people/w/wonkee-lee/>WonKee Lee</a>
|
<a href=/people/j/jaehun-shin/>Jaehun Shin</a>
|
<a href=/people/b/baikjin-jung/>Baikjin Jung</a>
|
<a href=/people/y/young-gil-kim/>Young-Kil Kim</a>
|
<a href=/people/j/jong-hyeok-lee/>Jong-Hyeok Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--82><div class="card-body p-3 small">This paper describes POSTECH-ETRI&#8217;s submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs : English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage ; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by <a href=https://en.wikipedia.org/wiki/Terminology>TER</a> of -3.58 and a BLEU score of +5.3 for the <a href=https://en.wikipedia.org/wiki/Terminology>En-De subtask</a> ; and <a href=https://en.wikipedia.org/wiki/Terminology>TER</a> of -5.29 and a BLEU score of +7.32 for the <a href=https://en.wikipedia.org/wiki/Terminology>En-Zh subtask</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939622 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.84/>Alibaba’s Submission for the WMT 2020 APE Shared Task : Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT<span class=acl-fixed-case>A</span>libaba’s Submission for the <span class=acl-fixed-case>WMT</span> 2020 <span class=acl-fixed-case>APE</span> Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/j/jiayi-wang/>Jiayi Wang</a>
|
<a href=/people/k/ke-wang/>Ke Wang</a>
|
<a href=/people/k/kai-fan/>Kai Fan</a>
|
<a href=/people/y/yuqi-zhang/>Yuqi Zhang</a>
|
<a href=/people/j/jun-lu/>Jun Lu</a>
|
<a href=/people/x/xin-ge/>Xin Ge</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/y/yu-zhao/>Yu Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--84><div class="card-body p-3 small">The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba&#8217;s submissions to the WMT 2020 APE Shared Task for the English-German language pair. We design a two-stage training pipeline. First, a BERT-like cross-lingual language model is pre-trained by randomly masking target sentences alone. Then, an additional neural decoder on the top of the pre-trained model is jointly fine-tuned for the APE task. We also apply an imitation learning strategy to augment a reasonable amount of pseudo APE training data, potentially preventing the model to overfit on the limited real training data and boosting the performance on held-out data. To verify our proposed model and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, we examine our approach with the well-known benchmarking English-German dataset from the WMT 2017 APE task. The experiment results demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> significantly outperforms all other <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and achieves the state-of-the-art performance. The final results on the WMT 2020 test dataset show that our <a href=https://en.wikipedia.org/wiki/Subscription_business_model>submission</a> can achieve +5.56 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a> and -4.57 TER with respect to the official MT baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939618 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.86/>LIMSI @ WMT 2020<span class=acl-fixed-case>LIMSI</span> @ <span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/j/jose-carlos-rosales-nunez/>José Carlos Rosales Núñez</a>
|
<a href=/people/m/minh-quang-pham/>Minh Quang Pham</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--86><div class="card-body p-3 small">This paper describes LIMSI&#8217;s submissions to the translation shared tasks at WMT&#8217;20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/French_language>French</a>, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/German_language>German</a> ; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions : zero-shot and few-shot domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939591 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.87/>Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation</a></strong><br><a href=/people/a/ander-corral/>Ander Corral</a>
|
<a href=/people/x/xabier-saralegi/>Xabier Saralegi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--87><div class="card-body p-3 small">This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine <a href=https://en.wikipedia.org/wiki/Open_data>open domain data</a> with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The <a href=https://en.wikipedia.org/wiki/System>systems</a> presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wmt-1.88.OptionalSupplementaryMaterial.tgz data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939644 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.88/>YerevaNN’s Systems for WMT20 Biomedical Translation Task : The Effect of Fixing Misaligned Sentence Pairs<span class=acl-fixed-case>Y</span>ereva<span class=acl-fixed-case>NN</span>’s Systems for <span class=acl-fixed-case>WMT</span>20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs</a></strong><br><a href=/people/k/karen-hambardzumyan/>Karen Hambardzumyan</a>
|
<a href=/people/h/hovhannes-tamoyan/>Hovhannes Tamoyan</a>
|
<a href=/people/h/hrant-khachatrian/>Hrant Khachatrian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--88><div class="card-body p-3 small">This report describes YerevaNN&#8217;s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide <a href=https://en.wikipedia.org/wiki/Language_planning>systems</a> for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with enru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.<tex-math>\\rightarrow</tex-math>ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939562 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.89/>Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>B</span>asque Biomedical Neural Machine Translation</a></strong><br><a href=/people/i/inigo-jauregi-unanue/>Inigo Jauregi Unanue</a>
|
<a href=/people/m/massimo-piccardi/>Massimo Piccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--89><div class="card-body p-3 small">This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training techniques</a>, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939645 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.90/>Lite Training Strategies for Portuguese-English and English-Portuguese Translation<span class=acl-fixed-case>P</span>ortuguese-<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>P</span>ortuguese Translation</a></strong><br><a href=/people/a/alexandre-lopes/>Alexandre Lopes</a>
|
<a href=/people/r/rodrigo-nogueira/>Rodrigo Nogueira</a>
|
<a href=/people/r/roberto-lotufo/>Roberto Lotufo</a>
|
<a href=/people/h/helio-pedrini/>Helio Pedrini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--90><div class="card-body p-3 small">Despite the widespread adoption of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as <a href=https://en.wikipedia.org/wiki/Diaeresis_(diacritic)>diaeresis</a>, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> have a competitive performance to state-of-the-art <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> while being trained on modest hardware (a single 8 GB gaming GPU for nine days). Our <a href=https://en.wikipedia.org/wiki/Data>data</a>, models and code are available in our GitHub repository.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939583 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.94/>Addressing Exposure Bias With Document Minimum Risk Training : Cambridge at the WMT20 Biomedical Translation Task<span class=acl-fixed-case>C</span>ambridge at the <span class=acl-fixed-case>WMT</span>20 Biomedical Translation Task</a></strong><br><a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--94><div class="card-body p-3 small">The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such <a href=https://en.wikipedia.org/wiki/Data>data</a> are susceptible to <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias effects</a>, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> if the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns to neglect the source sentence. The UNICAM entry addresses this problem during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove &#8216;problem&#8217; training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wmt-1.101.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939604 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.101/>Unbabel’s Participation in the WMT20 Metrics Shared Task<span class=acl-fixed-case>WMT</span>20 Metrics Shared Task</a></strong><br><a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/c/craig-stewart/>Craig Stewart</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/a/alon-lavie/>Alon Lavie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--101><div class="card-body p-3 small">We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the QE as a Metric track. Accordingly, we illustrate results of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in these <a href=https://en.wikipedia.org/wiki/Track_(rail_transport)>tracks</a> with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework : we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieve strong results for all language pairs on previous test sets and in many cases set a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939565 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.104/>Incorporate Semantic Structures into Machine Translation Evaluation via <a href=https://en.wikipedia.org/wiki/UCCA>UCCA</a><span class=acl-fixed-case>UCCA</span></a></strong><br><a href=/people/j/jin-xu/>Jin Xu</a>
|
<a href=/people/y/yinuo-guo/>Yinuo Guo</a>
|
<a href=/people/j/junfeng-hu/>Junfeng Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--104><div class="card-body p-3 small">Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939606 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.105/>Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning</a></strong><br><a href=/people/h/haluk-acarcicek/>Haluk Açarçiçek</a>
|
<a href=/people/t/talha-colakoglu/>Talha Çolakoğlu</a>
|
<a href=/people/p/pinar-ece-aktan-hatipoglu/>Pınar Ece Aktan Hatipoğlu</a>
|
<a href=/people/c/chong-hsuan-huang/>Chong Hsuan Huang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--105><div class="card-body p-3 small">This paper illustrates Huawei&#8217;s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the filtering capability for noisy parallel corpora. Such a supervised task also helps us to iterate much more quickly than using an existing <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation system</a> to perform the same <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. After performing empirical analyses of the finetuning task, we benchmark our approach by comparing the results with past years&#8217; state-of-theart records. This paper wraps up with a discussion of limitations and future work. The scripts for this study will be made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939612 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.106/>Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions</a></strong><br><a href=/people/m/muhammad-elnokrashy/>Muhammad ElNokrashy</a>
|
<a href=/people/a/amr-hendy/>Amr Hendy</a>
|
<a href=/people/m/mohamed-abdelghaffar/>Mohamed Abdelghaffar</a>
|
<a href=/people/m/mohamed-afify/>Mohamed Afify</a>
|
<a href=/people/a/ahmed-tawfik/>Ahmed Tawfik</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan Awadalla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--106><div class="card-body p-3 small">This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> built to distinguish positive and negative pairs and the original scores provided with the task. For the mBART setup, provided by the organizers, our method shows 7 % and 5 % relative improvement, over the baseline, in sacreBLEU score on the test set for <a href=https://en.wikipedia.org/wiki/Pashto>Pashto</a> and <a href=https://en.wikipedia.org/wiki/Khmer_language>Khmer</a> respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939649 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.108/>An exploratory approach to the Parallel Corpus Filtering shared task WMT20<span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/a/ankur-kejriwal/>Ankur Kejriwal</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--108><div class="card-body p-3 small">In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre / post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939610 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.113/>PATQUEST : Papago Translation Quality Estimation<span class=acl-fixed-case>PATQUEST</span>: Papago Translation Quality Estimation</a></strong><br><a href=/people/y/yujin-baek/>Yujin Baek</a>
|
<a href=/people/z/zae-myung-kim/>Zae Myung Kim</a>
|
<a href=/people/j/jihyung-moon/>Jihyung Moon</a>
|
<a href=/people/h/hyunjoong-kim/>Hyunjoong Kim</a>
|
<a href=/people/e/eunjeong-park/>Eunjeong Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--113><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation : (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>errors</a> that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment ; EN-DE only), and Task 3 (Document-Level Score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939546 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.118/>Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation</a></strong><br><a href=/people/d/dongjun-lee/>Dongjun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--118><div class="card-body p-3 small">In this paper, we describe the Bering Lab&#8217;s submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a huge artificially generated QE dataset, and then we fine-tune the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939643 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.119/>IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel Participation in the <span class=acl-fixed-case>WMT</span>20 Quality Estimation Shared Task</a></strong><br><a href=/people/j/joao-moura/>João Moura</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--119><div class="card-body p-3 small">We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939607 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.122/>TransQuest at WMT2020 : Sentence-Level Direct Assessment<span class=acl-fixed-case>T</span>rans<span class=acl-fixed-case>Q</span>uest at <span class=acl-fixed-case>WMT</span>2020: Sentence-Level Direct Assessment</a></strong><br><a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orasan</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--122><div class="card-body p-3 small">This paper presents the team TransQuest&#8217;s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> used in the shared task. We further fine tune the QE framework by performing ensemble and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939609 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.124/>Tencent submission for WMT20 Quality Estimation Shared Task<span class=acl-fixed-case>WMT</span>20 Quality Estimation Shared Task</a></strong><br><a href=/people/h/haijiang-wu/>Haijiang Wu</a>
|
<a href=/people/z/zixuan-wang/>Zixuan Wang</a>
|
<a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/x/xinjie-wen/>Xinjie Wen</a>
|
<a href=/people/r/ruichen-wang/>Ruichen Wang</a>
|
<a href=/people/x/xiaoli-wang/>Xiaoli Wang</a>
|
<a href=/people/y/yulin-zhang/>Yulin Zhang</a>
|
<a href=/people/z/zhipeng-yao/>Zhipeng Yao</a>
|
<a href=/people/s/siyao-peng/>Siyao Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--124><div class="card-body p-3 small">This paper presents Tencent&#8217;s submission to the WMT20 Quality Estimation (QE) Shared Task : Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM ; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications : using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> of 0.664, ranking first (tied) on <a href=https://en.wikipedia.org/wiki/Standard_Chinese>English-Chinese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939625 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.126/>NLPRL System for Very Low Resource Supervised Machine Translation<span class=acl-fixed-case>NLPRL</span> System for Very Low Resource Supervised Machine Translation</a></strong><br><a href=/people/r/rupjyoti-baruah/>Rupjyoti Baruah</a>
|
<a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/a/amit-kumar/>Amit Kumar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--126><div class="card-body p-3 small">This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939584 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.129/>UdS-DFKI@WMT20 : Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DFKI</span>@<span class=acl-fixed-case>WMT</span>20: Unsupervised <span class=acl-fixed-case>MT</span> and Very Low Resource Supervised <span class=acl-fixed-case>MT</span> for <span class=acl-fixed-case>G</span>erman-<span class=acl-fixed-case>U</span>pper <span class=acl-fixed-case>S</span>orbian</a></strong><br><a href=/people/s/sourav-dutta/>Sourav Dutta</a>
|
<a href=/people/j/jesujoba-alabi/>Jesujoba Alabi</a>
|
<a href=/people/s/saptarashmi-bandyopadhyay/>Saptarashmi Bandyopadhyay</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--129><div class="card-body p-3 small">This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submit <a href=https://en.wikipedia.org/wiki/System>systems</a> for both the supervised and unsupervised tracks. Apart from various experimental approaches like bitext mining, model pre-training, and iterative back-translation, we employ a factored machine translation approach on a small BPE vocabulary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939641 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.133/>CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20<span class=acl-fixed-case>CUNI</span> Systems for the Unsupervised and Very Low Resource Translation Task in <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/i/ivana-kvapilikova/>Ivana Kvapilíková</a>
|
<a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--133><div class="card-body p-3 small">This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between <a href=https://en.wikipedia.org/wiki/German_language>German</a> and Upper Sorbian. We experimented with training on <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939589 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.134/>The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki and Aalto University submissions to the <span class=acl-fixed-case>WMT</span> 2020 news and low-resource translation tasks</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/stig-arne-gronroos/>Stig-Arne Grönroos</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--134><div class="card-body p-3 small">This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020 : the news translation between Inuktitut and <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the low-resource translation between <a href=https://en.wikipedia.org/wiki/German_language>German</a> and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling</a>. Our submission obtained the highest score for <a href=https://en.wikipedia.org/wiki/Upper_Sorbian>Upper Sorbian-German</a> and was ranked second for German-Upper Sorbian according to <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a>. For EnglishInuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939575 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.135/>The NITS-CNLP System for the Unsupervised MT Task at WMT 2020<span class=acl-fixed-case>NITS</span>-<span class=acl-fixed-case>CNLP</span> System for the Unsupervised <span class=acl-fixed-case>MT</span> Task at <span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/s/salam-michael-singh/>Salam Michael Singh</a>
|
<a href=/people/t/thoudam-doren-singh/>Thoudam Doren Singh</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--135><div class="card-body p-3 small">We describe NITS-CNLP&#8217;s submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> using monolingual data from both the languages by jointly pre-training the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> and fine-tune using backtranslation loss. The final model uses the source side (de) monolingual data and the target side (hsb) synthetic data as a pseudo-parallel data to train a pseudo-supervised system which is tuned using the provided development set(dev set).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939621 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.136/>Adobe AMPS’s Submission for Very Low Resource Supervised Translation Task at WMT20<span class=acl-fixed-case>AMPS</span>’s Submission for Very Low Resource Supervised Translation Task at <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/k/keshaw-singh/>Keshaw Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--136><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>systems</a> submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pretraining</a> on a synthetic, backtranslated corpus followed by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on the original parallel training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939593 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.140/>Human-Paraphrased References Improve Neural Machine Translation</a></strong><br><a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--140><div class="card-body p-3 small">Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric scores</a> that correlate better with <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939650 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.141" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.141/>Incorporating Terminology Constraints in Automatic Post-Editing</a></strong><br><a href=/people/d/david-wan/>David Wan</a>
|
<a href=/people/c/chris-kedzie/>Chris Kedzie</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--141><div class="card-body p-3 small">Users of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> may want to ensure the use of specific <a href=https://en.wikipedia.org/wiki/Terminology>lexical terminologies</a>. While there exist techniques for incorporating terminology constraints during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> for MT, current APE approaches can not ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95 % of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>.</div></div></div><hr><div id=2020wnut-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wnut-1/>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.0/>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.5/>Combining BERT with Static Word Embeddings for Categorizing Social Media<span class=acl-fixed-case>BERT</span> with Static Word Embeddings for Categorizing Social Media</a></strong><br><a href=/people/i/israa-alghanmi/>Israa Alghanmi</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--5><div class="card-body p-3 small">Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the <a href=https://en.wikipedia.org/wiki/Social_media>social media genre</a>, despite the fact that <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> often has very different characteristics from the language that LMs have seen during training. A particularly striking example is the performance of AraBERT, an LM for the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>, which is successful in categorizing social media posts in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic dialects</a>, despite only having been trained on Modern Standard <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. Our hypothesis in this paper is that the performance of LMs for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can nonetheless be improved by incorporating static word vectors that have been specifically trained on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We show that a simple method for incorporating such word vectors is indeed successful in several Arabic and English benchmarks. Curiously, however, we also find that similar improvements are possible with word vectors that have been trained on traditional text sources (e.g. Wikipedia).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.7/>PHINC : A Parallel Hinglish Social Media Code-Mixed Corpus for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a><span class=acl-fixed-case>PHINC</span>: A Parallel <span class=acl-fixed-case>H</span>inglish Social Media Code-Mixed Corpus for Machine Translation</a></strong><br><a href=/people/v/vivek-srivastava/>Vivek Srivastava</a>
|
<a href=/people/m/mayank-singh/>Mayank Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--7><div class="card-body p-3 small">Code-mixing is the phenomenon of using more than one language in a sentence. In the <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual communities</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is a very frequently observed pattern of communication on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, the noisy user-generated code-mixed text adds to the challenge of processing and understanding <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to a much larger extent. Machine translation from monolingual source to the target language is a well-studied research problem. Here, we demonstrate that widely popular and sophisticated translation systems such as <a href=https://en.wikipedia.org/wiki/Google_Translate>Google Translate</a> fail at times to translate code-mixed text effectively. To address this challenge, we present a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> of the 13,738 code-mixed Hindi-English sentences and their corresponding human translation in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In addition, we also propose a translation pipeline build on top of <a href=https://en.wikipedia.org/wiki/Google_Translate>Google Translate</a>. The evaluation of the proposed <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> on PHINC demonstrates an increase in the performance of the underlying <a href=https://en.wikipedia.org/wiki/System>system</a>. With minimal effort, we can extend the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the proposed approach to other code-mixing language pairs.<tex-math>PHINC</tex-math> demonstrates an increase in the performance of the underlying system. With minimal effort, we can extend the dataset and the proposed approach to other code-mixing language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.11/>Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach</a></strong><br><a href=/people/y/yasuhiro-yamaguchi/>Yasuhiro Yamaguchi</a>
|
<a href=/people/s/shintaro-inuzuka/>Shintaro Inuzuka</a>
|
<a href=/people/m/makoto-hiramatsu/>Makoto Hiramatsu</a>
|
<a href=/people/j/jun-harashima/>Jun Harashima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--11><div class="card-body p-3 small">Recently, the number of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated recipes</a> on the Internet has increased. In such <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a>, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated recipe</a> are not actually edible ingredients. For example, headings, comments, and <a href=https://en.wikipedia.org/wiki/Kitchenware>kitchenware</a> sometimes appear in an ingredient list because users can freely write the list in their recipes. Such noise makes it difficult for computers to use <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> for a variety of <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>, such as calorie estimation. To address this issue, we propose a non-ingredient detection method inspired by a neural sequence tagging model. In our experiment, we annotated 6,675 ingredients in 600 user-generated recipes and showed that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieved a 93.3 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.14/>An Empirical Analysis of Human-Bot Interaction on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/m/ming-cheng-ma/>Ming-Cheng Ma</a>
|
<a href=/people/j/john-p-lalor/>John P. Lalor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--14><div class="card-body p-3 small">Automated agents (bots) have emerged as an ubiquitous and influential presence on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Bots engage on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of the activity of a single <a href=https://en.wikipedia.org/wiki/Internet_bot>bot</a> on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Our goal is to determine whether bot activity (in the form of posted comments on the website) has an effect on how humans engage on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. We find that (1) the sentiment of a bot comment has a significant, positive effect on the subsequent human reply, and (2) human Reddit users modify their comment behaviors to overlap with the text of the bot, similar to how humans modify their text to mimic other humans in conversation. Understanding human-bot interactions on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> with relatively simple <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a> is important for preparing for more advanced <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a> in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.15/>Detecting Trending Terms in Cybersecurity Forum Discussions</a></strong><br><a href=/people/j/jack-hughes/>Jack Hughes</a>
|
<a href=/people/s/seth-aycock/>Seth Aycock</a>
|
<a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a>
|
<a href=/people/a/alice-hutchings/>Alice Hutchings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--15><div class="card-body p-3 small">We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground hacking forum, spanning over ten years of activity, with posts containing misspellings, <a href=https://en.wikipedia.org/wiki/Orthography>orthographic variation</a>, <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a>, and <a href=https://en.wikipedia.org/wiki/Slang>slang</a>. Our statistical approach supports analysis of linguistic change and discussion topics over time, without a requirement to train a <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a> for each time interval for analysis. We evaluate the approach by comparing the results to TF-IDF using the discounted cumulative gain metric with human annotations, finding our method outperforms TF-IDF on <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.18/>Punctuation Restoration using Transformer Models for High-and Low-Resource Languages</a></strong><br><a href=/people/t/tanvirul-alam/>Tanvirul Alam</a>
|
<a href=/people/a/akib-khan/>Akib Khan</a>
|
<a href=/people/f/firoj-alam/>Firoj Alam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--18><div class="card-body p-3 small">Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> using different <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. Recently, transformer models have proven their success in downstream NLP tasks, and these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we obtain comparable state-of-the-art results, while for <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla</a>, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.20/>Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations<span class=acl-fixed-case>MT</span> systems for Robustness to Second-Language Speaker Variations</a></strong><br><a href=/people/m/md-mahfuz-ibn-alam/>Md Mahfuz Ibn Alam</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--20><div class="card-body p-3 small">The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate this issue. We show that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> using naturally occurring noise along with pseudo-references (i.e. corrected non-native inputs translated using the baseline NMT system) is a promising solution towards systems robust to such type of input variations. We focus on four translation pairs, from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new state-of-the-art on the JFLEG-ES dataset. All datasets and code are publicly available here : https://github.com/mahfuzibnalam/finetuning_for_robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.21.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wnut-1.21/>Impact of ASR on Alzheimer’s Disease Detection : All Errors are Equal, but Deletions are More Equal than Others<span class=acl-fixed-case>ASR</span> on <span class=acl-fixed-case>A</span>lzheimer’s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others</a></strong><br><a href=/people/a/aparna-balagopalan/>Aparna Balagopalan</a>
|
<a href=/people/k/ksenia-shkaruta/>Ksenia Shkaruta</a>
|
<a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--21><div class="card-body p-3 small">Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect <a href=https://en.wikipedia.org/wiki/Detection_theory>detection</a> performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for <a href=https://en.wikipedia.org/wiki/Deletion_(genetics)>deletion errors</a> in order to improve dementia detection performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.22/>Detecting Entailment in Code-Mixed Hindi-English Conversations<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Conversations</a></strong><br><a href=/people/s/sharanya-chakravarthy/>Sharanya Chakravarthy</a>
|
<a href=/people/a/anjana-umapathy/>Anjana Umapathy</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--22><div class="card-body p-3 small">The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages, and is commonly seen in informal conversations among <a href=https://en.wikipedia.org/wiki/Multilingualism>polyglots</a>. Given the rising importance of dialogue agents, it is imperative that they understand <a href=https://en.wikipedia.org/wiki/Code-mixing>code-mixing</a>, but the scarcity of code-mixed Natural Language Understanding (NLU) datasets has precluded research in this area. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by Khanuja et. al. for detecting conversational entailment in code-mixed Hindi-English text is the first of its kind. We investigate the effectiveness of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, and architectural approaches to address the code-mixed, conversational, and low-resource aspects of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We obtain an 8.09 % increase in test set accuracy over the current state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.24/>Annotation Efficient <a href=https://en.wikipedia.org/wiki/Language_identification>Language Identification</a> from Weak Labels</a></strong><br><a href=/people/s/shriphani-palakodety/>Shriphani Palakodety</a>
|
<a href=/people/a/ashiqur-khudabukhsh/>Ashiqur KhudaBukhsh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--24><div class="card-body p-3 small">India is home to several languages with more than 30 m speakers. These <a href=https://en.wikipedia.org/wiki/Language>languages</a> exhibit significant presence on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) models and resources. User generated social media content in these <a href=https://en.wikipedia.org/wiki/Language>languages</a> is also typically authored in the <a href=https://en.wikipedia.org/wiki/Latin_script>Roman script</a> as opposed to the traditional native script further contributing to resource scarcity. In this paper, we leverage a minimally supervised NLP technique to obtain weak language labels from a large-scale Indian social media corpus leading to a robust and annotation-efficient language-identification technique spanning nine Romanized Indian languages. In fast-spreading pandemic situations such as the current COVID-19 situation, information processing objectives might be heavily tilted towards under-served languages in densely populated regions. We release our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to facilitate downstream analyses in these low-resource languages. Experiments across multiple <a href=https://en.wikipedia.org/wiki/Social_media>social media corpora</a> demonstrate the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s robustness and provide several interesting insights on Indian language usage patterns on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We release an annotated data set of 1,000 comments in ten <a href=https://en.wikipedia.org/wiki/Romanization_(cultural)>Romanized languages</a> as a social media evaluation benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.25.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wnut-1.25/>Fantastic Features and Where to Find Them : Detecting Cognitive Impairment with a Subsequence Classification Guided Approach</a></strong><br><a href=/people/b/ben-eyre/>Ben Eyre</a>
|
<a href=/people/a/aparna-balagopalan/>Aparna Balagopalan</a>
|
<a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--25><div class="card-body p-3 small">Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from <a href=https://en.wikipedia.org/wiki/Noisy_text>noisy text</a> is time and resource consuming, and can potentially result in <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> that do not enhance <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance. To combat this, we describe a new approach to <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> that leverages sequential machine learning models and <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to predict which <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> help enhance performance. We provide a concrete example of this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3 % over a strong baseline when using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> produced by this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. This demonstration provides an example of how this method can be used to assist <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> in fields where <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> is important, such as <a href=https://en.wikipedia.org/wiki/Health_care>health care</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.28.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.28/>Civil Unrest on Twitter (CUT): A Dataset of Tweets to Support Research on Civil Unrest<span class=acl-fixed-case>T</span>witter (<span class=acl-fixed-case>CUT</span>): A Dataset of Tweets to Support Research on Civil Unrest</a></strong><br><a href=/people/j/justin-sech/>Justin Sech</a>
|
<a href=/people/a/alexandra-delucia/>Alexandra DeLucia</a>
|
<a href=/people/a/anna-l-buczak/>Anna L. Buczak</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--28><div class="card-body p-3 small">We present CUT, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for studying <a href=https://en.wikipedia.org/wiki/Civil_disorder>Civil Unrest</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Our dataset includes 4,381 tweets related to <a href=https://en.wikipedia.org/wiki/Civil_disorder>civil unrest</a>, hand-annotated with information related to the study of civil unrest discussion and events. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is drawn from 42 countries from 2014 to 2019. We present <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> for the identification of tweets related to <a href=https://en.wikipedia.org/wiki/Civil_disorder>civil unrest</a>. We include a discussion of ethical issues related to research on this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.30/>Representation learning of writing style</a></strong><br><a href=/people/j/julien-hay/>Julien Hay</a>
|
<a href=/people/b/bich-lien-doan/>Bich-Lien Doan</a>
|
<a href=/people/f/fabrice-popineau/>Fabrice Popineau</a>
|
<a href=/people/o/ouassim-ait-elhara/>Ouassim Ait Elhara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--30><div class="card-body p-3 small">In this paper, we introduce a new method of <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> that aims to embed documents in a stylometric space. Previous studies in the field of <a href=https://en.wikipedia.org/wiki/Authorship_analysis>authorship analysis</a> focused on feature engineering techniques in order to represent document styles and to enhance <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance in specific tasks. Instead, we directly embed documents in a stylometric space by relying on a reference set of authors and the intra-author consistency property which is one of two components in our definition of <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a>. The main intuition of this paper is that we can define a general stylometric space from a set of reference authors such that, in this space, the coordinates of different documents will be close when the documents are by the same author, and spread away when they are by different authors, even for documents by authors who are not in the set of reference authors. The method we propose allows for the clustering of documents based on stylistic clues reflecting the authorship of documents. For the empirical validation of the method, we train a deep neural network model to predict authors of a large reference dataset consisting of news and blog articles. Albeit the learning process is supervised, it does not require a dedicated labeling of the data but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> relies only on the metadata of the articles which are available in huge amounts. We evaluate the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on multiple <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, on both the authorship clustering and the authorship attribution tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.31.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wnut-1.31/>A Little Birdie Told Me... -Inductive Biases for Rumour Stance Detection on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/k/karthik-radhakrishnan/>Karthik Radhakrishnan</a>
|
<a href=/people/t/tushar-kanakagiri/>Tushar Kanakagiri</a>
|
<a href=/people/s/sharanya-chakravarthy/>Sharanya Chakravarthy</a>
|
<a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--31><div class="card-body p-3 small">The rise in the usage of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has placed <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> in a central position for <a href=https://en.wikipedia.org/wiki/Dissemination>news dissemination</a> and consumption. This greatly increases the potential for proliferation of <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a> and <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>. In an effort to mitigate the spread of rumours, we tackle the related task of identifying the stance (Support, Deny, Query, Comment) of a <a href=https://en.wikipedia.org/wiki/Social_media_marketing>social media post</a>. Unlike previous works, we impose <a href=https://en.wikipedia.org/wiki/Inductive_reasoning>inductive biases</a> that capture platform specific user behavior. These <a href=https://en.wikipedia.org/wiki/Bias>biases</a>, coupled with social media fine-tuning of BERT allow for better <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>, thus yielding an F1 score of 58.7 on the SemEval 2019 task on rumour stance detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.34/>IITKGP at W-NUT 2020 Shared Task-1 : Domain specific BERT representation for Named Entity Recognition of lab protocol<span class=acl-fixed-case>IITKGP</span> at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Shared Task-1: Domain specific <span class=acl-fixed-case>BERT</span> representation for Named Entity Recognition of lab protocol</a></strong><br><a href=/people/t/tejas-vaidhya/>Tejas Vaidhya</a>
|
<a href=/people/a/ayush-kaushal/>Ayush Kaushal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--34><div class="card-body p-3 small">Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks. For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistictasks. But the vocabulary used in the <a href=https://en.wikipedia.org/wiki/Medicine>medical field</a> contains a lot of different tokens used only in the <a href=https://en.wikipedia.org/wiki/Healthcare_industry>medical industry</a> such as the name of different diseases, devices, organisms, medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the <a href=https://en.wikipedia.org/wiki/System>System</a> for Named Entity Tagging based on Bio-Bert. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gives substantial improvements over the baseline and stood the fourth runner up in terms of <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>, and first runner up in terms of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> with just 2.21 <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> behind the best one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.38/>mgsohrab at WNUT 2020 Shared Task-1 : Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols<span class=acl-fixed-case>WNUT</span> 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols</a></strong><br><a href=/people/m/mohammad-golam-sohrab/>Mohammad Golam Sohrab</a>
|
<a href=/people/a/anh-khoa-duong-nguyen/>Anh-Khoa Duong Nguyen</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--38><div class="card-body p-3 small">We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60 % in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46 % in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.41/>WNUT-2020 Task 2 : Identification of Informative COVID-19 English Tweets<span class=acl-fixed-case>WNUT</span>-2020 Task 2: Identification of Informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/m/mai-hoang-dao/>Mai Hoang Dao</a>
|
<a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/l/long-doan/>Long Doan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--41><div class="card-body p-3 small">In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of 10 K Tweets</a> and organize the development and evaluation phases for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised training</a> performs well in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.45/>Siva at WNUT-2020 Task 2 : Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets<span class=acl-fixed-case>WNUT</span>-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets</a></strong><br><a href=/people/s/siva-sai/>Siva Sai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--45><div class="card-body p-3 small">Social media witnessed vast amounts of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as infodemic. The ill-effects of such <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> are multifarious. Thus, identifying and eliminating the sources of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> becomes very crucial, especially when <a href=https://en.wikipedia.org/wiki/Mass_psychogenic_illness>mass panic</a> can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. WNUT-2020 Task 2 aims at building <a href=https://en.wikipedia.org/wiki/System>systems</a> for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3 % in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.48/>CXP949 at WNUT-2020 Task 2 : Extracting Informative COVID-19 Tweets-RoBERTa Ensembles and The Continued Relevance of Handcrafted Features<span class=acl-fixed-case>CXP</span>949 at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Extracting Informative <span class=acl-fixed-case>COVID</span>-19 Tweets - <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a Ensembles and The Continued Relevance of Handcrafted Features</a></strong><br><a href=/people/c/calum-perrio/>Calum Perrio</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--48><div class="card-body p-3 small">This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation that makes use of corpus level information and a handcrafted feature. We test the effectiveness of including the aforementioned <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in accommodating the challenges of a noisy data set centred on a specific subject outside the remit of the pre-training data. We show that inclusion of additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> can improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results and achieve a score within 2 points of the top performing team.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.55/>CSECU-DSG at WNUT-2020 Task 2 : Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative COVID-19 English Tweets<span class=acl-fixed-case>CSECU</span>-<span class=acl-fixed-case>DSG</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/f/fareen-tasneem/>Fareen Tasneem</a>
|
<a href=/people/j/jannatun-naim/>Jannatun Naim</a>
|
<a href=/people/r/radiathun-tasnia/>Radiathun Tasnia</a>
|
<a href=/people/t/tashin-hossain/>Tashin Hossain</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--55><div class="card-body p-3 small">COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the situation as well as beneficial for public safety personnel for decision making. However, the informal nature of <a href=https://en.wikipedia.org/wiki/Twitter>twitter</a> makes it challenging to refine the informative tweets from the huge tweet streams. To address these challenges WNUT-2020 introduced a shared task focusing on COVID-19 related informative tweet identification. In this paper, we describe our participation in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We propose a neural model that adopts the strength of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and hand-crafted features in a unified architecture. To extract the transfer learning features, we utilize the state-of-the-art pre-trained sentence embedding model BERT, RoBERTa, and InferSent, whereas various twitter characteristics are exploited to extract the hand-crafted features. Next, various feature combinations are utilized to train a set of multilayer perceptron (MLP) as the base-classifier. Finally, a majority voting based fusion approach is employed to determine the informative tweets. Our approach achieved competitive performance and outperformed the baseline by 7 % (approx.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.56" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.56/>IRLab@IITBHU at WNUT-2020 Task 2 : Identification of informative COVID-19 English Tweets using BERT<span class=acl-fixed-case>IRL</span>ab@<span class=acl-fixed-case>IITBHU</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Identification of informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets using <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/s/supriya-chanda/>Supriya Chanda</a>
|
<a href=/people/e/eshita-nandy/>Eshita Nandy</a>
|
<a href=/people/s/sukomal-pal/>Sukomal Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--56><div class="card-body p-3 small">This paper reports our submission to the shared Task 2 : Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks : DistilBERT and <a href=https://en.wikipedia.org/wiki/FastText>FastText</a>. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.58/>DSC-IIT ISM at WNUT-2020 Task 2 : Detection of COVID-19 informative tweets using RoBERTa<span class=acl-fixed-case>DSC</span>-<span class=acl-fixed-case>IIT</span> <span class=acl-fixed-case>ISM</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Detection of <span class=acl-fixed-case>COVID</span>-19 informative tweets using <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a</a></strong><br><a href=/people/s/sirigireddy-dhana-laxmi/>Sirigireddy Dhana Laxmi</a>
|
<a href=/people/r/rohit-agarwal/>Rohit Agarwal</a>
|
<a href=/people/a/aman-sinha/>Aman Sinha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--58><div class="card-body p-3 small">Social media such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a public dataset with an <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.89 on the validation dataset and 0.87 on the <a href=https://en.wikipedia.org/wiki/Score_(statistics)>leaderboard</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.60/>NLPRL at WNUT-2020 Task 2 : ELMo-based System for Identification of COVID-19 Tweets<span class=acl-fixed-case>NLPRL</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: <span class=acl-fixed-case>ELM</span>o-based System for Identification of <span class=acl-fixed-case>COVID</span>-19 Tweets</a></strong><br><a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/r/rupjyoti-baruah/>Rupjyoti Baruah</a>
|
<a href=/people/b/bhavana-srivastava/>Bhavana Srivastava</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--60><div class="card-body p-3 small">The Coronavirus pandemic has been a dominating news on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, can help in <a href=https://en.wikipedia.org/wiki/Preventive_healthcare>prevention</a> and taking precautions. This is an example of using noisy text processing for <a href=https://en.wikipedia.org/wiki/Emergency_management>disaster management</a>. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as 80.85 % and 78.54 % as <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the provided test dataset. The experimental code is available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.63/>ComplexDataLab at W-NUT 2020 Task 2 : Detecting Informative COVID-19 Tweets by Attending over Linked Documents<span class=acl-fixed-case>C</span>omplex<span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>L</span>ab at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Task 2: Detecting Informative <span class=acl-fixed-case>COVID</span>-19 Tweets by Attending over Linked Documents</a></strong><br><a href=/people/k/kellin-pelrine/>Kellin Pelrine</a>
|
<a href=/people/j/jacob-danovitch/>Jacob Danovitch</a>
|
<a href=/people/a/albert-orozco-camacho/>Albert Orozco Camacho</a>
|
<a href=/people/r/reihaneh-rabbany/>Reihaneh Rabbany</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--63><div class="card-body p-3 small">Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph classification</a>, drawing on not only the tweet but connected webpages and entities. We leverage a pre-trained language model as well as the connections between nodes to learn a pooled representation for each document network. We show it outperforms several competitive baselines and present ablation studies supporting the benefit of the linked information. Code is available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.65/>LynyrdSkynyrd at WNUT-2020 Task 2 : Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets<span class=acl-fixed-case>L</span>ynyrd<span class=acl-fixed-case>S</span>kynyrd at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Semi-Supervised Learning for Identification of Informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/a/abhilasha-sancheti/>Abhilasha Sancheti</a>
|
<a href=/people/k/kushal-chawla/>Kushal Chawla</a>
|
<a href=/people/g/gaurav-verma/>Gaurav Verma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--65><div class="card-body p-3 small">In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.73/>SunBear at WNUT-2020 Task 2 : Improving BERT-Based Noisy Text Classification with Knowledge of the Data domain<span class=acl-fixed-case>S</span>un<span class=acl-fixed-case>B</span>ear at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Improving <span class=acl-fixed-case>BERT</span>-Based Noisy Text Classification with Knowledge of the Data domain</a></strong><br><a href=/people/l/linh-doan-bao/>Linh Doan Bao</a>
|
<a href=/people/v/viet-anh-nguyen/>Viet Anh Nguyen</a>
|
<a href=/people/q/quang-pham-huu/>Quang Pham Huu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--73><div class="card-body p-3 small">This paper proposes an improved custom model for WNUT task 2 : Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning methodologies</a> for state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> RoBERTa. We make a preliminary instantiation of this formal <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for the text classification approaches. With appropriate training techniques, our model is able to achieve 0.9218 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on public validation set and the ensemble version settles at top 9 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> (0.9005) and top 2 Recall (0.9301) on private test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.75/>COVCOR20 at WNUT-2020 Task 2 : An Attempt to Combine <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Expert rules<span class=acl-fixed-case>COVCOR</span>20 at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules</a></strong><br><a href=/people/a/ali-hurriyetoglu/>Ali Hürriyetoğlu</a>
|
<a href=/people/a/ali-safaya/>Ali Safaya</a>
|
<a href=/people/o/osman-mutlu/>Osman Mutlu</a>
|
<a href=/people/n/nelleke-oostdijk/>Nelleke Oostdijk</a>
|
<a href=/people/e/erdem-yoruk/>Erdem Yörük</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--75><div class="card-body p-3 small">In the scope of WNUT-2020 Task 2, we developed various text classification systems, using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of (the output of) the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the <a href=https://en.wikipedia.org/wiki/Integral>integration</a> was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.76/>TEST_POSITIVE at W-NUT 2020 Shared Task-3 : Cross-task modeling<span class=acl-fixed-case>TEST</span>_<span class=acl-fixed-case>POSITIVE</span> at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Shared Task-3: Cross-task modeling</a></strong><br><a href=/people/c/chacha-chen/>Chacha Chen</a>
|
<a href=/people/c/chieh-yang-huang/>Chieh-Yang Huang</a>
|
<a href=/people/y/yaqi-hou/>Yaqi Hou</a>
|
<a href=/people/y/yang-shi/>Yang Shi</a>
|
<a href=/people/e/enyan-dai/>Enyan Dai</a>
|
<a href=/people/j/jiaqi-wang/>Jiaqi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--76><div class="card-body p-3 small">The competition of extracting COVID-19 events from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> is to develop systems that can automatically extract related events from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. The built <a href=https://en.wikipedia.org/wiki/System>system</a> should identify different pre-defined slots for each event, in order to answer important questions (e.g., Who is tested positive? What is the age of the person? Where is he / she?). To tackle these challenges, we propose the Joint Event Multi-task Learning (JOELIN) model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Moreover, we implement a type-aware post-processing procedure using named entity recognition (NER) to further filter the predictions. JOELIN outperforms the BERT baseline by 17.2 % in micro F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.80/>HLTRI at W-NUT 2020 Shared Task-3 : COVID-19 Event Extraction from Twitter Using Multi-Task Hopfield Pooling<span class=acl-fixed-case>HLTRI</span> at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Shared Task-3: <span class=acl-fixed-case>COVID</span>-19 Event Extraction from <span class=acl-fixed-case>T</span>witter Using Multi-Task Hopfield Pooling</a></strong><br><a href=/people/m/maxwell-weinzierl/>Maxwell Weinzierl</a>
|
<a href=/people/s/sanda-harabagiu/>Sanda Harabagiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--80><div class="card-body p-3 small">Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has the potential to inform surveillance systems that play a critical role in <a href=https://en.wikipedia.org/wiki/Public_health>public health</a>. The event extraction challenge presented by the W-NUT 2020 Shared Task 3 focused on the identification of five types of events relevant to the COVID-19 pandemic and their respective set of pre-defined slots encoding demographic, epidemiological, clinical as well as spatial, temporal or subjective knowledge. Our participation in the challenge led to the design of a neural architecture for jointly identifying all Event Slots expressed in a tweet relevant to an event of interest. This <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> uses COVID-Twitter-BERT as the pre-trained language model. In addition, to learn text span embeddings for each Event Slot, we relied on a special case of <a href=https://en.wikipedia.org/wiki/Hopfield_network>Hopfield Networks</a>, namely Hopfield pooling. The results of the shared task evaluation indicate that our system performs best when it is trained on a larger dataset, while <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> remains competitive when training on smaller datasets.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>