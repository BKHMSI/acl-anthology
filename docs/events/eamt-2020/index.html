<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Conference of the European Association for Machine Translation (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Conference of the European Association for Machine Translation (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020eamt-1>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a>
<span class="badge badge-info align-middle ml-1">25&nbsp;papers</span></li></ul></div></div><div id=2020eamt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2020.eamt-1/>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.0/>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</a></strong><br><a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/h/helena-moniz/>Helena Moniz</a>
|
<a href=/people/s/sara-fumega/>Sara Fumega</a>
|
<a href=/people/b/bruno-martins/>Bruno Martins</a>
|
<a href=/people/f/fernando-batista/>Fernando Batista</a>
|
<a href=/people/l/luisa-coheur/>Luisa Coheur</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/j/joss-moorkens/>Joss Moorkens</a>
|
<a href=/people/a/ana-guerberof/>Ana Guerberof</a>
|
<a href=/people/m/mary-nurminen/>Mary Nurminen</a>
|
<a href=/people/l/lena-marg/>Lena Marg</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.3/>Efficiently Reusing Old Models Across Languages via Transfer Learning</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--3><div class="card-body p-3 small">Recent progress in neural machine translation (NMT) is directed towards larger <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> trained on an increasing amount of hardware resources. As a result, NMT models are costly to train, both financially, due to the electricity and hardware cost, and environmentally, due to the <a href=https://en.wikipedia.org/wiki/Carbon_footprint>carbon footprint</a>. It is especially true in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for its additional cost of training the parent model before transferring knowledge and training the desired <a href=https://en.wikipedia.org/wiki/Child_model>child model</a>. In this paper, we propose a simple method of re-using an already trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> for different language pairs where there is no need for modifications in <a href=https://en.wikipedia.org/wiki/Statistical_model>model architecture</a>. Our approach does not need a separate parent model for each investigated language pair, as it is typical in NMT transfer learning. To show the applicability of our method, we recycle a Transformer model trained by different researchers and use it to seed models for different language pairs. We achieve better translation quality and shorter <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence times</a> than when training from random initialization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.4/>Efficient <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> for Quality Estimation with Bottleneck Adapter Layer</a></strong><br><a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/n/ning-xie/>Ning Xie</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a>
|
<a href=/people/y/yao-deng/>Yao Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--4><div class="card-body p-3 small">The Predictor-Estimator framework for quality estimation (QE) is commonly used for its strong performance. Where the predictor and <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> works on <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> and <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality evaluation</a>, respectively. However, training the <a href=https://en.wikipedia.org/wiki/Prediction>predictor</a> from scratch is computationally expensive. In this paper, we propose an efficient transfer learning framework to transfer knowledge from NMT dataset into <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE models</a>. A Predictor-Estimator alike model named BAL-QE is also proposed, aiming to extract high quality features with pre-trained NMT model, and make classification with a fine-tuned Bottleneck Adapter Layer (BAL). The experiment shows that BAL-QE achieves 97 % of the SOTA performance in WMT19 En-De and En-Ru QE tasks by only training 3 % of parameters within 4 hours on 4 Titan XP GPUs. Compared with the commonly used NuQE baseline, BAL-QE achieves 47 % (En-Ru) and 75 % (En-De) of performance promotions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.6/>Incorporating External Annotation to improve Named Entity Translation in NMT<span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/m/maciej-modrzejewski/>Maciej Modrzejewski</a>
|
<a href=/people/m/miriam-exel/>Miriam Exel</a>
|
<a href=/people/b/bianka-buschbeck/>Bianka Buschbeck</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--6><div class="card-body p-3 small">The correct translation of named entities (NEs) still poses a challenge for conventional neural machine translation (NMT) systems. This study explores methods incorporating <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition (NER)</a> into <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>NMT</a> with the aim to improve <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity translation</a>. It proposes an annotation method that integrates <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and insideoutsidebeginning (IOB) tagging into the neural network input with the use of source factors. Our experiments on EnglishGerman and English Chinese show that just by including different NE classes and IOB tagging, we can increase the BLEU score by around 1 point using the standard test set from WMT2019 and achieve up to 12 % increase in NE translation rates over a strong baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.8/>A multi-source approach for BretonFrench hybrid machine translation<span class=acl-fixed-case>B</span>reton–<span class=acl-fixed-case>F</span>rench hybrid machine translation</a></strong><br><a href=/people/v/victor-m-sanchez-cartagena/>Víctor M. Sánchez-Cartagena</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a>
|
<a href=/people/f/felipe-sanchez-martinez/>Felipe Sánchez-Martínez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--8><div class="card-body p-3 small">Corpus-based approaches to machine translation (MT) have difficulties when the amount of parallel corpora to use for training is scarce, especially if the languages involved in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> are highly inflected. This problem can be addressed from different perspectives, including <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, and the use of additional resources, such as those used in rule-based MT. This paper focuses on the hybridisation of rule-based MT and neural MT for the BretonFrench under-resourced language pair in an attempt to study to what extent the rule-based MT resources help improve the translation quality of the neural MT system for this particular under-resourced language pair. We combine both translation approaches in a multi-source neural MT architecture and find out that, even though the rule-based system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.9/>Leveraging Multilingual Resources for Language Invariant Sentiment Analysis</a></strong><br><a href=/people/a/allen-antony/>Allen Antony</a>
|
<a href=/people/a/arghya-bhattacharya/>Arghya Bhattacharya</a>
|
<a href=/people/j/jaipal-goud/>Jaipal Goud</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--9><div class="card-body p-3 small">Sentiment analysis is a widely researched NLP problem with state-of-the-art solutions capable of attaining human-like accuracies for various languages. However, these methods rely heavily on large amounts of labeled data or sentiment weighted language-specific lexical resources that are unavailable for low-resource languages. Our work attempts to tackle this data scarcity issue by introducing a neural architecture for language invariant sentiment analysis capable of leveraging various monolingual datasets for training without any kind of cross-lingual supervision. The proposed architecture attempts to learn language agnostic sentiment features via adversarial training on multiple resource-rich languages which can then be leveraged for inferring sentiment information at a sentence level on a low resource language. Our model outperforms the current state-of-the-art methods on the Multilingual Amazon Review Text Classification dataset [ REF ] and achieves significant performance gains over prior work on the low resource Sentiraama corpus [ REF ]. A detailed analysis of our research highlights the ability of our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> to perform significantly well in the presence of minimal amounts of training data for low resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.12/>Double Attention-based Multimodal Neural Machine Translation with Semantic Image Regions</a></strong><br><a href=/people/y/yuting-zhao/>Yuting Zhao</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--12><div class="card-body p-3 small">Existing studies on multimodal neural machine translation (MNMT) have mainly focused on the effect of combining visual and textual modalities to improve translations. However, it has been suggested that the <a href=https://en.wikipedia.org/wiki/Visual_system>visual modality</a> is only marginally beneficial. Conventional visual attention mechanisms have been used to select the visual features from equally-sized grids generated by convolutional neural networks (CNNs), and may have had modest effects on aligning the visual concepts associated with textual objects, because the grid visual features do not capture semantic information. In contrast, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English-German and English-French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance benefited from semantic image regions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eamt-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.14/>Fine-grained Human Evaluation of Transformer and Recurrent Approaches to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> for English-to-Chinese<span class=acl-fixed-case>E</span>nglish-to-<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/y/yuying-ye/>Yuying Ye</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--14><div class="card-body p-3 small">This research presents a fine-grained human evaluation to compare the Transformer and recurrent approaches to neural machine translation (MT), on the translation direction English-to-Chinese. To this end, we develop an error taxonomy compliant with the Multidimensional Quality Metrics (MQM) framework that is customised to the relevant phenomena of this translation direction. We then conduct an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error annotation</a> using this customised error taxonomy on the output of state-of-the-art recurrent- and Transformer-based MT systems on a subset of WMT2019&#8217;s news test set. The resulting annotation shows that, compared to the best recurrent system, the best Transformer system results in a 31 % reduction of the total number of errors and it produced significantly less errors in 10 out of 22 error categories. We also note that two of the <a href=https://en.wikipedia.org/wiki/System>systems</a> evaluated do not produce any error for a category that was relevant for this translation direction prior to the advent of NMT systems : Chinese classifiers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eamt-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.15/>Correct Me If You Can : Learning from Error Corrections and Markings</a></strong><br><a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/n/nathaniel-berger/>Nathaniel Berger</a>
|
<a href=/people/s/stefan-riezler/>Stefan Riezler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--15><div class="card-body p-3 small">Sequence-to-sequence learning involves a trade-off between <a href=https://en.wikipedia.org/wiki/Signal_strength_in_telecommunications>signal strength</a> and annotation cost of training data. For example, machine translation data range from costly expert-generated translations that enable <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, to weak quality-judgment feedback that facilitate <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. We present the first user study on annotation cost and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learnability</a> for the less popular annotation mode of error markings. We show that error markings for translations of TED talks from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a> allow precise credit assignment while requiring significantly less human effort than correcting / post-editing, and that error-marked data can be used successfully to fine-tune neural machine translation models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.17/>Fine-Grained Error Analysis on English-to-Japanese Machine Translation in the Medical Domain<span class=acl-fixed-case>E</span>nglish-to-<span class=acl-fixed-case>J</span>apanese Machine Translation in the Medical Domain</a></strong><br><a href=/people/t/takeshi-hayakawa/>Takeshi Hayakawa</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--17><div class="card-body p-3 small">We performed a detailed <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> in domain-specific neural machine translation (NMT) for the English and Japanese language pair with fine-grained manual annotation. Despite its importance for advancing NMT technologies, research on the performance of domain-specific NMT and non-European languages has been limited. In this study, we designed an error typology based on the error types that were typically generated by NMT systems and might cause significant impact in technical translations : <a href=https://en.wikipedia.org/wiki/Addition>Addition</a>, Omission, <a href=https://en.wikipedia.org/wiki/Mistranslation>Mistranslation</a>, <a href=https://en.wikipedia.org/wiki/Grammar>Grammar</a>, and <a href=https://en.wikipedia.org/wiki/Terminology>Terminology</a>. The <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error annotation</a> was targeted to the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a> and was performed by experienced professional translators specialized in <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a> under careful quality control. The <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> detected 4,912 errors on 2,480 sentences, and the frequency and distribution of errors were analyzed. We found that the major errors in NMT were <a href=https://en.wikipedia.org/wiki/Mistranslation>Mistranslation</a> and Terminology rather than <a href=https://en.wikipedia.org/wiki/Addition>Addition</a> and Omission, which have been reported as typical problems of NMT. Interestingly, more errors occurred in documents for professionals compared with those for the general public. The results of our annotation work will be published as a parallel corpus with error labels, which are expected to contribute to developing better NMT models, automatic evaluation metrics, and quality estimation models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.21/>Modelling Source- and Target- Language Syntactic Information as Conditional Context in Interactive Neural Machine Translation</a></strong><br><a href=/people/k/kamal-kumar-gupta/>Kamal Kumar Gupta</a>
|
<a href=/people/r/rejwanul-haque/>Rejwanul Haque</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--21><div class="card-body p-3 small">In interactive machine translation (MT), human translators correct errors in automatic translations in collaboration with the MT systems, which is seen as an effective way to improve the productivity gain in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In this study, we model source-language syntactic constituency parse and target-language syntactic descriptions in the form of supertags as conditional context for interactive prediction in neural MT (NMT). We found that the supertags significantly improve productivity gain in <a href=https://en.wikipedia.org/wiki/Translation>translation</a> in interactive-predictive NMT (INMT), while syntactic parsing somewhat found to be effective in reducing human effort in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Furthermore, when we model this source- and target-language syntactic information together as the conditional context, both types complement each other and our fully syntax-informed INMT model statistically significantly reduces human efforts in a FrenchtoEnglish translation task, achieving 4.30 points absolute (corresponding to 9.18 % relative) improvement in terms of word prediction accuracy (WPA) and 4.84 points absolute (corresponding to 9.01 % relative) reduction in terms of word stroke ratio (WSR) over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.28/>Evaluating the usefulness of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> for the Polish translators in the European Commission<span class=acl-fixed-case>P</span>olish translators in the <span class=acl-fixed-case>E</span>uropean Commission</a></strong><br><a href=/people/k/karolina-stefaniak/>Karolina Stefaniak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--28><div class="card-body p-3 small">The mission of the Directorate General for Translation (DGT) is to provide high-quality translation to help the European Commission communicate with EU citizens. To this end <a href=https://en.wikipedia.org/wiki/Deutsche_Gesellschaft_f&#252;r_Internationale_Zusammenarbeit>DGT</a> employs almost 2000 translators from all EU official languages. But while the demand for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> has been continuously growing, following a global trend, the number of translators has decreased. To cope with the demand, DGT extensively uses a CAT environment encompassing <a href=https://en.wikipedia.org/wiki/Translation_memory>translation memories</a>, terminology databases and recently also <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. This paper examines the benefits and risks of using <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> to augment the productivity of inhouse DGT translators for the EnglishPolish language pair. Based on the analysis of a sample of NMTtranslated texts and on the observations of the working practices of Polish translators it is concluded that the possible productivity gain is still modest, while the risks to <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> are quite substantial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.29/>Terminology-Constrained Neural Machine Translation at SAP<span class=acl-fixed-case>SAP</span></a></strong><br><a href=/people/m/miriam-exel/>Miriam Exel</a>
|
<a href=/people/b/bianka-buschbeck/>Bianka Buschbeck</a>
|
<a href=/people/l/lauritz-brandt/>Lauritz Brandt</a>
|
<a href=/people/s/simona-doneva/>Simona Doneva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--29><div class="card-body p-3 small">This paper examines approaches to bias a neural machine translation model to adhere to terminology constraints in an industrial setup. In particular, we investigate variations of the <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> by Dinu et al. (2019), which uses inline annotation of the target terms in the source segment plus source factor embeddings during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, and compare them to constrained decoding. We describe the challenges with respect to terminology in our usage scenario at SAP and show how far the investigated methods can help to overcome them. We extend the original study to a new language pair and provide an in-depth evaluation including an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error classification</a> and a <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.eamt-1.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.31/>Bifixer and Bicleaner : two open-source tools to clean your parallel data</a></strong><br><a href=/people/g/gema-ramirez-sanchez/>Gema Ramírez-Sánchez</a>
|
<a href=/people/j/jaume-zaragoza-bernabeu/>Jaume Zaragoza-Bernabeu</a>
|
<a href=/people/m/marta-banon/>Marta Bañón</a>
|
<a href=/people/s/sergio-ortiz-rojas/>Sergio Ortiz Rojas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--31><div class="card-body p-3 small">This paper shows the utility of two open-source tools designed for parallel data cleaning : Bifixer and Bicleaner. Already used to clean highly noisy parallel content from crawled multilingual websites, we evaluate their performance in a different scenario : cleaning publicly available corpora commonly used to train machine translation systems. We choose four EnglishPortuguese corpora which we plan to use internally to compute <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> at a later stage. We clean the four <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> using both <a href=https://en.wikipedia.org/wiki/Tool>tools</a>, which are described in detail, and analyse the effect of some of the cleaning steps on them. We then compare machine translation training times and <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> before and after cleaning these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, showing a positive impact particularly for the noisiest ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.32/>An English-Swahili parallel corpus and its use for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> in the <a href=https://en.wikipedia.org/wiki/News_media>news domain</a><span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>S</span>wahili parallel corpus and its use for neural machine translation in the news domain</a></strong><br><a href=/people/f/felipe-sanchez-martinez/>Felipe Sánchez-Martínez</a>
|
<a href=/people/v/victor-m-sanchez-cartagena/>Víctor M. Sánchez-Cartagena</a>
|
<a href=/people/j/juan-antonio-perez-ortiz/>Juan Antonio Pérez-Ortiz</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a>
|
<a href=/people/m/miquel-espla-gomis/>Miquel Esplà-Gomis</a>
|
<a href=/people/a/andrew-secker/>Andrew Secker</a>
|
<a href=/people/s/susie-coleman/>Susie Coleman</a>
|
<a href=/people/j/julie-wall/>Julie Wall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--32><div class="card-body p-3 small">This paper describes our approach to create a neural machine translation system to translate between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Swahili (both directions) in the news domain, as well as the process we followed to crawl the necessary parallel corpora from the Internet. We report the results of a pilot human evaluation performed by the news media organisations participating in the H2020 EU-funded project GoURMET.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.34/>A User Study of the <a href=https://en.wikipedia.org/wiki/Incremental_learning>Incremental Learning</a> in NMT<span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/m/miguel-domingo/>Miguel Domingo</a>
|
<a href=/people/m/mercedes-garcia-martinez/>Mercedes García-Martínez</a>
|
<a href=/people/a/alvaro-peris/>Álvaro Peris</a>
|
<a href=/people/a/alexandre-helle/>Alexandre Helle</a>
|
<a href=/people/a/amando-estela/>Amando Estela</a>
|
<a href=/people/l/laurent-bie/>Laurent Bié</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a>
|
<a href=/people/m/manuel-herranz/>Manuel Herranz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--34><div class="card-body p-3 small">In the translation industry, human experts usually supervise and post-edit machine translation hypotheses. Adaptive neural machine translation systems, able to incrementally update the underlying models under an online learning regime, have been proven to be useful to improve the efficiency of this <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a>. However, this incremental adaptation is somewhat unstable, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> may lead to undesirable side effects. One of them is the sporadic appearance of made-up words, as a byproduct of an erroneous application of subword segmentation techniques. In this work, we extend previous studies on on-the-fly adaptation of neural machine translation systems. We perform a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> involving professional, experienced post-editors, delving deeper on the aforementioned problems. Results show that adaptive systems were able to learn how to generate the correct translation for task-specific terms, resulting in an improvement of the user&#8217;s productivity. We also observed a close similitude, in terms of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, between made-up words and the words that were expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.42/>How do LSPs compute MT discounts? Presenting a company’s pipeline and its use<span class=acl-fixed-case>LSP</span>s compute <span class=acl-fixed-case>MT</span> discounts? Presenting a company’s pipeline and its use</a></strong><br><a href=/people/r/randy-scansani/>Randy Scansani</a>
|
<a href=/people/l/lamis-mhedhbi/>Lamis Mhedhbi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--42><div class="card-body p-3 small">In this paper we present a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> developed at Acolad to test a Machine Translation (MT) engine and compute the <a href=https://en.wikipedia.org/wiki/Discounting>discount</a> to be applied when its output is used in production. Our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> includes three main steps where <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> and <a href=https://en.wikipedia.org/wiki/Productivity>productivity</a> are measured through automatic metrics, manual evaluation, and by keeping track of editing and temporal effort during a post-editing task. Thanks to this approach, it is possible to evaluate the output quality and compute an engine-specific discount. Our test pipeline tackles the complexity of transforming productivity measurements into discounts by comparing the outcome of each of the above-mentioned steps to an estimate of the average productivity of translation from scratch. The <a href=https://en.wikipedia.org/wiki/Discounting>discount</a> is obtained by subtracting the resulting coefficient from the per-word rate. After a description of the <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a>, the paper presents its application on four engines, discussing its results and showing that our method to estimate post-editing effort through manual evaluation seems to capture the actual productivity. The <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> relies heavily on the work of professional post-editors, with the aim of creating a mutually beneficial cooperation between users and developers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.45/>Comparing <a href=https://en.wikipedia.org/wiki/Post-editing>Post-editing</a> based on Four Editing Actions against Translating with an Auto-Complete Feature</a></strong><br><a href=/people/f/felix-do-carmo/>Félix Do Carmo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--45><div class="card-body p-3 small">This article describes the results of a workshop in which 50 translators tested two experimental translation interfaces, as part of a project which aimed at studying the details of editing work. In this work, <a href=https://en.wikipedia.org/wiki/Editing>editing</a> is defined as a selection of four actions : deleting, inserting, moving and replacing words. Four texts, machine-translated from English into <a href=https://en.wikipedia.org/wiki/European_Portuguese>European Portuguese</a>, were post-edited in four different sessions in which each translator swapped between texts and two work modes. One of the <a href=https://en.wikipedia.org/wiki/Mode_(user_interface)>work modes</a> involved a typical auto-complete feature, and the other was based on the four actions. The participants answered surveys before, during and after the workshop. A descriptive analysis of the answers to the surveys and of the logs recorded during the experiments was performed. The four editing actions mode is shown to be more intrusive, but to allow for more planned decisions : although they take more time in this mode, translators hesitate less and make fewer edits. The article shows the usefulness of the <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> for research on the <a href=https://en.wikipedia.org/wiki/Editing>editing task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.49/>Document-Level Machine Translation Evaluation Project : Methodology, Effort and Inter-Annotator Agreement</a></strong><br><a href=/people/s/sheila-castilho/>Sheila Castilho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--49><div class="card-body p-3 small">Document-level (doc-level) human eval-uation of machine translation (MT) has raised interest in the community after a fewattempts have disproved claims of human parity (Toral et al., 2018 ; Laubli et al.,2018). However, little is known about bestpractices regarding doc-level human evalu-ation. The goal of this project is to identifywhich methodologies better cope with i)the current state-of-the-art (SOTA) humanmetrics, ii) a possible complexity when as-signing a single score to a text consisted of&#8216;good&#8217; and &#8216;bad&#8217; sentences, iii) a possibletiredness bias in doc-level set-ups, and iv)the difference in inter-annotator agreement(IAA) between sentence and doc-level set-ups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.51/>CEF Data Marketplace : Powering a Long-term Supply of Language Data<span class=acl-fixed-case>CEF</span> Data Marketplace: Powering a Long-term Supply of Language Data</a></strong><br><a href=/people/a/amir-kamran/>Amir Kamran</a>
|
<a href=/people/d/dace-dzeguze/>Dace Dzeguze</a>
|
<a href=/people/j/jaap-van-der-meer/>Jaap van der Meer</a>
|
<a href=/people/m/milica-panic/>Milica Panic</a>
|
<a href=/people/a/alessandro-cattelan/>Alessandro Cattelan</a>
|
<a href=/people/d/daniele-patrioli/>Daniele Patrioli</a>
|
<a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--51><div class="card-body p-3 small">We describe the CEF Data Marketplace project, which focuses on the development of a trading platform of translation data for language professionals : translators, machine translation (MT) developers, language service providers (LSPs), translation buyers and government bodies. The CEF Data Marketplace platform will be designed and built to manage and trade data for all languages and domains. This project will open a continuous and longterm supply of <a href=https://en.wikipedia.org/wiki/Language_structure>language data</a> for MT and other machine learning applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.59/>MICE : a middleware layer for MT<span class=acl-fixed-case>MICE</span>: a middleware layer for <span class=acl-fixed-case>MT</span></a></strong><br><a href=/people/j/joachim-van-den-bogaert/>Joachim Van den Bogaert</a>
|
<a href=/people/t/tom-vanallemeersch/>Tom Vanallemeersch</a>
|
<a href=/people/h/heidi-depraetere/>Heidi Depraetere</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--59><div class="card-body p-3 small">The MICE project (2018-2020) will deliver a middleware layer for improving the output quality of the eTranslation system of EC&#8217;s Connecting Europe Facility through additional services, such as domain adaptation and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. It will also deliver a <a href=https://en.wikipedia.org/wiki/Web_portal>user portal</a>, allowing for <a href=https://en.wikipedia.org/wiki/Post-editing>human post-editing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.60/>Neural Translation for the European Union (NTEU) Project<span class=acl-fixed-case>E</span>uropean <span class=acl-fixed-case>U</span>nion (<span class=acl-fixed-case>NTEU</span>) Project</a></strong><br><a href=/people/l/laurent-bie/>Laurent Bié</a>
|
<a href=/people/a/aleix-cerda-i-cuco/>Aleix Cerdà-i-Cucó</a>
|
<a href=/people/h/hans-degroote/>Hans Degroote</a>
|
<a href=/people/a/amando-estela/>Amando Estela</a>
|
<a href=/people/m/mercedes-garcia-martinez/>Mercedes García-Martínez</a>
|
<a href=/people/m/manuel-herranz/>Manuel Herranz</a>
|
<a href=/people/a/alejandro-kohan/>Alejandro Kohan</a>
|
<a href=/people/m/maite-melero/>Maite Melero</a>
|
<a href=/people/t/tony-odowd/>Tony O’Dowd</a>
|
<a href=/people/s/sinead-ogorman/>Sinéad O’Gorman</a>
|
<a href=/people/m/marcis-pinnis/>Mārcis Pinnis</a>
|
<a href=/people/r/roberts-rozis/>Roberts Rozis</a>
|
<a href=/people/r/riccardo-superbo/>Riccardo Superbo</a>
|
<a href=/people/a/arturs-vasilevskis/>Artūrs Vasiļevskis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--60><div class="card-body p-3 small">The Neural Translation for the European Union (NTEU) project aims to build a neural engine farm with all European official language combinations for eTranslation, without the necessity to use a high-resourced language as a pivot. NTEU started in September 2019 and will run until August 2021.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.62/>OCR, Classification & Machine Translation (OCCAM)<span class=acl-fixed-case>OCR</span>, Classification& Machine Translation (<span class=acl-fixed-case>OCCAM</span>)</a></strong><br><a href=/people/j/joachim-van-den-bogaert/>Joachim Van den Bogaert</a>
|
<a href=/people/a/arne-defauw/>Arne Defauw</a>
|
<a href=/people/f/frederic-everaert/>Frederic Everaert</a>
|
<a href=/people/k/koen-van-winckel/>Koen Van Winckel</a>
|
<a href=/people/a/alina-kramchaninova/>Alina Kramchaninova</a>
|
<a href=/people/a/anna-bardadym/>Anna Bardadym</a>
|
<a href=/people/t/tom-vanallemeersch/>Tom Vanallemeersch</a>
|
<a href=/people/p/pavel-smrz/>Pavel Smrž</a>
|
<a href=/people/m/michal-hradis/>Michal Hradiš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--62><div class="card-body p-3 small">The OCCAM project (Optical Character recognition, ClassificAtion & Machine Translation) aims at integrating the CEF (Connecting Europe Facility) Automated Translation service with image classification, Translation Memories (TMs), Optical Character Recognition (OCR), and Machine Translation (MT). It will support the automated translation of scanned business documents (a document format that, currently, can not be processed by the CEF eTranslation service) and will also lead to a tool useful for the Digital Humanities domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.64/>Assessing the Comprehensibility of Automatic Translations (ArisToCAT)<span class=acl-fixed-case>A</span>ris<span class=acl-fixed-case>T</span>o<span class=acl-fixed-case>CAT</span>)</a></strong><br><a href=/people/l/lieve-macken/>Lieve Macken</a>
|
<a href=/people/m/margot-fonteyne/>Margot Fonteyne</a>
|
<a href=/people/a/arda-tezcan/>Arda Tezcan</a>
|
<a href=/people/j/joke-daems/>Joke Daems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--64><div class="card-body p-3 small">The ArisToCAT project aims to assess the comprehensibility of &#8216;raw&#8217; (unedited) MT output for readers who can only rely on the MT output. In this project description, we summarize the main results of the project and present future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.eamt-1.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--eamt-1--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.eamt-1.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.eamt-1.69/>MTrill project : <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> impact on language learning<span class=acl-fixed-case>MT</span>rill project: Machine Translation impact on language learning</a></strong><br><a href=/people/n/natalia-resende/>Natália Resende</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--eamt-1--69><div class="card-body p-3 small">Over the last decades, massive research investments have been made in the development of machine translation (MT) systems (Gupta and Dhawan, 2019). This has brought about a paradigm shift in the performance of these language tools, leading to widespread use of popular MT systems (Gaspari and Hutchins, 2007). Although the first MT engines were used for gisting purposes, in recent years, there has been an increasing interest in using MT tools, especially the freely available online MT tools, for language teaching and learning (Clifford et al., 2013). The literature on MT and Computer Assisted Language Learning (CALL) shows that, over the years, MT systems have been facilitating <a href=https://en.wikipedia.org/wiki/Language_education>language teaching</a> and also <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a> (Nin o, 2006). It has been shown that MT tools can increase awareness of <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical linguistic features</a> of a foreign language. Research also shows the positive role of MT systems in the development of writing skills in <a href=https://en.wikipedia.org/wiki/English_language>English</a> as well as in improving communication skills in English(Garcia and Pena, 2011). However, to date, the cognitive impact of MT on <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> and on the syntactic aspects of language processing has not yet been investigated and deserves further scrutiny. The MTril project aims at filling this gap in the literature by examining whether MT is contributing to a central aspect of <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> : the so-called <a href=https://en.wikipedia.org/wiki/Language_binding>language binding</a>, i.e., the ability to combine single words properly in a grammatical sentence (Heyselaar et al., 2017 ; Ferreira and Bock, 2006).</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>