<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>North American Chapter of the Association for Computational Linguistics (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>North American Chapter of the Association for Computational Linguistics (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#n19-1>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a>
<span class="badge badge-info align-middle ml-1">187&nbsp;papers</span></li><li><a class=align-middle href=#n19-2>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#n19-3>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#n19-4>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#n19-5>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-13>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-14>Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w19-15>Proceedings of the Third Workshop on Structured Prediction for NLP</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-16>Proceedings of the Combined Workshop on Spatial Language Understanding (SpLU) and Grounded Communication for Robotics (RoboNLP)</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-17>Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-18>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w19-19>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-20>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-21>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-22>Proceedings of the Natural Legal Language Processing Workshop 2019</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-23>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-24>Proceedings of the First Workshop on Narrative Understanding</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w19-25>Proceedings of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w19-26>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-27>Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-28>Proceedings of the Second Workshop on Computational Models of Reference, Anaphora and Coreference</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-29>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w19-30>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li></ul></div></div><div id=n19-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N19-1/>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</a></strong><br><a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/c/christy-doran/>Christy Doran</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347364761 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1001/>Entity Recognition at First Sight : Improving <a href=https://en.wikipedia.org/wiki/Near-sightedness>NER</a> with Eye Movement Information<span class=acl-fixed-case>I</span>mproving <span class=acl-fixed-case>NER</span> with Eye Movement Information</a></strong><br><a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1001><div class="card-body p-3 small">Previous research shows that eye-tracking data contains information about the lexical and syntactic properties of text, which can be used to improve <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language processing models</a>. In this work, we leverage eye movement features from three corpora with recorded gaze information to augment a state-of-the-art neural model for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition (NER)</a> with gaze embeddings. These <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> were manually annotated with named entity labels. Moreover, we show how gaze features, generalized on word type level, eliminate the need for recorded eye-tracking data at test time. The gaze-augmented models for NER using token-level and type-level features outperform the baselines. We present the benefits of eye-tracking features by evaluating the NER models on both individual datasets as well as in cross-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1002.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347368203 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1002/>The emergence of number and syntax units in LSTM language models<span class=acl-fixed-case>LSTM</span> language models</a></strong><br><a href=/people/y/yair-lakretz/>Yair Lakretz</a>
|
<a href=/people/g/german-kruszewski/>German Kruszewski</a>
|
<a href=/people/t/theo-desbordes/>Theo Desbordes</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/s/stanislas-dehaene/>Stanislas Dehaene</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1002><div class="card-body p-3 small">Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> that do not truly take <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a> into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two number units. Importantly, the behaviour of these <a href=https://en.wikipedia.org/wiki/Unit_of_measurement>units</a> is partially controlled by other units independently shown to track <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347377574 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1004/>Neural language models as psycholinguistic subjects : Representations of syntactic state</a></strong><br><a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/t/takashi-morita/>Takashi Morita</a>
|
<a href=/people/p/peng-qian/>Peng Qian</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1004><div class="card-body p-3 small">We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> have a representation of syntactic state, they also reveal the specific lexical cues that <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> use to update these states. We test four models : two publicly available LSTM sequence models of English (Jozefowicz et al., 2016 ; Gulordava et al., 2018) trained on large datasets ; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset ; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, but only the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347381430 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1005/>Understanding language-elicited EEG data by predicting it from a fine-tuned language model<span class=acl-fixed-case>EEG</span> data by predicting it from a fine-tuned language model</a></strong><br><a href=/people/d/dan-schwartz/>Dan Schwartz</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1005><div class="card-body p-3 small">Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a> and psycholinguistics communities as a tool to study <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension</a>. Several time-locked stereotyped EEG responses to word-presentations known collectively as event-related potentials (ERPs) are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual <a href=https://en.wikipedia.org/wiki/Enterprise_resource_planning>ERP</a> in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>language comprehension</a>. We take a step towards better understanding the ERPs by finetuning a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the <a href=https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential>ERPs</a> to be predictable. In addition to this <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a>, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of <a href=https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential>ERPs</a> previously identified in the literature as being related to each other benefit from joint training, while several other pairs of <a href=https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential>ERPs</a> that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353440477 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1007/>Measuring the perceptual availability of <a href=https://en.wikipedia.org/wiki/Phonology>phonological features</a> during <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> using unsupervised binary stochastic autoencoders</a></strong><br><a href=/people/c/cory-shain/>Cory Shain</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1007><div class="card-body p-3 small">In this paper, we deploy binary stochastic neural autoencoder networks as models of infant language learning in two typologically unrelated languages (Xitsonga and English). We show that the drive to model auditory percepts leads to latent clusters that partially align with theory-driven phonemic categories. We further evaluate the degree to which theory-driven phonological features are encoded in the latent bit patterns, finding that some (e.g. [ + -approximant ]), are well represented by the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> in both languages, while others (e.g. [ + -spread glottis ]) are less so. Together, these findings suggest that many reliable cues to phonemic structure are immediately available to infants from <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up perceptual characteristics</a> alone, but that these cues must eventually be supplemented by <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down lexical and phonotactic information</a> to achieve adult-like phone discrimination. Our results also suggest differences in degree of perceptual availability between <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>, yielding testable predictions as to which <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> might depend more or less heavily on <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down cues</a> during child language acquisition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353444632 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1008/>Giving Attention to the Unexpected : Using Prosody Innovations in Disfluency Detection</a></strong><br><a href=/people/v/vicky-zayats/>Vicky Zayats</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1008><div class="card-body p-3 small">Disfluencies in spontaneous speech are known to be associated with prosodic disruptions. However, most <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for disfluency detection use only <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>word transcripts</a>. Integrating prosodic cues has proved difficult because of the many sources of variability affecting the acoustic correlates. This paper introduces a new approach to extracting acoustic-prosodic cues using text-based distributional prediction of acoustic cues to derive vector z-score features (innovations). We explore both early and late fusion techniques for integrating text and prosody, showing gains over a high-accuracy text-only model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353450338 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1009/>Massively Multilingual Adversarial Speech Recognition</a></strong><br><a href=/people/o/oliver-adams/>Oliver Adams</a>
|
<a href=/people/m/matthew-wiesner/>Matthew Wiesner</a>
|
<a href=/people/s/shinji-watanabe/>Shinji Watanabe</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1009><div class="card-body p-3 small">We report on adaptation of multilingual end-to-end speech recognition models trained on as many as 100 languages. Our findings shed light on the relative importance of similarity between the target and pretraining languages along the dimensions of <a href=https://en.wikipedia.org/wiki/Phonetics>phonetics</a>, <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>, <a href=https://en.wikipedia.org/wiki/Language_family>language family</a>, geographical location, and <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>. In this context, experiments demonstrate the effectiveness of two additional pretraining objectives in encouraging language-independent encoder representations : a context-independent phoneme objective paired with a language-adversarial classification objective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1013.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353418933 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1013/>Answer-based Adversarial Training for Generating Clarification Questions<span class=acl-fixed-case>A</span>nswer-based <span class=acl-fixed-case>A</span>dversarial <span class=acl-fixed-case>T</span>raining for <span class=acl-fixed-case>G</span>enerating <span class=acl-fixed-case>C</span>larification <span class=acl-fixed-case>Q</span>uestions</a></strong><br><a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1013><div class="card-body p-3 small">We present an approach for generating clarification questions with the goal of eliciting new information that would make the given textual context more complete. We propose that modeling hypothetical answers (to clarification questions) as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> can guide our approach into generating more useful clarification questions. We develop a Generative Adversarial Network (GAN) where the generator is a sequence-to-sequence model and the discriminator is a utility function that models the value of updating the context with the answer to the clarification question. We evaluate on two datasets, using both automatic metrics and human judgments of usefulness, specificity and relevance, showing that our approach outperforms both a retrieval-based model and ablations that exclude the utility model and the adversarial training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353425373 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1014/>Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/l/liang-wang/>Liang Wang</a>
|
<a href=/people/k/kewei-shen/>Kewei Shen</a>
|
<a href=/people/r/ruoyu-jia/>Ruoyu Jia</a>
|
<a href=/people/j/jingming-liu/>Jingming Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1014><div class="card-body p-3 small">Neural machine translation systems have become state-of-the-art approaches for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence. Since the GEC suffers from not having enough labeled training data to achieve high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We pre-train the copy-augmented architecture with a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons between the fully pre-trained model and a partially pre-trained model. It is the first time copying words from the source context and fully pre-training a sequence to sequence model are experimented on the GEC task. Moreover, We add token-level and sentence-level multi-task learning for the GEC task. The evaluation results on the CoNLL-2014 test set show that our approach outperforms all recently published state-of-the-art results by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353433493 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1015/>Topic-Guided Variational Auto-Encoder for Text Generation</a></strong><br><a href=/people/w/wenlin-wang/>Wenlin Wang</a>
|
<a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/h/hongteng-xu/>Hongteng Xu</a>
|
<a href=/people/r/ruiyi-zhang/>Ruiyi Zhang</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/c/changyou-chen/>Changyou Chen</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1015><div class="card-body p-3 small">We propose a topic-guided variational auto-encoder (TGVAE) model for text generation. Distinct from existing variational auto-encoder (VAE) based approaches, which assume a simple Gaussian prior for latent code, our model specifies the prior as a Gaussian mixture model (GMM) parametrized by a neural topic module. Each mixture component corresponds to a latent topic, which provides a guidance to generate sentences under the topic. The neural topic module and the VAE-based neural sequence module in our model are learned jointly. In particular, a sequence of invertible Householder transformations is applied to endow the approximate posterior of the latent code with high flexibility during the <a href=https://en.wikipedia.org/wiki/Statistical_inference>model inference</a>. Experimental results show that our TGVAE outperforms its competitors on both unconditional and conditional text generation, which can also generate semantically-meaningful sentences with various topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360494509 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1018/>Discontinuous Constituency Parsing with a Stack-Free Transition System and a Dynamic Oracle</a></strong><br><a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1018><div class="card-body p-3 small">We introduce a novel <a href=https://en.wikipedia.org/wiki/Transition_system>transition system</a> for discontinuous constituency parsing. Instead of storing subtrees in a <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>stack</a> i.e. a <a href=https://en.wikipedia.org/wiki/Data_structure>data structure</a> with linear-time sequential access the proposed system uses a set of parsing items, with constant-time random access. This change makes it possible to construct any discontinuous constituency tree in exactly 4n2 transitions for a sentence of length n. At each parsing step, the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> considers every item in the set to be combined with a focus item and to construct a new constituent in a bottom-up fashion. The parsing strategy is based on the assumption that most syntactic structures can be parsed incrementally and that the set the memory of the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> remains reasonably small on average. Moreover, we introduce a provably correct dynamic oracle for the new transition system, and present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> obtains state-of-the-art results on three English and German discontinuous treebanks.<tex-math>4n&#8211;2</tex-math> transitions for a sentence of length n. At each parsing step, the parser considers every item in the set to be combined with a focus item and to construct a new constituent in a bottom-up fashion. The parsing strategy is based on the assumption that most syntactic structures can be parsed incrementally and that the set &#8211;the memory of the parser&#8211; remains reasonably small on average. Moreover, we introduce a provably correct dynamic oracle for the new transition system, and present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our parser obtains state-of-the-art results on three English and German discontinuous treebanks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360516550 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1020/>CCG Parsing Algorithm with Incremental Tree Rotation<span class=acl-fixed-case>CCG</span> Parsing Algorithm with Incremental Tree Rotation</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1020><div class="card-body p-3 small">The main obstacle to incremental sentence processing arises from right-branching constituent structures, which are present in the majority of English sentences, as well as optional constituents that adjoin on the right, such as <a href=https://en.wikipedia.org/wiki/Adjunct_(grammar)>right adjuncts</a> and right conjuncts. In CCG, many right-branching derivations can be replaced by semantically equivalent left-branching incremental derivations. The problem of right-adjunction is more resistant to solution, and has been tackled in the past using revealing-based approaches that often rely either on the <a href=https://en.wikipedia.org/wiki/Unification_(computer_science)>higher-order unification</a> over lambda terms (Pareschi and Steedman,1987) or heuristics over dependency representations that do not cover the whole CCGbank (Ambati et al., 2015). We propose a new incremental parsing algorithm for CCG following the same revealing tradition of work but having a purely syntactic approach that does not depend on access to a distinct level of semantic representation. This <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can cover the whole CCGbank, with greater incrementality and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than previous proposals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1021.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1021/>Cyclical Annealing Schedule : A Simple Approach to Mitigating KL Vanishing<span class=acl-fixed-case>KL</span> Vanishing</a></strong><br><a href=/people/h/hao-fu/>Hao Fu</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1021><div class="card-body p-3 small">Variational autoencoders (VAE) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. VAE objective consists of two terms, the KL regularization term and the reconstruction term, balanced by a weighting hyper-parameter. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a>. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, dialog response generation and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised text classification</a>.<tex-math>\\beta</tex-math>. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for <tex-math>\\beta</tex-math>, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of optimization. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing <tex-math>\\beta</tex-math> multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including language modeling, dialog response generation and semi-supervised text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1022/>Recurrent models and lower bounds for projective syntactic decoding</a></strong><br><a href=/people/n/natalie-schluter/>Natalie Schluter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1022><div class="card-body p-3 small">The current state-of-the-art in neural graph-based parsing uses only approximate decoding at the training phase. In this paper aim to understand this result better. We show how recurrent models can carry out projective maximum spanning tree decoding. This result holds for both current state-of-the-art models for shift-reduce and graph-based parsers, projective or not. We also provide the first proof on the <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>lower bounds</a> of projective maximum spanning tree decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1023/>Evaluating Composition Models for Verb Phrase Elliptical Sentence Embeddings</a></strong><br><a href=/people/g/gijs-wijnholds/>Gijs Wijnholds</a>
|
<a href=/people/m/mehrnoosh-sadrzadeh/>Mehrnoosh Sadrzadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1023><div class="card-body p-3 small">Ellipsis is a natural language phenomenon where part of a sentence is missing and its information must be recovered from its surrounding context, as in Cats chase dogs and so do foxes.. Formal semantics has different methods for resolving <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a> and recovering the missing information, but the problem has not been considered for <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>, where words have vector embeddings and combinations thereof provide <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for sentences. In elliptical sentences these combinations go beyond linear as copying of elided information is necessary. In this paper, we develop different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for embedding VP-elliptical sentences. We extend existing verb disambiguation and sentence similarity datasets to ones containing elliptical phrases and evaluate our models on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for a variety of non-linear combinations and their linear counterparts. We compare results of these compositional models to state of the art holistic sentence encoders. Our results show that non-linear addition and a non-linear tensor-based composition outperform the naive non-compositional baselines and the linear models, and that sentence encoders perform well on sentence similarity, but not on verb disambiguation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1025.Software.tar data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1025.Presentation.pptx data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1025" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1025/>Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for Text Modeling<span class=acl-fixed-case>R</span>iemannian Normalizing Flow on Variational <span class=acl-fixed-case>W</span>asserstein Autoencoder for Text Modeling</a></strong><br><a href=/people/p/prince-zizhuang-wang/>Prince Zizhuang Wang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1025><div class="card-body p-3 small">Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks. These models often face a difficult optimization problem, also known as KL vanishing, where the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior</a> easily collapses to the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> and model will ignore latent codes in generative tasks. To address this problem, we introduce an improved Variational Wasserstein Autoencoder (WAE) with Riemannian Normalizing Flow (RNF) for text modeling. The RNF transforms a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> into a space that respects the geometric characteristics of input space, which makes posterior impossible to collapse to the <a href=https://en.wikipedia.org/wiki/Non-informative_prior>non-informative prior</a>. The Wasserstein objective minimizes the distance between <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginal distribution</a> and the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> directly and therefore does not force the posterior to match the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a>. Empirical experiments show that our model avoids KL vanishing over a range of datasets and has better performance in tasks such as <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Likelihood_function>likelihood approximation</a>, and text generation. Through a series of experiments and analysis over latent space, we show that our model learns latent distributions that respect latent space geometry and is able to generate sentences that are more diverse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1027/>ComQA : A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters<span class=acl-fixed-case>C</span>om<span class=acl-fixed-case>QA</span>: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters</a></strong><br><a href=/people/a/abdalghani-abujabal/>Abdalghani Abujabal</a>
|
<a href=/people/r/rishiraj-saha-roy/>Rishiraj Saha Roy</a>
|
<a href=/people/m/mohamed-yahya/>Mohamed Yahya</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1027><div class="card-body p-3 small">To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these <a href=https://en.wikipedia.org/wiki/Questionnaire>questions</a> are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a>, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> with their answers. ComQA contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We also present an extensive analysis of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the results achieved by state-of-the-art systems on ComQA, demonstrating that our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be a driver of future research on <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1030/>Learning to Attend On Essential Terms : An Enhanced Retriever-Reader Model for Open-domain Question Answering</a></strong><br><a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1030><div class="card-body p-3 small">Open-domain question answering remains a challenging task as it requires <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that are capable of understanding questions and answers, collecting useful information, and reasoning over evidence. Previous work typically formulates this task as a reading comprehension or entailment problem given evidence retrieved from <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a>. However, existing techniques struggle to retrieve indirectly related evidence when no directly related evidence is provided, especially for complex questions where it is hard to parse precisely what the question asks. In this paper we propose a retriever-reader model that learns to attend on essential terms during the <a href=https://en.wikipedia.org/wiki/Question_answering>question answering process</a>. We build (1) an essential term selector which first identifies the most important words in a question, then reformulates the query and searches for related evidence ; and (2) an enhanced reader that distinguishes between essential terms and distracting words to predict the answer. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on multiple open-domain QA datasets, notably achieving the level of the state-of-the-art on the AI2 Reasoning Challenge (ARC) dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1034/>Multi-task Learning for Multi-modal Emotion Recognition and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/d/dushyant-chauhan/>Dushyant Chauhan</a>
|
<a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1034><div class="card-body p-3 small">Related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multi-modal inputs</a> (i.e. text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance. We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment and emotion analysis. Evaluation results suggest that multi-task learning framework offers improvement over the single-task framework. The proposed approach reports new state-of-the-art performance for both <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and emotion analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1038.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1038/>Learning Interpretable Negation Rules via Weak Supervision at Document Level : A Reinforcement Learning Approach</a></strong><br><a href=/people/n/nicolas-prollochs/>Nicolas Pröllochs</a>
|
<a href=/people/s/stefan-feuerriegel/>Stefan Feuerriegel</a>
|
<a href=/people/d/dirk-neumann/>Dirk Neumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1038><div class="card-body p-3 small">Negation scope detection is widely performed as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning task</a> which relies upon <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation labels</a> at word level. This suffers from two key drawbacks : (1) such granular annotations are costly and (2) highly subjective, since, due to the absence of explicit linguistic resolution rules, human annotators often disagree in the perceived negation scopes. To the best of our knowledge, our work presents the first approach that eliminates the need for world-level negation labels, replacing it instead with document-level sentiment annotations. For this, we present a novel strategy for learning fully interpretable negation rules via weak supervision : we apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to find a policy that reconstructs negation rules from sentiment predictions at document level. Our experiments demonstrate that our approach for weak supervision can effectively learn negation rules. Furthermore, an <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>out-of-sample evaluation</a> via <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> reveals consistent improvements (of up to 4.66 %) over both a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> with (i) no <a href=https://en.wikipedia.org/wiki/Negation>negation handling</a> and (ii) the use of word-level annotations from humans. Moreover, the inferred negation rules are fully interpretable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1041.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1041/>ReWE : Regressing Word Embeddings for Regularization of Neural Machine Translation Systems<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>WE</span>: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems</a></strong><br><a href=/people/i/inigo-jauregi-unanue/>Inigo Jauregi Unanue</a>
|
<a href=/people/e/ehsan-zare-borzeshi/>Ehsan Zare Borzeshi</a>
|
<a href=/people/n/nazanin-esmaili/>Nazanin Esmaili</a>
|
<a href=/people/m/massimo-piccardi/>Massimo Piccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1041><div class="card-body p-3 small">Regularization of neural machine translation is still a significant problem, especially in <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>low-resource settings</a>. To mollify this problem, we propose regressing word embeddings (ReWE) as a new regularization technique in a system that is jointly trained to predict the next word in the translation (categorical value) and its word embedding (continuous value). Such a joint training allows the proposed system to learn the distributional properties represented by the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, empirically improving the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> to unseen sentences. Experiments over three translation datasets have showed a consistent improvement over a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, ranging between 0.91 and 2.4 BLEU points, and also a marked improvement over a <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1042" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1042/>Lost in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> : A Method to Reduce Meaning Loss</a></strong><br><a href=/people/r/reuben-cohn-gordon/>Reuben Cohn-Gordon</a>
|
<a href=/people/n/noah-goodman/>Noah Goodman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1042><div class="card-body p-3 small">A desideratum of high-quality translation systems is that they preserve meaning, in the sense that two sentences with different meanings should not translate to one and the same sentence in another language. However, state-of-the-art systems often fail in this regard, particularly in cases where the source and target languages partition the meaning space in different ways. For instance, I cut my finger. and I cut my finger off. describe different states of the world but are translated to <a href=https://en.wikipedia.org/wiki/French_language>French</a> (by both Fairseq and Google Translate) as Je me suis coup le doigt., which is ambiguous as to whether the finger is detached. More generally, <a href=https://en.wikipedia.org/wiki/Translation>translation systems</a> are typically many-to-one (non-injective) functions from source to target language, which in many cases results in important distinctions in meaning being lost in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Building on Bayesian models of informative utterance production, we present a method to define a less ambiguous translation system in terms of an underlying pre-trained neural sequence-to-sequence model. This method increases <a href=https://en.wikipedia.org/wiki/Injectivity>injectivity</a>, resulting in greater preservation of meaning as measured by improvement in cycle-consistency, without impeding translation quality (measured by BLEU score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1044/>Code-Switching for Enhancing <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> with Pre-Specified Translation<span class=acl-fixed-case>NMT</span> with Pre-Specified Translation</a></strong><br><a href=/people/k/kai-song/>Kai Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/h/heng-yu/>Heng Yu</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a>
|
<a href=/people/k/kun-wang/>Kun Wang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1044><div class="card-body p-3 small">Leveraging user-provided translation to constrain <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> has practical significance. Existing methods can be classified into two main categories, namely the use of <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>placeholder tags</a> for lexicon words and the use of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>hard constraints</a> during decoding. Both methods can hurt translation fidelity for various reasons. We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations. Our method does not change the MNT model or decoding algorithm, allowing the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to learn lexicon translations by copying source-side target words. Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1047.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1047.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1047.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1047" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1047/>Content Differences in Syntactic and Semantic Representation</a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1047><div class="card-body p-3 small">Syntactic analysis plays an important role in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, but the nature of this <a href=https://en.wikipedia.org/wiki/Role>role</a> remains a topic of ongoing debate. The debate has been constrained by the scarcity of empirical comparative studies between syntactic and semantic schemes, which hinders the development of parsing methods informed by the details of target schemes and constructions. We target this gap, and take Universal Dependencies (UD) and <a href=https://en.wikipedia.org/wiki/UCCA>UCCA</a> as a test case. After abstracting away from differences of <a href=https://en.wikipedia.org/wiki/Convention_(norm)>convention</a> or formalism, we find that most content divergences can be ascribed to : (1) UCCA&#8217;s distinction between a Scene and a non-Scene ; (2) UCCA&#8217;s distinction between primary relations, secondary ones and participants ; (3) different treatment of multi-word expressions, and (4) different treatment of inter-clause linkage. We further discuss the long tail of cases where the two <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>schemes</a> take markedly different approaches. Finally, we show that the proposed comparison methodology can be used for fine-grained evaluation of UCCA parsing, highlighting both challenges and potential sources for improvement. The substantial differences between the schemes suggest that semantic parsers are likely to benefit downstream text understanding applications beyond their syntactic counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1048.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1048/>Attentive Mimicking : Better Word Embeddings by Attending to Informative Contexts</a></strong><br><a href=/people/t/timo-schick/>Timo Schick</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1048><div class="card-body p-3 small">Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution : given <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learned by a standard algorithm, a model is first trained to reproduce <a href=https://en.wikipedia.org/wiki/Embedding>embeddings of frequent words</a> from their surface form and then used to compute <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for rare words. In this paper, we introduce attentive mimicking : the mimicking model is given access not only to a word&#8217;s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a>. In an evaluation on four tasks, we show that attentive mimicking outperforms previous <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a much larger part of the vocabulary, including the <a href=https://en.wikipedia.org/wiki/Medium_frequency>medium-frequency range</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1049/>Evaluating Style Transfer for Text</a></strong><br><a href=/people/r/remi-mir/>Remi Mir</a>
|
<a href=/people/b/bjarke-felbo/>Bjarke Felbo</a>
|
<a href=/people/n/nick-obradovich/>Nick Obradovich</a>
|
<a href=/people/i/iyad-rahwan/>Iyad Rahwan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1049><div class="card-body p-3 small">Research in the area of style transfer for <a href=https://en.wikipedia.org/wiki/Writing>text</a> is currently bottlenecked by a lack of standard evaluation practices. This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset. We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work. We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment : direction-corrected Earth Mover&#8217;s Distance, Word Mover&#8217;s Distance on style-masked texts, and adversarial classification for the respective aspects. We also show that the three examined <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> exhibit tradeoffs between aspects of interest, demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots. We release software with our evaluation metrics to facilitate research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1051/>Outlier Detection for Improved Data Quality and Diversity in Dialog Systems</a></strong><br><a href=/people/s/stefan-larson/>Stefan Larson</a>
|
<a href=/people/a/anish-mahendran/>Anish Mahendran</a>
|
<a href=/people/a/andrew-lee/>Andrew Lee</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/p/parker-hill/>Parker Hill</a>
|
<a href=/people/m/michael-a-laurenzano/>Michael A. Laurenzano</a>
|
<a href=/people/j/johann-hauswald/>Johann Hauswald</a>
|
<a href=/people/l/lingjia-tang/>Lingjia Tang</a>
|
<a href=/people/j/jason-mars/>Jason Mars</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1051><div class="card-body p-3 small">In a <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus of data</a>, <a href=https://en.wikipedia.org/wiki/Outlier>outliers</a> are either errors : mistakes in the data that are counterproductive, or are unique : informative samples that improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>. Identifying outliers can lead to better datasets by (1) removing noise in datasets and (2) guiding collection of additional data to fill gaps. However, the problem of detecting both outlier types has received relatively little attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, particularly for dialog systems. We introduce a simple and effective technique for detecting both erroneous and unique samples in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of short texts</a> using neural sentence embeddings combined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iteratively mine unique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1053/>Seeing Things from a Different Angle : Discovering Diverse Perspectives about Claims</a></strong><br><a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1053><div class="card-body p-3 small">One key consequence of the <a href=https://en.wikipedia.org/wiki/Information_revolution>information revolution</a> is a significant increase and a contamination of our information supply. The practice of <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a> wo n&#8217;t suffice to eliminate the biases in text data we observe, as the degree of <a href=https://en.wikipedia.org/wiki/Fact>factuality</a> alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as animals should have lawful rights, and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding task</a>, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct PERSPECTRUM, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of claims, perspectives and evidence, making use of online debate websites to create the initial <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, and augmenting it using <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> in order to expand and diversify our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We use <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> to filter out noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353455637 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1057/>Improving Dialogue State Tracking by Discerning the Relevant Context</a></strong><br><a href=/people/s/sanuj-sharma/>Sanuj Sharma</a>
|
<a href=/people/p/prafulla-kumar-choubey/>Prafulla Kumar Choubey</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1057><div class="card-body p-3 small">A typical conversation comprises of multiple turns between participants where they go back and forth between different topics. At each user turn, dialogue state tracking (DST) aims to estimate user&#8217;s goal by processing the current utterance. However, in many turns, users implicitly refer to the previous goal, necessitating the use of relevant dialogue history. Nonetheless, distinguishing relevant history is challenging and a popular method of using dialogue recency for that is inefficient. We, therefore, propose a novel framework for DST that identifies relevant historical context by referring to the past utterances where a particular slot-value changes and uses that together with weighted system utterance to identify the relevant context. Specifically, we use the current user utterance and the most recent system utterance to determine the relevance of a system utterance. Empirical analyses show that our method improves <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>joint goal accuracy</a> by 2.75 % and 2.36 % on WoZ 2.0 and Multi-WoZ restaurant domain datasets respectively over the previous state-of-the-art GLAD model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347386459 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1060/>Detection of Abusive Language : the Problem of Biased Datasets<span class=acl-fixed-case>D</span>etection of <span class=acl-fixed-case>A</span>busive <span class=acl-fixed-case>L</span>anguage: the <span class=acl-fixed-case>P</span>roblem of <span class=acl-fixed-case>B</span>iased <span class=acl-fixed-case>D</span>atasets</a></strong><br><a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/t/thomas-kleinbauer/>Thomas Kleinbauer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1060><div class="card-body p-3 small">We discuss the impact of data bias on abusive language detection. We show that classification scores on popular <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that are created by focused sampling instead of <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a>. Datasets with a higher proportion of implicit abuse are more affected than <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with a lower proportion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347389631 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1061/>Lipstick on a Pig : Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them<span class=acl-fixed-case>D</span>ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1061><div class="card-body p-3 small">Word embeddings are widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for a vast range of tasks. It was shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> derived from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, demonstrating convincing results. However, we argue that this removal is superficial. While the <a href=https://en.wikipedia.org/wiki/Bias>bias</a> is indeed substantially reduced according to the provided <a href=https://en.wikipedia.org/wiki/Bias>bias definition</a>, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between gender-neutralized words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1063.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1063.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347394290 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1063" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1063/>On Measuring Social Biases in Sentence Encoders</a></strong><br><a href=/people/c/chandler-may/>Chandler May</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/s/shikha-bordia/>Shikha Bordia</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1063><div class="card-body p-3 small">The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test&#8217;s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347396468 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1064/>Gender Bias in Contextualized Word Embeddings</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1064><div class="card-body p-3 small">In this paper, we quantify, analyze and mitigate <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> exhibited in ELMo&#8217;s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356020948 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1065" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1065/>Combining Sentiment Lexica with a Multi-View Variational Autoencoder<span class=acl-fixed-case>C</span>ombining <span class=acl-fixed-case>S</span>entiment <span class=acl-fixed-case>L</span>exica with a <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>V</span>iew <span class=acl-fixed-case>V</span>ariational <span class=acl-fixed-case>A</span>utoencoder</a></strong><br><a href=/people/a/alexander-miserlis-hoyle/>Alexander Miserlis Hoyle</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1065><div class="card-body p-3 small">When assigning quantitative labels to a dataset, different <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets ; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1067.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1067.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355760337 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1067/>Frowning Frodo, Wincing Leia, and a Seriously Great Friendship : Learning to Classify Emotional Relationships of Fictional Characters<span class=acl-fixed-case>F</span>rodo, Wincing <span class=acl-fixed-case>L</span>eia, and a Seriously Great Friendship: Learning to Classify Emotional Relationships of Fictional Characters</a></strong><br><a href=/people/e/evgeny-kim/>Evgeny Kim</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1067><div class="card-body p-3 small">The development of a <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>fictional plot</a> is centered around characters who closely interact with each other forming dynamic social networks. In literature analysis, such networks have mostly been analyzed without particular relation types or focusing on roles which the characters take with respect to each other. We argue that an important aspect for the analysis of stories and their development is the emotion between characters. In this paper, we combine these aspects into a unified framework to classify emotional relationships of fictional characters. We formalize it as a new task and describe the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, based on <a href=https://en.wikipedia.org/wiki/Fan_fiction>fan-fiction short stories</a>. The extraction pipeline which we propose consists of character identification (which we treat as given by an oracle here) and the relation classification. For the latter, we provide results using several approaches previously proposed for relation identification with <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a>. The best result of 0.45 F1 is achieved with a <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>GRU</a> with character position indicators on the task of predicting undirected emotion relations in the associated <a href=https://en.wikipedia.org/wiki/Social_network>social network graph</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1071.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353462534 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1071/>SEQ3 : Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression<span class=acl-fixed-case>SEQ</span>ˆ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</a></strong><br><a href=/people/c/christos-baziotis/>Christos Baziotis</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1071><div class="card-body p-3 small">Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from <a href=https://en.wikipedia.org/wiki/Categorical_distribution>categorical distributions</a>, allowing gradient-based optimization, unlike alternatives that rely on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353467177 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1072" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1072/>Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation</a></strong><br><a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/d/david-gabay/>David Gabay</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/h/hadar-ronen/>Hadar Ronen</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/y/yael-amsterdamer/>Yael Amsterdamer</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1072><div class="card-body p-3 small">Conducting a manual evaluation is considered an essential part of summary evaluation methodology. Traditionally, the Pyramid protocol, which exhaustively compares system summaries to references, has been perceived as very reliable, providing objective scores. Yet, due to the high cost of the Pyramid method and the required expertise, researchers resorted to cheaper and less thorough manual evaluation methods, such as <a href=https://en.wikipedia.org/wiki/Responsiveness>Responsiveness</a> and <a href=https://en.wikipedia.org/wiki/Pairwise_comparison>pairwise comparison</a>, attainable via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We revisit the Pyramid approach, proposing a lightweight sampling-based version that is crowdsourcable. We analyze the performance of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in comparison to original expert-based Pyramid evaluations, showing higher <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a> relative to the common Responsiveness method. We release our crowdsourced Summary-Content-Units, along with all crowdsourcing scripts, for future evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1076" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1076/>Left-to-Right Dependency Parsing with Pointer Networks</a></strong><br><a href=/people/d/daniel-fernandez-gonzalez/>Daniel Fernández-González</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1076><div class="card-body p-3 small">We propose a novel transition-based algorithm that straightforwardly parses sentences from left to right by building n attachments, with n being the length of the input sentence. Similarly to the recent stack-pointer parser by Ma et al. (2018), we use the pointer network framework that, given a word, can directly point to a position from the sentence. However, our left-to-right approach is simpler than the original top-down stack-pointer parser (not requiring a stack) and reduces transition sequence length in half, from 2n-1 actions to n. This results in a quadratic non-projective parser that runs twice as fast as the original while achieving the best accuracy to date on the English PTB dataset (96.04 % UAS, 94.43 % LAS) among fully-supervised single-model dependency parsers, and improves over the former top-down transition system in the majority of languages tested.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1079.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1079.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360565437 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1079/>Better Modeling of Incomplete Annotations for Named Entity Recognition</a></strong><br><a href=/people/z/zhanming-jie/>Zhanming Jie</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/r/ruixue-ding/>Ruixue Ding</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1079><div class="card-body p-3 small">Supervised approaches to named entity recognition (NER) are largely developed based on the assumption that the training data is fully annotated with named entity information. However, in practice, annotated data can often be imperfect with one typical issue being the training data may contain incomplete annotations. We highlight several pitfalls associated with learning under such a setup in the context of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>NER</a> and identify limitations associated with existing approaches, proposing a novel yet easy-to-implement approach for recognizing named entities with incomplete data annotations. We demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> through extensive experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1088" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1088/>Adversarial Decomposition of Text Representation</a></strong><br><a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/d/david-donahue/>David Donahue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1088><div class="card-body p-3 small">In this paper, we present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for adversarial decomposition of text representation. This method can be used to decompose a representation of an input sentence into several independent vectors, each of them responsible for a specific aspect of the input sentence. We evaluate the proposed method on two <a href=https://en.wikipedia.org/wiki/Case_study>case studies</a> : the conversion between different <a href=https://en.wikipedia.org/wiki/Register_(sociolinguistics)>social registers</a> and <a href=https://en.wikipedia.org/wiki/Language_change>diachronic language change</a>. We show that the proposed method is capable of fine-grained controlled change of these aspects of the input sentence. It is also learning a continuous (rather than categorical) representation of the style of the sentence, which is more linguistically realistic. The model uses adversarial-motivational training and includes a special motivational loss, which acts opposite to the discriminator and encourages a better decomposition. Furthermore, we evaluate the obtained meaning embeddings on a downstream task of <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> and show that they significantly outperform the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> of a regular autoencoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1095" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1095/>Recovering dropped pronouns in Chinese conversations via modeling their referents<span class=acl-fixed-case>C</span>hinese conversations via modeling their referents</a></strong><br><a href=/people/j/jingxuan-yang/>Jingxuan Yang</a>
|
<a href=/people/j/jianzhuo-tong/>Jianzhuo Tong</a>
|
<a href=/people/s/si-li/>Si Li</a>
|
<a href=/people/s/sheng-gao/>Sheng Gao</a>
|
<a href=/people/j/jun-guo/>Jun Guo</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1095><div class="card-body p-3 small">Pronouns are often dropped in Chinese sentences, and this happens more frequently in conversational genres as their referents can be easily understood from context. Recovering dropped pronouns is essential to applications such as <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> where the referents of these dropped pronouns need to be resolved, or <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> when Chinese is the source language. In this work, we present a novel end-to-end neural network model to recover <a href=https://en.wikipedia.org/wiki/Pronoun>dropped pronouns</a> in <a href=https://en.wikipedia.org/wiki/Conversation>conversational data</a>. Our model is based on a structured attention mechanism that models the referents of dropped pronouns utilizing both sentence-level and word-level information. Results on three different conversational genres show that our approach achieves a significant improvement over the current state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1097" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1097/>A Systematic Study of Leveraging Subword Information for Learning Word Representations</a></strong><br><a href=/people/y/yi-zhu/>Yi Zhu</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1097><div class="card-body p-3 small">The use of subword-level information (e.g., <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>, <a href=https://en.wikipedia.org/wiki/Character_(symbol)>character n-grams</a>, morphemes) has become ubiquitous in modern word representation learning. Its importance is attested especially for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a> which generate a large number of rare words. Despite a steadily increasing interest in such subword-informed word representations, their systematic comparative analysis across typologically diverse languages and different tasks is still missing. In this work, we deliver such a study focusing on the variation of two crucial components required for subword-level integration into word representation models : 1) segmentation of words into subword units, and 2) subword composition functions to obtain final word representations. We propose a general framework for learning subword-informed word representations that allows for easy experimentation with different segmentation and composition components, also including more advanced techniques based on position embeddings and self-attention. Using the unified framework, we run experiments over a large number of subword-informed word representation configurations (60 in total) on 3 tasks (general and rare word similarity, dependency parsing, fine-grained entity typing) for 5 languages representing 3 language types. Our main results clearly indicate that there is no one-size-fits-all configuration, as performance is both language- and task-dependent. We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1099/>Integration of Knowledge Graph Embedding Into <a href=https://en.wikipedia.org/wiki/Topic_model>Topic Modeling</a> with Hierarchical Dirichlet Process<span class=acl-fixed-case>D</span>irichlet Process</a></strong><br><a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/s/siamak-zamani/>Siamak Zamani</a>
|
<a href=/people/j/jingyuan-zhang/>Jingyuan Zhang</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1099><div class="card-body p-3 small">Leveraging <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a>, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.<i>topic modeling with knowledge graph embedding</i> (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1101/>Generating Token-Level Explanations for Natural Language Inference</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1101><div class="card-body p-3 small">The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on a single piece of text, there have been no attempts to generate explanations of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment relation</a>. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as the black-box methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/353480570 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1103/>Adaptive Convolution for Multi-Relational Learning</a></strong><br><a href=/people/x/xiaotian-jiang/>Xiaotian Jiang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1103><div class="card-body p-3 small">We consider the problem of learning <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> for entities and relations of multi-relational data so as to predict missing links therein. Convolutional neural networks have recently shown their superiority for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, bringing increased model expressiveness while remaining parameter efficient. Despite the success, previous convolution designs fail to model full interactions between input entities and relations, which potentially limits the performance of link prediction. In this work we introduce ConvR, an adaptive convolutional network designed to maximize entity-relation interactions in a convolutional fashion. ConvR adaptively constructs convolution filters from <a href=https://en.wikipedia.org/wiki/Binary_relation>relation representations</a>, and applies these filters across <a href=https://en.wikipedia.org/wiki/Binary_relation>entity representations</a> to generate <a href=https://en.wikipedia.org/wiki/Convolution>convolutional features</a>. As such, ConvR enables rich interactions between entity and relation representations at diverse regions, and all the <a href=https://en.wikipedia.org/wiki/Convolution>convolutional features</a> generated will be able to capture such <a href=https://en.wikipedia.org/wiki/Interaction>interactions</a>. We evaluate ConvR on multiple benchmark datasets. Experimental results show that : (1) ConvR performs substantially better than competitive baselines in almost all the metrics and on all the datasets ; (2) Compared with state-of-the-art convolutional models, ConvR is not only more effective but also more efficient. It offers a 7 % increase in MRR and a 6 % increase in Hits@10, while saving 12 % in parameter storage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360608466 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1107" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1107/>Relation Extraction with Temporal Reasoning Based on Memory Augmented Distant Supervision</a></strong><br><a href=/people/j/jianhao-yan/>Jianhao Yan</a>
|
<a href=/people/l/lin-he/>Lin He</a>
|
<a href=/people/r/ruqin-huang/>Ruqin Huang</a>
|
<a href=/people/j/jian-li/>Jian Li</a>
|
<a href=/people/y/ying-liu/>Ying Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1107><div class="card-body p-3 small">Distant supervision (DS) is an important paradigm for automatically extracting relations. It utilizes existing <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> to collect examples for the relation we intend to extract, and then uses these examples to automatically generate the training data. However, the examples collected can be very noisy, and pose significant challenge for obtaining high quality labels. Previous work has made remarkable progress in predicting the relation from distant supervision, but typically ignores the temporal relations among those supervising instances. This paper formulates the problem of relation extraction with temporal reasoning and proposes a solution to predict whether two given entities participate in a relation at a given time spot. For this purpose, we construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called WIKI-TIME which additionally includes the valid period of a certain relation of two entities in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. We propose a novel neural model to incorporate both the temporal information encoding and sequential reasoning. The experimental results show that, compared with the best of existing models, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves better performance in both WIKI-TIME dataset and the well-studied NYT-10 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1108.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355765532 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1108/>Integrating Semantic Knowledge to Tackle Zero-shot Text Classification</a></strong><br><a href=/people/j/jingqing-zhang/>Jingqing Zhang</a>
|
<a href=/people/p/piyawat-lertvittayakumjorn/>Piyawat Lertvittayakumjorn</a>
|
<a href=/people/y/yike-guo/>Yike Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1108><div class="card-body p-3 small">Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is therefore difficult and only limited previous works tackled this problem. In this paper, we propose a two-phase framework together with <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and feature augmentation to solve this problem. Four kinds of semantic knowledge (word embeddings, class descriptions, class hierarchy, and a general knowledge graph) are incorporated into the proposed framework to deal with instances of unseen classes effectively. Experimental results show that each and the combination of the two phases achieve the best overall accuracy compared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355773895 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1109/>Word-Node2Vec : Improving Word Embedding with Document-Level Non-Local Word Co-occurrences<span class=acl-fixed-case>N</span>ode2<span class=acl-fixed-case>V</span>ec: Improving Word Embedding with Document-Level Non-Local Word Co-occurrences</a></strong><br><a href=/people/p/procheta-sen/>Procheta Sen</a>
|
<a href=/people/d/debasis-ganguly/>Debasis Ganguly</a>
|
<a href=/people/g/gareth-jones/>Gareth Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1109><div class="card-body p-3 small">A standard word embedding algorithm, such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and glove, makes a strong assumption that words are likely to be semantically related only if they co-occur locally within a window of fixed size. However, this strong assumption may not capture the semantic association between words that co-occur frequently but non-locally within documents. In this paper, we propose a graph-based word embedding method, named &#8216;word-node2vec&#8217;. By relaxing the strong constraint of <a href=https://en.wikipedia.org/wiki/Principle_of_locality>locality</a>, our method is able to capture both the local and non-local co-occurrences. Word-node2vec constructs a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> where every <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a> represents a word and an edge between two nodes represents a combination of both local (e.g. word2vec) and document-level co-occurrences. Our experiments show that word-node2vec outperforms <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and glove on a range of different tasks, such as predicting word-pair similarity, word analogy and concept categorization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1111.Software.txt data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356031269 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1111/>What just happened? Evaluating retrofitted distributional word vectors<span class=acl-fixed-case>E</span>valuating retrofitted distributional word vectors</a></strong><br><a href=/people/d/dmetri-hayes/>Dmetri Hayes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1111><div class="card-body p-3 small">Recent work has attempted to enhance vector space representations using information from structured semantic resources. This process, dubbed <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a> (Faruqui et al., 2015), has yielded improvements in word similarity performance. Research has largely focused on the retrofitting algorithm, or on the kind of structured semantic resources used, but little research has explored why some <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a> perform better than others. We conducted a fine-grained analysis of the original <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting process</a>, and found that the utility of different lexical resources for <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a> depends on two factors : the coverage of the resource and the evaluation metric. Our assessment suggests that the common practice of using correlation measures to evaluate increases in performance against full word similarity benchmarks 1) obscures the benefits offered by smaller resources, and 2) overlooks incremental gains in word similarity performance. We propose root-mean-square error (RMSE) as an alternative evaluation metric, and demonstrate that <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation measures</a> and RMSE sometimes yield opposite conclusions concerning the efficacy of <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a>. This point is illustrated by word vectors retrofitted with novel treatments of the FrameNet data (Fillmore and Baker, 2010).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1115.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364675378 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1115/>Cooperative Learning of Disjoint Syntax and Semantics</a></strong><br><a href=/people/s/serhii-havrylov/>Serhii Havrylov</a>
|
<a href=/people/g/german-kruszewski/>Germán Kruszewski</a>
|
<a href=/people/a/armand-joulin/>Armand Joulin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1115><div class="card-body p-3 small">There has been considerable attention devoted to <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that learn to jointly infer an expression&#8217;s <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> and its <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. Yet, Nangia and Bowman (2018) has recently shown that the current best systems fail to learn the correct parsing strategy on mathematical expressions generated from a simple <a href=https://en.wikipedia.org/wiki/Context-free_grammar>context-free grammar</a>. In this work, we present a <a href=https://en.wikipedia.org/wiki/Recursion_(computer_science)>recursive model</a> inspired by Choi et al. (2018) that reaches near perfect <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is composed of two separated <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> for syntax and semantics. They are cooperatively trained with standard continuous and discrete optimisation schemes. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not require any linguistic structure for <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>, and its recursive nature allows for out-of-domain generalisation. Additionally, our approach performs competitively on several <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a>, such as <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1116/>Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/m/mohit-yadav/>Mohit Yadav</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1116><div class="card-body p-3 small">We introduce the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence. During training we use <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a> to consider all possible binary trees over the sentence, and for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> we use the <a href=https://en.wikipedia.org/wiki/CKY_algorithm>CKY algorithm</a> to extract the highest scoring parse. DIORA outperforms previously reported results for unsupervised binary constituency parsing on the benchmark WSJ dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347403902 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1118/>Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations</a></strong><br><a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/g/guohong-fu/>Guohong Fu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1118><div class="card-body p-3 small">Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntax</a> by representing 1-best tree outputs from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from <a href=https://en.wikipedia.org/wiki/Propagation_of_uncertainty>error propagation</a>. In this work, we propose a novel method to integrate source-side syntax implicitly for <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a>. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to enhance basic NMT models. The <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> can be straightforwardly integrated into the widely-used sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>, 1.74 points for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese-English translation</a> and 0.80 point for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>English-Vietnamese translation</a>, respectively. In addition, the <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> also outperforms the explicit Tree-RNN and Tree-Linearization methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347406566 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1119/>Competence-based Curriculum Learning for Neural Machine Translation</a></strong><br><a href=/people/e/emmanouil-antonios-platanios/>Emmanouil Antonios Platanios</a>
|
<a href=/people/o/otilia-stretcu/>Otilia Stretcu</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/b/barnabas-poczos/>Barnabas Poczos</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1119><div class="card-body p-3 small">Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> at different times during training, based on the estimated difficulty of a sample and the current competence of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the <a href=https://en.wikipedia.org/wiki/Time_complexity>training time</a> and the performance of both <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network models</a> and Transformers, achieving up to a 70 % decrease in <a href=https://en.wikipedia.org/wiki/Time_complexity>training time</a>, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347411013 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1121" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1121/>Consistency by Agreement in Zero-Shot Neural Machine Translation</a></strong><br><a href=/people/m/maruan-al-shedivat/>Maruan Al-Shedivat</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1121><div class="card-body p-3 small">Generalization and reliability of multilingual translation often highly depend on the amount of available parallel data for each language pair of interest. In this paper, we focus on zero-shot generalizationa challenging setup that tests <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on translation directions they have not been optimized for at training time. To solve the problem, we (i) reformulate multilingual translation as probabilistic inference, (ii) define the notion of zero-shot consistency and show why standard training often results in models unsuitable for zero-shot tasks, and (iii) introduce a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages. We test our multilingual NMT models on multiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl) and show that agreement-based learning often results in 2-3 BLEU zero-shot improvement over strong baselines without any loss in performance on supervised translation directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360620730 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1123" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1123/>Rethinking Action Spaces for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> in End-to-end Dialog Agents with Latent Variable Models</a></strong><br><a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/k/kaige-xie/>Kaige Xie</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1123><div class="card-body p-3 small">Defining action spaces for conversational agents and optimizing their <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making process</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> is an enduring challenge. Common practice has been to use handcrafted dialog acts, or the output vocabulary, e.g. in neural encoder decoders, as the action spaces. Both have their own limitations. This paper proposes a novel latent action framework that treats the action spaces of an end-to-end dialog agent as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> and develops <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> in order to induce its own action space from the data. Comprehensive experiments are conducted examining both continuous and discrete action types and two different <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization methods</a> based on stochastic variational inference. Results show that the proposed latent actions achieve superior empirical performance improvement over previous word-level policy gradient methods on both DealOrNoDeal and MultiWoz dialogs. Our detailed analysis also provides insights about various latent variable approaches for <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a> and can serve as a foundation for developing better latent actions in future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1128/>WiC : the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations<span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>C</span>: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations</a></strong><br><a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1128><div class="card-body p-3 small">By design, word embeddings are unable to model the <a href=https://en.wikipedia.org/wiki/Semantics>dynamic nature of words&#8217; semantics</a>, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few <a href=https://en.wikipedia.org/wiki/Benchmarking>evaluation benchmarks</a> exist that specifically focus on the dynamic semantics of words. In this paper we show that existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1130/>Casting Light on Invisible Cities : Computationally Engaging with Literary Criticism<span class=acl-fixed-case>C</span>asting <span class=acl-fixed-case>L</span>ight on <span class=acl-fixed-case>I</span>nvisible <span class=acl-fixed-case>C</span>ities: <span class=acl-fixed-case>C</span>omputationally <span class=acl-fixed-case>E</span>ngaging with <span class=acl-fixed-case>L</span>iterary <span class=acl-fixed-case>C</span>riticism</a></strong><br><a href=/people/s/shufan-wang/>Shufan Wang</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1130><div class="card-body p-3 small">Literary critics often attempt to uncover meaning in a single work of literature through careful reading and analysis. Applying <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing methods</a> to aid in such <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary analyses</a> remains a challenge in <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>. While most previous work focuses on distant reading by algorithmically discovering high-level patterns from large collections of literary works, here we sharpen the focus of our methods to a single <a href=https://en.wikipedia.org/wiki/Literary_theory>literary theory</a> about Italo Calvino&#8217;s postmodern novel Invisible Cities, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these <a href=https://en.wikipedia.org/wiki/City>cities</a> into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate : we leverage pretrained contextualized representations to embed each city&#8217;s description and use <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> to cluster these embeddings. Additionally, we compare results of our <a href=https://en.wikipedia.org/wiki/Computational_model>computational approach</a> to similarity judgments generated by <a href=https://en.wikipedia.org/wiki/User_(computing)>human readers</a>. Our work is a first step towards incorporating <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> into <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary criticism</a>.<i>Invisible Cities</i>, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these cities into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate: we leverage pretrained contextualized representations to embed each city&#8217;s description and use unsupervised methods to cluster these embeddings. Additionally, we compare results of our computational approach to similarity judgments generated by human readers. Our work is a first step towards incorporating natural language processing into literary criticism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1131" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1131/>PAWS : Paraphrase Adversaries from Word Scrambling<span class=acl-fixed-case>PAWS</span>: Paraphrase Adversaries from Word Scrambling</a></strong><br><a href=/people/y/yuan-zhang/>Yuan Zhang</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/l/luheng-he/>Luheng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1131><div class="card-body p-3 small">Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (40 % accuracy) ; however, including PAWS training data for these models improves their accuracy to 85 % while maintaining performance on existing tasks. In contrast, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that better exploit <a href=https://en.wikipedia.org/wiki/Structure>structure</a>, <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>, and <a href=https://en.wikipedia.org/wiki/Pairwise_comparison>pairwise comparisons</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1134 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1134/>Adaptation of Hierarchical Structured Models for <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Act Recognition</a> in Asynchronous Conversation</a></strong><br><a href=/people/m/muhammad-tasnim-mohiuddin/>Tasnim Mohiuddin</a>
|
<a href=/people/t/thanh-tung-nguyen/>Thanh-Tung Nguyen</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1134><div class="card-body p-3 small">We address the problem of speech act recognition (SAR) in asynchronous conversations (forums, emails). Unlike <a href=https://en.wikipedia.org/wiki/Synchronization>synchronous conversations</a> (e.g., meetings, phone), asynchronous domains lack large labeled datasets to train an effective SAR model. In this paper, we propose methods to effectively leverage abundant unlabeled conversational data and the available labeled data from synchronous domains. We carry out our research in three main steps. First, we introduce a neural architecture based on hierarchical LSTMs and conditional random fields (CRF) for SAR, and show that our method outperforms existing methods when trained on in-domain data only. Second, we improve our initial SAR models by <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> in the form of pretrained word embeddings learned from a large unlabeled conversational corpus. Finally, we employ adversarial training to improve the results further by leveraging the labeled data from synchronous domains and by explicitly modeling the distributional shift in two domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1137/>Multi-Channel Convolutional Neural Network for Twitter Emotion and Sentiment Recognition<span class=acl-fixed-case>T</span>witter Emotion and Sentiment Recognition</a></strong><br><a href=/people/j/jumayel-islam/>Jumayel Islam</a>
|
<a href=/people/r/robert-e-mercer/>Robert E. Mercer</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1137><div class="card-body p-3 small">The advent of micro-blogging sites has paved the way for researchers to collect and analyze huge volumes of data in recent years. Twitter, being one of the leading social networking sites worldwide, provides a great opportunity to its users for expressing their states of mind via short messages which are called <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. The urgency of identifying emotions and sentiments conveyed through <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> has led to several research works. It provides a great way to understand <a href=https://en.wikipedia.org/wiki/Human_psychology>human psychology</a> and impose a challenge to researchers to analyze their content easily. In this paper, we propose a novel use of a multi-channel convolutional neural architecture which can effectively use different emotion and sentiment indicators such as <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> and <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> that are present in the <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and improve the performance of emotion and sentiment identification. We also investigate the incorporation of different lexical features in the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> and its effect on the emotion and sentiment identification task. We analyze our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on some standard datasets and compare its effectiveness with existing techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1138.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1138/>Detecting Cybersecurity Events from Noisy Short Text</a></strong><br><a href=/people/s/semih-yagcioglu/>Semih Yagcioglu</a>
|
<a href=/people/m/mehmet-saygin-seyfioglu/>Mehmet Saygin Seyfioglu</a>
|
<a href=/people/b/begum-citamak/>Begum Citamak</a>
|
<a href=/people/b/batuhan-bardak/>Batuhan Bardak</a>
|
<a href=/people/s/seren-guldamlasioglu/>Seren Guldamlasioglu</a>
|
<a href=/people/a/azmi-yuksel/>Azmi Yuksel</a>
|
<a href=/people/e/emin-islam-tatli/>Emin Islam Tatli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1138><div class="card-body p-3 small">It is very critical to analyze messages shared over <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> for <a href=https://en.wikipedia.org/wiki/Cyber_threat_intelligence>cyber threat intelligence</a> and cyber-crime prevention. In this study, we propose a method that leverages both domain-specific word embeddings and task-specific features to detect cyber security events from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. Our model employs a convolutional neural network (CNN) and a long short-term memory (LSTM) recurrent neural network which takes word level meta-embeddings as inputs and incorporates contextual embeddings to classify noisy short text. We collected a new dataset of cyber security related tweets from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and manually annotated a subset of 2 K of them. We experimented with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and concluded that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms both traditional and neural baselines. The results suggest that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> works well for detecting cyber security events from noisy short text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1139/>White-to-Black : Efficient Distillation of Black-Box Adversarial Attacks</a></strong><br><a href=/people/y/yotam-gil/>Yotam Gil</a>
|
<a href=/people/y/yoav-chai/>Yoav Chai</a>
|
<a href=/people/o/or-gorodissky/>Or Gorodissky</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1139><div class="card-body p-3 small">Adversarial examples are important for understanding the behavior of neural models, and can improve their <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> through adversarial training. Recent work in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization procedure</a> can be distilled into another more efficient <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability. Our attack flips the API-predicted label in 42 % of the generated examples, while humans maintain high-accuracy in predicting the gold label.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1141/>Fake News Detection using Deep Markov Random Fields<span class=acl-fixed-case>M</span>arkov Random Fields</a></strong><br><a href=/people/d/duc-minh-nguyen/>Duc Minh Nguyen</a>
|
<a href=/people/t/tien-huu-do/>Tien Huu Do</a>
|
<a href=/people/r/robert-calderbank/>Robert Calderbank</a>
|
<a href=/people/n/nikos-deligiannis/>Nikos Deligiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1141><div class="card-body p-3 small">Deep-learning-based models have been successfully applied to the problem of <a href=https://en.wikipedia.org/wiki/Fake_news>detecting fake news</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. While the correlations among news articles have been shown to be effective cues for online news analysis, existing deep-learning-based methods often ignore this information and only consider each news article individually. To overcome this limitation, we develop a graph-theoretic method that inherits the power of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> while at the same time utilizing the correlations among the articles. We formulate fake news detection as an inference problem in a Markov random field (MRF) which can be solved by the iterative mean-field algorithm. We then unfold the mean-field algorithm into hidden layers that are composed of common neural network operations. By integrating these hidden layers on top of a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a>, which produces the MRF potentials, we obtain our deep MRF model for fake news detection. Experimental results on well-known datasets show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves upon various <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1143/>Vector of Locally Aggregated Embeddings for Text Representation</a></strong><br><a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1143><div class="card-body p-3 small">We present Vector of Locally Aggregated Embeddings (VLAE) for effective and, ultimately, lossless representation of textual content. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> encodes each input text by effectively identifying and integrating the representations of its semantically-relevant parts. The proposed model generates high quality representation of textual content and improves the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance of current state-of-the-art deep averaging networks across several text classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1145 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1145/>Biomedical Event Extraction based on Knowledge-driven Tree-LSTM<span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/d/diya-li/>Diya Li</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1145><div class="card-body p-3 small">Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features : (1) dependency structures to capture wide contexts ; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1150/>Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/o/oshin-agarwal/>Oshin Agarwal</a>
|
<a href=/people/c/chris-tar/>Chris Tar</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1150><div class="card-body p-3 small">Modern NLP systems require high-quality annotated data. For specialized domains, expert annotations may be prohibitively expensive ; the alternative is to rely on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to reduce costs at the risk of introducing noise. In this paper we demonstrate that directly modeling instance difficulty can be used to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance and to route instances to appropriate annotators. Our difficulty prediction model combines two learned representations : a &#8216;universal&#8217; encoder trained on out of domain data, and a task-specific encoder. Experiments on a complex biomedical information extraction task using expert and lay annotators show that : (i) simply excluding from the training data instances predicted to be difficult yields a small boost in performance ; (ii) using difficulty scores to weight instances during training provides further, consistent gains ; (iii) assigning instances predicted to be difficult to domain experts is an effective strategy for task routing. Further, our experiments confirm the expectation that for such domain-specific tasks expert annotations are of much higher quality and preferable to obtain if practical and that augmenting small amounts of expert data with a larger set of lay annotations leads to further improvements in model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1151 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1151/>Detecting Depression in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> using Fine-Grained Emotions</a></strong><br><a href=/people/m/mario-ezra-aragon/>Mario Ezra Aragón</a>
|
<a href=/people/a/adrian-pastor-lopez-monroy/>Adrian Pastor López-Monroy</a>
|
<a href=/people/l/luis-carlos-gonzalez-gurrola/>Luis Carlos González-Gurrola</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes-y-Gómez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1151><div class="card-body p-3 small">Nowadays <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> are the most popular way for people to share information, from work issues to personal matters. For example, people with health disorders tend to share their concerns for advice, support or simply to relieve suffering. This provides a great opportunity to proactively detect these users and refer them as soon as possible to professional help. We propose a new representation called Bag of Sub-Emotions (BoSE), which represents social media documents by a set of fine-grained emotions automatically generated using a lexical resource of emotions and subword embeddings. The proposed <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> is evaluated in the task of <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression detection</a>. The results are encouraging ; the usage of fine-grained emotions improved the results from a representation based on the <a href=https://en.wikipedia.org/wiki/Emotion>core emotions</a> and obtained competitive results in comparison to state of the art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360694967 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1154/>One Size Does Not Fit All : Comparing NMT Representations of Different Granularities<span class=acl-fixed-case>NMT</span> Representations of Different Granularities</a></strong><br><a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1154><div class="card-body p-3 small">Recent work has shown that contextualized word representations derived from <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a> have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. We found that while representations derived from subwords are slightly better for modeling <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, character-based representations are superior for modeling <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and are also more robust to noisy input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/360705702 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1155/>A Simple Joint Model for Improved Contextual Neural Lemmatization</a></strong><br><a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1155><div class="card-body p-3 small">English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1159.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364704101 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1159/>Recursive Subtree Composition in LSTM-Based Dependency Parsing<span class=acl-fixed-case>LSTM</span>-Based Dependency Parsing</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1159><div class="card-body p-3 small">The need for tree structure modelling on top of sequence modelling is an open issue in neural dependency parsing. We investigate the impact of adding a <a href=https://en.wikipedia.org/wiki/Tree_layer>tree layer</a> on top of a <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a> by recursively composing subtree representations (composition) in a transition-based parser that uses features extracted by a BiLSTM. Composition seems superfluous with such a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, suggesting that BiLSTMs capture information about <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>subtrees</a>. We perform model ablations to tease out the conditions under which <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> helps. When ablating the backward LSTM, performance drops and <a href=https://en.wikipedia.org/wiki/Musical_composition>composition</a> does not recover much of the gap. When ablating the forward LSTM, performance drops less dramatically and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> recovers a substantial part of the gap, indicating that a forward LSTM and <a href=https://en.wikipedia.org/wiki/Composition_(music)>composition</a> capture similar information. We take the backward LSTM to be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved <a href=https://en.wikipedia.org/wiki/Ahead-of-time_compilation>lookahead</a> of a backward LSTM is especially important for head-final languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1161 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364706803 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1161" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1161/>Density Matching for Bilingual Word Embedding</a></strong><br><a href=/people/c/chunting-zhou/>Chunting Zhou</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/d/di-wang/>Di Wang</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1161><div class="card-body p-3 small">Recent approaches to cross-lingual word embedding have generally been based on <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a> between the sets of embedding vectors in the two languages. In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as <a href=https://en.wikipedia.org/wiki/Probability_density_function>probability densities</a> defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> to mappings between difficult language pairs or word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1162 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364708233 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1162/>Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing</a></strong><br><a href=/people/t/tal-schuster/>Tal Schuster</a>
|
<a href=/people/o/ori-ram/>Ori Ram</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/a/amir-globerson/>Amir Globerson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1162><div class="card-body p-3 small">We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> to derive an <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> for the context-dependent spaces. This <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> readily supports processing of a target language, improving <a href=https://en.wikipedia.org/wiki/Language_transfer>transfer</a> by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> consistently outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on 6 tested languages, yielding an improvement of 6.8 LAS points on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364687803 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1164" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1164/>Microblog Hashtag Generation via Encoding Conversation Contexts</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/i/irwin-king/>Irwin King</a>
|
<a href=/people/m/michael-r-lyu/>Michael R. Lyu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1164><div class="card-body p-3 small">Automatic hashtag annotation plays an important role in content understanding for <a href=https://en.wikipedia.org/wiki/Microblogging>microblog posts</a>. To date, progress made in this field has been restricted to phrase selection from limited candidates, or word-level hashtag discovery using <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>. Different from previous work considering <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> to be inseparable, our work is the first effort to annotate <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> with a novel sequence generation framework via viewing the <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> as a short sequence of words. Moreover, to address the data sparsity issue in processing short microblog posts, we propose to jointly model the target posts and the conversation contexts initiated by them with bidirectional attention. Extensive experimental results on two large-scale datasets, newly collected from <a href=https://en.wikipedia.org/wiki/Twitter>English Twitter</a> and <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Chinese Weibo</a>, show that our model significantly outperforms state-of-the-art models based on <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Further studies demonstrate our ability to effectively generate rare and even unseen hashtags, which is however not possible for most existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1166.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364697819 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1166/>Something’s Brewing ! Early Prediction of Controversy-causing Posts from Discussion Features</a></strong><br><a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1166><div class="card-body p-3 small">Controversial posts are those that split the preferences of a community, receiving both significant positive and significant negative feedback. Our inclusion of the word community here is deliberate : what is controversial to some audiences may not be so to others. Using data from several different communities on <a href=https://en.wikipedia.org/wiki/Reddit>reddit.com</a>, we predict the ultimate controversiality of posts, leveraging features drawn from both the textual content and the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> of the early comments that initiate the discussion. We find that even when only a handful of comments are available, e.g., the first 5 comments made within 15 minutes of the original post, discussion features often add predictive capacity to strong content-and- rate only baselines. Additional experiments on <a href=https://en.wikipedia.org/wiki/Domain_transfer>domain transfer</a> suggest that conversation- structure features often generalize to other communities better than conversation-content features do.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1167 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364700832 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1167" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1167/>No Permanent Friends or Enemies : Tracking Relationships between Nations from News<span class=acl-fixed-case>F</span>riends or Enemies: Tracking Relationships between Nations from News</a></strong><br><a href=/people/x/xiaochuang-han/>Xiaochuang Han</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1167><div class="card-body p-3 small">Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised neural models</a> to infer <a href=https://en.wikipedia.org/wiki/International_relations>relations between nations</a> from <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding <a href=https://en.wikipedia.org/wiki/International_relations>international relations</a> requires carefully analyzing <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>complex relationships</a>, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and give insightful feedback that suggests future directions for human-centered models. Furthermore, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> reveals interesting regional differences in <a href=https://en.wikipedia.org/wiki/News_media>news coverage</a>. For instance, with respect to <a href=https://en.wikipedia.org/wiki/China&#8211;United_States_relations>US-China relations</a>, <a href=https://en.wikipedia.org/wiki/Media_of_Singapore>Singaporean media</a> focus more on strengthening and purchasing, while <a href=https://en.wikipedia.org/wiki/Media_of_the_United_States>US media</a> focus more on criticizing and denouncing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361580764 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1168/>Improving Human Text Comprehension through Semi-Markov CRF-based Neural Section Title Generation<span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>CRF</span>-based Neural Section Title Generation</a></strong><br><a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/s/steven-layne/>Steven Layne</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1168><div class="card-body p-3 small">Titles of short sections within long documents support readers by guiding their focus towards relevant passages and by providing anchor-points that help to understand the progression of the document. The positive effects of section titles are even more pronounced when measured on readers with less developed reading abilities, for example in communities with limited labeled text resources. We, therefore, aim to develop <a href=https://en.wikipedia.org/wiki/Scientific_technique>techniques</a> to generate section titles in <a href=https://en.wikipedia.org/wiki/Developing_country>low-resource environments</a>. In particular, we present an extractive pipeline for section title generation by first selecting the most salient sentence and then applying deletion-based compression. Our compression approach is based on a Semi-Markov Conditional Random Field that leverages unsupervised word-representations such as ELMo or BERT, eliminating the need for a complex encoder-decoder architecture. The results show that this approach leads to competitive performance with sequence-to-sequence models with <a href=https://en.wikipedia.org/wiki/High-throughput_screening>high resources</a>, while strongly outperforming it with low resources. In a human-subject study across subjects with varying reading abilities, we find that our section titles improve the speed of completing comprehension tasks while retaining similar <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1172 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359670150 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1172" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1172/>Pun Generation with Surprise</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1172><div class="card-body p-3 small">We tackle the problem of generating a pun sentence given a pair of homophones (e.g., died and dyed). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a <a href=https://en.wikipedia.org/wiki/Text_corpus>large corpus</a>. In this paper, we propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> to <a href=https://en.wikipedia.org/wiki/Pun>pun generation</a> based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., dyed) and the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>distant context</a>, but a strong association between the alternative word (e.g., died) and the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>immediate context</a>. We instantiate the surprisal principle in two ways : (i) as a measure based on the ratio of probabilities given by a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30 % of the time, doubling the success rate of a neural generation baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1173/>Single Document Summarization as Tree Induction</a></strong><br><a href=/people/y/yang-liu-edinburgh/>Yang Liu</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1173><div class="card-body p-3 small">In this paper, we conceptualize single-document extractive summarization as a tree induction problem. In contrast to previous approaches which have relied on linguistically motivated document representations to generate summaries, our model induces a multi-root dependency tree while predicting the output summary. Each root node in the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> is a summary sentence, and the subtrees attached to it are sentences whose content relates to or explains the summary sentence. We design a new iterative refinement algorithm : it induces the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> through repeatedly refining the structures predicted by previous iterations. We demonstrate experimentally on two benchmark datasets that our <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarizer</a> performs competitively against state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1174/>Fixed That for You : Generating Contrastive Claims with Semantic Edits</a></strong><br><a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1174><div class="card-body p-3 small">Understanding contrastive opinions is a key component of argument generation. Central to an argument is the claim, a statement that is in dispute. Generating a counter-argument then requires generating a response in contrast to the main claim of the original argument. To generate contrastive claims, we create a corpus of Reddit comment pairs self-labeled by posters using the acronym FTFY (fixed that for you). We then train neural models on these pairs to edit the original claim and produce a new claim with a different view. We demonstrate significant improvement over a sequence-to-sequence baseline in BLEU score and a human evaluation for <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>, and <a href=https://en.wikipedia.org/wiki/Contrast_(vision)>contrast</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1178/>Unsupervised Dialog Structure Learning</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1178><div class="card-body p-3 small">Learning a shared dialog structure from a set of task-oriented dialogs is an important challenge in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. The learned dialog structure can shed light on how to analyze human dialogs, and more importantly contribute to the design and evaluation of dialog systems. We propose to extract dialog structures using a modified VRNN model with discrete latent vectors. Different from existing HMM-based models, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on variational-autoencoder (VAE). Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to capture more dynamics in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogs</a> beyond the surface forms of the language. We find that qualitatively, our method extracts meaningful dialog structure, and quantitatively, outperforms previous models on the ability to predict unseen data. We further evaluate the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s effectiveness in a downstream task, the dialog system building task. Experiments show that, by integrating the learned dialog structure into the reward function design, the model converges faster and to a better outcome in a reinforcement learning setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1181/>Text Similarity Estimation Based on Word Embeddings and Matrix Norms for Targeted Marketing</a></strong><br><a href=/people/t/tim-vor-der-bruck/>Tim vor der Brück</a>
|
<a href=/people/m/marc-pouly/>Marc Pouly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1181><div class="card-body p-3 small">The prevalent way to estimate the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> of two documents based on word embeddings is to apply the cosine similarity measure to the two <a href=https://en.wikipedia.org/wiki/Centroid>centroids</a> obtained from the embedding vectors associated with the words in each document. Motivated by an industrial application from the domain of <a href=https://en.wikipedia.org/wiki/Youth_marketing>youth marketing</a>, where this approach produced only mediocre results, we propose an alternative way of combining the <a href=https://en.wikipedia.org/wiki/Word_vectors>word vectors</a> using <a href=https://en.wikipedia.org/wiki/Matrix_norms>matrix norms</a>. The evaluation shows superior results for most of the investigated matrix norms in comparison to both the classical cosine measure and several other document similarity estimates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1182/>Glocal : Incorporating Global Information in Local Convolution for Keyphrase Extraction<span class=acl-fixed-case>G</span>local: Incorporating Global Information in Local Convolution for Keyphrase Extraction</a></strong><br><a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1182><div class="card-body p-3 small">Graph Convolutional Networks (GCNs) are a class of spectral clustering techniques that leverage localized convolution filters to perform <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classification</a> directly on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphical structures</a>. While such methods model nodes&#8217; local pairwise importance, they lack the capability to model global importance relative to other nodes of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. This causes such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to miss critical information in tasks where global ranking is a key component for the task, such as in <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a>. We address this shortcoming by allowing the proper incorporation of global information into the GCN family of models through the use of scaled node weights. In the context of <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a>, incorporating global random walk scores obtained from TextRank boosts performance significantly. With our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we achieve state-of-the-art results, bettering a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by an absolute 2 % increase in F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1183/>A Study of Latent Structured Prediction Approaches to Passage Reranking</a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1183><div class="card-body p-3 small">The structured output framework provides a helpful tool for learning to rank problems. In this paper, we propose a structured output approach which regards <a href=https://en.wikipedia.org/wiki/Ranking>rankings</a> as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. Our approach addresses the complex optimization of Mean Average Precision (MAP) ranking metric. We provide an <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> to find the max-violating ranking based on the decomposition of the corresponding loss. The results of our experiments on WikiQA and TREC13 datasets show that our reranking based on <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> is a promising research direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1185/>Tweet Stance Detection Using an Attention based Neural Ensemble Model<span class=acl-fixed-case>T</span>weet Stance Detection Using an Attention based Neural Ensemble Model</a></strong><br><a href=/people/u/umme-aymun-siddiqua/>Umme Aymun Siddiqua</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a>
|
<a href=/people/m/masaki-aono/>Masaki Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1185><div class="card-body p-3 small">Stance detection in twitter aims at mining user stances expressed in a tweet towards a single or multiple target entities. To tackle this problem, most of the prior studies have been explored the traditional <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, e.g., <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTM</a> and GRU. However, in compared to these traditional approaches, recently proposed densely connected Bi-LSTM and nested LSTMs architectures effectively address the vanishing-gradient and overfitting problems as well as dealing with long-term dependencies. In this paper, we propose a neural ensemble model that adopts the strengths of these two LSTM variants to learn better long-term dependencies, where each module coupled with an attention mechanism that amplifies the contribution of important elements in the final representation. We also employ a multi-kernel convolution on top of them to extract the higher-level tweet representations. Results of extensive experiments on single and multi-target stance detection datasets show that our proposed method achieves substantial improvement over the current state-of-the-art deep learning based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1188/>Learning Unsupervised Multilingual Word Embeddings with Incremental Multilingual Hubs</a></strong><br><a href=/people/g/geert-heyman/>Geert Heyman</a>
|
<a href=/people/b/bregt-verreet/>Bregt Verreet</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1188><div class="card-body p-3 small">Recent research has discovered that a shared bilingual word embedding space can be induced by projecting monolingual word embedding spaces from two languages using a self-learning paradigm without any bilingual supervision. However, it has also been shown that for distant language pairs such fully unsupervised self-learning methods are unstable and often get stuck in poor local optima due to reduced isomorphism between starting monolingual spaces. In this work, we propose a new <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robust framework</a> for learning unsupervised multilingual word embeddings that mitigates the instability issues. We learn a shared multilingual embedding space for a variable number of languages by incrementally adding new languages one by one to the current multilingual space. Through the gradual language addition the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can leverage the <a href=https://en.wikipedia.org/wiki/Interconnection>interdependencies</a> between the new language and all other languages in the current multilingual space. We find that it is beneficial to project more distant languages later in the iterative process. Our fully unsupervised multilingual embedding spaces yield results that are on par with the state-of-the-art methods in the bilingual lexicon induction (BLI) task, and simultaneously obtain state-of-the-art scores on two downstream tasks : multilingual document classification and multilingual dependency parsing, outperforming even supervised baselines. This finding also accentuates the need to establish evaluation protocols for cross-lingual word embeddings beyond the omnipresent intrinsic BLI task in future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1189/>Curriculum Learning for Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/x/xuan-zhang/>Xuan Zhang</a>
|
<a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/g/gaurav-kumar/>Gaurav Kumar</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1189><div class="card-body p-3 small">We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithm</a> with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1192/>Online Distilling from Checkpoints for Neural Machine Translation</a></strong><br><a href=/people/h/hao-ran-wei/>Hao-Ran Wei</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/r/ran-wang/>Ran Wang</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1192><div class="card-body p-3 small">Current predominant neural machine translation (NMT) models often have a deep structure with large amounts of parameters, making these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> hard to train and easily suffering from <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. A common practice is to utilize a validation set to evaluate the training process and select the best checkpoint. Average and ensemble techniques on checkpoints can lead to further performance improvement. However, as these methods do not affect the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training process</a>, the <a href=https://en.wikipedia.org/wiki/System>system</a> performance is restricted to the checkpoints generated in original <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a>. In contrast, we propose an online knowledge distillation method. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on-the-fly generates a teacher model from <a href=https://en.wikipedia.org/wiki/Checkpoint>checkpoints</a>, guiding the <a href=https://en.wikipedia.org/wiki/Training>training process</a> to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. Furthermore, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> leads to an improvement in a machine reading experiment as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1193/>Value-based Search in Execution Space for Mapping Instructions to Programs</a></strong><br><a href=/people/d/dor-muhlgay/>Dor Muhlgay</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1193><div class="card-body p-3 small">Training models to map natural language instructions to <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a>, given target world supervision only, requires searching for good programs at training time. Search is commonly done using <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> in the space of partial programs or program trees, but as the length of the instructions grows finding a good program becomes difficult. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a> that uses the target world state, known at training time, to train a critic network that predicts the expected reward of every search state. We then score search states on the beam by interpolating their expected reward with the likelihood of programs represented by the search state. Moreover, we search not in the space of programs but in a more compressed state of program executions, augmented with recent entities and actions. On the SCONE dataset, we show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> dramatically improves performance on all three domains compared to standard <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354228781 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1200/>Cross-lingual Visual Verb Sense Disambiguation</a></strong><br><a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1200><div class="card-body p-3 small">Recent work has shown that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a> improves cross-lingual sense disambiguation for <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>. We extend this line of work to the more challenging task of cross-lingual verb sense disambiguation, introducing the MultiSense dataset of 9,504 images annotated with English, German, and Spanish verbs. Each image in MultiSense is annotated with an English verb and its translation in <a href=https://en.wikipedia.org/wiki/German_language>German</a> or Spanish. We show that cross-lingual verb sense disambiguation models benefit from <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a>, compared to unimodal baselines. We also show that the verb sense predicted by our best disambiguation model can improve the results of a text-only machine translation system when used for a multimodal translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264673 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1201/>Subword-Level Language Identification for Intra-Word Code-Switching</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/o/ozlem-cetinoglu/>Özlem Çetinoğlu</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1201><div class="card-body p-3 small">Language identification for code-switching (CS), the phenomenon of alternating between two or more languages in conversations, has traditionally been approached under the assumption of a single language per token. However, if at least one language is morphologically rich, a large number of words can be composed of morphemes from more than one language (intra-word CS). In this paper, we extend the language identification task to the subword-level, such that it includes splitting mixed words while tagging each part with a language ID. We further propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for this task, which is based on a segmental recurrent neural network. In experiments on a new SpanishWixarika dataset and on an adapted GermanTurkish dataset, our proposed model performs slightly better than or roughly on par with our best baseline, respectively. Considering only mixed words, however, it strongly outperforms all baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354264026 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1203/>Contextualization of Morphological Inflection</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1203><div class="card-body p-3 small">Critical to <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> is the production of correctly inflected text. In this paper, we isolate the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> or surface realization, our task input does not provide gold tags that specify what morphological features to realize on each lemmatized word ; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a> before predicting the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a>, and compare this to a system that directly predicts the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1206.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355794917 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1206/>Measuring Immediate Adaptation Performance for Neural Machine Translation</a></strong><br><a href=/people/p/patrick-simianer/>Patrick Simianer</a>
|
<a href=/people/j/joern-wuebker/>Joern Wuebker</a>
|
<a href=/people/j/john-denero/>John DeNero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1206><div class="card-body p-3 small">Incremental domain adaptation, in which a system learns from the correct output for each input immediately after making its prediction for that input, can dramatically improve <a href=https://en.wikipedia.org/wiki/System>system</a> performance for <a href=https://en.wikipedia.org/wiki/Interactive_machine_translation>interactive machine translation</a>. Users of interactive systems are sensitive to the speed of adaptation and how often a system repeats mistakes, despite being corrected. Adaptation is most commonly assessed using corpus-level BLEU- or TER-derived metrics that do not explicitly take adaptation speed into account. We find that these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> often do not capture immediate adaptation effects, such as zero-shot and one-shot learning of domain-specific lexical items. To this end, we propose new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that directly evaluate immediate adaptation performance for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We use these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to choose the most suitable adaptation method from a range of different adaptation techniques for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355798547 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1208/>Reinforcement Learning based Curriculum Optimization for Neural Machine Translation</a></strong><br><a href=/people/g/gaurav-kumar/>Gaurav Kumar</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/m/maxim-krikun/>Maxim Krikun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1208><div class="card-body p-3 small">We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, we seek an optimal <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a>, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes <a href=https://en.wikipedia.org/wiki/Weighting>data weighting</a>, <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering</a>, and fine-tuning schemes. Rather than relying on prior knowledge to design a <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a>, we use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>uniform baselines</a> on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356056256 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1209/>Overcoming <a href=https://en.wikipedia.org/wiki/Catastrophic_Forgetting>Catastrophic Forgetting</a> During <a href=https://en.wikipedia.org/wiki/Domain_adaptation>Domain Adaptation</a> of Neural Machine Translation</a></strong><br><a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1209><div class="card-body p-3 small">Continued training is an effective method for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354246126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1210/>Short-Term Meaning Shift : A Distributional Exploration</a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1210><div class="card-body p-3 small">We present the first exploration of meaning shift over short periods of time in <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for meaning shift detection on short-term meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/354239263 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1213/>An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</a></strong><br><a href=/people/a/alexandra-chronopoulou/>Alexandra Chronopoulou</a>
|
<a href=/people/c/christos-baziotis/>Christos Baziotis</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1213><div class="card-body p-3 small">A growing number of state-of-the-art transfer learning methods employ <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning methods</a> with greater level of <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1217 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355805085 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1217" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1217/>Joint Detection and Location of English Puns<span class=acl-fixed-case>E</span>nglish Puns</a></strong><br><a href=/people/y/yanyan-zou/>Yanyan Zou</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1217><div class="card-body p-3 small">A pun is a form of <a href=https://en.wikipedia.org/wiki/Word_play>wordplay</a> for an intended humorous or rhetorical effect, where a word suggests two or more meanings by exploiting <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> (homographic pun) or phonological similarity to another word (heterographic pun). This paper presents an approach that addresses pun detection and pun location jointly from a sequence labeling perspective. We employ a new tagging scheme such that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is capable of performing such a joint task, where useful structural information can be properly captured. We show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective in handling both homographic and heterographic puns. Empirical results on the benchmark datasets demonstrate that our approach can achieve new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1219.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355808962 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1219/>Argument Mining for Understanding Peer Reviews</a></strong><br><a href=/people/x/xinyu-hua/>Xinyu Hua</a>
|
<a href=/people/m/mitko-nikolov/>Mitko Nikolov</a>
|
<a href=/people/n/nikhil-badugu/>Nikhil Badugu</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1219><div class="card-body p-3 small">Peer-review plays a critical role in the scientific writing and publication ecosystem. To assess the efficiency and efficacy of the reviewing process, one essential element is to understand and evaluate the reviews themselves. In this work, we study the content and structure of <a href=https://en.wikipedia.org/wiki/Peer_review>peer reviews</a> under the argument mining framework, through automatically detecting (1) the <a href=https://en.wikipedia.org/wiki/Argument>argumentative propositions</a> put forward by reviewers, and (2) their types (e.g., evaluating the work or making suggestions for improvement). We first collect 14.2 K reviews from major machine learning and natural language processing venues. 400 reviews are annotated with 10,386 propositions and corresponding types of Evaluation, Request, Fact, Reference, or Quote. We then train state-of-the-art proposition segmentation and classification models on the data to evaluate their utilities and identify new challenges for this new domain, motivating future directions for <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. Further experiments show that proposition usage varies across venues in amount, type, and topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355811189 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1221/>Abusive Language Detection with Graph Convolutional Networks<span class=acl-fixed-case>A</span>busive <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>D</span>etection with <span class=acl-fixed-case>G</span>raph <span class=acl-fixed-case>C</span>onvolutional <span class=acl-fixed-case>N</span>etworks</a></strong><br><a href=/people/p/pushkar-mishra/>Pushkar Mishra</a>
|
<a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1221><div class="card-body p-3 small">Abuse on the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> represents a significant societal problem of our time. Previous research on automated abusive language detection in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling followerfollowing relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364735719 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1223/>Factorising AMR generation through syntax<span class=acl-fixed-case>AMR</span> generation through syntax</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1223><div class="card-body p-3 small">Generating from Abstract Meaning Representation (AMR) is an underspecified problem, as many syntactic decisions are not specified by the semantic graph. To explicitly account for this variation, we break down generating from AMR into two steps : first generate a <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>, and then generate the surface form. We show that decomposing the generation process this way leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1224.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1224.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364709844 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1224/>A Crowdsourced Frame Disambiguation Corpus with Ambiguity</a></strong><br><a href=/people/a/anca-dumitrache/>Anca Dumitrache</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a>
|
<a href=/people/c/chris-welty/>Chris Welty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1224><div class="card-body p-3 small">We present a resource for the task of FrameNet semantic frame disambiguation of over 5,000 word-sentence pairs from the Wikipedia corpus. The <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> were collected using a novel crowdsourcing approach with multiple workers per sentence to capture inter-annotator disagreement. In contrast to the typical approach of attributing the best single frame to each word, we provide a list of frames with disagreement-based scores that express the confidence with which each frame applies to the word. This is based on the idea that inter-annotator disagreement is at least partly caused by <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> that is inherent to the text and frames. We have found many examples where the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of individual frames overlap sufficiently to make them acceptable alternatives for interpreting a sentence. We have argued that ignoring this <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> creates an overly arbitrary target for training and evaluating natural language processing systems-if humans can not agree, why would we expect the correct answer from a machine to be any different? To process this data we also utilized an expanded lemma-set provided by the Framester system, which merges FN with <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> to enhance coverage. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> includes annotations of 1,000 sentence-word pairs whose lemmas are not part of FN. Finally we present <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for evaluating frame disambiguation systems that account for <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1227 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1227.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1227/>Partial Or Complete, That’s The Question</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/h/hangfeng-he/>Hangfeng He</a>
|
<a href=/people/c/chuchu-fan/>Chuchu Fan</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1227><div class="card-body p-3 small">For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>structures</a> consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>, while allowing for more <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>structures</a> to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1228/>Sequential Attention with Keyword Mask Model for Community-based Question Answering<span class=acl-fixed-case>S</span>equential <span class=acl-fixed-case>A</span>ttention with <span class=acl-fixed-case>K</span>eyword <span class=acl-fixed-case>M</span>ask <span class=acl-fixed-case>M</span>odel for <span class=acl-fixed-case>C</span>ommunity-based <span class=acl-fixed-case>Q</span>uestion <span class=acl-fixed-case>A</span>nswering</a></strong><br><a href=/people/j/jianxin-yang/>Jianxin Yang</a>
|
<a href=/people/w/wenge-rong/>Wenge Rong</a>
|
<a href=/people/l/libin-shi/>Libin Shi</a>
|
<a href=/people/z/zhang-xiong/>Zhang Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1228><div class="card-body p-3 small">In Community-based Question Answering system(CQA), Answer Selection(AS) is a critical task, which focuses on finding a suitable answer within a list of candidate answers. For neural network models, the key issue is how to model the representations of QA text pairs and calculate the interactions between them. We propose a Sequential Attention with Keyword Mask model(SAKM) for CQA to imitate human reading behavior. Question and answer text regard each other as context within keyword-mask attention when encoding the representations, and repeat multiple times(hops) in a sequential style. So the QA pairs capture features and information from both question text and answer text, interacting and improving vector representations iteratively through hops. The flexibility of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> allows to extract meaningful <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> from the sentences and enhance diverse mutual information. We perform on <a href=https://en.wikipedia.org/wiki/Question_answering>answer selection tasks</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>multi-level answer ranking tasks</a>. Experiment results demonstrate the superiority of our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on community-based QA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1229/>Simple Attention-Based Representation Learning for Ranking Short Social Media Posts</a></strong><br><a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1229><div class="card-body p-3 small">This paper explores the problem of ranking short social media posts with respect to user queries using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Instead of starting with a complex <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic soft matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but are also much faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1230/>AttentiveChecker : A Bi-Directional Attention Flow Mechanism for Fact Verification<span class=acl-fixed-case>A</span>ttentive<span class=acl-fixed-case>C</span>hecker: A Bi-Directional Attention Flow Mechanism for Fact Verification</a></strong><br><a href=/people/s/santosh-tokala/>Santosh Tokala</a>
|
<a href=/people/v/vishal-g/>Vishal G</a>
|
<a href=/people/a/avirup-saha/>Avirup Saha</a>
|
<a href=/people/n/niloy-ganguly/>Niloy Ganguly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1230><div class="card-body p-3 small">The recently released FEVER dataset provided benchmark results on a fact-checking task in which given a factual claim, the system must extract textual evidence (sets of sentences from Wikipedia pages) that support or refute the claim. In this paper, we present a completely task-agnostic pipelined system, AttentiveChecker, consisting of three homogeneous Bi-Directional Attention Flow (BIDAF) networks, which are multi-layer hierarchical networks that represent the context at different levels of granularity. We are the first to apply to this task a bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. AttentiveChecker can be used to perform <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, sentence selection, and claim verification. Experiments on the FEVER dataset indicate that AttentiveChecker is able to achieve the state-of-the-art results on the FEVER test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1231 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1231/>Practical, Efficient, and Customizable <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a> for Named Entity Recognition in the <a href=https://en.wikipedia.org/wiki/Digital_humanities>Digital Humanities</a></a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/d/david-joseph-wrisley/>David Joseph Wrisley</a>
|
<a href=/people/b/benjamin-allen/>Benjamin Allen</a>
|
<a href=/people/c/christopher-brown/>Christopher Brown</a>
|
<a href=/people/s/sophie-cohen-bodenes/>Sophie Cohen-Bodénès</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/b/brian-joseph/>Brian Joseph</a>
|
<a href=/people/b/beatrice-joyeux-prunel/>Béatrice Joyeux-Prunel</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1231><div class="card-body p-3 small">Scholars in inter-disciplinary fields like the <a href=https://en.wikipedia.org/wiki/Digital_humanities>Digital Humanities</a> are increasingly interested in semantic annotation of specialized corpora. Yet, under-resourced languages, imperfect or noisily structured data, and user-specific classification tasks make it difficult to meet their needs using off-the-shelf models. Manual annotation of large corpora from scratch, meanwhile, can be prohibitively expensive. Thus, we propose an active learning solution for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, attempting to maximize a custom model&#8217;s improvement per additional unit of manual annotation. Our system robustly handles any domain or user-defined label set and requires no external resources, enabling quality <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Humanities corpora where such resources are not available. Evaluating on typologically disparate languages and datasets, we reduce required annotation by 20-60 % and greatly outperform a competitive active learning baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1232 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1232/>Doc2hash : Learning Discrete Latent variables for Documents Retrieval<span class=acl-fixed-case>D</span>oc2hash: Learning Discrete Latent variables for Documents Retrieval</a></strong><br><a href=/people/y/yifei-zhang/>Yifei Zhang</a>
|
<a href=/people/h/hao-zhu/>Hao Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1232><div class="card-body p-3 small">Learning to hash via <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> has become a powerful paradigm for fast similarity search in documents retrieval. To get <a href=https://en.wikipedia.org/wiki/Binary_number>binary representation</a> (i.e., hash codes), the discrete distribution prior (i.e., <a href=https://en.wikipedia.org/wiki/Bernoulli_distribution>Bernoulli Distribution</a>) is applied to train the variational autoencoder (VAE). However, the discrete stochastic layer is usually incompatible with the <a href=https://en.wikipedia.org/wiki/Backpropagation>backpropagation</a> in the training stage, and thus causes a gradient flow problem because of non-differentiable operators. The reparameterization trick of sampling from a <a href=https://en.wikipedia.org/wiki/Probability_distribution>discrete distribution</a> usually inc <a href=https://en.wikipedia.org/wiki/Differentiable_function>non-differentiable operators</a>. In this paper, we propose a method, Doc2hash, that solves the gradient flow problem of the discrete stochastic layer by using continuous relaxation on priors, and trains the generative model in an end-to-end manner to generate hash codes. In qualitative and quantitative experiments, we show the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other state-of-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1235" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1235/>Neural Text Generation from Rich Semantic Representations</a></strong><br><a href=/people/v/valerie-hajdik/>Valerie Hajdik</a>
|
<a href=/people/j/jan-buys/>Jan Buys</a>
|
<a href=/people/m/michael-wayne-goodman/>Michael Wayne Goodman</a>
|
<a href=/people/e/emily-m-bender/>Emily M. Bender</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1235><div class="card-body p-3 small">We propose neural models to generate high-quality text from structured representations based on Minimal Recursion Semantics (MRS). MRS is a rich semantic representation that encodes more precise semantic detail than other representations such as Abstract Meaning Representation (AMR). We show that a sequence-to-sequence model that maps a linearization of Dependency MRS, a graph-based representation of MRS, to <a href=https://en.wikipedia.org/wiki/Plain_text>text</a> can achieve a BLEU score of 66.11 when trained on gold data. The performance of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can be improved further using a high-precision, broad coverage grammar-based parser to generate a large silver training corpus, achieving a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1239 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1239/>Open Information Extraction from Question-Answer Pairs</a></strong><br><a href=/people/n/nikita-bhutani/>Nikita Bhutani</a>
|
<a href=/people/y/yoshihiko-suhara/>Yoshihiko Suhara</a>
|
<a href=/people/w/wang-chiew-tan/>Wang-Chiew Tan</a>
|
<a href=/people/a/alon-halevy/>Alon Halevy</a>
|
<a href=/people/h/h-v-jagadish/>H. V. Jagadish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1239><div class="card-body p-3 small">Open Information Extraction (OpenIE) extracts meaningful structured tuples from free-form text. Most previous work on OpenIE considers extracting data from one sentence at a time. We describe NeurON, a <a href=https://en.wikipedia.org/wiki/System>system</a> for extracting tuples from question-answer pairs. One of the main motivations for <a href=https://en.wikipedia.org/wiki/Neuron>NeurON</a> is to be able to extend <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> in a way that considers precisely the information that users care about. NeurON addresses several challenges. First, an answer text is often hard to understand without knowing the question, and second, relevant information can span multiple sentences. To address these, NeurON formulates extraction as a multi-source sequence-to-sequence learning task, wherein it combines distributed representations of a question and an answer to generate knowledge facts. We describe experiments on two real-world datasets that demonstrate that NeurON can find a significant number of new and interesting facts to extend a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> compared to state-of-the-art OpenIE methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1240/>Question Answering by Reasoning Across Documents with Graph Convolutional Networks</a></strong><br><a href=/people/n/nicola-de-cao/>Nicola De Cao</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1240><div class="card-body p-3 small">Most research in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Mentions of entities are <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> of this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> while <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> encode relations between different <a href=https://en.wikipedia.org/wiki/Note_(typography)>mentions</a> (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1241 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1241" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1241/>A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>QA</span>, <span class=acl-fixed-case>SQ</span>u<span class=acl-fixed-case>AD</span> 2.0 and <span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>AC</span></a></strong><br><a href=/people/m/mark-yatskar/>Mark Yatskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1241><div class="card-body p-3 small">We compare three new datasets for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> : <a href=https://en.wikipedia.org/wiki/Question_answering>SQuAD 2.0</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>QuAC</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>CoQA</a>, along several of their new features : (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers. We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third. Because of the datasets&#8217; structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both <a href=https://en.wikipedia.org/wiki/Question_answering>SQuAD 2.0</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>CoQA</a>. Despite the similarity, <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on one <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are ineffective on another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, but we find moderate performance improvement through pretraining. To encourage cross-evaluation, we release code for conversion between datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1242/>BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis<span class=acl-fixed-case>BERT</span> Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a></strong><br><a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1242><div class="card-body p-3 small">Question-answering plays an important role in <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a> as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions. We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1243 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1243/>Old is Gold : Linguistic Driven Approach for Entity and Relation Linking of Short Text</a></strong><br><a href=/people/a/ahmad-sakor/>Ahmad Sakor</a>
|
<a href=/people/i/isaiah-onando-mulang/>Isaiah Onando Mulang’</a>
|
<a href=/people/k/kuldeep-singh/>Kuldeep Singh</a>
|
<a href=/people/s/saeedeh-shekarpour/>Saeedeh Shekarpour</a>
|
<a href=/people/m/maria-esther-vidal/>Maria Esther Vidal</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a>
|
<a href=/people/s/soren-auer/>Sören Auer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1243><div class="card-body p-3 small">Short texts challenge NLP tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation</a>, linking and relation inference because they do not provide sufficient context or are partially malformed (e.g. wrt. capitalization, <a href=https://en.wikipedia.org/wiki/Long_tail>long tail entities</a>, implicit relations). In this work, we present the Falcon approach which effectively maps entities and relations within a short text to its mentions of a background knowledge graph. Falcon overcomes the challenges of short text using a light-weight linguistic approach relying on a background knowledge graph. Falcon performs joint entity and relation linking of a short text by leveraging several fundamental principles of <a href=https://en.wikipedia.org/wiki/English_language>English morphology</a> (e.g. compounding, headword identification) and utilizes an extended <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> created by merging entities and relations from various knowledge sources. It uses the context of entities for finding relations and does not require <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>. Our empirical study using several standard benchmarks and datasets show that Falcon significantly outperforms state-of-the-art entity and relation linking for short text query inventories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1244 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1244" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1244/>Be Consistent ! Improving Procedural Text Comprehension using Label Consistency</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1244><div class="card-body p-3 small">Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging as the world is changing throughout the text, and despite recent advances, current <a href=https://en.wikipedia.org/wiki/System>systems</a> still struggle with this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1246.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1246" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1246/>DROP : A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs<span class=acl-fixed-case>DROP</span>: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</a></strong><br><a href=/people/d/dheeru-dua/>Dheeru Dua</a>
|
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1246><div class="card-body p-3 small">Reading comprehension has recently seen rapid progress, with <a href=https://en.wikipedia.org/wiki/Computer>systems</a> matching humans on the most popular datasets for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, a large body of work has highlighted the brittleness of these <a href=https://en.wikipedia.org/wiki/System>systems</a>, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and show that the best <a href=https://en.wikipedia.org/wiki/System>systems</a> only achieve 38.4 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on our generalized accuracy metric, while expert human performance is 96 %. We additionally present a new model that combines reading comprehension methods with simple <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical reasoning</a> to achieve 51 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1251/>A Simple and Robust Approach to Detecting Subject-Verb Agreement Errors</a></strong><br><a href=/people/s/simon-flachs/>Simon Flachs</a>
|
<a href=/people/o/ophelie-lacroix/>Ophélie Lacroix</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1251><div class="card-body p-3 small">While rule-based detection of subject-verb agreement (SVA) errors is sensitive to syntactic parsing errors and irregularities and exceptions to the main rules, neural sequential labelers have a tendency to overfit their training data. We observe that rule-based error generation is less sensitive to syntactic parsing errors and irregularities than <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> and explore a simple, yet efficient approach to getting the best of both worlds : We train neural sequential labelers on the combination of large volumes of silver standard data, obtained through rule-based error generation, and gold standard data. We show that our simple protocol leads to more robust detection of SVA errors on both in-domain and out-of-domain data, as well as in the context of other errors and long-distance dependencies ; and across four standard benchmarks, the induced model on average achieves a new state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1252 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1252" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1252/>A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages</a></strong><br><a href=/people/r/ronald-cardenas/>Ronald Cardenas</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1252><div class="card-body p-3 small">Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a &#8216;ciphertext&#8217; and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our <a href=https://en.wikipedia.org/wiki/POS_tagger>POS tagger</a> into a name tagger leads to state-of-the-art tagging performance in <a href=https://en.wikipedia.org/wiki/Sinhala_language>Sinhalese</a> and <a href=https://en.wikipedia.org/wiki/Kinyarwanda>Kinyarwanda</a>, two languages with nearly no labeled POS data available. We further demonstrate our tagger&#8217;s utility by incorporating it into a true &#8216;zero-resource&#8217; variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a> makes up much of the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> lost when gold POS tags are unavailable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1253.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1253" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1253/>On Difficulties of Cross-Lingual Transfer with Order Differences : A Case Study on Dependency Parsing</a></strong><br><a href=/people/w/wasi-ahmad/>Wasi Ahmad</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1253><div class="card-body p-3 small">Different languages might have different <a href=https://en.wikipedia.org/wiki/Part_of_speech>word orders</a>. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an <a href=https://en.wikipedia.org/wiki/English_language>English corpus</a> and evaluate their transfer performance on 30 other languages. Specifically, we compare <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> and <a href=https://en.wikipedia.org/wiki/Code>decoders</a> based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks (RNNs)</a> and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355814096 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1255/>Self-Discriminative Learning for Unsupervised Document Embedding</a></strong><br><a href=/people/h/hong-you-chen/>Hong-You Chen</a>
|
<a href=/people/c/chin-hua-hu/>Chin-Hua Hu</a>
|
<a href=/people/l/leila-wehbe/>Leila Wehbe</a>
|
<a href=/people/s/shou-de-lin/>Shou-De Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1255><div class="card-body p-3 small">Unsupervised document representation learning is an important <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> providing pre-trained features for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Unlike most previous work which learn the <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> based on self-prediction of the surface of text, we explicitly exploit the inter-document information and directly model the relations of documents in <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a> with a discriminative network and a novel objective. Extensive experiments on both small and large public datasets show the competitiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. In evaluations on standard <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has errors that are 5 to 13 % lower than state-of-the-art unsupervised embedding models. The reduction in error is even more pronounced in scarce label setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1256.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1256/>Adaptive Convolution for Text Classification</a></strong><br><a href=/people/b/byung-ju-choi/>Byung-Ju Choi</a>
|
<a href=/people/j/jun-hyung-park/>Jun-Hyung Park</a>
|
<a href=/people/s/sangkeun-lee/>SangKeun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1256><div class="card-body p-3 small">In this paper, we present an adaptive convolution for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> to give flexibility to convolutional neural networks (CNNs). Unlike traditional convolutions which utilize the same set of <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filters</a> regardless of different inputs, the adaptive convolution employs adaptively generated convolutional filters conditioned on inputs. We achieve this by attaching filter-generating networks, which are carefully designed to generate input-specific filters, to convolution blocks in existing CNNs. We show the efficacy of our approach in existing CNNs based on the <a href=https://en.wikipedia.org/wiki/Performance_evaluation>performance evaluation</a>. Our evaluation indicates that all of our baselines achieve performance improvements with adaptive convolutions as much as up to 2.6 percentage point in seven benchmark text classification datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359684150 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1257/>Zero-Shot Cross-Lingual Opinion Target Extraction<span class=acl-fixed-case>Z</span>ero-Shot Cross-Lingual Opinion Target Extraction</a></strong><br><a href=/people/s/soufian-jebbara/>Soufian Jebbara</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1257><div class="card-body p-3 small">Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction. Our experiments with 5 languages give promising results : We can successfully train a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on annotated data of a source language and perform accurate <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> on a target language without ever using any annotated samples in that target language. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77 % of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on target language data. Furthermore, we can increase this performance up to 87 % of a baseline model trained on target language data by performing cross-lingual learning from multiple source languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1260 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1260/>Abstractive Summarization of Reddit Posts with Multi-level Memory Networks<span class=acl-fixed-case>R</span>eddit Posts with Multi-level Memory Networks</a></strong><br><a href=/people/b/byeongchang-kim/>Byeongchang Kim</a>
|
<a href=/people/h/hyunwoo-kim/>Hyunwoo Kim</a>
|
<a href=/people/g/gunhee-kim/>Gunhee Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1260><div class="card-body p-3 small">We address the problem of abstractive summarization in two directions : proposing a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and a new <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. First, we collect Reddit TIFU dataset, consisting of 120 K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that mostly use formal documents as source such as <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. Thus, our dataset could less suffer from some biases that key sentences usually located at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks (MMN), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a>, we show the Reddit TIFU dataset is highly abstractive and the MMN outperforms the state-of-the-art summarization models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1263 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1263.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1263/>Text Generation with Exemplar-based Adaptive Decoding</a></strong><br><a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/d/dipanjan-das/>Dipanjan Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1263><div class="card-body p-3 small">We propose a novel conditioned text generation model. It draws inspiration from traditional template-based text generation techniques, where the source provides the content (i.e., what to say), and the template influences how to say it. Building on the successful encoder-decoder paradigm, it first encodes the content representation from the given input text ; to produce the output, it retrieves exemplar text from the training data as soft templates, which are then used to construct an exemplar-specific decoder. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on abstractive text summarization and data-to-text generation. Empirical results show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves strong performance and outperforms comparable baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1267.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364226255 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1267/>Strong and Simple Baselines for Multimodal Utterance Embeddings</a></strong><br><a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/y/yao-chong-lim/>Yao Chong Lim</a>
|
<a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1267><div class="card-body p-3 small">Human language is a rich multimodal signal consisting of <a href=https://en.wikipedia.org/wiki/Speech>spoken words</a>, <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, <a href=https://en.wikipedia.org/wiki/Gesture>body gestures</a>, and <a href=https://en.wikipedia.org/wiki/Intonation_(linguistics)>vocal intonations</a>. Learning representations for these spoken utterances is a complex research problem due to the presence of multiple heterogeneous sources of information. Recent advances in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a> have followed the general trend of building more complex models that utilize various attention, memory and recurrent components. In this paper, we propose two simple but strong baselines to learn embeddings of multimodal utterances. The first baseline assumes a conditional factorization of the utterance into unimodal factors. Each <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal factor</a> is modeled using the simple form of a <a href=https://en.wikipedia.org/wiki/Likelihood_function>likelihood function</a> obtained via a linear transformation of the embedding. We show that the optimal embedding can be derived in closed form by taking a weighted average of the unimodal features. In order to capture richer representations, our second baseline extends the first by factorizing into unimodal, bimodal, and trimodal factors, while retaining simplicity and efficiency during <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. From a set of experiments across two tasks, we show strong performance on both supervised and semi-supervised multimodal prediction, as well as significant (10 times) speedups over neural models during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. Overall, we believe that our strong baseline models offer new benchmarking options for future research in <a href=https://en.wikipedia.org/wiki/Multimodal_learning>multimodal learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1269 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364740187 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1269/>Towards Content Transfer through Grounded Text Generation</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1269><div class="card-body p-3 small">Recent work in neural generation has attracted significant interest in controlling the form of text, such as <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a>, <a href=https://en.wikipedia.org/wiki/Persona>persona</a>, and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a>. However, there has been less work on controlling neural text generation for <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>. This paper introduces the notion of Content Transfer for long-form text generation, where the task is to generate a next sentence in a document that both fits its context and is grounded in a content-rich external textual source such as a news story. Our experiments on Wikipedia data show significant improvements against competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1270 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364746823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1270" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1270/>Improving Machine Reading Comprehension with General Reading Strategies</a></strong><br><a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1270><div class="card-body p-3 small">Reading strategies have been shown to improve <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension levels</a>, especially for readers lacking adequate prior knowledge. Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep language model</a> via pre-training. Inspired by reading strategies identified in <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, and given limited computational resources-just a pre-trained model and a fixed number of training instances-we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice questions and candidate answers directly from the text in an unsupervised manner. By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a 5.8 % absolute increase in accuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1271 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/364750438 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1271/>Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension</a></strong><br><a href=/people/y/yichong-xu/>Yichong Xu</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/j/jingjing-liu/>Jingjing Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1271><div class="card-body p-3 small">We propose a multi-task learning framework to learn a joint Machine Reading Comprehension (MRC) model that can be applied to a wide range of MRC tasks in different domains. Inspired by recent ideas of data selection in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, we develop a novel sample re-weighting scheme to assign sample-specific weights to the loss. Empirical study shows that our approach can be applied to many existing MRC models. Combined with contextual representations from pre-trained language models (such as ELMo), we achieve new state-of-the-art results on a set of MRC benchmark datasets. We release our code at.<url>https://github.com/xycforgithub/MultiTask-MRC</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1273 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1273.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361691015 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1273/>Iterative Search for Weakly Supervised Semantic Parsing</a></strong><br><a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1273><div class="card-body p-3 small">Training semantic parsers from question-answer pairs typically involves searching over an exponentially large space of <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>, and an unguided search can easily be misled by spurious logical forms that coincidentally evaluate to the correct answer. We propose a novel iterative training algorithm that alternates between searching for consistent logical forms and maximizing the marginal likelihood of the retrieved ones. This training scheme lets us iteratively train <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that provide guidance to subsequent ones to search for logical forms of increasing <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a>, thus dealing with the problem of spuriousness. We evaluate these techniques on two hard datasets : WikiTableQuestions (WTQ) and Cornell Natural Language Visual Reasoning (NLVR), and show that our training algorithm outperforms the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1275 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1275" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1275/>Bridging the Gap : Attending to Discontinuity in Identification of Multiword Expressions<span class=acl-fixed-case>B</span>ridging the Gap: <span class=acl-fixed-case>A</span>ttending to Discontinuity in Identification of Multiword Expressions</a></strong><br><a href=/people/o/omid-rohanian/>Omid Rohanian</a>
|
<a href=/people/s/shiva-taslimipoor/>Shiva Taslimipoor</a>
|
<a href=/people/s/samaneh-kouchaki/>Samaneh Kouchaki</a>
|
<a href=/people/l/le-an-ha/>Le An Ha</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1275><div class="card-body p-3 small">We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target <a href=https://en.wikipedia.org/wiki/Classification_of_discontinuities>discontinuity</a>, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored : Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We finally propose a combined <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that integrates complementary information from both, through a gating mechanism. The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1277 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1277" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1277/>VCWE : Visual Character-Enhanced Word Embeddings<span class=acl-fixed-case>VCWE</span>: Visual Character-Enhanced Word Embeddings</a></strong><br><a href=/people/c/chi-sun/>Chi Sun</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1277><div class="card-body p-3 small">Chinese is a <a href=https://en.wikipedia.org/wiki/Logogram>logographic writing system</a>, and the shape of Chinese characters contain rich syntactic and semantic information. In this paper, we propose a model to learn Chinese word embeddings via three-level composition : (1) a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to extract the intra-character compositionality from the visual shape of a character ; (2) a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> with self-attention to compose character representation into word embeddings ; (3) the Skip-Gram framework to capture non-compositionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks : word similarity, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1278 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1278.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1278" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1278/>Subword Encoding in Lattice LSTM for Chinese Word Segmentation<span class=acl-fixed-case>LSTM</span> for <span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/s/shuailong-liang/>Shuailong Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1278><div class="card-body p-3 small">We investigate subword information for Chinese word segmentation, by integrating sub word embeddings trained using byte-pair encoding into a Lattice LSTM (LaLSTM) network over a character sequence. Experiments on standard benchmark show that subword information brings significant gains over strong character-based segmentation models. To our knowledge, this is the first research on the effectiveness of <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> on neural word segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1281 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1281/>Shrinking Japanese Morphological Analyzers With <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-supervised Learning</a><span class=acl-fixed-case>J</span>apanese Morphological Analyzers With Neural Networks and Semi-supervised Learning</a></strong><br><a href=/people/a/arseny-tolmachev/>Arseny Tolmachev</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1281><div class="card-body p-3 small">For languages without natural word boundaries, like <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> is a prerequisite for downstream analysis. For <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, segmentation is often done jointly with <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part of speech tagging</a>, and this process is usually referred to as <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>. Morphological analyzers are trained on data hand-annotated with segmentation boundaries and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>part of speech tags</a>. A segmentation dictionary or character n-gram information is also provided as additional inputs to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Incorporating this extra information makes <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> large. Modern neural morphological analyzers can consume gigabytes of <a href=https://en.wikipedia.org/wiki/Computer_memory>memory</a>. We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations. The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network, and independently infers both segmentation and part of speech information. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained in an end-to-end and semi-supervised fashion, on labels produced by a state-of-the-art analyzer. We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> itself is significantly smaller than the dictionary-based one : it uses less than 15 megabytes of space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1282 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1282/>Neural Constituency Parsing of Speech Transcripts</a></strong><br><a href=/people/p/paria-jamshid-lou/>Paria Jamshid Lou</a>
|
<a href=/people/y/yufei-wang/>Yufei Wang</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1282><div class="card-body p-3 small">This paper studies the performance of a neural self-attentive parser on <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed speech</a>. Speech presents parsing challenges that do not appear in written text, such as the lack of <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> and the presence of <a href=https://en.wikipedia.org/wiki/Speech_disfluency>speech disfluencies</a> (including filled pauses, <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetitions</a>, corrections, etc.). Disfluencies are especially problematic for conventional syntactic parsers, which typically fail to find any EDITED disfluency nodes at all. This motivated the development of special disfluency detection systems, and special mechanisms added to <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> specifically to handle <a href=https://en.wikipedia.org/wiki/Disfluency>disfluencies</a>. However, we show here that <a href=https://en.wikipedia.org/wiki/Parsing>neural parsers</a> can find EDITED disfluency nodes, and the best <a href=https://en.wikipedia.org/wiki/Parsing>neural parsers</a> find them with an accuracy surpassing that of specialized disfluency detection systems, thus making these specialized mechanisms unnecessary. This paper also investigates a modified <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> that puts more weight on EDITED nodes. It also describes tree-transformations that simplify the disfluency detection task by providing alternative encodings of disfluencies and syntactic information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1283 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1283/>Acoustic-to-Word Models with Conversational Context Information</a></strong><br><a href=/people/s/suyoun-kim/>Suyoun Kim</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1283><div class="card-body p-3 small">Conversational context information, higher-level knowledge that spans across sentences, can help to recognize a long conversation. However, existing speech recognition models are typically built at a <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>, and thus it may not capture important <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context information</a>. The recent progress in end-to-end speech recognition enables integrating context with other available information (e.g., acoustic, linguistic resources) and directly recognizing words from <a href=https://en.wikipedia.org/wiki/Speech>speech</a>. In this work, we present a direct acoustic-to-word, end-to-end speech recognition model capable of utilizing the conversational context to better process long conversations. We evaluate our proposed approach on the Switchboard conversational speech corpus and show that our system outperforms a standard end-to-end speech recognition system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1286/>Relation Classification Using Segment-Level Attention-based CNN and Dependency-based RNN<span class=acl-fixed-case>CNN</span> and Dependency-based <span class=acl-fixed-case>RNN</span></a></strong><br><a href=/people/v/van-hien-tran/>Van-Hien Tran</a>
|
<a href=/people/v/van-thuy-phi/>Van-Thuy Phi</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1286><div class="card-body p-3 small">Recently, relation classification has gained much success by exploiting <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. In this paper, we propose a new model effectively combining Segment-level Attention-based Convolutional Neural Networks (SACNNs) and Dependency-based Recurrent Neural Networks (DepRNNs). While SACNNs allow the model to selectively focus on the important information segment from the raw sequence, DepRNNs help to handle the long-distance relations from the shortest dependency path of relation entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> without using any external lexical features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1288 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1288/>Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions</a></strong><br><a href=/people/z/zhi-xiu-ye/>Zhi-Xiu Ye</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1288><div class="card-body p-3 small">This paper presents a neural relation extraction method to deal with the noisy training data generated by distant supervision. Previous studies mainly focus on sentence-level de-noising by designing <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with intra-bag attentions. In this paper, both intra-bag and inter-bag attentions are considered in order to deal with the <a href=https://en.wikipedia.org/wiki/Noise>noise</a> at sentence-level and bag-level respectively. First, relation-aware bag representations are calculated by weighting <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> using intra-bag attentions. Here, each possible relation is utilized as the query for attention calculation instead of only using the target relation in conventional methods. Furthermore, the representation of a group of bags in the training set which share the same relation label is calculated by weighting bag representations using a similarity-based inter-bag attention module. Finally, a bag group is utilized as a training sample when building our relation extractor. Experimental results on the New York Times dataset demonstrate the effectiveness of our proposed intra-bag and inter-bag attention modules. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also achieves better relation extraction accuracy than state-of-the-art methods on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1289 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1289/>Ranking-Based Autoencoder for Extreme Multi-label Classification</a></strong><br><a href=/people/b/bingyu-wang/>Bingyu Wang</a>
|
<a href=/people/l/li-chen/>Li Chen</a>
|
<a href=/people/w/wei-sun/>Wei Sun</a>
|
<a href=/people/k/kechen-qin/>Kechen Qin</a>
|
<a href=/people/k/kefeng-li/>Kefeng Li</a>
|
<a href=/people/h/hui-zhou/>Hui Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1289><div class="card-body p-3 small">Extreme Multi-label classification (XML) is an important yet challenging machine learning task, that assigns to each instance its most relevant candidate labels from an extremely large label collection, where the numbers of labels, <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and instances could be thousands or millions. XML is more and more on demand in the Internet industries, accompanied with the increasing business scale / scope and data accumulation. The extremely large label collections yield challenges such as <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>, inter-label dependency and noisy labeling. Many methods have been proposed to tackle these challenges, based on different mathematical formulations. In this paper, we propose a deep learning XML method, with a word-vector-based self-attention, followed by a ranking-based AutoEncoder architecture. The proposed method has three major advantages : 1) the autoencoder simultaneously considers the inter-label dependencies and the feature-label dependencies, by projecting labels and <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> onto a common embedding space ; 2) the ranking loss not only improves the training efficiency and accuracy but also can be extended to handle noisy labeled data ; 3) the efficient attention mechanism improves feature representation by highlighting feature importance. Experimental results on benchmark datasets show the proposed method is competitive to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1290 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1290/>Posterior-regularized REINFORCE for Instance Selection in Distant Supervision<span class=acl-fixed-case>REINFORCE</span> for Instance Selection in Distant Supervision</a></strong><br><a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/s/shiliang-pu/>Shiliang Pu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1290><div class="card-body p-3 small">This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to the task of <a href=https://en.wikipedia.org/wiki/Instance_selection>instance selection</a> in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>unbiased methods</a>, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1291 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1291" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1291/>Scalable Collapsed Inference for High-Dimensional Topic Models</a></strong><br><a href=/people/r/rashidul-islam/>Rashidul Islam</a>
|
<a href=/people/j/james-foulds/>James Foulds</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1291><div class="card-body p-3 small">The bigger the corpus, the more topics it can potentially support. To truly make full use of massive text corpora, a topic model inference algorithm must therefore scale efficiently in 1) documents and 2) topics, while 3) achieving accurate <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. Previous methods have achieved two out of three of these criteria simultaneously, but never all three at once. In this paper, we develop an online inference algorithm for <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> which leverages <a href=https://en.wikipedia.org/wiki/Stochastic>stochasticity</a> to scale well in the number of documents, sparsity to scale well in the number of topics, and which operates in the collapsed representation of the topic model for improved accuracy and run-time performance. We use a <a href=https://en.wikipedia.org/wiki/Monte_Carlo_method>Monte Carlo inner loop</a> in the online setting to approximate the collapsed variational Bayes updates in a sparse and efficient way, which we accomplish via the MetropolisHastings Walker method. We showcase our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on LDA and the recently proposed mixed membership skip-gram topic model. Our method requires only amortized O(k_d) computation per word token instead of O(K) operations, where the number of topics occurring for a particular document k_d the total number of topics in the corpus K, to converge to a high-quality solution.<tex-math>O(k_{d})</tex-math> computation per word token instead of <tex-math>O(K)</tex-math> operations, where the number of topics occurring for a particular document <tex-math>k_{d}\\ll</tex-math> the total number of topics in the corpus <tex-math>K</tex-math>, to converge to a high-quality solution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1293 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1293/>Predicting Malware Attributes from Cybersecurity Texts</a></strong><br><a href=/people/a/arpita-roy/>Arpita Roy</a>
|
<a href=/people/y/youngja-park/>Youngja Park</a>
|
<a href=/people/s/shimei-pan/>Shimei Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1293><div class="card-body p-3 small">Text analytics is a useful tool for studying <a href=https://en.wikipedia.org/wiki/Malware>malware behavior</a> and <a href=https://en.wikipedia.org/wiki/Threat_(computer)>tracking emerging threats</a>. The task of automated malware attribute identification based on cybersecurity texts is very challenging due to a large number of malware attribute labels and a small number of training instances. In this paper, we propose a novel feature learning method to leverage diverse knowledge sources such as small amount of human annotations, unlabeled text and specifications about malware attribute labels. Our evaluation has demonstrated the effectiveness of our method over the state-of-the-art malware attribute prediction systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1298.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1298 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1298 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1298" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1298/>A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/d/duy-cat-can/>Duy-Cat Can</a>
|
<a href=/people/h/hoang-quynh-le/>Hoang-Quynh Le</a>
|
<a href=/people/q/quang-thuy-ha/>Quang-Thuy Ha</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1298><div class="card-body p-3 small">To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> suffers from its own disadvantage of either <a href=https://en.wikipedia.org/wiki/Information_asymmetry>missing or redundant information</a>. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> that combines the advantages of these two <a href=https://en.wikipedia.org/wiki/Scientific_modelling>approaches</a>. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. The data and source code are available at https://github.com/catcd/RbSP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356071812 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1299" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1299/>Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases</a></strong><br><a href=/people/y/yu-chen/>Yu Chen</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/m/mohammed-j-zaki/>Mohammed J. Zaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1299><div class="card-body p-3 small">When answering <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> over knowledge bases (KBs), different question components and KB aspects play different roles. However, most existing embedding-based methods for knowledge base question answering (KBQA) ignore the subtle inter-relationships between the question and the KB (e.g., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a>, <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation paths</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>). In this work, we propose to directly model the two-way flow of interactions between the questions and the KB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring no external resources and only very few hand-crafted features, on the WebQuestions benchmark, our method significantly outperforms existing information-retrieval based methods, and remains competitive with (hand-crafted) semantic parsing based methods. Also, since we use <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, our method offers better <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> compared to other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356088995 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1301/>Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering</a></strong><br><a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1301><div class="card-body p-3 small">Traditional Key-value Memory Neural Networks (KV-MemNNs) are proved to be effective to support shallow reasoning over a collection of documents in domain specific Question Answering or Reading Comprehension tasks. However, extending KV-MemNNs to Knowledge Based Question Answering (KB-QA) is not trivia, which should properly decompose a complex question into a sequence of queries against the <a href=https://en.wikipedia.org/wiki/Random-access_memory>memory</a>, and update the query representations to support multi-hop reasoning over the <a href=https://en.wikipedia.org/wiki/Random-access_memory>memory</a>. In this paper, we propose a novel mechanism to enable conventional KV-MemNNs models to perform interpretable reasoning for complex questions. To achieve this, we design a new query updating strategy to mask previously-addressed memory information from the query representations, and introduce a novel STOP strategy to avoid invalid or repeated memory reading without strong annotation signals. This also enables KV-MemNNs to produce structured queries and work in a semantic parsing fashion. Experimental results on benchmark datasets show that our solution, trained with question-answer pairs only, can provide conventional KV-MemNNs models with better reasoning abilities on complex questions, and achieve state-of-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1304 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359689303 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1304" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1304/>Analyzing Polarization in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : Method and Application to <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> on 21 Mass Shootings</a></strong><br><a href=/people/d/dorottya-demszky/>Dorottya Demszky</a>
|
<a href=/people/n/nikhil-garg/>Nikhil Garg</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/j/james-zou/>James Zou</a>
|
<a href=/people/j/jesse-shapiro/>Jesse Shapiro</a>
|
<a href=/people/m/matthew-gentzkow/>Matthew Gentzkow</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1304><div class="card-body p-3 small">We provide an NLP framework to uncover four linguistic dimensions of <a href=https://en.wikipedia.org/wiki/Political_polarization>political polarization</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> : topic choice, <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a>, <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> and <a href=https://en.wikipedia.org/wiki/Illocutionary_force>illocutionary force</a>. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events ; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to study 4.4 M tweets on 21 <a href=https://en.wikipedia.org/wiki/Mass_shooting>mass shootings</a>. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> rather than topic choice. We identify <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing devices</a>, such as grounding and the contrasting use of the terms terrorist and crazy, that contribute to <a href=https://en.wikipedia.org/wiki/Political_polarization>polarization</a>. Results pertaining to topic choice, <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational methods</a> for studying them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355830579 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1306/>Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks</a></strong><br><a href=/people/n/ningyu-zhang/>Ningyu Zhang</a>
|
<a href=/people/s/shumin-deng/>Shumin Deng</a>
|
<a href=/people/z/zhanlin-sun/>Zhanlin Sun</a>
|
<a href=/people/g/guanying-wang/>Guanying Wang</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/h/huajun-chen/>Huajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1306><div class="card-body p-3 small">We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate few-shot models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that <a href=https://en.wikipedia.org/wiki/Relational_model>relational knowledge</a> into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a>, especially for long-tail relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355837778 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1309/>OpenCeres : When <a href=https://en.wikipedia.org/wiki/Open_information_extraction>Open Information Extraction</a> Meets the Semi-Structured Web<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>C</span>eres: <span class=acl-fixed-case>W</span>hen Open Information Extraction Meets the Semi-Structured Web</a></strong><br><a href=/people/c/colin-lockard/>Colin Lockard</a>
|
<a href=/people/p/prashant-shiralkar/>Prashant Shiralkar</a>
|
<a href=/people/x/xin-luna-dong/>Xin Luna Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1309><div class="card-body p-3 small">Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade. However, this research has largely ignored the vast quantity of facts available in semi-structured webpages. In this paper, we define the problem of OpenIE from <a href=https://en.wikipedia.org/wiki/Semi-structured_model>semi-structured websites</a> to extract such facts, and present an approach for solving it. We also introduce a labeled evaluation dataset to motivate research in this area. Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for the relations present on the site. We then use this <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> to learn a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for relation extraction. Experimental results of this method on our new benchmark dataset obtained a <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of over 70 %. A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1313 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1313.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361725345 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1313/>Selective Attention for Context-aware Neural Machine Translation</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1313><div class="card-body p-3 small">Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document. Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents. To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences. We also propose single-level attention approaches based on sentence or word-level information in the context. The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context. Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1315 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1315.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356125366 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1315/>Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction</a></strong><br><a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1315><div class="card-body p-3 small">A major obstacle in reinforcement learning-based sentence generation is the large action space whose size is equal to the vocabulary size of the target-side language. To improve the efficiency of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, we present a novel approach for reducing the action space based on dynamic vocabulary prediction. Our method first predicts a fixed-size small vocabulary for each input to generate its target sentence. The input-specific vocabularies are then used at supervised and reinforcement learning steps, and also at test time. In our experiments on six <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and two image captioning datasets, our method achieves faster <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> (~2.7x faster) with less <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU memory</a> (~2.3x less) than the full-vocabulary counterpart. We also show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> more effectively receives rewards with fewer iterations of supervised pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347415373 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1316/>Mitigating Uncertainty in Document Classification</a></strong><br><a href=/people/x/xuchao-zhang/>Xuchao Zhang</a>
|
<a href=/people/f/fanglan-chen/>Fanglan Chen</a>
|
<a href=/people/c/chang-tien-lu/>Chang-Tien Lu</a>
|
<a href=/people/n/naren-ramakrishnan/>Naren Ramakrishnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1316><div class="card-body p-3 small">The uncertainty measurement of classifiers&#8217; predictions is especially important in applications such as <a href=https://en.wikipedia.org/wiki/Medical_diagnosis>medical diagnoses</a> that need to ensure limited human resources can focus on the most uncertain predictions returned by <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. However, few existing uncertainty models attempt to improve overall prediction accuracy where <a href=https://en.wikipedia.org/wiki/Human_resources>human resources</a> are involved in the text classification task. In this paper, we propose a novel neural-network-based model that applies a new dropout-entropy method for uncertainty measurement. We also design a metric learning method on <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature representations</a>, which can boost the performance of dropout-based uncertainty methods with smaller prediction variance in accurate prediction trials. Extensive experiments on real-world data sets demonstrate that our method can achieve a considerable improvement in overall prediction accuracy compared to existing approaches. In particular, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improved the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from 0.78 to 0.92 when 30 % of the most uncertain predictions were handed over to <a href=https://en.wikipedia.org/wiki/Expert_witness>human experts</a> in 20NewsGroup data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1322 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1322/>Customizing Grapheme-to-Phoneme System for Non-Trivial Transcription Problems in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla Language</a><span class=acl-fixed-case>B</span>angla Language</a></strong><br><a href=/people/s/sudipta-saha-shubha/>Sudipta Saha Shubha</a>
|
<a href=/people/n/nafis-sadeq/>Nafis Sadeq</a>
|
<a href=/people/s/shafayat-ahmed/>Shafayat Ahmed</a>
|
<a href=/people/m/md-nahidul-islam/>Md. Nahidul Islam</a>
|
<a href=/people/m/muhammad-abdullah-adnan/>Muhammad Abdullah Adnan</a>
|
<a href=/people/m/md-yasin-ali-khan/>Md. Yasin Ali Khan</a>
|
<a href=/people/m/mohammad-zuberul-islam/>Mohammad Zuberul Islam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1322><div class="card-body p-3 small">Grapheme to phoneme (G2P) conversion is an integral part in various text and speech processing systems, such as : <a href=https://en.wikipedia.org/wiki/Speech_synthesis>Text to Speech system</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition system</a>, etc. The existing <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> for G2P conversion in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a> are mostly rule-based. However, data-driven approaches have proved their superiority over rule-based approaches for large-scale G2P conversion in other languages, such as : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, etc. As the performance of data-driven approaches for G2P conversion depend largely on pronunciation lexicon on which the system is trained, in this paper, we investigate on developing an improved training lexicon by identifying and categorizing the critical cases in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a> and include those critical cases in training lexicon for developing a robust G2P conversion system in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a>. Additionally, we have incorporated <a href=https://en.wikipedia.org/wiki/Nasal_vowel>nasal vowels</a> in our proposed phoneme list. Our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> outperforms other state-of-the-art approaches for G2P conversion in <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1325/>Exploiting Noisy Data in Distant Supervision Relation Classification</a></strong><br><a href=/people/k/kaijia-yang/>Kaijia Yang</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/x/xinyu-dai/>Xin-yu Dai</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1325><div class="card-body p-3 small">Distant supervision has obtained great progress on relation classification task. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> still suffers from noisy labeling problem. Different from previous works that underutilize <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy data</a> which inherently characterize the property of classification, in this paper, we propose RCEND, a novel framework to enhance Relation Classification by Exploiting <a href=https://en.wikipedia.org/wiki/Noisy_data>Noisy Data</a>. First, an instance discriminator with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> is designed to split the noisy data into correctly labeled data and incorrectly labeled data. Second, we learn a robust relation classifier in semi-supervised learning way, whereby the correctly and incorrectly labeled data are treated as labeled and unlabeled data respectively. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1327 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1327/>Learning Relational Representations by Analogy using Hierarchical Siamese Networks<span class=acl-fixed-case>S</span>iamese Networks</a></strong><br><a href=/people/g/gaetano-rossiello/>Gaetano Rossiello</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/r/robert-farrell/>Robert Farrell</a>
|
<a href=/people/n/nicolas-r-fauceglia/>Nicolas Fauceglia</a>
|
<a href=/people/m/michael-glass/>Michael Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1327><div class="card-body p-3 small">We address <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> as an analogy problem by proposing a novel approach to learn representations of relations expressed by their textual mentions. In our assumption, if two pairs of entities belong to the same relation, then those two pairs are analogous. Following this idea, we collect a large set of analogous pairs by matching triples in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> with web-scale corpora through distant supervision. We leverage this dataset to train a hierarchical siamese network in order to learn entity-entity embeddings which encode relational information through the different linguistic paraphrasing expressing the same relation. We evaluate our model in a one-shot learning task by showing a promising generalization capability in order to classify unseen relation types, which makes this approach suitable to perform automatic knowledge base population with minimal supervision. Moreover, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be used to generate pre-trained embeddings which provide a valuable signal when integrated into an existing neural-based model by outperforming the state-of-the-art methods on a downstream relation extraction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1328 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1328/>An Effective Label Noise Model for DNN Text Classification<span class=acl-fixed-case>DNN</span> Text Classification</a></strong><br><a href=/people/i/ishan-jindal/>Ishan Jindal</a>
|
<a href=/people/d/daniel-pressel/>Daniel Pressel</a>
|
<a href=/people/b/brian-lester/>Brian Lester</a>
|
<a href=/people/m/matthew-nokleby/>Matthew Nokleby</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1328><div class="card-body p-3 small">Because large, human-annotated datasets suffer from labeling errors, it is crucial to be able to train <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> in the presence of label noise. While training <a href=https://en.wikipedia.org/wiki/Image_classification>image classification models</a> with label noise have received much attention, training <a href=https://en.wikipedia.org/wiki/Text_classification>text classification models</a> have not. In this paper, we propose an approach to training <a href=https://en.wikipedia.org/wiki/Deep_learning>deep networks</a> that is robust to label noise. This approach introduces a non-linear processing layer (noise model) that models the statistics of the label noise into a convolutional neural network (CNN) architecture. The noise model and the CNN weights are learned jointly from noisy training data, which prevents the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> from overfitting to erroneous labels. Through extensive experiments on several text classification datasets, we show that this approach enables the CNN to learn better sentence representations and is robust even to extreme label noise. We find that proper initialization and <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> of this noise model is critical. Further, by contrast to results focusing on large batch sizes for mitigating label noise for image classification, we find that altering the batch size does not have much effect on <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1330/>Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language Models</a></strong><br><a href=/people/y/yiben-yang/>Yiben Yang</a>
|
<a href=/people/j/ji-ping-wang/>Ji-Ping Wang</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1330><div class="card-body p-3 small">Recurrent neural network language models (RNNLM) form a valuable foundation for many NLP systems, but training the models can be computationally expensive, and may take days to train on a large corpus. We explore a technique that uses large corpus n-gram statistics as a regularizer for training a neural network LM on a smaller corpus. In experiments with the Billion-Word and Wikitext corpora, we show that the technique is effective, and more time-efficient than simply training on a larger sequential corpus. We also introduce new <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> for selecting the most informative n-grams, and show that these boost efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1332 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1332/>Relation Discovery with Out-of-Relation Knowledge Base as Supervision</a></strong><br><a href=/people/y/yan-liang/>Yan Liang</a>
|
<a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/j/jianwen-zhang/>Jianwen Zhang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1332><div class="card-body p-3 small">Unsupervised relation discovery aims to discover new relations from a given <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> without <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a>. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> does not consider existing human annotated knowledge bases even when they are relevant to the relations to be discovered. In this paper, we study the problem of how to use out-of-relation knowledge bases to supervise the discovery of unseen relations, where out-of-relation means that relations to discover from the <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and those in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are not overlapped. We construct a set of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> between entity pairs based on the knowledge base embedding and then incorporate <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> into the relation discovery by a variational auto-encoder based algorithm. Experiments show that our new approach can improve the state-of-the-art relation discovery performance by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1336 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1336/>Evaluating and Enhancing the Robustness of Dialogue Systems : A Case Study on a Negotiation Agent</a></strong><br><a href=/people/m/minhao-cheng/>Minhao Cheng</a>
|
<a href=/people/w/wei-wei/>Wei Wei</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1336><div class="card-body p-3 small">Recent research has demonstrated that goal-oriented dialogue agents trained on large datasets can achieve striking performance when interacting with human users. In real world applications, however, it is important to ensure that the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> performs smoothly interacting with not only regular users but also those malicious ones who would attack the <a href=https://en.wikipedia.org/wiki/System>system</a> through interactions in order to achieve goals for their own advantage. In this paper, we develop <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to evaluate the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of a dialogue agent by carefully designed attacks using <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial agents</a>. Those <a href=https://en.wikipedia.org/wiki/Attack_(computing)>attacks</a> are performed in both black-box and white-box settings. Furthermore, we demonstrate that adversarial training using our attacks can significantly improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of a goal-oriented dialogue system. On a case-study of the negotiation agent developed by (Lewis et al., 2017), our attacks reduced the average advantage of rewards between the attacker and the trained RL-based agent from 2.68 to -5.76 on a scale from -10 to 10 for randomized goals. Moreover, we show that with the adversarial training, we are able to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of negotiation agents by 1.5 points on average against all our attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1340 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1340/>Semantic Role Labeling with Associated Memory Network</a></strong><br><a href=/people/c/chaoyu-guan/>Chaoyu Guan</a>
|
<a href=/people/y/yuhao-cheng/>Yuhao Cheng</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1340><div class="card-body p-3 small">Semantic role labeling (SRL) is a task to recognize all the predicate-argument pairs of a sentence, which has been in a performance improvement bottleneck after a series of latest works were presented. This paper proposes a novel syntax-agnostic SRL model enhanced by the proposed associated memory network (AMN), which makes use of inter-sentence attention of label-known associated sentences as a kind of <a href=https://en.wikipedia.org/wiki/Memory>memory</a> to further enhance dependency-based SRL. In detail, we use sentences and their labels from train dataset as an <a href=https://en.wikipedia.org/wiki/Association_(psychology)>associated memory cue</a> to help label the target sentence. Furthermore, we compare several associated sentences selecting strategies and label merging methods in AMN to find and utilize the label of associated sentences while attending them. By leveraging the attentive memory from known training data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark datasets for syntax-agnostic setting, showing a new effective research line of SRL enhancement other than exploiting external resources such as well pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1341 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1341" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1341/>Better, Faster, Stronger Sequence Tagging Constituent Parsers</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1341><div class="card-body p-3 small">Sequence tagging models for constituent parsing are faster, but less accurate than other types of <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. In this work, we address the following weaknesses of such constituent parsers : (a) high error rates around closing brackets of long constituents, (b) large label sets, leading to sparsity, and (c) error propagation arising from greedy decoding. To effectively close brackets, we train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that learns to switch between tagging schemes. To reduce sparsity, we decompose the label set and use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to jointly learn to predict sublabels. Finally, we mitigate issues from greedy decoding through auxiliary losses and sentence-level fine-tuning with policy gradient. Combining these techniques, we clearly surpass the performance of sequence tagging constituent parsers on the English and Chinese Penn Treebanks, and reduce their parsing time even further. On the SPMRL datasets, we observe even greater improvements across the board, including a new state of the art on <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1347 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1347" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1347/>Learning Hierarchical Discourse-level Structure for Fake News Detection</a></strong><br><a href=/people/h/hamid-karimi/>Hamid Karimi</a>
|
<a href=/people/j/jiliang-tang/>Jiliang Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1347><div class="card-body p-3 small">On the one hand, nowadays, <a href=https://en.wikipedia.org/wiki/Fake_news>fake news articles</a> are easily propagated through various online media platforms and have become a grand threat to the trustworthiness of information. On the other hand, our understanding of the language of fake news is still minimal. Incorporating hierarchical discourse-level structure of fake and real news articles is one crucial step toward a better understanding of how these <a href=https://en.wikipedia.org/wiki/Article_(publishing)>articles</a> are structured. Nevertheless, this has rarely been investigated in the fake news detection domain and faces tremendous challenges. First, existing methods for capturing discourse-level structure rely on annotated corpora which are not available for fake news datasets. Second, how to extract out useful information from such discovered <a href=https://en.wikipedia.org/wiki/Biomolecular_structure>structures</a> is another challenge. To address these challenges, we propose Hierarchical Discourse-level Structure for Fake news detection. HDSF learns and constructs a discourse-level structure for fake / real news articles in an automated and data-driven manner. Moreover, we identify insightful structure-related properties, which can explain the discovered structures and boost our understating of fake news. Conducted experiments show the effectiveness of the proposed approach. Further structural analysis suggests that real and fake news present substantial differences in the hierarchical discourse-level structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1357 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359703968 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1357" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1357/>Attention is not Explanation<span class=acl-fixed-case>A</span>ttention is not <span class=acl-fixed-case>E</span>xplanation</a></strong><br><a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1357><div class="card-body p-3 small">Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency : models equipped with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a> that aim to assess the degree to which attention weights provide meaningful explanations for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard <a href=https://en.wikipedia.org/wiki/Attentional_control>attention modules</a> do not provide meaningful explanations and should not be treated as though they do.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1358 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359702665 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1358" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1358/>Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning</a></strong><br><a href=/people/p/prithviraj-ammanabrolu/>Prithviraj Ammanabrolu</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1358><div class="card-body p-3 small">Text-based adventure games provide a platform on which to explore <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> in the context of a combinatorial action space, such as <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. We present a deep reinforcement learning architecture that represents the game state as a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> which is learned during exploration. This <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> is used to prune the action space, enabling more efficient <a href=https://en.wikipedia.org/wiki/Exploration>exploration</a>. The question of which action to take can be reduced to a question-answering task, a form of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> that pre-trains certain parts of our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1360 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1360.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359699975 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1360/>Context Dependent Semantic Parsing over Temporally Structured Data</a></strong><br><a href=/people/c/charles-chen-jr/>Charles Chen</a>
|
<a href=/people/r/razvan-bunescu/>Razvan Bunescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1360><div class="card-body p-3 small">We describe a new semantic parsing setting that allows users to query the system using both natural language questions and actions within a <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>graphical user interface</a>. Multiple <a href=https://en.wikipedia.org/wiki/Time_series>time series</a> belonging to an entity of interest are stored in a database and the user interacts with the system to obtain a better understanding of the entity&#8217;s state and behavior, entailing sequences of actions and questions whose answers may depend on previous factual or navigational interactions. We design an LSTM-based encoder-decoder architecture that models context dependency through copying mechanisms and multiple levels of attention over inputs and previous outputs. When trained to predict tokens using <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, the proposed <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> substantially outperforms standard sequence generation baselines. Training the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> using policy gradient leads to further improvements in performance, reaching a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>sequence-level accuracy</a> of 88.7 % on artificial data and 74.8 % on real data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1362 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356133444 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1362" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1362/>pair2vec : Compositional Word-Pair Embeddings for Cross-Sentence Inference</a></strong><br><a href=/people/m/mandar-joshi/>Mandar Joshi</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1362><div class="card-body p-3 small">Reasoning about implied relationships (e.g. paraphrastic, <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>, encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. Our pairwise embeddings are computed as a compositional function of each word&#8217;s representation, which is learned by maximizing the pointwise mutual information (PMI) with the contexts in which the the two words co-occur. We add these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> to the cross-sentence attention layer of existing <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference models</a> (e.g. BiDAF for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>, ESIM for NLI), instead of extending or replacing existing <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Experiments show a gain of 2.7 % on the recently released SQuAD 2.0 and 1.3 % on MultiNLI. Our representations also aid in better generalization with gains of around 6-7 % on adversarial SQuAD datasets, and 8.8 % on the adversarial entailment test set by Glockner et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1364 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356153695 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1364/>Let’s Make Your Request More Persuasive : Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms</a></strong><br><a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1364><div class="card-body p-3 small">Modeling what makes a request persuasive-eliciting the desired response from a reader-is critical to the study of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>, <a href=https://en.wikipedia.org/wiki/Behavioral_economics>behavioral economics</a>, and <a href=https://en.wikipedia.org/wiki/Advertising>advertising</a>. Yet current <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> ca n&#8217;t quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>, we propose a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies-offering increased interpretability of persuasive speech-and has applications for other situations with document-level supervision but only partial sentence supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1365 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356167288 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1365/>Recursive Routing Networks : Learning to Compose Modules for Language Understanding</a></strong><br><a href=/people/i/ignacio-cases/>Ignacio Cases</a>
|
<a href=/people/c/clemens-rosenbaum/>Clemens Rosenbaum</a>
|
<a href=/people/m/matthew-riemer/>Matthew Riemer</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/t/tim-klinger/>Tim Klinger</a>
|
<a href=/people/a/alex-tamkin/>Alex Tamkin</a>
|
<a href=/people/o/olivia-li/>Olivia Li</a>
|
<a href=/people/s/sandhini-agarwal/>Sandhini Agarwal</a>
|
<a href=/people/j/joshua-d-greene/>Joshua D. Greene</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/l/lauri-karttunen/>Lauri Karttunen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1365><div class="card-body p-3 small">We introduce Recursive Routing Networks (RRNs), which are modular, adaptable models that learn effectively in diverse environments. RRNs consist of a set of <a href=https://en.wikipedia.org/wiki/Subroutine>functions</a>, typically organized into a <a href=https://en.wikipedia.org/wiki/Grid_(spatial_index)>grid</a>, and a meta-learner decision-making component called the <a href=https://en.wikipedia.org/wiki/Router_(computing)>router</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly optimizes the parameters of the <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> and the meta-learner&#8217;s policy for routing inputs through those <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. RRNs can be incorporated into existing architectures in a number of ways ; we explore adding them to word representation layers, recurrent network hidden layers, and classifier layers. Our evaluation task is natural language inference (NLI). Using the MultiNLI corpus, we show that an RRN&#8217;s routing decisions reflect the high-level genre structure of that <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. To show that RRNs can learn to specialize to more fine-grained semantic distinctions, we introduce a new corpus of NLI examples involving implicative predicates, and show that the model components become fine-tuned to the inferential signatures that are characteristic of these <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1366 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1366.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/356184145 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1366/>Structural Neural Encoders for AMR-to-text Generation<span class=acl-fixed-case>AMR</span>-to-text Generation</a></strong><br><a href=/people/m/marco-damonte/>Marco Damonte</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1366><div class="card-body p-3 small">AMR-to-text generation is a problem recently introduced to the NLP community, in which the goal is to generate sentences from Abstract Meaning Representation (AMR) graphs. Sequence-to-sequence models can be used to this end by converting the AMR graphs to strings. Approaching the problem while working directly with <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> requires the use of graph-to-sequence models that encode the AMR graph into a <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representation</a>. Such <a href=https://en.wikipedia.org/wiki/Code>encoding</a> has been shown to be beneficial in the past, and unlike sequential encoding, it allows us to explicitly capture reentrant structures in the AMR graphs. We investigate the extent to which <a href=https://en.wikipedia.org/wiki/Reentrancy_(computing)>reentrancies</a> (nodes with multiple parents) have an impact on AMR-to-text generation by comparing graph encoders to tree encoders, where <a href=https://en.wikipedia.org/wiki/Reentrancy_(computing)>reentrancies</a> are not preserved. We show that improvements in the treatment of reentrancies and <a href=https://en.wikipedia.org/wiki/Long-range_dependence>long-range dependencies</a> contribute to higher overall scores for graph encoders. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 24.40 BLEU on LDC2015E86, outperforming the state of the art by 1.1 points and 24.54 BLEU on LDC2017T10, outperforming the <a href=https://en.wikipedia.org/wiki/State_(computer_science)>state</a> of the art by 1.24 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1378 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1378/>What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue</a></strong><br><a href=/people/l/laura-aina/>Laura Aina</a>
|
<a href=/people/c/carina-silberer/>Carina Silberer</a>
|
<a href=/people/i/ionut-sorodoc/>Ionut-Teodor Sorodoc</a>
|
<a href=/people/m/matthijs-westera/>Matthijs Westera</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1378><div class="card-body p-3 small">Humans use <a href=https://en.wikipedia.org/wiki/Language>language</a> to refer to entities in the external world. Motivated by this, in recent years several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> do not really build entity representations and that they make poor use of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>linguistic context</a>. These negative results underscore the need for model analysis, to test whether the motivations for particular <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a> are borne out in how <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> behave when deployed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1380 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1380/>Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog</a></strong><br><a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a>
|
<a href=/people/r/rushin-shah/>Rushin Shah</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1380><div class="card-body p-3 small">One of the first steps in the utterance interpretation pipeline of many task-oriented conversational AI systems is to identify user intents and the corresponding slots. Since data collection for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> for this task is time-consuming, it is desirable to make use of existing <a href=https://en.wikipedia.org/wiki/Data>data</a> in a high-resource language to train models in low-resource languages. However, development of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> has largely been hindered by the lack of multilingual training data. In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 57k annotated utterances in <a href=https://en.wikipedia.org/wiki/English_language>English</a> (43k), <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> (8.6k) and <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a> (5k) across the domains weather, alarm, and reminder. We use this data set to evaluate three different cross-lingual transfer methods : (1) translating the training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel method of using a multilingual machine translation encoder as contextual word representations. We find that given several hundred training examples in the the target language, the latter two methods outperform translating the training data. Further, in very low-resource settings, multilingual contextual word representations give better results than using cross-lingual static embeddings. We also compare the cross-lingual methods to using monolingual resources in the form of contextual ELMo representations and find that given just small amounts of target language data, this method outperforms all cross-lingual methods, which highlights the need for more sophisticated cross-lingual methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1381 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1381/>Evaluating Coherence in <a href=https://en.wikipedia.org/wiki/Dialogue_system>Dialogue Systems</a> using Entailment</a></strong><br><a href=/people/n/nouha-dziri/>Nouha Dziri</a>
|
<a href=/people/e/ehsan-kamalloo/>Ehsan Kamalloo</a>
|
<a href=/people/k/kory-mathewson/>Kory Mathewson</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaiane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1381><div class="card-body p-3 small">Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1382.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1382 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1382 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1382/>On Knowledge distillation from <a href=https://en.wikipedia.org/wiki/Complex_network>complex networks</a> for response prediction</a></strong><br><a href=/people/s/siddhartha-arora/>Siddhartha Arora</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/h/harish-g-ramaswamy/>Harish G. Ramaswamy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1382><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> have lead to the development of very complex models which compute rich representations for query and documents by capturing all pairwise interactions between query and document words. This makes these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> expensive in space and time, and in practice one has to restrict the length of the documents that can be fed to these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have also been recently employed for the task of predicting dialog responses from available background documents (e.g., Holl-E dataset). However, here the documents are longer, thereby rendering these complex models infeasible except in select restricted settings. In order to overcome this, we use standard simple models which do not capture all pairwise interactions, but learn to emulate certain characteristics of a complex teacher network. Specifically, we first investigate the conicity of representations learned by a complex model and observe that it is significantly lower than that of simpler models. Based on this insight, we modify the simple architecture to mimic this <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>characteristic</a>. We go further by using knowledge distillation approaches, where the simple model acts as a student and learns to match the output from the complex teacher network. We experiment with the Holl-E dialog data set and show that by mimicking characteristics and matching outputs from a teacher, even a simple network can give improved performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1384 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1384/>Unsupervised Extraction of Partial Translations for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1384><div class="card-body p-3 small">In neural machine translation (NMT), monolingual data are usually exploited through a so-called <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> : sentences in the target language are translated into the source language to synthesize new parallel data. While this method provides more training data to better model the target language, on the source side, it only exploits translations that the NMT system is already able to generate using a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on existing parallel data. In this work, we assume that new translation knowledge can be extracted from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>, without relying at all on existing parallel data. We propose a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for extracting from monolingual data what we call partial translations : pairs of source and target sentences that contain sequences of tokens that are translations of each other. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is fully unsupervised and takes only source and target monolingual data as input. Our empirical evaluation points out that our partial translations can be used in combination with <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> to further improve NMT models. Furthermore, while partial translations are particularly useful for low-resource language pairs, they can also be successfully exploited in resource-rich scenarios to improve translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1385 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1385/>Low-Resource Syntactic Transfer with Unsupervised Source Reordering</a></strong><br><a href=/people/m/mohammad-sadegh-rasooli/>Mohammad Sadegh Rasooli</a>
|
<a href=/people/m/michael-collins/>Michael Collins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1385><div class="card-body p-3 small">We describe a cross-lingual transfer method for dependency parsing that takes into account the problem of word order differences between source and target languages. Our model only relies on the <a href=https://en.wikipedia.org/wiki/Bible>Bible</a>, a considerably smaller parallel data than the commonly used parallel data in transfer methods. We use the concatenation of projected trees from the <a href=https://en.wikipedia.org/wiki/Text_corpus>Bible corpus</a>, and the gold-standard treebanks in multiple source languages along with cross-lingual word representations. We demonstrate that reordering the source treebanks before training on them for a target language improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of languages outside the <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European language family</a>. Our experiments on 68 treebanks (38 languages) in the Universal Dependencies corpus achieve a high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for all languages. Among them, our experiments on 16 <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> of 12 <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>non-European languages</a> achieve an average UAS absolute improvement of 3.3 % over a <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1388.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1388 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1388 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1388/>Massively Multilingual Neural Machine Translation</a></strong><br><a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/m/melvin-johnson/>Melvin Johnson</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1388><div class="card-body p-3 small">Multilingual Neural Machine Translation enables training a single model that supports <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1390 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1390/>Combining Discourse Markers and Cross-lingual Embeddings for SynonymAntonym Classification</a></strong><br><a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1390><div class="card-body p-3 small">It is well-known that distributional semantic approaches have difficulty in distinguishing between <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> (Grefenstette, 1992 ; Pad and Lapata, 2003). Recent work has shown that supervision available in <a href=https://en.wikipedia.org/wiki/English_language>English</a> for this <a href=https://en.wikipedia.org/wiki/Task_force>task</a> (e.g., lexical resources) can be transferred to other languages via cross-lingual word embeddings. However, this kind of transfer misses monolingual distributional information available in a target language, such as <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>contrast relations</a> that are indicative of <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonymy</a> (e.g. hot... while... cold). In this work, we improve the transfer by exploiting <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual information</a>, expressed in the form of co-occurrences with <a href=https://en.wikipedia.org/wiki/Discourse_marker>discourse markers</a> that convey contrast. Our approach makes use of less than a dozen <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>markers</a>, which can easily be obtained for many languages. Compared to a baseline using only cross-lingual embeddings, we show absolute improvements of 410 % F1-score in <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1391 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1391/>Context-Aware Cross-Lingual Mapping</a></strong><br><a href=/people/h/hanan-aldarmaki/>Hanan Aldarmaki</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1391><div class="card-body p-3 small">Cross-lingual word vectors are typically obtained by fitting an <a href=https://en.wikipedia.org/wiki/Orthogonal_matrix>orthogonal matrix</a> that maps the entries of a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a> from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in the <a href=https://en.wikipedia.org/wiki/Transformation_matrix>transformation matrix</a> by directly mapping the averaged embeddings of aligned sentences in a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1394 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1394" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1394/>Recommendations for Datasets for Source Code Summarization</a></strong><br><a href=/people/a/alexander-leclair/>Alexander LeClair</a>
|
<a href=/people/c/collin-mcmillan/>Collin McMillan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1394><div class="card-body p-3 small">Source Code Summarization is the task of writing short, natural language descriptions of source code. The main use for these descriptions is in <a href=https://en.wikipedia.org/wiki/Software_documentation>software documentation</a> e.g. the one-sentence Java method descriptions in JavaDocs. Code summarization is rapidly becoming a popular research problem, but progress is restrained due to a lack of suitable datasets. In addition, a lack of community standards for creating datasets leads to confusing and unreproducible research results we observe swings in performance of more than 33 % due only to changes in dataset design. In this paper, we make recommendations for these standards from experimental results. We release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> based on prior work of over 2.1 m pairs of Java methods and one sentence method descriptions from over 28k Java projects. We describe the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and point out key differences from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language data</a>, to guide and support future researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1396.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1396 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1396 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-1396/>Understanding the Behaviour of Neural Abstractive Summarizers using Contrastive Examples<span class=acl-fixed-case>U</span>nderstanding the <span class=acl-fixed-case>B</span>ehaviour of <span class=acl-fixed-case>N</span>eural <span class=acl-fixed-case>A</span>bstractive <span class=acl-fixed-case>S</span>ummarizers using <span class=acl-fixed-case>C</span>ontrastive <span class=acl-fixed-case>E</span>xamples</a></strong><br><a href=/people/k/krtin-kumar/>Krtin Kumar</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1396><div class="card-body p-3 small">Neural abstractive summarizers generate summary texts using a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> conditioned on the input source text, and have recently achieved high ROUGE scores on benchmark summarization datasets. We investigate how they achieve this performance with respect to human-written gold-standard abstracts, and whether the systems are able to understand deeper syntactic and semantic structures. We generate a set of contrastive summaries which are perturbed, deficient versions of human-written summaries, and test whether existing neural summarizers score them more highly than the human-written summaries. We analyze their performance on different datasets and find that these <a href=https://en.wikipedia.org/wiki/System>systems</a> fail to understand the source text, in a majority of the cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1401 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1401" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-1401/>Positional Encoding to Control Output Sequence Length</a></strong><br><a href=/people/s/sho-takase/>Sho Takase</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1401><div class="card-body p-3 small">Neural encoder-decoder models have been successful in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation tasks</a>. However, real applications of abstractive summarization must consider an additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) so that a neural encoder-decoder model preserves the length constraint. Unlike previous studies that learn length embeddings, the proposed method can generate a text of any length even if the target length is unseen in training data. The experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is able not only to control generation length but also improve ROUGE scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1404 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361773751 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1404/>Saliency Learning : Teaching the Model Where to Pay Attention<span class=acl-fixed-case>S</span>aliency <span class=acl-fixed-case>L</span>earning: <span class=acl-fixed-case>T</span>eaching the <span class=acl-fixed-case>M</span>odel <span class=acl-fixed-case>W</span>here to <span class=acl-fixed-case>P</span>ay <span class=acl-fixed-case>A</span>ttention</a></strong><br><a href=/people/r/reza-ghaeini/>Reza Ghaeini</a>
|
<a href=/people/x/xiaoli-fern/>Xiaoli Fern</a>
|
<a href=/people/h/hamed-shahbazi/>Hamed Shahbazi</a>
|
<a href=/people/p/prasad-tadepalli/>Prasad Tadepalli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1404><div class="card-body p-3 small">Deep learning has emerged as a compelling solution to many NLP tasks with remarkable performances. However, due to their opacity, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are hard to interpret and trust. Recent work on explaining deep models has introduced approaches to provide insights toward the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s behaviour and predictions, which are helpful for assessing the reliability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s predictions. However, such methods do not improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s reliability</a>. In this paper, we aim to teach the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to make the right prediction for the right reason by providing explanation training and ensuring the alignment of the model&#8217;s explanation with the ground truth explanation. Our experimental results on multiple tasks and datasets demonstrate the effectiveness of the proposed method, which produces more reliable predictions while delivering better results compared to traditionally trained models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1407 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359716954 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1407/>Convolutional Self-Attention Networks</a></strong><br><a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/l/lidia-s-chao/>Lidia S. Chao</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1407><div class="card-body p-3 small">Self-attention networks (SANs) have drawn increasing interest due to their high <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallelization in computation</a> and flexibility in modeling <a href=https://en.wikipedia.org/wiki/Coupling_(computer_programming)>dependencies</a>. SANs can be further enhanced with multi-head attention by allowing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is parameter free in terms of introducing no more parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1415 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359721173 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1415/>On the Idiosyncrasies of the Mandarin Chinese Classifier System<span class=acl-fixed-case>M</span>andarin <span class=acl-fixed-case>C</span>hinese Classifier System</a></strong><br><a href=/people/s/shijia-liu/>Shijia Liu</a>
|
<a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1415><div class="card-body p-3 small">While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973 ; Erbaugh, 1986 ; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy ; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> modify. Using the empirical distribution of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> (in bits) between the distribution over <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic ; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1416 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361815756 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1416/>Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging</a></strong><br><a href=/people/s/sara-meftah/>Sara Meftah</a>
|
<a href=/people/y/youssef-tamaazousti/>Youssef Tamaazousti</a>
|
<a href=/people/n/nasredine-semmar/>Nasredine Semmar</a>
|
<a href=/people/h/hassane-essafi/>Hassane Essafi</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1416><div class="card-body p-3 small">Fine-tuning neural networks is widely used to transfer valuable knowledge from high-resource to low-resource domains. In a standard fine-tuning scheme, source and target problems are trained using the same <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>. Although capable of adapting to new domains, pre-trained units struggle with learning uncommon target-specific patterns. In this paper, we propose to augment the target-network with normalised, weighted and randomly initialised units that beget a better <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> while maintaining the valuable source knowledge. Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1418.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361822826 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1418/>Data Augmentation for Context-Sensitive Neural Lemmatization Using Inflection Tables and Raw Text</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1418><div class="card-body p-3 small">Lemmatization aims to reduce the sparse data problem by relating the <a href=https://en.wikipedia.org/wiki/Inflection>inflected forms</a> of a word to its <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary form</a>. Using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> can help, both for unseen and ambiguous words. Yet most context-sensitive approaches require full lemma-annotated sentences for training, which may be scarce or unavailable in low-resource languages. In addition (as shown here), in a low-resource setting, a <a href=https://en.wikipedia.org/wiki/Lemmatizer>lemmatizer</a> can learn more from n labeled examples of distinct words (types) than from n (contiguous) labeled tokens, since the latter contain far fewer distinct types. To combine the efficiency of type-based learning with the benefits of context, we propose a way to train a context-sensitive lemmatizer with little or no labeled corpus data, using inflection tables from the UniMorph project and raw text examples from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that provide sentence contexts for the unambiguous UniMorph examples. Despite these being unambiguous examples, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> successfully generalizes from them, leading to improved results (both overall, and especially on unseen words) in comparison to a baseline that does not use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1419 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/361827125 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-1419" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1419/>A Structural Probe for Finding Syntax in Word Representations<span class=acl-fixed-case>A</span> Structural Probe for Finding Syntax in Word Representations</a></strong><br><a href=/people/j/john-hewitt/>John Hewitt</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1419><div class="card-body p-3 small">Recent work has improved our ability to detect linguistic knowledge in <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>word representations</a>. However, current methods for detecting syntactic knowledge do not test whether <a href=https://en.wikipedia.org/wiki/Syntax_tree>syntax trees</a> are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> of a neural network&#8217;s word representation space. The probe identifies a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> under which squared L2 distance encodes the distance between words in the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a>, and one in which squared L2 norm encodes depth in the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a>. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models&#8217; vector geometry.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1422 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Short Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-1422.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation>
<i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/365146894 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1422/>Probing the Need for Visual Context in Multimodal Machine Translation</a></strong><br><a href=/people/o/ozan-caglayan/>Ozan Caglayan</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1422><div class="card-body p-3 small">Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30 K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are capable of leveraging the <a href=https://en.wikipedia.org/wiki/Visual_system>visual input</a> to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-1424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-1424 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-1424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Thematic Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/365132300 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-1424/>What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes<span class=acl-fixed-case>R</span>educing Bias in Bios without Access to Protected Attributes</a></strong><br><a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/m/maria-de-arteaga/>Maria De-Arteaga</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/j/jennifer-chayes/>Jennifer Chayes</a>
|
<a href=/people/c/christian-borgs/>Christian Borgs</a>
|
<a href=/people/a/alexandra-chouldechova/>Alexandra Chouldechova</a>
|
<a href=/people/s/sahin-geyik/>Sahin Geyik</a>
|
<a href=/people/k/krishnaram-kenthapadi/>Krishnaram Kenthapadi</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/adam-kalai/>Adam Kalai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-1424><div class="card-body p-3 small">There is a growing body of work that proposes methods for mitigating bias in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a>. These methods typically rely on access to protected attributes such as <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, or age. However, this raises two significant challenges : (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual&#8217;s true occupation and a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> of their name. This method leverages the societal biases that are encoded in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, eliminating the need for access to <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>protected attributes</a>. Crucially, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> only requires access to individuals&#8217; names at training time and not at deployment time. We evaluate two variations of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier&#8217;s overall true positive rate.</div></div></div><hr><div id=n19-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N19-2/>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)</a></strong><br><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/m/michelle-morales/>Michelle Morales</a>
|
<a href=/people/r/rohit-kumar/>Rohit Kumar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-2001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-2001/>Enabling Real-time Neural IME with Incremental Vocabulary Selection<span class=acl-fixed-case>IME</span> with Incremental Vocabulary Selection</a></strong><br><a href=/people/j/jiali-yao/>Jiali Yao</a>
|
<a href=/people/r/raphael-shu/>Raphael Shu</a>
|
<a href=/people/x/xinjian-li/>Xinjian Li</a>
|
<a href=/people/k/katsutoshi-ohtsuki/>Katsutoshi Ohtsuki</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2001><div class="card-body p-3 small">Input method editor (IME) converts sequential alphabet key inputs to words in a target language. It is an indispensable service for billions of Asian users. Although the neural-based language model is extensively studied and shows promising results in sequence-to-sequence tasks, applying a neural-based language model to IME was not considered feasible due to high <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> when converting words on user devices. In this work, we articulate the bottleneck of neural IME decoding to be the heavy softmax computation over a large vocabulary. We propose an approach that incrementally builds a subset vocabulary from the word lattice. Our approach always computes the probability with a selected subset vocabulary. When the selected vocabulary is updated, the stale probabilities in previous steps are fixed by recomputing the missing logits. The experiments on Japanese IME benchmark shows an over 50x speedup for the softmax computations comparing to the baseline, reaching real-time speed even on commodity CPU without losing conversion accuracy. The approach is potentially applicable to other incremental sequence-to-sequence decoding tasks such as real-time continuous speech recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2011/>Neural Lexicons for Slot Tagging in Spoken Language Understanding</a></strong><br><a href=/people/k/kyle-williams/>Kyle Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2011><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> or <a href=https://en.wikipedia.org/wiki/Gazette>gazettes</a> in neural models for slot tagging in spoken language understanding. We develop models that encode lexicon information as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>neural features</a> for use in a Long-short term memory neural network. Experiments are performed on <a href=https://en.wikipedia.org/wiki/Data>data</a> from 4 domains from an intelligent assistant under conditions that often occur in an industry setting, where there may be : 1) large amounts of training data, 2) limited amounts of training data for new domains, and 3) cross domain training. Results show that the use of neural lexicon information leads to a significant improvement in slot tagging, with improvements in the <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of up to 12 %. Our findings have implications for how <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> can be used to improve the performance of neural slot tagging models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2012/>Active Learning for New Domains in Natural Language Understanding</a></strong><br><a href=/people/s/stanislav-peshterliev/>Stanislav Peshterliev</a>
|
<a href=/people/j/john-kearney/>John Kearney</a>
|
<a href=/people/a/abhyuday-jagannatha/>Abhyuday Jagannatha</a>
|
<a href=/people/i/imre-kiss/>Imre Kiss</a>
|
<a href=/people/s/spyros-matsoukas/>Spyros Matsoukas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2012><div class="card-body p-3 small">We explore active learning (AL) for improving the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of new domains in a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding (NLU) system</a>. We propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> called Majority-CRF that uses an ensemble of classification models to guide the selection of relevant utterances, as well as a sequence labeling model to help prioritize informative examples. Experiments with three domains show that Majority-CRF achieves 6.6%-9 % relative error rate reduction compared to <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a> with the same annotation budget, and statistically significant improvements compared to other AL approaches. Additionally, case studies with human-in-the-loop AL on six <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>new domains</a> show 4.6%-9 % improvement on an existing NLU system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2014/>Are the Tools up to the Task? an Evaluation of Commercial Dialog Tools in Developing Conversational Enterprise-grade Dialog Systems</a></strong><br><a href=/people/m/marie-meteer/>Marie Meteer</a>
|
<a href=/people/m/meghan-hickey/>Meghan Hickey</a>
|
<a href=/people/c/carmi-rothberg/>Carmi Rothberg</a>
|
<a href=/people/d/david-nahamoo/>David Nahamoo</a>
|
<a href=/people/e/ellen-eide-kislal/>Ellen Eide Kislal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2014><div class="card-body p-3 small">There has been a significant investment in dialog systems (tools and runtime) for building conversational systems by major companies including <a href=https://en.wikipedia.org/wiki/Google>Google</a>, <a href=https://en.wikipedia.org/wiki/IBM>IBM</a>, <a href=https://en.wikipedia.org/wiki/Microsoft>Microsoft</a>, and <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon</a>. The question remains whether these tools are up to the task of building conversational, task-oriented dialog applications at the enterprise level. In our company, we are exploring and comparing several toolsets in an effort to determine their strengths and weaknesses in meeting our goals for dialog system development : <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, time to market, ease of replicating and extending applications, and efficiency and ease of use by developers. In this paper, we provide both quantitative and qualitative results in three main areas : <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a>, and text generation. While existing toolsets were all incomplete, we hope this paper will provide a roadmap of where they need to go to meet the goal of building effective dialog systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2015/>Development and Deployment of a Large-Scale Dialog-based Intelligent Tutoring System</a></strong><br><a href=/people/s/shazia-afzal/>Shazia Afzal</a>
|
<a href=/people/t/tejas-dhamecha/>Tejas Dhamecha</a>
|
<a href=/people/n/nirmal-mukhi/>Nirmal Mukhi</a>
|
<a href=/people/r/renuka-sindhgatta/>Renuka Sindhgatta</a>
|
<a href=/people/s/smit-marvaniya/>Smit Marvaniya</a>
|
<a href=/people/m/matthew-ventura/>Matthew Ventura</a>
|
<a href=/people/j/jessica-yarbro/>Jessica Yarbro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2015><div class="card-body p-3 small">There are significant challenges involved in the design and implementation of a dialog-based tutoring system (DBT) ranging from <a href=https://en.wikipedia.org/wiki/Domain_engineering>domain engineering</a> to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language classification</a> and eventually instantiating an adaptive, personalized dialog strategy. These issues are magnified when implementing such a <a href=https://en.wikipedia.org/wiki/System>system</a> at scale and across domains. In this paper, we describe and reflect on the design, methods, decisions and assessments that led to the successful deployment of our AI driven DBT currently being used by several hundreds of college level students for practice and self-regulated study in diverse subjects like <a href=https://en.wikipedia.org/wiki/Sociology>Sociology</a>, <a href=https://en.wikipedia.org/wiki/Communication>Communications</a>, and <a href=https://en.wikipedia.org/wiki/Federal_government_of_the_United_States>American Government</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2016/>Learning When Not to Answer : a Ternary Reward Structure for Reinforcement Learning Based Question Answering</a></strong><br><a href=/people/f/frederic-godin/>Fréderic Godin</a>
|
<a href=/people/a/anjishnu-kumar/>Anjishnu Kumar</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2016><div class="card-body p-3 small">In this paper, we investigate the challenges of using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning agents</a> for question-answering over knowledge graphs for real-world applications. We examine the <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> used by state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> and determine that they are inadequate for such settings. More specifically, they do not evaluate the <a href=https://en.wikipedia.org/wiki/System>systems</a> correctly for situations when there is no answer available and thus <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> optimized for these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are poor at modeling confidence. We introduce a simple new <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metric</a> for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2017/>Extraction of Message Sequence Charts from Software Use-Case Descriptions</a></strong><br><a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2017><div class="card-body p-3 small">Software Requirement Specification documents provide natural language descriptions of the core functional requirements as a set of <a href=https://en.wikipedia.org/wiki/Use_case>use-cases</a>. Essentially, each use-case contains a set of <a href=https://en.wikipedia.org/wiki/Actor_(disambiguation)>actors</a> and sequences of steps describing the interactions among them. Goals of use-case reviews and analyses include their correctness, completeness, detection of ambiguities, <a href=https://en.wikipedia.org/wiki/Software_prototyping>prototyping</a>, <a href=https://en.wikipedia.org/wiki/Software_verification>verification</a>, <a href=https://en.wikipedia.org/wiki/Test_case>test case generation</a> and <a href=https://en.wikipedia.org/wiki/Traceability>traceability</a>. Message Sequence Chart (MSC) have been proposed as a expressive, rigorous yet intuitive visual representation of use-cases. In this paper, we describe a linguistic knowledge-based approach to extract MSCs from <a href=https://en.wikipedia.org/wiki/Use_case>use-cases</a>. Compared to existing techniques, we extract richer constructs of the MSC notation such as <a href=https://en.wikipedia.org/wiki/Timer>timers</a>, <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>conditions</a> and alt-boxes. We apply this <a href=https://en.wikipedia.org/wiki/Tool>tool</a> to extract MSCs from several real-life software use-case descriptions and show that it performs better than the existing techniques. We also discuss the benefits and limitations of the extracted MSCs to meet the above goals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2018/>Improving Knowledge Base Construction from Robust Infobox Extraction</a></strong><br><a href=/people/b/boya-peng/>Boya Peng</a>
|
<a href=/people/y/yejin-huh/>Yejin Huh</a>
|
<a href=/people/x/xiao-ling/>Xiao Ling</a>
|
<a href=/people/m/michele-banko/>Michele Banko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2018><div class="card-body p-3 small">A capable, automatic Question Answering (QA) system can provide more complete and accurate answers using a comprehensive knowledge base (KB). One important approach to constructing a comprehensive knowledge base is to extract information from Wikipedia infobox tables to populate an existing KB. Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DBpedia), three major challenges remain : 1) Deterministic extraction patterns used in DBpedia are vulnerable to template changes ; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors ; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB. This paper presents a <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robust approach</a> that tackles all three challenges. We build probabilistic models to predict relations between entity mentions directly from the infobox tables in <a href=https://en.wikipedia.org/wiki/HTML>HTML</a>. The entity mentions are linked to <a href=https://en.wikipedia.org/wiki/Identifier>identifiers</a> in an existing <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> if possible. The unlinkable ones are also parsed and preserved in the final output. Training data for both the <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> and the entity linking models are automatically generated using distant supervision. We demonstrate the empirical effectiveness of the proposed method in both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> compared to a strong IBE baseline, <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a>, with an absolute improvement of 41.3 % in average F1. We also show that our <a href=https://en.wikipedia.org/wiki/Information_extraction>extraction</a> makes the final KB significantly more complete, improving the <a href=https://en.wikipedia.org/wiki/Completeness_(logic)>completeness score</a> of list-value relation types by 61.4 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-2019.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-2019/>A k-Nearest Neighbor Approach towards Multi-level Sequence Labeling</a></strong><br><a href=/people/y/yue-chen/>Yue Chen</a>
|
<a href=/people/j/john-chen/>John Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2019><div class="card-body p-3 small">In this paper we present a new method for intent recognition for complex dialog management in low resource situations. Complex dialog management is required because our target domain is real world mixed initiative food ordering between agents and their customers, where individual customer utterances may contain multiple intents and refer to food items with complex structure. For example, a customer might say Can I get a deluxe burger with large fries and oh put extra mayo on the burger would you? We approach this task as a multi-level sequence labeling problem, with the constraint of limited real training data. Both traditional methods like HMM, <a href=https://en.wikipedia.org/wiki/Microelectromechanical_systems>MEMM</a>, or CRF and newer methods like <a href=https://en.wikipedia.org/wiki/Deep_learning>DNN</a> or BiLSTM use only homogeneous feature sets. Newer <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> perform better but also require considerably more data. Previous research has done pseudo-data synthesis to obtain the required amounts of training data. We propose to use a k-NN learner with heterogeneous feature set. We used windowed word n-grams, POS tag n-grams and pre-trained word embeddings as features. For the experiments we perform a comparison between using pseudo-data and real world data. We also perform <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised self-training</a> to obtain additional labeled data, in order to better model real world scenarios. Instead of using massive pseudo-data, we show that with only less than 1 % of the data size, we can achieve better result than any of the methods above by annotating real world data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2024/>Neural Text Normalization with Subword Units</a></strong><br><a href=/people/c/courtney-mansfield/>Courtney Mansfield</a>
|
<a href=/people/m/ming-sun/>Ming Sun</a>
|
<a href=/people/y/yuzong-liu/>Yuzong Liu</a>
|
<a href=/people/a/ankur-gandhe/>Ankur Gandhe</a>
|
<a href=/people/b/bjorn-hoffmeister/>Björn Hoffmeister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2024><div class="card-body p-3 small">Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech synthesis</a>. Finite state transducers (FSTs) are commonly used to build <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammars</a> that handle <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>. However, translating <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a> into <a href=https://en.wikipedia.org/wiki/Grammar>grammars</a> requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2026/>In Other News : a Bi-style Text-to-speech Model for Synthesizing Newscaster Voice with Limited Data</a></strong><br><a href=/people/n/nishant-prateek/>Nishant Prateek</a>
|
<a href=/people/m/mateusz-lajszczak/>Mateusz Łajszczak</a>
|
<a href=/people/r/roberto-barra-chicote/>Roberto Barra-Chicote</a>
|
<a href=/people/t/thomas-drugman/>Thomas Drugman</a>
|
<a href=/people/j/jaime-lorenzo-trueba/>Jaime Lorenzo-Trueba</a>
|
<a href=/people/t/thomas-merritt/>Thomas Merritt</a>
|
<a href=/people/s/srikanth-ronanki/>Srikanth Ronanki</a>
|
<a href=/people/t/trevor-wood/>Trevor Wood</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2026><div class="card-body p-3 small">Neural text-to-speech synthesis (NTTS) models have shown significant progress in generating high-quality speech, however they require a large quantity of training data. This makes creating <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> for multiple styles expensive and time-consuming. In this paper different styles of speech are analysed based on <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic variations</a>, from this a model is proposed to synthesise <a href=https://en.wikipedia.org/wiki/Speech>speech</a> in the style of a <a href=https://en.wikipedia.org/wiki/News_presenter>newscaster</a>, with just a few hours of supplementary data. We pose the problem of synthesising in a target style using limited data as that of creating a bi-style model that can synthesise both neutral-style and newscaster-style speech via a one-hot vector which factorises the two styles. We also propose conditioning the model on contextual word embeddings, and extensively evaluate it against neutral NTTS, and neutral concatenative-based synthesis. This model closes the gap in perceived style-appropriateness between <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural recordings</a> for <a href=https://en.wikipedia.org/wiki/News_style>newscaster-style of speech</a>, and neutral speech synthesis by approximately two-thirds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-2028/>Content-based Dwell Time Engagement Prediction Model for News Articles</a></strong><br><a href=/people/h/heidar-davoudi/>Heidar Davoudi</a>
|
<a href=/people/a/aijun-an/>Aijun An</a>
|
<a href=/people/g/gordon-edall/>Gordon Edall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-2028><div class="card-body p-3 small">The article dwell time (i.e., expected time that users spend on an article) is among the most important factors showing the article engagement. It is of great interest to predict the dwell time of an article before its release. This allows <a href=https://en.wikipedia.org/wiki/Digital_newspaper>digital newspapers</a> to make informed decisions and publish more engaging articles. In this paper, we propose a novel content-based approach based on a deep neural network architecture for predicting article dwell times. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> extracts <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, event and entity features from an article, learns interactions among them, and combines the interactions with the word-based features of the article to learn a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for predicting the dwell time. The experimental results on a real dataset from a major newspaper show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other state-of-the-art baselines.</div></div></div><hr><div id=n19-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N19-3/>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/f/farah-nadeem/>Farah Nadeem</a>
|
<a href=/people/l/laura-burdick/>Laura Burdick</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/n/na-rae-han/>Na-Rae Han</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3001/>Is It Dish Washer Safe? Automatically Answering Yes / No Questions Using Customer Reviews</a></strong><br><a href=/people/d/daria-dzendzik/>Daria Dzendzik</a>
|
<a href=/people/c/carl-vogel/>Carl Vogel</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3001><div class="card-body p-3 small">It has become commonplace for people to share their opinions about all kinds of products by posting reviews online. It has also become commonplace for potential customers to do research about the quality and limitations of these <a href=https://en.wikipedia.org/wiki/Product_(business)>products</a> by posting questions online. We test the extent to which reviews are useful in <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a> by combining two <a href=https://en.wikipedia.org/wiki/Amazon_Web_Services>Amazon datasets</a> and focusing our attention on <a href=https://en.wikipedia.org/wiki/Yes&#8211;no_question>yes / no questions</a>. A manual analysis of 400 cases reveals that the reviews directly contain the answer to the question just over a third of the time. Preliminary reading comprehension experiments with this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> prove inconclusive, with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in the range 50-66 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-3002.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-3002.Note.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347400639 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-3002/>Identifying and Reducing Gender Bias in Word-Level Language Models</a></strong><br><a href=/people/s/shikha-bordia/>Shikha Bordia</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3002><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> exhibit socially problematic biases, which can be propagated or amplified in the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on such <a href=https://en.wikipedia.org/wiki/Data>data</a>. For example, doctor cooccurs more frequently with <a href=https://en.wikipedia.org/wiki/Sex_and_gender_distinction>male pronouns</a> than <a href=https://en.wikipedia.org/wiki/Sex_and_gender_distinction>female pronouns</a>. In this study we (i) propose a metric to measure <a href=https://en.wikipedia.org/wiki/Gender>gender bias</a> ; (ii) measure bias in a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and the text generated from a recurrent neural network language model trained on the <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> ; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes <a href=https://en.wikipedia.org/wiki/Gender>gender</a> ; (iv) finally, evaluate efficacy of our proposed method on reducing <a href=https://en.wikipedia.org/wiki/Gender>gender bias</a>. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> becomes unstable as the perplexity increases. We replicate this study on three training corporaPenn Treebank, WikiText-2, and CNN / Daily Mailresulting in similar conclusions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3010/>Computational Investigations of Pragmatic Effects in <a href=https://en.wikipedia.org/wiki/Natural_language>Natural Language</a></a></strong><br><a href=/people/j/jad-kabbara/>Jad Kabbara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3010><div class="card-body p-3 small">Semantics and <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> are two complimentary and intertwined aspects of meaning in language. The <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>former</a> is concerned with the literal (context-free) meaning of words and sentences, the <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>latter</a> focuses on the intended meaning, one that is context-dependent. While NLP research has focused in the past mostly on <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, the goal of this thesis is to develop computational models that leverage this pragmatic knowledge in language that is crucial to performing many NLP tasks correctly. In this proposal, we begin by reviewing the current progress in this thesis, namely, on the tasks of definiteness prediction and adverbial presupposition triggering. Then we discuss the proposed research for the remainder of the thesis which builds on this progress towards the goal of building better and more pragmatically-aware natural language generation and understanding systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3011 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-3011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-3011/>SEDTWik : Segmentation-based Event Detection from <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> Using Wikipedia<span class=acl-fixed-case>SEDTW</span>ik: Segmentation-based Event Detection from Tweets Using <span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/k/keval-morabia/>Keval Morabia</a>
|
<a href=/people/n/neti-lalita-bhanu-murthy/>Neti Lalita Bhanu Murthy</a>
|
<a href=/people/a/aruna-malapati/>Aruna Malapati</a>
|
<a href=/people/s/surender-samant/>Surender Samant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3011><div class="card-body p-3 small">Event Detection has been one of the research areas in <a href=https://en.wikipedia.org/wiki/Text_mining>Text Mining</a> that has attracted attention during this decade due to the widespread availability of social media data specifically twitter data. Twitter has become a major source for information about real-world events because of the use of <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> and the small word limit of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> that ensures concise presentation of events. Previous works on event detection from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> are either applicable to detect localized events or breaking news only or miss out on many important events. This paper presents the problems associated with event detection from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories. The main idea is to split each tweet and <a href=https://en.wikipedia.org/wiki/Hashtag>hash-tag</a> into segments, extract bursty segments, cluster them, and summarize them. We evaluated our results on the well-known Events2012 corpus and achieved state-of-the-art results. Keywords : Event detection, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>, <a href=https://en.wikipedia.org/wiki/Microblogging>Microblogging</a>, Tweet segmentation, <a href=https://en.wikipedia.org/wiki/Text_mining>Text Mining</a>, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/Hashtag>Hashtag</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-3012.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/355800547 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-3012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-3012/>Multimodal Machine Translation with Embedding Prediction</a></strong><br><a href=/people/t/tosho-hirasawa/>Tosho Hirasawa</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3012><div class="card-body p-3 small">Multimodal machine translation is an attractive application of neural machine translation (NMT). It helps computers to deeply understand <a href=https://en.wikipedia.org/wiki/Visual_system>visual objects</a> and their relations with <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a>. However, multimodal NMT systems suffer from a shortage of available training data, resulting in poor performance for translating rare words. In NMT, pretrained word embeddings have been shown to improve NMT of low-resource domains, and a search-based approach is proposed to address the rare word problem. In this study, we effectively combine these two approaches in the context of multimodal NMT and explore how we can take full advantage of pretrained word embeddings to better translate rare words. We report overall performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement of 7.67 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> for rare word translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3013/>Deep Learning and Sociophonetics : Automatic Coding of Rhoticity Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/s/sarah-gupta/>Sarah Gupta</a>
|
<a href=/people/a/anthony-dipadova/>Anthony DiPadova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3013><div class="card-body p-3 small">Automated extraction methods are widely available for vowels, but automated methods for coding rhoticity have lagged far behind. R-fulness versus <a href=https://en.wikipedia.org/wiki/R-lessness>r-lessness</a> (in words like park, store, etc.) is a classic and frequently cited variable, but it is still commonly coded by human analysts rather than automated methods. Human-coding requires extensive resources and lacks replicability, making it difficult to compare large datasets across research groups. Can reliable automated methods be developed to aid in coding rhoticity? In this study, we use <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> / Deep Learning, training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 208 Boston-area speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3014/>Data Augmentation by Data Noising for Open-vocabulary Slots in Spoken Language Understanding</a></strong><br><a href=/people/h/hwa-yeon-kim/>Hwa-Yeon Kim</a>
|
<a href=/people/y/yoon-hyung-roh/>Yoon-Hyung Roh</a>
|
<a href=/people/y/young-gil-kim/>Young-Kil Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3014><div class="card-body p-3 small">One of the main challenges in Spoken Language Understanding (SLU) is dealing with &#8216;open-vocabulary&#8217; slots. Recently, SLU models based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> were proposed, but it is still difficult to recognize the slots of unknown words or &#8216;open-vocabulary&#8217; slots because of the high cost of creating a manually tagged SLU dataset. This paper proposes data noising, which reflects the characteristics of the &#8216;open-vocabulary&#8217; slots, for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We applied it to an attention based bi-directional recurrent neural network (Liu and Lane, 2016) and experimented with three datasets : Airline Travel Information System (ATIS), Snips, and MIT-Restaurant. We achieved performance improvements of up to 0.57 % and 3.25 in intent prediction (accuracy) and slot filling (f1-score), respectively. Our method is advantageous because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> does not require additional memory and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can be applied simultaneously with the training process of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-3015/>Expectation and Locality Effects in the Prediction of Disfluent Fillers and Repairs in <a href=https://en.wikipedia.org/wiki/English_language>English Speech</a><span class=acl-fixed-case>E</span>nglish Speech</a></strong><br><a href=/people/s/samvit-dammalapati/>Samvit Dammalapati</a>
|
<a href=/people/r/rajakrishnan-rajkumar/>Rajakrishnan Rajkumar</a>
|
<a href=/people/s/sumeet-agarwal/>Sumeet Agarwal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-3015><div class="card-body p-3 small">This study examines the role of three influential theories of <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a>, viz., Surprisal Theory, Uniform Information Density (UID) hypothesis and Dependency Locality Theory (DLT), in predicting disfluencies in speech production. To this end, we incorporate features based on lexical surprisal, word duration and DLT integration and storage costs into logistic regression classifiers aimed to predict disfluencies in the Switchboard corpus of English conversational speech. We find that <a href=https://en.wikipedia.org/wiki/Speech_disfluency>disfluencies</a> occur in the face of upcoming difficulties and speakers tend to handle this by lessening <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> before <a href=https://en.wikipedia.org/wiki/Speech_disfluency>disfluencies</a> occur. Further, we see that reparandums behave differently from disfluent fillers possibly due to the lessening of the <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> also happening in the word choice of the reparandum, i.e., in the <a href=https://en.wikipedia.org/wiki/Disfluency>disfluency</a> itself. While the UID hypothesis does not seem to play a significant role in disfluency prediction, lexical surprisal and DLT costs do give promising results in explaining <a href=https://en.wikipedia.org/wiki/Language_production>language production</a>. Further, we also find that as a means to lessen <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> for upcoming difficulties speakers take more time on words preceding disfluencies, making duration a key element in understanding <a href=https://en.wikipedia.org/wiki/Speech_disfluency>disfluencies</a>.<i>viz.</i>, Surprisal Theory, Uniform Information Density (UID) hypothesis and Dependency Locality Theory (DLT), in predicting disfluencies in speech production. To this end, we incorporate features based on lexical surprisal, word duration and DLT integration and storage costs into logistic regression classifiers aimed to predict disfluencies in the Switchboard corpus of English conversational speech. We find that disfluencies occur in the face of upcoming difficulties and speakers tend to handle this by lessening cognitive load before disfluencies occur. Further, we see that reparandums behave differently from disfluent fillers possibly due to the lessening of the cognitive load also happening in the word choice of the reparandum, i.e., in the disfluency itself. While the UID hypothesis does not seem to play a significant role in disfluency prediction, lexical surprisal and DLT costs do give promising results in explaining language production. Further, we also find that as a means to lessen cognitive load for upcoming difficulties speakers take more time on words preceding disfluencies, making duration a key element in understanding disfluencies.</div></div></div><hr><div id=n19-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N19-4/>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics (Demonstrations)</a></strong><br><a href=/people/w/waleed-ammar/>Waleed Ammar</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a>
|
<a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4002/>ADIDA : Automatic Dialect Identification for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a><span class=acl-fixed-case>ADIDA</span>: Automatic Dialect Identification for <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/o/ossama-obeid/>Ossama Obeid</a>
|
<a href=/people/m/mohammad-salameh/>Mohammad Salameh</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4002><div class="card-body p-3 small">This demo paper describes ADIDA, a web-based system for automatic dialect identification for <a href=https://en.wikipedia.org/wiki/Arabic_alphabet>Arabic text</a>. The system distinguishes among the dialects of 25 Arab cities (from Rabat to Muscat) in addition to Modern Standard <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. The results are presented with either a point map or a <a href=https://en.wikipedia.org/wiki/Heat_map>heat map</a> visualizing the automatic identification probabilities over a geographical map of the Arab World.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4004/>INS : An Interactive Chinese News Synthesis System<span class=acl-fixed-case>INS</span>: An Interactive <span class=acl-fixed-case>C</span>hinese News Synthesis System</a></strong><br><a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/w/wentao-qin/>Wentao Qin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4004><div class="card-body p-3 small">Nowadays, we are surrounded by more and more <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news articles</a>. Tens or hundreds of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> need to be read if we wish to explore a hot news event or topic. So it is of vital importance to automatically synthesize a batch of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> related to the event or topic into a new synthesis article (or overview article) for reader&#8217;s convenience. It is so challenging to make news synthesis fully automatic that there is no successful solution by now. In this paper, we put forward a novel Interactive News Synthesis system (i.e. INS), which can help generate news overview articles automatically or by interacting with users. More importantly, <a href=https://en.wikipedia.org/wiki/Immigration_and_Naturalization_Service>INS</a> can serve as a tool for editors to help them finish their jobs. In our experiments, INS performs well on both <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topic representation</a> and synthesis article generation. A <a href=https://en.wikipedia.org/wiki/User_study>user study</a> also demonstrates the usefulness and users&#8217; satisfaction with the INS tool. A demo video is available at.<url>https://youtu.be/7ItteKW3GEk</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4006 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-4006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-4006/>Train, Sort, Explain : Learning to Diagnose Translation Models</a></strong><br><a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/d/david-harbecke/>David Harbecke</a>
|
<a href=/people/v/vivien-macketanz/>Vivien Macketanz</a>
|
<a href=/people/e/eleftherios-avramidis/>Eleftherios Avramidis</a>
|
<a href=/people/s/sebastian-moller/>Sebastian Möller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4006><div class="card-body p-3 small">Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial settings</a>, we train a neural text classifier to distinguish human from machine translations. A <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75 % and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4012 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-4012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-4012/>LeafNATS : An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization<span class=acl-fixed-case>L</span>eaf<span class=acl-fixed-case>NATS</span>: An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization</a></strong><br><a href=/people/t/tian-shi/>Tian Shi</a>
|
<a href=/people/p/ping-wang/>Ping Wang</a>
|
<a href=/people/c/chandan-k-reddy/>Chandan K. Reddy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4012><div class="card-body p-3 small">Neural abstractive text summarization (NATS) has received a lot of attention in the past few years from both industry and academia. In this paper, we introduce an open-source toolkit, namely LeafNATS, for training and evaluation of different sequence-to-sequence based models for the NATS task, and for deploying the pre-trained models to real-world applications. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is modularized and extensible in addition to maintaining competitive performance in the NATS task. A live news blogging system has also been implemented to demonstrate how these models can aid blog / news editors by providing them suggestions of headlines and summaries of their articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4014 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4014/>FAKTA : An Automatic End-to-End Fact Checking System<span class=acl-fixed-case>FAKTA</span>: An Automatic End-to-End Fact Checking System</a></strong><br><a href=/people/m/moin-nadeem/>Moin Nadeem</a>
|
<a href=/people/w/wei-fang/>Wei Fang</a>
|
<a href=/people/b/brian-xu/>Brian Xu</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4014><div class="card-body p-3 small">We present FAKTA which is a unified framework that integrates various components of a fact-checking process : document retrieval from media sources with various types of reliability, stance detection of documents with respect to given claims, evidence extraction, and linguistic analysis. FAKTA predicts the factuality of given claims and provides evidence at the document and sentence level to explain its predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4016 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-4016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-4016/>Plan, Write, and Revise : an Interactive System for Open-Domain Story Generation</a></strong><br><a href=/people/s/seraphina-goldfarb-tarrant/>Seraphina Goldfarb-Tarrant</a>
|
<a href=/people/h/haining-feng/>Haining Feng</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4016><div class="card-body p-3 small">Story composition is a challenging problem for machines and even for humans. We present a neural narrative generation system that interacts with humans to generate stories. Our system has different levels of human interaction, which enables us to understand at what stage of story-writing human collaboration is most productive, both to improving story quality and human engagement in the writing process. We compare different varieties of interaction in story-writing, story-planning, and diversity controls under time constraints, and show that increased types of human collaboration at both planning and writing stages results in a 10-50 % improvement in story quality as compared to less interactive baselines. We also show an accompanying increase in user engagement and satisfaction with stories as compared to our own less interactive systems and to previous turn-taking approaches to <a href=https://en.wikipedia.org/wiki/Interaction>interaction</a>. Finally, we find that humans tasked with collaboratively improving a particular characteristic of a story are in fact able to do so, which has implications for future uses of human-in-the-loop systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=N19-4017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/N19-4017/>LT Expertfinder : An Evaluation Framework for Expert Finding Methods<span class=acl-fixed-case>LT</span> Expertfinder: An Evaluation Framework for Expert Finding Methods</a></strong><br><a href=/people/t/tim-fischer/>Tim Fischer</a>
|
<a href=/people/s/steffen-remus/>Steffen Remus</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4017><div class="card-body p-3 small">Expert finding is the task of ranking persons for a predefined topic or search query. Finding experts for a specified area is an important task and has attracted much attention in the <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval community</a>. Most approaches for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> are evaluated in a supervised fashion, which depend on predefined topics of interest as well as gold standard expert rankings. Famous representatives of such datasets are enriched versions of <a href=https://en.wikipedia.org/wiki/DBLP>DBLP</a> provided by the ArnetMiner projet or the W3C Corpus of TREC. However, manually ranking experts can be considered highly subjective and detailed rankings are hardly distinguishable. Evaluating these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> does not necessarily guarantee a good or bad performance of the <a href=https://en.wikipedia.org/wiki/System>system</a>. Particularly for <a href=https://en.wikipedia.org/wiki/Dynamical_system>dynamic systems</a>, where topics are not predefined but formulated as a <a href=https://en.wikipedia.org/wiki/Web_search_query>search query</a>, we believe a more informative approach is to perform <a href=https://en.wikipedia.org/wiki/User_study>user studies</a> for directly comparing different methods in the same view. In order to accomplish this in a user-friendly way, we present the LT Expert Finder web-application, which is equipped with various query-based expert finding methods that can be easily extended, a detailed expert profile view, detailed evidence in form of relevant documents and statistics, and an evaluation component that allows the qualitative comparison between different rankings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4020 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4020/>Litigation Analytics : Extracting and querying motions and orders from US federal courts<span class=acl-fixed-case>US</span> federal courts</a></strong><br><a href=/people/t/thomas-vacek/>Thomas Vacek</a>
|
<a href=/people/d/dezhao-song/>Dezhao Song</a>
|
<a href=/people/h/hugo-molina-salgado/>Hugo Molina-Salgado</a>
|
<a href=/people/r/ronald-teo/>Ronald Teo</a>
|
<a href=/people/c/conner-cowling/>Conner Cowling</a>
|
<a href=/people/f/frank-schilder/>Frank Schilder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4020><div class="card-body p-3 small">Legal litigation planning can benefit from statistics collected from past decisions made by judges. Information on the typical duration for a submitted motion, for example, can give valuable clues for developing a successful <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>. Such information is encoded in semi-structured documents called dockets. In order to extract and aggregate this information, we deployed various <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and machine learning techniques. The aggregated data can be queried in real time within the Westlaw Edge search engine. In addition to a <a href=https://en.wikipedia.org/wiki/Keyword_search>keyword search</a> for <a href=https://en.wikipedia.org/wiki/Judge>judges</a>, <a href=https://en.wikipedia.org/wiki/Lawyer>lawyers</a>, <a href=https://en.wikipedia.org/wiki/Law_firm>law firms</a>, parties and courts, we also implemented a question answering interface that offers targeted questions in order to get to the respective answers quicker.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4023 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4023/>A Research Platform for Multi-Robot Dialogue with Humans<span class=acl-fixed-case>R</span>esearch <span class=acl-fixed-case>P</span>latform for <span class=acl-fixed-case>M</span>ulti-<span class=acl-fixed-case>R</span>obot <span class=acl-fixed-case>D</span>ialogue with <span class=acl-fixed-case>H</span>umans</a></strong><br><a href=/people/m/matthew-marge/>Matthew Marge</a>
|
<a href=/people/s/stephen-nogar/>Stephen Nogar</a>
|
<a href=/people/c/cory-hayes/>Cory J. Hayes</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a>
|
<a href=/people/j/jesse-bloecker/>Jesse Bloecker</a>
|
<a href=/people/e/eric-holder/>Eric Holder</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4023><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Computing_platform>research platform</a> that supports spoken dialogue interaction with multiple robots. The demonstration showcases our crafted MultiBot testing scenario in which users can verbally issue search, navigate, and follow instructions to two robotic teammates : a simulated ground robot and an <a href=https://en.wikipedia.org/wiki/Autonomous_robot>aerial robot</a>. This flexible language and robotic platform takes advantage of existing tools for <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> and dialogue management that are compatible with new domains, and implements an inter-agent communication protocol (tactical behavior specification), where verbal instructions are encoded for tasks assigned to the appropriate robot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-4024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-4024 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-4024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-4024/>Chat-crowd : A Dialog-based Platform for Visual Layout Composition</a></strong><br><a href=/people/p/paola-cascante-bonilla/>Paola Cascante-Bonilla</a>
|
<a href=/people/x/xuwang-yin/>Xuwang Yin</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/s/song-feng/>Song Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-4024><div class="card-body p-3 small">In this paper we introduce Chat-crowd, an interactive environment for visual layout composition via conversational interactions. Chat-crowd supports multiple agents with two conversational roles : agents who play the role of a designer are in charge of placing objects in an editable canvas according to instructions or commands issued by agents with a director role. The system can be integrated with <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing platforms</a> for both synchronous and asynchronous data collection and is equipped with comprehensive <a href=https://en.wikipedia.org/wiki/Quality_control>quality controls</a> on the performance of both types of <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a>. We expect that this system will be useful to build multimodal goal-oriented dialog tasks that require spatial and geometric reasoning.</div></div></div><hr><div id=n19-5><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-5.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/N19-5/>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/N19-5000/>Proceedings of the 2019 Conference of the North <span class=acl-fixed-case>A</span>merican Chapter of the Association for Computational Linguistics: Tutorials</a></strong><br><a href=/people/a/anoop-sarkar/>Anoop Sarkar</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-5001.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/359555654 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-5001/>Deep Adversarial Learning for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a><span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/w/william-yang-wang/>William Yang Wang</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-5001><div class="card-body p-3 small">Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of <a href=https://en.wikipedia.org/wiki/Computer_vision>Computer Vision</a> recently. Adversarial learning is also a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We describe recent advances in deep adversarial learning for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, with a special focus on generation, adversarial examples & rules, and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. We provide an overview of the research area, categorize different types of <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning models</a>, and discuss pros and cons, aiming at providing some practical perspectives on the future of <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> for solving real-world NLP problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/N19-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-N19-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-N19-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/N19-5003.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/347475879 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/N19-5003/>Measuring and Modeling Language Change</a></strong><br><a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-N19-5003><div class="card-body p-3 small">This tutorial is designed to help researchers answer the following sorts of questions :-Are people happier on the weekend?-What was 1861&#8217;s word of the year?-Are Democrats and Republicans more different than ever?-When did gay stop meaning happy?-Are <a href=https://en.wikipedia.org/wiki/Gender_role>gender stereotypes</a> getting weaker, stronger, or just different?-Who is a linguistic leader?-How can we get internet users to be more polite and objective? Such questions are fundamental to the <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a> and humanities, and scholars in these disciplines are increasingly turning to <a href=https://en.wikipedia.org/wiki/Computational_science>computational techniques</a> for answers. Meanwhile, the ACL community is increasingly engaged with data that varies across time, and with the social insights that can be offered by analyzing temporal patterns and trends. The purpose of this tutorial is to facilitate this convergence in two main ways : 1. By synthesizing recent <a href=https://en.wikipedia.org/wiki/Computational_science>computational techniques</a> for handling and modeling <a href=https://en.wikipedia.org/wiki/Temporal_database>temporal data</a>, such as dynamic word embeddings, the tutorial will provide a starting point for future <a href=https://en.wikipedia.org/wiki/Computational_science>computational research</a>. It will also identify useful tools for <a href=https://en.wikipedia.org/wiki/Social_science>social scientists</a> and <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities scholars</a>. The tutorial will provide an overview of techniques and <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from the quantitative social sciences and the <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>, which are not well-known in the <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics community</a>. These techniques include <a href=https://en.wikipedia.org/wiki/Vector_autoregressive_model>vector autoregressive models</a>, multiple comparisons corrections for <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>hypothesis testing</a>, and <a href=https://en.wikipedia.org/wiki/Causal_inference>causal inference</a>. Datasets include historical newspaper archives and corpora of contemporary political speech.</div></div></div><hr><div id=w19-13><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-13.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-13/>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1300/>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></strong><br><a href=/people/a/alexandra-balahur/>Alexandra Balahur</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1302/>A Soft Label Strategy for Target-Level Sentiment Classification</a></strong><br><a href=/people/d/da-yin/>Da Yin</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/x/xiuyu-wu/>Xiuyu Wu</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1302><div class="card-body p-3 small">In this paper, we propose a soft label approach to target-level sentiment classification task, in which a history-based soft labeling model is proposed to measure the possibility of a context word as an opinion word. We also apply a <a href=https://en.wikipedia.org/wiki/Convolution>convolution layer</a> to extract local active features, and introduce positional weights to take relative distance information into consideration. In addition, we obtain more informative target representation by training with context tokens together to make deeper interaction between target and context tokens. We conduct experiments on SemEval 2014 datasets and the experimental results show that our approach significantly outperforms previous models and gives state-of-the-art results on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-1303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-1303/>Online abuse detection : the value of <a href=https://en.wikipedia.org/wiki/Data_preprocessing>preprocessing</a> and neural attention models</a></strong><br><a href=/people/d/dhruv-kumar/>Dhruv Kumar</a>
|
<a href=/people/r/robin-cohen/>Robin Cohen</a>
|
<a href=/people/l/lukasz-golab/>Lukasz Golab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1303><div class="card-body p-3 small">We propose an attention-based neural network approach to detect abusive speech in online social networks. Our approach enables more effective modeling of context and the semantic relationships between words. We also empirically evaluate the value of text pre-processing techniques in addressing the challenge of out-of-vocabulary words in toxic content. Finally, we conduct extensive experiments on the Wikipedia Talk page datasets, showing improved <a href=https://en.wikipedia.org/wiki/Predictive_power>predictive power</a> over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1306/>Using Structured Representation and Data : A Hybrid Model for Negation and Sentiment in Customer Service Conversations</a></strong><br><a href=/people/a/amita-misra/>Amita Misra</a>
|
<a href=/people/m/mansurul-bhuiyan/>Mansurul Bhuiyan</a>
|
<a href=/people/j/jalal-mahmud/>Jalal Mahmud</a>
|
<a href=/people/s/saurabh-tripathy/>Saurabh Tripathy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1306><div class="card-body p-3 small">Twitter customer service interactions have recently emerged as an effective platform to respond and engage with customers. In this work, we explore the role of <a href=https://en.wikipedia.org/wiki/Negation>negation</a> in customer service interactions, particularly applied to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We define rules to identify true negation cues and scope more suited to conversational data than existing general review data. Using semantic knowledge and syntactic structure from constituency parse trees, we propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for scope detection that performs comparable to state of the art BiLSTM. We further investigate the results of negation scope detection for the sentiment prediction task on customer service conversation data using both a traditional SVM and a <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a>. We propose an antonym dictionary based method for <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> applied to a combination CNN-LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Experimental results show that the antonym-based method outperforms the previous lexicon-based and Neural Network methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1309/>When Numbers Matter ! : Detecting Sarcasm in Numerical Portions of Text</a></strong><br><a href=/people/a/abhijeet-dubey/>Abhijeet Dubey</a>
|
<a href=/people/l/lakshya-kumar/>Lakshya Kumar</a>
|
<a href=/people/a/arpan-somani/>Arpan Somani</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1309><div class="card-body p-3 small">Research in sarcasm detection spans almost a decade. However a particular form of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> remains unexplored : <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> expressed through numbers, which we estimate, forms about 11 % of the sarcastic tweets in our dataset. The sentence &#8216;Love waking up at 3 am&#8217; is sarcastic because of the number. In this paper, we focus on detecting <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> in tweets arising out of numbers. Initially, to get an insight into the problem, we implement a rule-based and a statistical machine learning-based (ML) classifier. The rule-based classifier conveys the crux of the numerical sarcasm problem, namely, incongruity arising out of numbers. The statistical ML classifier uncovers the indicators i.e., features of such <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>. The actual system in place, however, are two deep learning (DL) models, CNN and attention network that obtains an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.93 and 0.91 on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets containing numbers. To the best of our knowledge, this is the first line of research investigating the phenomenon of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> arising out of numbers, culminating in a detector thereof.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1310/>Cross-lingual Subjectivity Detection for Resource Lean Languages</a></strong><br><a href=/people/i/ida-amini/>Ida Amini</a>
|
<a href=/people/s/samane-karimi/>Samane Karimi</a>
|
<a href=/people/a/azadeh-shakery/>Azadeh Shakery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1310><div class="card-body p-3 small">Wide and universal changes in the <a href=https://en.wikipedia.org/wiki/Web_content>web content</a> due to the growth of <a href=https://en.wikipedia.org/wiki/Web_2.0>web 2 applications</a> increase the importance of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. Therefore, the related research areas such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> and subjectivity detection receives much attention from the research community. Due to the diverse languages that web-users use to express their opinions and sentiments, research areas like subjectivity detection should present methods which are practicable on all languages. An important prerequisite to effectively achieve this aim is considering the limitations in resource-lean languages. In this paper, cross-lingual subjectivity detection on resource lean languages is investigated using two different approaches : a language-model based and a learning-to-rank approach. Experimental results show the impact of different factors on the performance of subjectivity detection methods using <a href=https://en.wikipedia.org/wiki/English_language>English resources</a> to detect the subjectivity score of <a href=https://en.wikipedia.org/wiki/Persian_language>Persian documents</a>. The experiments demonstrate that the proposed learning-to-rank method outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline method</a> in ranking documents based on their <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity degree</a>.</div></div></div><hr><div id=w19-14><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-14.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-14/>Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1400/>Proceedings of the Sixth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-1405.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-1405/>Modeling Global Syntactic Variation in English Using Dialect Classification<span class=acl-fixed-case>E</span>nglish Using Dialect Classification</a></strong><br><a href=/people/j/jonathan-dunn/>Jonathan Dunn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1405><div class="card-body p-3 small">This paper evaluates global-scale dialect identification for 14 national varieties of English on both <a href=https://en.wikipedia.org/wiki/Web_crawler>web-crawled data</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter data</a>. The paper makes three main contributions : (i) introducing data-driven language mapping as a method for selecting the inventory of national varieties to include in the task ; (ii) producing a large and dynamic set of syntactic features using <a href=https://en.wikipedia.org/wiki/Grammar_induction>grammar induction</a> rather than focusing on a few hand-selected features such as <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> ; and (iii) comparing models across both web corpora and social media corpora in order to measure the robustness of syntactic variation across registers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1406/>Language Discrimination and Transfer Learning for Similar Languages : Experiments with Feature Combinations and Adaptation</a></strong><br><a href=/people/n/nianheng-wu/>Nianheng Wu</a>
|
<a href=/people/e/eric-demattos/>Eric DeMattos</a>
|
<a href=/people/k/kwok-him-so/>Kwok Him So</a>
|
<a href=/people/p/pin-zhen-chen/>Pin-zhen Chen</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1406><div class="card-body p-3 small">This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines : <a href=https://en.wikipedia.org/wiki/Support_Vector_Machine>SVM</a> with a flat combination of features and <a href=https://en.wikipedia.org/wiki/Support_Vector_Machine>SVM ensembles</a>. We participated in all language / dialect identification tasks, as well as the Moldavian vs. Romanian cross-dialect topic identification (MRC) task. Our team achieved first place in German Dialect identification (GDI) and MRC subtasks 2 and 3, second place in the simplified variant of Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT) as well as Cuneiform Language Identification (CLI), and third and fifth place in DMT traditional and MRC subtask 1 respectively. In most cases, the <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM</a> with a flat combination of features performed better than <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM ensembles</a>. Besides describing the systems and the results obtained by them, we provide a tentative comparison between the feature combination methods, and present additional experiments with a method of adaptation to the test set, which may indicate potential pitfalls with some of the data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1411/>Toward a deep dialectological representation of Indo-Aryan<span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>A</span>ryan</a></strong><br><a href=/people/c/chundra-cathcart/>Chundra Cathcart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1411><div class="card-body p-3 small">This paper presents a new approach to disentangling inter-dialectal and intra-dialectal relationships within one such group, the Indo-Aryan subgroup of Indo-European. We draw upon admixture models and deep generative models to tease apart historic language contact and language-specific behavior in the overall patterns of sound change displayed by <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indo-Aryan languages</a>. We show that a deep model of Indo-Aryan dialectology sheds some light on questions regarding inter-relationships among the Indo-Aryan languages, and performs better than a shallow model in terms of certain qualities of the posterior distribution (e.g., entropy of posterior distributions), and outline future pathways for model development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1412/>Naive Bayes and BiLSTM Ensemble for Discriminating between Mainland and Taiwan Variation of Mandarin Chinese<span class=acl-fixed-case>B</span>ayes and <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Ensemble for Discriminating between Mainland and <span class=acl-fixed-case>T</span>aiwan Variation of <span class=acl-fixed-case>M</span>andarin <span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/l/li-yang/>Li Yang</a>
|
<a href=/people/y/yang-xiang/>Yang Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1412><div class="card-body p-3 small">Automatic dialect identification is a more challengingctask than <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a>, as it requires the ability to discriminate between varieties of one language. In this paper, we propose an ensemble based system, which combines traditional machine learning models trained on bag of n-gram fetures, with deep learning models trained on word embeddings, to solve the Discriminating between Mainland and Taiwan Variation of Mandarin Chinese (DMT) shared task at VarDial 2019. Our experiments show that a character bigram-trigram combination based Naive Bayes is a very strong model for identifying varieties of <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin Chinense</a>. Through further ensemble of Navie Bayes and BiLSTM, our system (team : itsalexyang) achived an macro-averaged F1 score of 0.8530 and 0.8687 in two tracks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1413/>BAM : A combination of deep and shallow models for German Dialect Identification.<span class=acl-fixed-case>BAM</span>: A combination of deep and shallow models for <span class=acl-fixed-case>G</span>erman Dialect Identification.</a></strong><br><a href=/people/a/andrei-butnaru/>Andrei M. Butnaru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1413><div class="card-body p-3 small">* This is a submission for the Third VarDial Evaluation Campaign * In this paper, we present a machine learning approach for the German Dialect Identification (GDI) Closed Shared Task of the DSL 2019 Challenge. The proposed approach combines deep and shallow models, by applying a voting scheme on the outputs resulted from a Character-level Convolutional Neural Networks (Char-CNN), a Long Short-Term Memory (LSTM) network, and a model based on String Kernels. The first <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used is the Char-CNN model that merges multiple convolutions computed with <a href=https://en.wikipedia.org/wiki/Kernel_(statistics)>kernels</a> of different sizes. The second model is the LSTM network which applies a global max pooling over the returned sequences over time. Both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> pass the activation maps to two <a href=https://en.wikipedia.org/wiki/Connected_space>fully-connected layers</a>. The final model is based on <a href=https://en.wikipedia.org/wiki/String_kernel>String Kernels</a>, computed on character p-grams extracted from <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech transcripts</a>. The model combines two blended kernel functions, one is the presence bits kernel, and the other is the intersection kernel. The empirical results obtained in the shared task prove that the <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> can achieve good results. The <a href=https://en.wikipedia.org/wiki/System>system</a> proposed in this paper obtained the fourth place with a macro-F1 score of 62.55 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1415/>Initial Experiments In Cross-Lingual Morphological Analysis Using Morpheme Segmentation</a></strong><br><a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/l/lorenzo-tosi/>Lorenzo Tosi</a>
|
<a href=/people/a/anastasia-khorosheva/>Anastasia Khorosheva</a>
|
<a href=/people/o/oleg-serikov/>Oleg Serikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1415><div class="card-body p-3 small">The paper describes initial experiments in data-driven cross-lingual morphological analysis of open-category words using a combination of unsupervised morpheme segmentation, annotation projection and an LSTM encoder-decoder model with attention. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> provides <a href=https://en.wikipedia.org/wiki/Lemmatisation>lemmatisation</a> and morphological analysis generation for previously unseen low-resource language surface forms with only annotated data on the related languages given. Despite the inherently lossy annotation projection, we achieved the best lemmatisation F1-score in the VarDial 2019 Shared Task on Cross-Lingual Morphological Analysis for both Karachay-Balkar (Turkic languages, agglutinative morphology) and Sardinian (Romance languages, fusional morphology).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1416/>Neural and Linear Pipeline Approaches to Cross-lingual Morphological Analysis</a></strong><br><a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/j/jeremy-barnes/>Jeremy Barnes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1416><div class="card-body p-3 small">This paper describes Tbingen-Oslo team&#8217;s participation in the cross-lingual morphological analysis task in the VarDial 2019 evaluation campaign. We participated in the shared task with a standard <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a>. Our model achieved analysis F1-scores of 31.48 and 23.67 on test languages <a href=https://en.wikipedia.org/wiki/Karachay-Balkar_language>Karachay-Balkar (Turkic)</a> and <a href=https://en.wikipedia.org/wiki/Sardinian_language>Sardinian (Romance)</a> respectively. The scores are comparable to the scores obtained by the other participants in both <a href=https://en.wikipedia.org/wiki/Language_family>language families</a>, and the <a href=https://en.wikipedia.org/wiki/Score_(statistics)>analysis score</a> on the Romance data set was also the best result obtained in the shared task. Besides describing the <a href=https://en.wikipedia.org/wiki/System>system</a> used in our shared task participation, we describe another, simpler, <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifiers</a>, and present further analyses using both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our analyses, besides revealing some of the difficult cases, also confirm that the usefulness of a source language in this task is highly correlated with the similarity of source and target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1418/>SC-UPB at the VarDial 2019 Evaluation Campaign : Moldavian vs. Romanian Cross-Dialect Topic Identification<span class=acl-fixed-case>SC</span>-<span class=acl-fixed-case>UPB</span> at the <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2019 Evaluation Campaign: <span class=acl-fixed-case>M</span>oldavian vs. <span class=acl-fixed-case>R</span>omanian Cross-Dialect Topic Identification</a></strong><br><a href=/people/c/cristian-onose/>Cristian Onose</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/s/stefan-trausan-matu/>Stefan Trausan-Matu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1418><div class="card-body p-3 small">This paper describes our models for the Moldavian vs. Romanian Cross-Topic Identification (MRC) evaluation campaign, part of the VarDial 2019 workshop. We focus on the three subtasks for MRC : binary classification between the Moldavian (MD) and the Romanian (RO) dialects and two cross-dialect multi-class classification between six news topics, MD to RO and RO to MD. We propose several deep learning models based on long short-term memory cells, Bidirectional Gated Recurrent Unit (BiGRU) and Hierarchical Attention Networks (HAN). We also employ three word embedding models to represent the text as a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>low dimensional vector</a>. Our official submission includes two runs of the BiGRU and HAN models for each of the three subtasks. The best submitted <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtained the following macro-averaged F1 scores : 0.708 for subtask 1, 0.481 for subtask 2 and 0.480 for the last one. Due to a read error caused by the quoting behaviour over the test file, our final submissions contained a smaller number of items than expected. More than 50 % of the submission files were corrupted. Thus, we also present the results obtained with the corrected labels for which the HAN model achieves the following results : 0.930 for subtask 1, 0.590 for subtask 2 and 0.687 for the third one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1420/>Investigating Machine Learning Methods for Language and Dialect Identification of Cuneiform Texts</a></strong><br><a href=/people/e/ehsan-doostmohammadi/>Ehsan Doostmohammadi</a>
|
<a href=/people/m/minoo-nassajian/>Minoo Nassajian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1420><div class="card-body p-3 small">Identification of the languages written using <a href=https://en.wikipedia.org/wiki/Cuneiform>cuneiform symbols</a> is a difficult task due to the lack of resources and the problem of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>. The Cuneiform Language Identification task in VarDial 2019 addresses the problem of identifying seven languages and dialects written in cuneiform ; Sumerian and six dialects of <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian language</a> : Old Babylonian, Middle Babylonian Peripheral, Standard Babylonian, <a href=https://en.wikipedia.org/wiki/Neo-Babylonian_language>Neo-Babylonian</a>, Late Babylonian, and <a href=https://en.wikipedia.org/wiki/Neo-Assyrian_language>Neo-Assyrian</a>. This paper describes the approaches taken by SharifCL team to this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> in VarDial 2019. The best result belongs to an ensemble of Support Vector Machines and a <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>naive Bayes classifier</a>, both working on <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>character-level features</a>, with macro-averaged F1-score of 72.10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1422/>DTeam @ VarDial 2019 : Ensemble based on skip-gram and triplet loss neural networks for Moldavian vs. Romanian cross-dialect topic identification<span class=acl-fixed-case>DT</span>eam @ <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2019: Ensemble based on skip-gram and triplet loss neural networks for <span class=acl-fixed-case>M</span>oldavian vs. <span class=acl-fixed-case>R</span>omanian cross-dialect topic identification</a></strong><br><a href=/people/d/diana-tudoreanu/>Diana Tudoreanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1422><div class="card-body p-3 small">This paper presents the solution proposed by DTeam in the VarDial 2019 Evaluation Campaign for the Moldavian vs. Romanian cross-topic identification task. The solution proposed is a Support Vector Machines (SVM) ensemble composed of a two character-level neural networks. The first network is a skip-gram classification model formed of an embedding layer, three convolutional layers and two fully-connected layers. The second <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> has a similar architecture, but is trained using the triplet loss function.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1424/>Comparing Pipelined and Integrated Approaches to Dialectal Arabic Neural Machine Translation<span class=acl-fixed-case>A</span>rabic Neural Machine Translation</a></strong><br><a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1424><div class="card-body p-3 small">When translating diglossic languages such as <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, situations may arise where we would like to translate a text but do not know which dialect it is. A traditional approach to this problem is to design dialect identification systems and dialect-specific machine translation systems. However, under the recent paradigm of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, shared multi-dialectal systems have become a natural alternative. Here we explore under which conditions it is beneficial to perform dialect identification for Arabic neural machine translation versus using a general system for all dialects.</div></div></div><hr><div id=w19-15><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-15.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-15/>Proceedings of the Third Workshop on Structured Prediction for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1500/>Proceedings of the Third Workshop on Structured Prediction for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/a/andre-f-t-martins/>Andre Martins</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/g/gerasimos-lampouras/>Gerasimos Lampouras</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1504/>Lightly-supervised Representation Learning with Global Interpretability</a></strong><br><a href=/people/a/andrew-zupon/>Andrew Zupon</a>
|
<a href=/people/m/maria-alexeeva/>Maria Alexeeva</a>
|
<a href=/people/m/marco-valenzuela-escarcega/>Marco Valenzuela-Escárcega</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1504><div class="card-body p-3 small">We propose a lightly-supervised approach for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, in particular named entity classification, which combines the benefits of traditional <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a>, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets : CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a>, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> can be used to produce interpretable models with small loss in performance. This <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a> can be edited by human experts to mitigate some of that loss and in some cases outperform the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1505/>Semi-Supervised Teacher-Student Architecture for Relation Extraction</a></strong><br><a href=/people/f/fan-luo/>Fan Luo</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1505><div class="card-body p-3 small">Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi-supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt <a href=https://en.wikipedia.org/wiki/Mean_Teacher>Mean Teacher</a> (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Additionally, different syntax representations are incorporated into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10 % of the labeled corpus. Further, the syntax-aware model outperforms other syntax-free approaches across all levels of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>.</div></div></div><hr><div id=w19-16><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-16.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-16/>Proceedings of the Combined Workshop on Spatial Language Understanding (SpLU) and Grounded Communication for Robotics (RoboNLP)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1600/>Proceedings of the Combined Workshop on Spatial Language Understanding (<span class=acl-fixed-case>S</span>p<span class=acl-fixed-case>LU</span>) and Grounded Communication for Robotics (<span class=acl-fixed-case>R</span>obo<span class=acl-fixed-case>NLP</span>)</a></strong><br><a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a>
|
<a href=/people/j/jesse-thomason/>Jesse Thomason</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1603/>From Virtual to Real : A Framework for Verbal Interaction with Robots</a></strong><br><a href=/people/e/eugene-joseph/>Eugene Joseph</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1603><div class="card-body p-3 small">A Natural Language Understanding (NLU) pipeline integrated with a 3D physics-based scene is a flexible way to develop and test language-based human-robot interaction, by virtualizing people, robot hardware and the target 3D environment. Here, <a href=https://en.wikipedia.org/wiki/Interaction>interaction</a> means both controlling robots using language and conversing with them about the user&#8217;s physical environment and her daily life. Such a virtual development framework was initially developed for the Bot Colony videogame launched on Steam in June 2014, and has been undergoing improvements since. The <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> is focused of developing intuitive verbal interaction with various types of <a href=https://en.wikipedia.org/wiki/Robot>robots</a>. Key robot functions (robot vision and object recognition, path planning and obstacle avoidance, task planning and constraints, grabbing and inverse kinematics), the human participants in the interaction, and the impact of gravity and other forces on the environment are all simulated using commercial 3D tools. The framework can be used as a robotics testbed : the results of our simulations can be compared with the output of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> in real robots, to validate such <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. A novelty of our framework is support for social interaction with robots-enabling robots to converse about people and objects in the user&#8217;s environment, as well as learning about human needs and everyday life topics from their owner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1605/>Multi-modal Discriminative Model for Vision-and-Language Navigation</a></strong><br><a href=/people/h/haoshuo-huang/>Haoshuo Huang</a>
|
<a href=/people/v/vihan-jain/>Vihan Jain</a>
|
<a href=/people/h/harsh-mehta/>Harsh Mehta</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/e/eugene-ie/>Eugene Ie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1605><div class="card-body p-3 small">Vision-and-Language Navigation (VLN) is a natural language grounding task where agents have to interpret natural language instructions in the context of visual scenes in a dynamic environment to achieve prescribed navigation goals. Successful agents must have the ability to parse natural language of varying linguistic styles, ground them in potentially unfamiliar scenes, plan and react with ambiguous environmental feedback. Generalization ability is limited by the amount of human annotated data. In particular, paired vision-language sequence data is expensive to collect. We develop a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> that evaluates how well an instruction explains a given path in VLN task using multi-modal alignment. Our study reveals that only a small fraction of the high-quality augmented data from Fried et al., as scored by our <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a>, is useful for training VLN agents with similar performance. We also show that a VLN agent warm-started with pre-trained components from the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> outperforms the benchmark success rates of 35.5 by 10 % relative measure.</div></div></div><hr><div id=w19-17><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-17.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-17/>Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1700/>Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies</a></strong><br><a href=/people/u/university-of-sheffield-heidi-christensen/>University of Sheffield Heidi Christensen</a>
|
<a href=/people/f/florida-institute-for-human-kristy-hollingshead/>Florida Institute for Human Kristy Hollingshead</a>
|
<a href=/people/m/machine-cognition/>Machine Cognition</a>
|
<a href=/people/b/boston-college-emily-prudhommeaux/>Boston College Emily Prud’hommeaux</a>
|
<a href=/people/u/university-of-toronto-frank-rudzicz/>University of Toronto Frank Rudzicz</a>
|
<a href=/people/m/michigan-technological-university-keith-vertanen/>Michigan Technological University Keith Vertanen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1703/>Permanent Magnetic Articulograph (PMA) vs Electromagnetic Articulograph (EMA) in Articulation-to-Speech Synthesis for Silent Speech Interface<span class=acl-fixed-case>PMA</span>) vs Electromagnetic Articulograph (<span class=acl-fixed-case>EMA</span>) in Articulation-to-Speech Synthesis for Silent Speech Interface</a></strong><br><a href=/people/b/beiming-cao/>Beiming Cao</a>
|
<a href=/people/n/nordine-sebkhi/>Nordine Sebkhi</a>
|
<a href=/people/t/ted-mau/>Ted Mau</a>
|
<a href=/people/o/omer-t-inan/>Omer T. Inan</a>
|
<a href=/people/j/jun-wang/>Jun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1703><div class="card-body p-3 small">Silent speech interfaces (SSIs) are devices that enable <a href=https://en.wikipedia.org/wiki/Speech>speech communication</a> when <a href=https://en.wikipedia.org/wiki/Speech>audible speech</a> is unavailable. Articulation-to-speech (ATS) synthesis is a software design in <a href=https://en.wikipedia.org/wiki/Speech_synthesis>SSI</a> that directly converts articulatory movement information into audible speech signals. Permanent magnetic articulograph (PMA) is a wireless articulator motion tracking technology that is similar to commercial, wired Electromagnetic Articulograph (EMA). PMA has shown great potential for practical SSI applications, because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is wireless. The <a href=https://en.wikipedia.org/wiki/Autostereoscopy>ATS</a> performance of <a href=https://en.wikipedia.org/wiki/Poly(methyl_methacrylate)>PMA</a>, however, is unknown when compared with current <a href=https://en.wikipedia.org/wiki/Electromagnetic_spectrum>EMA</a>. In this study, we compared the performance of ATS using a PMA we recently developed and a commercially available EMA (NDI Wave system). Datasets with same stimuli and size that were collected from tongue tip were used in the comparison. The experimental results indicated the performance of PMA was close to, although not as equally good as that of EMA. Furthermore, in PMA, converting the raw magnetic signals to positional signals did not significantly affect the performance of <a href=https://en.wikipedia.org/wiki/Autostereoscopy>ATS</a>, which support the future direction in PMA-based ATS can be focused on the use of positional signals to maximize the benefit of <a href=https://en.wikipedia.org/wiki/Spatial_analysis>spatial analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1706/>Investigating <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a> for Improving Predictive AAC<span class=acl-fixed-case>AAC</span></a></strong><br><a href=/people/j/jiban-adhikary/>Jiban Adhikary</a>
|
<a href=/people/r/robbie-watling/>Robbie Watling</a>
|
<a href=/people/c/crystal-fletcher/>Crystal Fletcher</a>
|
<a href=/people/a/alex-stanage/>Alex Stanage</a>
|
<a href=/people/k/keith-vertanen/>Keith Vertanen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1706><div class="card-body p-3 small">Making good letter or word predictions can help accelerate the communication of users of high-tech AAC devices. This is particularly important for real-time person-to-person conversations. We investigate whether per forming speech recognition on the speaking-side of a conversation can improve language model based predictions. We compare the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of three plausible microphone deployment options and the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of two commercial speech recognition engines (Google and IBM Watson). We found that despite recognition word error rates of 7-16 %, our ensemble of N-gram and recurrent neural network language models made predictions nearly as good as when they used the reference transcripts.</div></div></div><hr><div id=w19-18><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-18.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-18/>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1800/>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a></strong><br><a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernandez</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/k/kushal-kafle/>Kushal Kafle</a>
|
<a href=/people/c/christopher-kanan/>Christopher Kanan</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-1802.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-1802/>Referring to Objects in Videos Using Spatio-Temporal Identifying Descriptions</a></strong><br><a href=/people/p/peratham-wiriyathammabhum/>Peratham Wiriyathammabhum</a>
|
<a href=/people/a/abhinav-shrivastava/>Abhinav Shrivastava</a>
|
<a href=/people/v/vlad-morariu/>Vlad Morariu</a>
|
<a href=/people/l/larry-davis/>Larry Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1802><div class="card-body p-3 small">This paper presents a new task, the grounding of spatio-temporal identifying descriptions in videos. Previous work suggests potential bias in existing datasets and emphasizes the need for a new data creation schema to better model linguistic structure. We introduce a new data collection scheme based on grammatical constraints for surface realization to enable us to investigate the problem of grounding spatio-temporal identifying descriptions in videos. We then propose a two-stream modular attention network that learns and grounds spatio-temporal identifying descriptions based on appearance and motion. We show that motion modules help to ground motion-related words and also help to learn in appearance modules because modular neural networks resolve task interference between modules. Finally, we propose a future challenge and a need for a robust system arising from replacing ground truth visual annotations with automatic video object detector and temporal event localization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-1803" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-1803/>A Survey on Biomedical Image Captioning</a></strong><br><a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/v/vasiliki-kougia/>Vasiliki Kougia</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1803><div class="card-body p-3 small">Image captioning applied to biomedical images can assist and accelerate the diagnosis process followed by clinicians. This article is the first survey of biomedical image captioning, discussing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation measures</a>, and <a href=https://en.wikipedia.org/wiki/Scientific_method>state of the art methods</a>. Additionally, we suggest two <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, a weak and a stronger one ; the latter outperforms all current state of the art systems on one of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1807/>Learning Multilingual Word Embeddings Using Image-Text Data</a></strong><br><a href=/people/k/karan-singhal/>Karan Singhal</a>
|
<a href=/people/k/karthik-raman/>Karthik Raman</a>
|
<a href=/people/b/balder-ten-cate/>Balder ten Cate</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1807><div class="card-body p-3 small">There has been significant interest recently in learning multilingual word embeddings in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.</div></div></div><hr><div id=w19-19><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-19.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-19/>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1900/>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a></strong><br><a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/t/tristan-naumann/>Tristan Naumann</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1902/>An Analysis of Attention over Clinical Notes for Predictive Tasks</a></strong><br><a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/r/ramin-mohammadi/>Ramin Mohammadi</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1902><div class="card-body p-3 small">The shift to electronic medical records (EMRs) has engendered research into machine learning and natural language technologies to analyze patient records, and to predict from these clinical outcomes of interest. Two observations motivate our aims here. First, <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured notes</a> contained within EMR often contain key information, and hence should be exploited by <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Second, while strong predictive performance is important, interpretability of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is perhaps equally so for applications in this domain. Together, these points suggest that neural models for <a href=https://en.wikipedia.org/wiki/Electroencephalography>EMR</a> may benefit from incorporation of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> over notes, which one may hope will both yield performance gains and afford transparency in predictions. In this work we perform experiments to explore this question using two EMR corpora and four different predictive tasks, that : (i) inclusion of attention mechanisms is critical for neural encoder modules that operate over notes fields in order to yield competitive performance, but, (ii) unfortunately, while these boost predictive performance, it is decidedly less clear whether they provide meaningful support for predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-1904.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-1904/>Hierarchical Nested Named Entity Recognition</a></strong><br><a href=/people/z/zita-marinho/>Zita Marinho</a>
|
<a href=/people/a/alfonso-mendes/>Afonso Mendes</a>
|
<a href=/people/s/sebastiao-miranda/>Sebastião Miranda</a>
|
<a href=/people/d/david-nogueira/>David Nogueira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1904><div class="card-body p-3 small">In the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a> and other scientific areas, it is often important to recognize different levels of hierarchy in mentions, such as those related to specific symptoms or diseases associated with different <a href=https://en.wikipedia.org/wiki/Anatomical_terms_of_location>anatomical regions</a>. Unlike previous approaches, we build a transition-based parser that explicitly models an arbitrary number of hierarchical and nested mentions, and propose a <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> that encourages correct predictions of higher-level mentions. We further introduce a set of modifier classes which introduces certain concepts that change the meaning of an entity, such as absence, or uncertainty about a given disease. Our proposed model achieves state-of-the-art results in medical entity recognition datasets, using both nested and hierarchical mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-1905" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-1905/>Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models</a></strong><br><a href=/people/o/oren-melamud/>Oren Melamud</a>
|
<a href=/people/c/chaitanya-shivade/>Chaitanya Shivade</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1905><div class="card-body p-3 small">Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding <a href=https://en.wikipedia.org/wiki/Patient_privacy>patient privacy</a> hinder the open dissemination of such <a href=https://en.wikipedia.org/wiki/Data>data</a> and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in <a href=https://en.wikipedia.org/wiki/Medical_record>clinical notes</a> and propose to automatically generate synthetic clinical notes that are more amenable to sharing using <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as <a href=https://en.wikipedia.org/wiki/Utility>utility</a> in training clinical NLP models. Experiments using neural language models yield notes whose <a href=https://en.wikipedia.org/wiki/Utility>utility</a> is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1906/>A Novel System for Extractive Clinical Note Summarization using <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR Data</a><span class=acl-fixed-case>EHR</span> Data</a></strong><br><a href=/people/j/jennifer-liang/>Jennifer Liang</a>
|
<a href=/people/c/ching-huei-tsou/>Ching-Huei Tsou</a>
|
<a href=/people/a/ananya-poddar/>Ananya Poddar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1906><div class="card-body p-3 small">While much data within a patient&#8217;s electronic health record (EHR) is coded, crucial information concerning the patient&#8217;s care and management remain buried in unstructured clinical notes, making it difficult and time-consuming for physicians to review during their usual clinical workflow. In this paper, we present our clinical note processing pipeline, which extends beyond basic medical natural language processing (NLP) with concept recognition and relation detection to also include components specific to EHR data, such as structured data associated with the encounter, sentence-level clinical aspects, and structures of the clinical notes. We report on the use of this <a href=https://en.wikipedia.org/wiki/Drug_pipeline>pipeline</a> in a disease-specific extractive text summarization task on <a href=https://en.wikipedia.org/wiki/Medical_record>clinical notes</a>, focusing primarily on progress notes by physicians and nurse practitioners. We show how the addition of EHR-specific components to the pipeline resulted in an improvement in our overall system performance and discuss the potential impact of EHR-specific components on other higher-level clinical NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1912/>Medical Entity Linking using Triplet Network</a></strong><br><a href=/people/i/ishani-mondal/>Ishani Mondal</a>
|
<a href=/people/s/sukannya-purkayastha/>Sukannya Purkayastha</a>
|
<a href=/people/s/sudeshna-sarkar/>Sudeshna Sarkar</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/j/jitesh-pillai/>Jitesh Pillai</a>
|
<a href=/people/a/amitava-bhattacharyya/>Amitava Bhattacharyya</a>
|
<a href=/people/m/mahanandeeshwar-gattu/>Mahanandeeshwar Gattu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1912><div class="card-body p-3 small">Entity linking (or Normalization) is an essential task in <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a> that maps the entity mentions in the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical text</a> to standard entities in a given Knowledge Base (KB). This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is of great importance in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>. It can also be used for merging different <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>medical and clinical ontologies</a>. In this paper, we center around the problem of disease linking or normalization. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is executed in two phases : candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for <a href=https://en.wikipedia.org/wiki/Ranked_voting>candidate ranking</a>. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1914/>Extracting Factual Min / Max Age Information from Clinical Trial Studies<span class=acl-fixed-case>M</span>in/Max Age Information from Clinical Trial Studies</a></strong><br><a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/d/debasis-ganguly/>Debasis Ganguly</a>
|
<a href=/people/l/lea-deleris/>Léa Deleris</a>
|
<a href=/people/f/francesca-bonin/>Francesca Bonin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1914><div class="card-body p-3 small">Population age information is an essential characteristic of <a href=https://en.wikipedia.org/wiki/Clinical_trial>clinical trials</a>. In this paper, we focus on extracting minimum and maximum (min / max) age values for the study samples from clinical research articles. Specifically, we investigate the use of a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> to address this information extraction task. The min / max age QA model is trained on the massive structured clinical study records from <a href=https://en.wikipedia.org/wiki/ClinicalTrials.gov>ClinicalTrials.gov</a>. For each article, based on multiple min and max age values extracted from the QA model, we predict both actual min / max age values for the study samples and filter out non-factual age expressions. Our system improves the results over (i) a passage retrieval based IE system and (ii) a CRF-based system by a large margin when evaluated on an annotated dataset consisting of 50 research papers on <a href=https://en.wikipedia.org/wiki/Smoking_cessation>smoking cessation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1915/>Distinguishing Clinical Sentiment : The Importance of Domain Adaptation in Psychiatric Patient Health Records</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/p/philip-cawkwell/>Philip Cawkwell</a>
|
<a href=/people/k/kirsten-bolton/>Kirsten Bolton</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/mei-hua-hall/>Mei-Hua Hall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1915><div class="card-body p-3 small">Recently <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tools</a> have been developed to identify and extract salient risk indicators in <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health records (EHRs)</a>. Sentiment analysis, although widely used in non-medical areas for improving <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to psychiatric EHRs by defining psychiatric clinical sentiment, performing an annotation project, and evaluating multiple sentence-level sentiment machine learning (ML) models. Results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity, and that the definition of clinical sentiment that we provide is learnable with relatively small amounts of training data. This project is an initial step towards further refining <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis methods</a> for clinical use. Our long-term objective is to incorporate the results of this project as part of a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> that predicts inpatient readmission risk. We hope that this work will initiate a discussion concerning domain adaptation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to the clinical setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1917/>Attention Neural Model for Temporal Relation Extraction</a></strong><br><a href=/people/s/sijia-liu/>Sijia Liu</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a>
|
<a href=/people/v/vipin-chaudhary/>Vipin Chaudhary</a>
|
<a href=/people/h/hongfang-liu/>Hongfang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1917><div class="card-body p-3 small">Neural network models have shown promise in the temporal relation extraction task. In this paper, we present the attention based neural network model to extract the containment relations within sentences from clinical narratives. The attention mechanism used on top of GRU model outperforms the existing state-of-the-art neural network models on THYME corpus in intra-sentence temporal relation extraction.</div></div></div><hr><div id=w19-20><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-20.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-20/>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2000/>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2002.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2002.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2002/>Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance</a></strong><br><a href=/people/b/brendan-whitaker/>Brendan Whitaker</a>
|
<a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/aparajita-haldar/>Aparajita Haldar</a>
|
<a href=/people/h/hakan-ferhatosmanoglu/>Hakan Ferhatosmanoglu</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2002><div class="card-body p-3 small">Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbors</a>. However, <a href=https://en.wikipedia.org/wiki/Geometry>geometric properties</a> of the continuous feature space contribute directly to the use of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>embedding features</a> in downstream models, and are largely unexplored. We consider four properties of word embedding geometry, namely : position relative to the origin, distribution of features in the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, global pairwise distances, and local pairwise distances. We define a sequence of <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> to generate new embeddings that expose subsets of these <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>properties</a> to downstream models and evaluate change in task performance to understand the contribution of each property to NLP models. We transform publicly available pretrained embeddings from three popular toolkits (word2vec, GloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model linguistic information in the vector space, and extrinsic tasks, which use vectors as input to machine learning models. We find that intrinsic evaluations are highly sensitive to absolute position, while extrinsic tasks rely primarily on local similarity. Our findings suggest that future embedding models and post-processing techniques should focus primarily on <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> to nearby points in <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2003/>The Influence of Down-Sampling Strategies on SVD Word Embedding Stability<span class=acl-fixed-case>SVD</span> Word Embedding Stability</a></strong><br><a href=/people/j/johannes-hellrich/>Johannes Hellrich</a>
|
<a href=/people/b/bernd-kampe/>Bernd Kampe</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2003><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Consistency>stability</a> of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD-PPMI-type embeddings. This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior <a href=https://en.wikipedia.org/wiki/BIBO_stability>stability</a> as well as <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on par with skip-gram embedding</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2004/>How Well Do Embedding Models Capture Non-compositionality? A View from Multiword Expressions</a></strong><br><a href=/people/n/navnita-nandakumar/>Navnita Nandakumar</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/b/bahar-salehi/>Bahar Salehi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2004><div class="card-body p-3 small">In this paper, we apply various <a href=https://en.wikipedia.org/wiki/Embedding>embedding methods</a> on multiword expressions to study how well they capture the nuances of non-compositional data. Our results from a pool of word-, character-, and document-level embbedings suggest that <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a> performs the best, followed by <a href=https://en.wikipedia.org/wiki/FastText>FastText</a> and Infersent. Moreover, we find that recently-proposed contextualised embedding models such as Bert and ELMo are not adept at handling non-compositionality in multiword expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2005.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2005/>Measuring Semantic Abstraction of Multilingual NMT with Paraphrase Recognition and Generation Tasks<span class=acl-fixed-case>NMT</span> with Paraphrase Recognition and Generation Tasks</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2005><div class="card-body p-3 small">In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> when applied to <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> of the source language. The intuition is that an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> produces better representations if a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is capable of recognizing synonymous sentences in the same language even though the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> is significantly reduced in each of the cases, indicating that meaning can be grounded in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. This is further supported by a study on <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> that we also include at the end of the paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2008/>CODAH : An Adversarially-Authored Question Answering Dataset for Common Sense<span class=acl-fixed-case>CODAH</span>: An Adversarially-Authored Question Answering Dataset for Common Sense</a></strong><br><a href=/people/m/michael-chen/>Michael Chen</a>
|
<a href=/people/m/mike-darcy/>Mike D’Arcy</a>
|
<a href=/people/a/alisa-liu/>Alisa Liu</a>
|
<a href=/people/j/jared-fernandez/>Jared Fernandez</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2008><div class="card-body p-3 small">Commonsense reasoning is a critical AI capability, but it is difficult to construct challenging <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that test <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to achieve human-level scores. We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>. CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedure</a> and evaluate the performance of multiple state-of-the-art <a href=https://en.wikipedia.org/wiki/Question_answering>question answering systems</a> on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We observe a significant gap between <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a>, which is 95.3 %, and the performance of the best baseline accuracy of 65.3 % by the OpenAI GPT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2009.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2009.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2009/>Syntactic Interchangeability in Word Embedding Models</a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/a/assaf-toledo/>Assaf Toledo</a>
|
<a href=/people/a/alon-halfon/>Alon Halfon</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2009><div class="card-body p-3 small">Nearest neighbors in word embedding models are commonly observed to be semantically similar, but the relations between them can vary greatly. We investigate the extent to which word embedding models preserve syntactic interchangeability, as reflected by distances between word vectors, and the effect of hyper-parameterscontext window size in particular. We use part of speech (POS) as a proxy for syntactic interchangeability, as generally speaking, words with the same POS are syntactically valid in the same contexts. We also investigate the relationship between <a href=https://en.wikipedia.org/wiki/Interchangeability>interchangeability</a> and <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> as judged by commonly-used word similarity benchmarks, and correlate the result with the performance of word embedding models on these benchmarks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2011/>Probing Biomedical Embeddings from Language Models</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/x/xinghua-lu/>Xinghua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2011><div class="card-body p-3 small">Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained <a href=https://en.wikipedia.org/wiki/Linear_model>LMs</a> as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT (Devlin et al. 2018), ELMo (Peters et al., 2018), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10 M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a> and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2012/>Dyr Bul Shchyl. Proxying Sound Symbolism With Word Embeddings</a></strong><br><a href=/people/i/ivan-p-yamshchikov/>Ivan P. Yamshchikov</a>
|
<a href=/people/v/viascheslav-shibaev/>Viascheslav Shibaev</a>
|
<a href=/people/a/alexey-tikhonov/>Alexey Tikhonov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2012><div class="card-body p-3 small">This paper explores modern <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in the context of <a href=https://en.wikipedia.org/wiki/Sound_symbolism>sound symbolism</a>. Using basic properties of the representations space one can construct semantic axes. A method is proposed to measure if the presence of individual sounds in a given word shifts its <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of that word along a specific axis. It is shown that, in accordance with several experimental and statistical results, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> capture <a href=https://en.wikipedia.org/wiki/Symbol>symbolism</a> for certain sounds.</div></div></div><hr><div id=w19-21><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-21.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-21/>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2100/>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</a></strong><br><a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/d/david-bamman/>David Bamman</a>
|
<a href=/people/o/oren-tsur/>Oren Tsur</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2104/>Geolocating Political Events in Text</a></strong><br><a href=/people/a/andrew-halterman/>Andrew Halterman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2104><div class="card-body p-3 small">This work introduces a general <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for automatically finding the locations where <a href=https://en.wikipedia.org/wiki/Politics>political events</a> in text occurred. Using a novel set of 8,000 labeled sentences, I create a method to link automatically extracted events and locations in text. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves human level performance on the annotation task and outperforms previous event geolocation systems. It can be applied to most <a href=https://en.wikipedia.org/wiki/Event_(computing)>event extraction systems</a> across <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geographic contexts</a>. I formalize the eventlocation linking task, describe the neural network model, describe the potential uses of such a system in <a href=https://en.wikipedia.org/wiki/Political_science>political science</a>, and demonstrate a workflow to answer an open question on the role of conventional military offensives in causing civilian casualties in the Syrian civil war.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2105/>Neural Network Prediction of Censorable Language</a></strong><br><a href=/people/k/kei-yin-ng/>Kei Yin Ng</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/j/jing-peng/>Jing Peng</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2105><div class="card-body p-3 small">Internet censorship imposes restrictions on what information can be publicized or viewed on the Internet. According to Freedom House&#8217;s annual Freedom on the Net report, more than half the world&#8217;s Internet users now live in a place where the Internet is censored or restricted. China has built the world&#8217;s most extensive and sophisticated <a href=https://en.wikipedia.org/wiki/Internet_censorship_in_China>online censorship system</a>. In this paper, we describe a new corpus of censored and uncensored social media tweets from a Chinese microblogging website, <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a>, collected by tracking posts that mention &#8216;sensitive&#8217; topics or authored by &#8216;sensitive&#8217; users. We use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to build a neural network classifier to predict <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs with a 88.50 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> using only <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. We discuss these features in detail and hypothesize that they could potentially be used for <a href=https://en.wikipedia.org/wiki/Censorship_circumvention>censorship circumvention</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2107/>Using <a href=https://en.wikipedia.org/wiki/Time_series>time series and natural language processing</a> to identify <a href=https://en.wikipedia.org/wiki/Viral_phenomenon>viral moments</a> in the 2016 U.S. Presidential Debate<span class=acl-fixed-case>U</span>.<span class=acl-fixed-case>S</span>. Presidential Debate</a></strong><br><a href=/people/j/josephine-lukito/>Josephine Lukito</a>
|
<a href=/people/p/prathusha-kameswara-sarma/>Prathusha K Sarma</a>
|
<a href=/people/j/jordan-foley/>Jordan Foley</a>
|
<a href=/people/a/aman-abhishek/>Aman Abhishek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2107><div class="card-body p-3 small">This paper proposes a method for identifying and studying viral moments or highlights during a <a href=https://en.wikipedia.org/wiki/Political_debate>political debate</a>. Using a combined strategy of <a href=https://en.wikipedia.org/wiki/Time_series>time series analysis</a> and domain adapted word embeddings, this study provides an in-depth analysis of several key moments during the 2016 U.S. Presidential election. First, a time series outlier analysis is used to identify key moments during the debate. These <a href=https://en.wikipedia.org/wiki/Moment_(mathematics)>moments</a> had to result in a long-term shift in attention towards either <a href=https://en.wikipedia.org/wiki/Hillary_Clinton>Hillary Clinton</a> or Donald Trump (i.e., a transient change outlier or an intervention, resulting in a permanent change in the time series). To assess whether these moments also resulted in a discursive shift, two corpora are produced for each potential viral moment (a pre-viral corpus and post-viral corpus). A domain adaptation layer learns weights to combine a generic and domain-specific (DS) word embedding into a domain adapted (DA) embedding. Words are then classified using a generic encoder+ classifier framework that relies on these <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as inputs. Results suggest that both <a href=https://en.wikipedia.org/wiki/Hillary_Clinton>Clinton</a> and <a href=https://en.wikipedia.org/wiki/Donald_Trump>Trump</a> were able to induce discourse-shifting viral moments, though the former is much better at producing a topically-specific discursive shift.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2108/>Stance Classification, Outcome Prediction, and <a href=https://en.wikipedia.org/wiki/Impact_assessment>Impact Assessment</a> : NLP Tasks for Studying Group Decision-Making<span class=acl-fixed-case>NLP</span> Tasks for Studying Group Decision-Making</a></strong><br><a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/a/alan-w-black/>Alan Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2108><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Group_decision-making>group decision-making</a>, the nuanced process of conflict and resolution that leads to <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>consensus formation</a> is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>process variables</a>, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> alongside a large new corpus of over 400,000 <a href=https://en.wikipedia.org/wiki/Internet_forum>group debates</a> on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2112/>Modeling Behavioral Aspects of Social Media Discourse for Moral Classification</a></strong><br><a href=/people/k/kristen-johnson/>Kristen Johnson</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2112><div class="card-body p-3 small">Political discourse on <a href=https://en.wikipedia.org/wiki/Microblogging>social media microblogs</a>, specifically <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, has become an undeniable part of mainstream U.S. politics. Given the length constraint of tweets, politicians must carefully word their statements to ensure their message is understood by their intended audience. This constraint often eliminates the context of the tweet, making automatic analysis of social media political discourse a difficult task. To overcome this challenge, we propose simultaneous modeling of high-level abstractions of political language, such as political slogans and framing strategies, with <a href=https://en.wikipedia.org/wiki/Abstraction>abstractions</a> of how politicians behave on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. These behavioral abstractions can be further leveraged as forms of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> in order to increase prediction accuracy, while reducing the burden of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. In this work, we use Probabilistic Soft Logic (PSL) to build <a href=https://en.wikipedia.org/wiki/Relational_model>relational models</a> to capture the similarities in language and behavior that obfuscate political messages on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. When combined, these descriptors reveal the moral foundations underlying the discourse of U.S. politicians online, across differing governing administrations, showing how <a href=https://en.wikipedia.org/wiki/List_of_political_parties_in_the_United_States>party talking points</a> remain cohesive or change over time.<i>across</i> differing governing administrations, showing how party talking points remain cohesive or change over time.</div></div></div><hr><div id=w19-22><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-22.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-22/>Proceedings of the Natural Legal Language Processing Workshop 2019</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2200/>Proceedings of the Natural Legal Language Processing Workshop 2019</a></strong><br><a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a>
|
<a href=/people/e/elliott-ash/>Elliott Ash</a>
|
<a href=/people/l/leslie-barrett/>Leslie Barrett</a>
|
<a href=/people/d/daniel-chen/>Daniel Chen</a>
|
<a href=/people/a/adam-meyers/>Adam Meyers</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preotiuc-Pietro</a>
|
<a href=/people/d/david-rosenberg/>David Rosenberg</a>
|
<a href=/people/a/amanda-stent/>Amanda Stent</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2202/>Scalable Methods for Annotating Legal-Decision Corpora</a></strong><br><a href=/people/l/lisa-ferro/>Lisa Ferro</a>
|
<a href=/people/j/john-aberdeen/>John Aberdeen</a>
|
<a href=/people/k/karl-branting/>Karl Branting</a>
|
<a href=/people/c/craig-pfeifer/>Craig Pfeifer</a>
|
<a href=/people/a/alexander-yeh/>Alexander Yeh</a>
|
<a href=/people/a/amartya-chakraborty/>Amartya Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2202><div class="card-body p-3 small">Recent research has demonstrated that judicial and administrative decisions can be predicted by <a href=https://en.wikipedia.org/wiki/Machine_learning>machine-learning models</a> trained on prior decisions. However, to have any practical application, these predictions must be explainable, which in turn requires modeling a rich set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Such approaches face a roadblock if the <a href=https://en.wikipedia.org/wiki/Knowledge_engineering>knowledge engineering</a> required to create these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> is not scalable. We present an approach to developing a feature-rich corpus of administrative rulings about domain name disputes, an approach which leverages a small amount of manual annotation and prototypical patterns present in the case documents to automatically extend feature labels to the entire corpus. To demonstrate the feasibility of this approach, we report results from <a href=https://en.wikipedia.org/wiki/Computer>systems</a> trained on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2203/>The Extent of Repetition in Contract Language</a></strong><br><a href=/people/d/dan-simonson/>Dan Simonson</a>
|
<a href=/people/d/daniel-broderick/>Daniel Broderick</a>
|
<a href=/people/j/jonathan-herr/>Jonathan Herr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2203><div class="card-body p-3 small">Contract language is repetitive (Anderson and Manns, 2017), but so is all language (Zipf, 1949). In this paper, we measure the extent to which contract language in <a href=https://en.wikipedia.org/wiki/English_language>English</a> is repetitive compared with the language of other <a href=https://en.wikipedia.org/wiki/English_language>English language corpora</a>. Contracts have much smaller vocabulary sizes compared with similarly sized non-contract corpora across multiple contract types, contain 1/5th as many <a href=https://en.wikipedia.org/wiki/Hapax_legomena>hapax legomena</a>, pattern differently on a log-log plot, use fewer pronouns, and contain sentences that are about 20 % more similar to one another than in other corpora. These suggest that the study of <a href=https://en.wikipedia.org/wiki/Contract>contracts</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> controls for some linguistic phenomena and allows for more in depth study of others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2204/>Sentence Boundary Detection in Legal Text</a></strong><br><a href=/people/g/george-sanchez/>George Sanchez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2204><div class="card-body p-3 small">In this paper, we examined several <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to detect <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence boundaries</a> in <a href=https://en.wikipedia.org/wiki/Legal_writing>legal text</a>. Legal text presents challenges for sentence tokenizers because of the variety of <a href=https://en.wikipedia.org/wiki/Punctuation>punctuations</a> and syntax of legal text. Out-of-the-box algorithms perform poorly on <a href=https://en.wikipedia.org/wiki/Legal_writing>legal text</a> affecting further analysis of the text. A novel and domain-specific approach is needed to detect <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence boundaries</a> to further analyze <a href=https://en.wikipedia.org/wiki/Legal_text>legal text</a>. We present the results of our investigation in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2206/>Litigation Analytics : Case Outcomes Extracted from US Federal Court Dockets<span class=acl-fixed-case>US</span> Federal Court Dockets</a></strong><br><a href=/people/t/thomas-vacek/>Thomas Vacek</a>
|
<a href=/people/r/ronald-teo/>Ronald Teo</a>
|
<a href=/people/d/dezhao-song/>Dezhao Song</a>
|
<a href=/people/t/timothy-nugent/>Timothy Nugent</a>
|
<a href=/people/c/conner-cowling/>Conner Cowling</a>
|
<a href=/people/f/frank-schilder/>Frank Schilder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2206><div class="card-body p-3 small">Dockets contain a wealth of information for planning a litigation strategy, but the information is locked up in semi-structured text. Manually deriving the outcomes for each party (e.g., settlement, verdict) would be very labor intensive. Having such information available for every past court case, however, would be very useful for developing a <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> because it potentially reveals tendencies and trends of judges and courts and the opposing counsel. We used Natural Language Processing (NLP) techniques and deep learning methods allowing us to scale the automatic analysis of millions of US federal court dockets. The automatically extracted information is fed into a Litigation Analytics tool that is used by lawyers to plan how they approach concrete litigations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2208/>Legal Area Classification : A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments<span class=acl-fixed-case>S</span>ingapore <span class=acl-fixed-case>S</span>upreme <span class=acl-fixed-case>C</span>ourt Judgments</a></strong><br><a href=/people/j/jerrold-soh/>Jerrold Soh</a>
|
<a href=/people/h/how-khang-lim/>How Khang Lim</a>
|
<a href=/people/i/ian-ernst-chai/>Ian Ernst Chai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2208><div class="card-body p-3 small">This paper conducts a comparative study on the performance of various machine learning approaches for classifying judgments into legal areas. Using a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> for the legal domain.</div></div></div><hr><div id=w19-23><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-23.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-23/>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2300/>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></strong><br><a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/u/urvashi-khandelwal/>Urvashi Khandelwal</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2301/>An Adversarial Learning Framework For A Persona-Based Multi-Turn Dialogue Model</a></strong><br><a href=/people/o/oluwatobi-olabiyi/>Oluwatobi Olabiyi</a>
|
<a href=/people/a/anish-khazane/>Anish Khazane</a>
|
<a href=/people/a/alan-salimov/>Alan Salimov</a>
|
<a href=/people/e/erik-mueller/>Erik Mueller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2301><div class="card-body p-3 small">In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator : (1) phredGANa, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGANd, a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona Seq2Seq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of <a href=https://en.wikipedia.org/wiki/Quantitative_research>quantitative measures</a> as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with <a href=https://en.wikipedia.org/wiki/Big_Bang>Big Bang Theory and Friends</a>) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2303/>How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature</a></strong><br><a href=/people/s/simeng-sun/>Simeng Sun</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2303><div class="card-body p-3 small">We show that plain ROUGE F1 scores are not ideal for comparing current neural systems which on average produce different lengths. This is due to a <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linear pattern</a> between ROUGE F1 and summary length. To alleviate the effect of <a href=https://en.wikipedia.org/wiki/Length>length</a> during evaluation, we have proposed a new method which normalizes the ROUGE F1 scores of a <a href=https://en.wikipedia.org/wiki/System>system</a> by that of a random system with same average output length. A pilot human evaluation has shown that humans prefer short summaries in terms of the verbosity of a summary but overall consider longer summaries to be of higher quality. While human evaluations are more expensive in time and resources, it is clear that <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a>, such as the one we proposed for automatic evaluation, will make human evaluations more meaningful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2304" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2304/>BERT has a Mouth, and It Must Speak : BERT as a Markov Random Field Language Model<span class=acl-fixed-case>BERT</span> has a Mouth, and It Must Speak: <span class=acl-fixed-case>BERT</span> as a <span class=acl-fixed-case>M</span>arkov Random Field Language Model</a></strong><br><a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2304><div class="card-body p-3 small">We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from <a href=https://en.wikipedia.org/wiki/Boolean_satisfiability_problem>BERT</a>. We generate from <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and find that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2307/>Bilingual-GAN : A Step Towards Parallel Text Generation<span class=acl-fixed-case>GAN</span>: A Step Towards Parallel Text Generation</a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/a/alan-do-omri/>Alan Do-Omri</a>
|
<a href=/people/m/md-akmal-haidar/>Md. Akmal Haidar</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2307><div class="card-body p-3 small">Latent space based GAN methods and attention based sequence to sequence models have achieved impressive results in text generation and unsupervised machine translation respectively. Leveraging the two domains, we propose an adversarial latent space based model capable of generating parallel sentences in two languages concurrently and translating bidirectionally. The bilingual generation goal is achieved by sampling from the latent space that is shared between both languages. First two denoising autoencoders are trained, with shared encoders and back-translation to enforce a shared latent state between the two languages. The <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is shared for the two translation directions. Next, a GAN is trained to generate synthetic &#8216;code&#8217; mimicking the languages&#8217; shared latent space. This <a href=https://en.wikipedia.org/wiki/Code>code</a> is then fed into the <a href=https://en.wikipedia.org/wiki/Code_generation_(compiler)>decoder</a> to generate text in either language. We perform our experiments on Europarl and Multi30k datasets, on the English-French language pair, and document our performance using both supervised and unsupervised machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2310/>Better Automatic Evaluation of Open-Domain Dialogue Systems with Contextualized Embeddings</a></strong><br><a href=/people/s/sarik-ghazarian/>Sarik Ghazarian</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2310><div class="card-body p-3 small">Despite advances in open-domain dialogue systems, automatic evaluation of such <a href=https://en.wikipedia.org/wiki/System>systems</a> is still a challenging problem. Traditional reference-based metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> are ineffective because there could be many valid responses for a given context that share no common words with reference responses. A recent work proposed Referenced metric and Unreferenced metric Blended Evaluation Routine (RUBER) to combine a learning-based metric, which predicts relatedness between a generated response and a given query, with reference-based metric ; it showed high correlation with human judgments. In this paper, we explore using contextualized word embeddings to compute more accurate <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness scores</a>, thus better evaluation metrics. Experiments show that our evaluation metrics outperform RUBER, which is trained on static embeddings.</div></div></div><hr><div id=w19-24><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-24.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-24/>Proceedings of the First Workshop on Narrative Understanding</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2400/>Proceedings of the First Workshop on Narrative Understanding</a></strong><br><a href=/people/d/david-bamman/>David Bamman</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/m/madalina-fiterau/>Madalina Fiterau</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2404/>Extraction of Message Sequence Charts from Narrative History Text</a></strong><br><a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/h/harsimran-bedi/>Harsimran Bedi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2404><div class="card-body p-3 small">In this paper, we advocate the use of Message Sequence Chart (MSC) as a <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> to capture and visualize multi-actor interactions and their temporal ordering. We propose <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to automatically extract an <a href=https://en.wikipedia.org/wiki/Most_recent_common_ancestor>MSC</a> from a <a href=https://en.wikipedia.org/wiki/Narrative>history narrative</a>. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extraction, we employ a state-of-the art algorithm to temporally re-order these interactions. Our evaluation on multiple publicly available narratives shows improvements over four <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div></div><hr><div id=w19-25><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-25.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-25/>Proceedings of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2500/>Proceedings of the 3rd Joint <span class=acl-fixed-case>SIGHUM</span> Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/b/beatrice-alex/>Beatrice Alex</a>
|
<a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2501/>Modeling Word Emotion in Historical Language : Quantity Beats Supposed Stability in Seed Word Selection</a></strong><br><a href=/people/j/johannes-hellrich/>Johannes Hellrich</a>
|
<a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2501><div class="card-body p-3 small">To understand historical texts, we must be aware that languageincluding the emotional connotation attached to wordschanges over time. In this paper, we aim at estimating the <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> which is associated with a given word in former language stages of <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Emotion is represented following the popular Valence-Arousal-Dominance (VAD) annotation scheme. While being more expressive than <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>polarity</a> alone, existing word emotion induction methods are typically not suited for addressing it. To overcome this limitation, we present adaptations of two popular <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to VAD. To measure their effectiveness in diachronic settings, we present the first <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a> for historical word emotions, which was created by scholars with proficiency in the respective language stages and covers both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. In contrast to claims in previous work, our findings indicate that hand-selecting small sets of seed words with supposedly stable emotional meaning is actually harm- rather than helpful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2504/>Are Fictional Voices Distinguishable? Classifying Character Voices in Modern Drama</a></strong><br><a href=/people/k/krishnapriya-vishnubhotla/>Krishnapriya Vishnubhotla</a>
|
<a href=/people/a/adam-hammond/>Adam Hammond</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2504><div class="card-body p-3 small">According to the <a href=https://en.wikipedia.org/wiki/Literary_theory>literary theory</a> of Mikhail Bakhtin, a dialogic novel is one in which characters speak in their own distinct voices, rather than serving as mouthpieces for their authors. We use <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> to determine which authors best achieve <a href=https://en.wikipedia.org/wiki/Dialogism>dialogism</a>, looking at a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of plays</a> from the late nineteenth and early twentieth centuries. We find that the SAGE model of text generation, which highlights deviations from a background lexical distribution, is an effective method of weighting the words of characters&#8217; utterances. Our results show that it is indeed possible to distinguish characters by their speech in the plays of canonical writers such as George Bernard Shaw, whereas characters are clustered more closely in the works of lesser-known playwrights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2505/>Automatic Alignment and Annotation Projection for Literary Texts</a></strong><br><a href=/people/u/uli-steinbach/>Uli Steinbach</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2505><div class="card-body p-3 small">This paper presents a modular NLP pipeline for the creation of a parallel literature corpus, followed by annotation transfer from the source to the target language. The test case we use to evaluate our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> is the automatic transfer of quote and speaker mention annotations from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We evaluate the different components of the <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> and discuss challenges specific to <a href=https://en.wikipedia.org/wiki/Literature>literary texts</a>. Our experiments show that after applying a reasonable amount of semi-automatic postprocessing we can obtain high-quality aligned and annotated resources for a new language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2506/>Inferring missing metadata from environmental policy texts</a></strong><br><a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/e/egoitz-laparra/>Egoitz Laparra</a>
|
<a href=/people/s/sophia-wang/>Sophia Wang</a>
|
<a href=/people/y/yiyun-zhao/>Yiyun Zhao</a>
|
<a href=/people/r/ragheb-al-ghezi/>Ragheb Al-Ghezi</a>
|
<a href=/people/a/aaron-lien/>Aaron Lien</a>
|
<a href=/people/l/laura-lopez-hoffman/>Laura López-Hoffman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2506><div class="card-body p-3 small">The National Environmental Policy Act (NEPA) provides a trove of data on how environmental policy decisions have been made in the United States over the last 50 years. Unfortunately, there is no central database for this information and it is too voluminous to assess manually. We describe our efforts to enable systematic research over <a href=https://en.wikipedia.org/wiki/Environmental_policy_of_the_United_States>US environmental policy</a> by extracting and organizing <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> from the text of NEPA documents. Our contributions include collecting more than 40,000 NEPA-related documents, and evaluating rule-based baselines that establish the difficulty of three important tasks : identifying lead agencies, aligning document versions, and detecting reused text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2508/>A framework for streamlined statistical prediction using <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a></a></strong><br><a href=/people/v/vanessa-glenny/>Vanessa Glenny</a>
|
<a href=/people/j/jonathan-tuke/>Jonathan Tuke</a>
|
<a href=/people/n/nigel-bean/>Nigel Bean</a>
|
<a href=/people/l/lewis-mitchell/>Lewis Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2508><div class="card-body p-3 small">In the Humanities and Social Sciences, there is increasing interest in approaches to <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>, intelligent linkage, and <a href=https://en.wikipedia.org/wiki/Dimension_reduction>dimension reduction</a> applicable to large text corpora. With approaches in these fields being grounded in traditional <a href=https://en.wikipedia.org/wiki/Statistics>statistical techniques</a>, the need arises for frameworks whereby advanced NLP techniques such as topic modelling may be incorporated within classical methodologies. This paper provides a classical, supervised, statistical learning framework for prediction from text, using topic models as a data reduction method and the topics themselves as predictors, alongside typical statistical tools for predictive modelling. We apply this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> in a Social Sciences context (applied animal behaviour) as well as a Humanities context (narrative analysis) as examples of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>. The results show that topic regression models perform comparably to their much less efficient equivalents that use individual words as predictors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2510/>Graph convolutional networks for exploring authorship hypotheses</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2510><div class="card-body p-3 small">This work considers a task from traditional <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary criticism</a> : annotating a structured, composite document with information about its sources. We take the <a href=https://en.wikipedia.org/wiki/Documentary_hypothesis>Documentary Hypothesis</a>, a prominent theory regarding the composition of the first five books of the <a href=https://en.wikipedia.org/wiki/Hebrew_Bible>Hebrew bible</a>, extract stylistic features designed to avoid bias or overfitting, and train several classification models. Our main result is that the recently-introduced graph convolutional network architecture outperforms structurally-uninformed models. We also find that including information about the granularity of text spans is a crucial ingredient when employing hidden layers, in contrast to simple <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>. We perform error analysis at several levels, noting how some characteristic limitations of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and simple <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> lead to <a href=https://en.wikipedia.org/wiki/Statistical_classification>misclassifications</a>, and conclude with an overview of future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2511/>Semantics and Homothetic Clustering of Hafez Poetry</a></strong><br><a href=/people/a/arya-rahgozar/>Arya Rahgozar</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2511><div class="card-body p-3 small">We have created two sets of labels for Hafez (1315-1390) poems, using <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a>. Our labels are the only semantic clustering alternative to the previously existing, hand-labeled, gold-standard classification of Hafez poems, to be used for <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary research</a>. We have cross-referenced, measured and analyzed the agreements of our clustering labels with Houman&#8217;s chronological classes. Our <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are based on <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We also introduced a similarity of similarities&#8217; features, we called homothetic clustering approach that proved effective, in case of Hafez&#8217;s small corpus of ghazals2. Although all our experiments showed different clusters when compared with Houman&#8217;s classes, we think they were valid in their own right to have provided further insights, and have proved useful as a contrasting alternative to Houman&#8217;s classes. Our homothetic clusterer and its feature design and engineering framework can be used for further semantic analysis of Hafez&#8217;s poetry and other similar literary research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2512/>Computational Linguistics Applications for Multimedia Services</a></strong><br><a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a>
|
<a href=/people/k/kelley-lynch/>Kelley Lynch</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2512><div class="card-body p-3 small">We present Computational Linguistics Applications for Multimedia Services (CLAMS), a platform that provides access to computational content analysis tools for archival multimedia material that appear in different media, such as <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, <a href=https://en.wikipedia.org/wiki/Sound>audio</a>, <a href=https://en.wikipedia.org/wiki/Image>image</a>, and <a href=https://en.wikipedia.org/wiki/Video>video</a>. The primary goal of CLAMS is : (1) to develop an interchange format between multimodal metadata generation tools to ensure interoperability between tools ; (2) to provide users with a portable, user-friendly workflow engine to chain selected tools to extract meaningful analyses ; and (3) to create a public software development kit (SDK) for developers that eases deployment of analysis tools within the CLAMS platform. CLAMS is designed to help archives and libraries enrich the <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> associated with their mass-digitized multimedia collections, that would otherwise be largely unsearchable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2514/>On the Feasibility of Automated Detection of Allusive Text Reuse</a></strong><br><a href=/people/e/enrique-manjavacas/>Enrique Manjavacas</a>
|
<a href=/people/b/brian-long/>Brian Long</a>
|
<a href=/people/m/mike-kestemont/>Mike Kestemont</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2514><div class="card-body p-3 small">The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely commonly based on none or very few shared words. Arguably, <a href=https://en.wikipedia.org/wiki/Lexical_semantics>lexical semantics</a> can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> performance with respect to a windowing approach, and that (ii) <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> performance can be moderately boosted with <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2516/>Sign Clustering and Topic Extraction in Proto-Elamite<span class=acl-fixed-case>P</span>roto-<span class=acl-fixed-case>E</span>lamite</a></strong><br><a href=/people/l/logan-born/>Logan Born</a>
|
<a href=/people/k/kate-kelley/>Kate Kelley</a>
|
<a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/c/carolyn-chen/>Carolyn Chen</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2516><div class="card-body p-3 small">We describe a first attempt at using techniques from <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> to analyze the undeciphered proto-Elamite script. Using <a href=https://en.wikipedia.org/wiki/Hierarchical_clustering>hierarchical clustering</a>, n-gram frequencies, and LDA topic models, we both replicate results obtained by manual decipherment and reveal previously-unobserved relationships between signs. This demonstrates the utility of these techniques as an aid to manual decipherment.</div></div></div><hr><div id=w19-26><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-26.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-26/>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2600/>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></strong><br><a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a>
|
<a href=/people/l/laura-dietz/>Laura Dietz</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2601/>Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention</a></strong><br><a href=/people/q/qin-dai/>Qin Dai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/r/ryo-takahashi/>Ryo Takahashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2601><div class="card-body p-3 small">The increased demand for structured scientific knowledge has attracted considerable attention in extracting <a href=https://en.wikipedia.org/wiki/Scientific_method>scientific relation</a> from the ever growing <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a>. Distant supervision is widely applied approach to automatically generate large amounts of <a href=https://en.wikipedia.org/wiki/Data_type>labelled data</a> with low manual annotation cost. However, distant supervision inevitably accompanies the wrong labelling problem, which will negatively affect the performance of Relation Extraction (RE). To address this issue, (Han et al., 2018) proposes a novel framework for jointly training both RE model and Knowledge Graph Completion (KGC) model to extract structured knowledge from non-scientific dataset. In this work, we firstly investigate the feasibility of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> on <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific dataset</a>, specifically on <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical dataset</a>. Secondly, to achieve better performance on the biomedical dataset, we extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> with other competitive KGC models. Moreover, we proposed a new end-to-end KGC model to extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Experimental results not only show the feasibility of the framework on the biomedical dataset, but also indicate the effectiveness of our extensions, because our extended model achieves significant and consistent improvements on distant supervised RE as compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2603/>Understanding the Polarity of Events in the Biomedical Literature : <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> vs. <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistically-informed Methods</a></a></strong><br><a href=/people/e/enrique-noriega-atala/>Enrique Noriega-Atala</a>
|
<a href=/people/z/zhengzhong-liang/>Zhengzhong Liang</a>
|
<a href=/people/j/john-bachman/>John Bachman</a>
|
<a href=/people/c/clayton-morrison/>Clayton Morrison</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2603><div class="card-body p-3 small">An important task in the machine reading of biochemical events expressed in biomedical texts is correctly reading the <a href=https://en.wikipedia.org/wiki/Chemical_polarity>polarity</a>, i.e., attributing whether the biochemical event is a promotion or an inhibition. Here we present a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for studying polarity attribution accuracy. We use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to train and evaluate several <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for polarity identification, and compare these to a linguistically-informed model. The best performing deep learning architecture achieves 0.968 average F1 performance in a five-fold cross-validation study, a considerable improvement over the linguistically informed model average F1 of 0.862.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2604/>Dataset Mention Extraction and Classification</a></strong><br><a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/c/chenglei-si/>Chenglei Si</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2604><div class="card-body p-3 small">Datasets are integral artifacts of <a href=https://en.wikipedia.org/wiki/Empirical_research>empirical scientific research</a>. However, due to <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>natural language variation</a>, their recognition can be difficult and even when identified, can often be inconsistently referred across and within publications. We report our approach to the Coleridge Initiative&#8217;s Rich Context Competition, which tasks participants with identifying dataset surface forms (dataset mention extraction) and associating the extracted mention to its referred dataset (dataset classification). In this work, we propose various neural baselines and evaluate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on one-plus and zero-shot classification scenarios. We further explore various joint learning approaches-exploring the synergy between the tasks-and report the issues with such techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2605" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2605/>Annotating with Pros and Cons of Technologies in Computer Science Papers</a></strong><br><a href=/people/h/hono-shirai/>Hono Shirai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2605><div class="card-body p-3 small">This paper explores a task for extracting a technological expression and its pros / cons from computer science papers. We report ongoing efforts on an annotated corpus of pros / cons and an analysis of the nature of the automatic extraction task. Specifically, we show how to adapt the targeted sentiment analysis task for pros / cons extraction in computer science papers and conduct an annotation study. In order to identify the challenges of the automatic extraction task, we construct a strong baseline model and conduct an error analysis. The experiments show that pros / cons can be consistently annotated by several annotators, and that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging due to domain-specific knowledge. The annotated dataset is made publicly available for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2607/>An Analysis of Deep Contextual Word Embeddings and Neural Architectures for Toponym Mention Detection in Scientific Publications</a></strong><br><a href=/people/m/matthew-magnusson/>Matthew Magnusson</a>
|
<a href=/people/l/laura-dietz/>Laura Dietz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2607><div class="card-body p-3 small">Toponym detection in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a> is an open task and a key first step in place entity enrichment of documents. We examine three common neural architectures in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> : 1) <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, 2) multi-layer perceptron (both applied in a sliding window context) and 3) bidirectional LSTM and apply contextual and non-contextual word embedding layers to these models. We find that deep contextual word embeddings improve the performance of the bi-LSTM with CRF neural architecture achieving the best performance when multiple layers of deep contextual embeddings are concatenated. Our best performing model achieves an average F1 of 0.910 when evaluated on overlap macro exceeding previous state-of-the-art models in the toponym detection task.</div></div></div><hr><div id=w19-27><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-27.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-27/>Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2700/>Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a>
|
<a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/e/erick-maziero-galani/>Erick Maziero Galani</a>
|
<a href=/people/j/juliano-desiderato-antonio/>Juliano Desiderato Antonio</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2705/>Nuclearity in RST and signals of coherence relations<span class=acl-fixed-case>RST</span> and signals of coherence relations</a></strong><br><a href=/people/d/debopam-das/>Debopam Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2705><div class="card-body p-3 small">We investigate the relationship between the notion of nuclearity as proposed in <a href=https://en.wikipedia.org/wiki/Rhetorical_structure_theory>Rhetorical Structure Theory (RST)</a> and the signalling of coherence relations. RST relations are categorized as either mononuclear (comprising a <a href=https://en.wikipedia.org/wiki/Atomic_nucleus>nucleus</a> and a satellite span) or multinuclear (comprising two or more nuclei spans). We examine how mononuclear relations (e.g., <a href=https://en.wikipedia.org/wiki/Antithesis>Antithesis</a>, Condition) and multinuclear relations (e.g., <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>Contrast</a>, List) are indicated by relational signals, more particularly by <a href=https://en.wikipedia.org/wiki/Discourse_marker>discourse markers</a> (e.g., because, however, if, therefore). We conduct a corpus study, examining the distribution of either type of relations in the RST Discourse Treebank (Carlson et al., 2002) and the distribution of discourse markers for those relations in the RST Signalling Corpus (Das et al., 2015). Our results show that <a href=https://en.wikipedia.org/wiki/Discourse_marker>discourse markers</a> are used more often to signal multinuclear relations than mononuclear relations. The findings also suggest a complex relationship between the relation types and syntactic categories of <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>discourse markers</a> (subordinating and coordinating conjunctions).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2707/>Annotating Shallow Discourse Relations in Twitter Conversations<span class=acl-fixed-case>T</span>witter Conversations</a></strong><br><a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/b/berfin-aktas/>Berfin Aktaş</a>
|
<a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2707><div class="card-body p-3 small">We introduce our pilot study applying PDTB-style annotation to Twitter conversations. Lexically grounded coherence annotation for Twitter threads will enable detailed investigations of the discourse structure of conversations on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Here, we present our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 185 threads and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, including an inter-annotator agreement study. We discuss our observations as to how Twitter discourses differ from written news text wrt. discourse connectives and <a href=https://en.wikipedia.org/wiki/Social_relation>relations</a>. We confirm our hypothesis that <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> in written social media conversations are expressed differently than in (news) text. We find that in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, connective arguments frequently are not full syntactic clauses, and that a few general connectives expressing EXPANSION and CONTINGENCY make up the majority of the explicit relations in our data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2708/>A Discourse Signal Annotation System for RST Trees<span class=acl-fixed-case>RST</span> Trees</a></strong><br><a href=/people/l/luke-gessler/>Luke Gessler</a>
|
<a href=/people/y/yang-liu-georgetown/>Yang Liu</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2708><div class="card-body p-3 small">This paper presents a new system for open-ended discourse relation signal annotation in the framework of Rhetorical Structure Theory (RST), implemented on top of an online tool for RST annotation. We discuss existing projects annotating textual signals of discourse relations, which have so far not allowed simultaneously structuring and annotating words signaling hierarchical discourse trees, and demonstrate the design and applications of our interface by extending existing RST annotations in the freely available GUM corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2709/>EusDisParser : improving an under-resourced discourse parser with cross-lingual data<span class=acl-fixed-case>E</span>us<span class=acl-fixed-case>D</span>is<span class=acl-fixed-case>P</span>arser: improving an under-resourced discourse parser with cross-lingual data</a></strong><br><a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a>
|
<a href=/people/c/chloe-braud/>Chloé Braud</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2709><div class="card-body p-3 small">Development of discourse parsers to annotate the relational discourse structure of a text is crucial for many downstream tasks. However, most of the existing work focuses on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, assuming a quite large dataset. Discourse data have been annotated for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, but training a system on these <a href=https://en.wikipedia.org/wiki/Data>data</a> is challenging since the corpus is very small. In this paper, we create the first demonstrator based on RST for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, and we investigate the use of data in another language to improve the performance of a <a href=https://en.wikipedia.org/wiki/Basque_language>Basque discourse parser</a>. More precisely, we build a monolingual system using the small set of data available and investigate the use of multilingual word embeddings to train a system for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> using data annotated for another language. We found that our approach to building a system limited to the small set of data available for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> allowed us to get an improvement over previous approaches making use of many data annotated in other languages. At best, we get 34.78 in F1 for the full discourse structure. More <a href=https://en.wikipedia.org/wiki/Annotation>data annotation</a> is necessary in order to improve the results obtained with these techniques. We also describe which relations match with the gold standard, in order to understand these results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2711.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2711/>Towards the Data-driven System for Rhetorical Parsing of Russian Texts<span class=acl-fixed-case>R</span>ussian Texts</a></strong><br><a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/d/dina-pisarevskaya/>Dina Pisarevskaya</a>
|
<a href=/people/e/elena-chistova/>Elena Chistova</a>
|
<a href=/people/s/svetlana-toldova/>Svetlana Toldova</a>
|
<a href=/people/m/maria-kobozeva/>Maria Kobozeva</a>
|
<a href=/people/i/ivan-smirnov/>Ivan Smirnov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2711><div class="card-body p-3 small">Results of the first experimental evaluation of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> trained on Ru-RSTreebank first Russian corpus annotated within RST framework are presented. Various lexical, quantitative, morphological, and semantic features were used. In rhetorical relation classification, ensemble of CatBoost model with selected <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and a linear SVM model provides the best score (macro F1 = 54.67 0.38). We discover that most of the important features for rhetorical relation classification are related to discourse connectives derived from the connectives lexicon for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and from other sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2713/>The DISRPT 2019 Shared Task on Elementary Discourse Unit Segmentation and Connective Detection<span class=acl-fixed-case>DISRPT</span> 2019 Shared Task on Elementary Discourse Unit Segmentation and Connective Detection</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a>
|
<a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/e/erick-galani-maziero/>Erick Galani Maziero</a>
|
<a href=/people/j/juliano-antonio/>Juliano Antonio</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2713><div class="card-body p-3 small">In 2019, we organized the first iteration of a shared task dedicated to the underlying units used in discourse parsing across formalisms : the DISRPT Shared Task on Elementary Discourse Unit Segmentation and Connective Detection. In this paper we review the data included in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which cover 2.6 million manually annotated tokens from 15 datasets in 10 languages, survey and compare submitted systems and report on system performance on each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for both annotated and plain-tokenized versions of the <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2716.Software.pdf data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2716/>Multilingual segmentation based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and pre-trained word embeddings</a></strong><br><a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a>
|
<a href=/people/k/kepa-bengoetxea/>Kepa Bengoetxea</a>
|
<a href=/people/a/aitziber-atutxa-salazar/>Aitziber Atutxa Salazar</a>
|
<a href=/people/a/arantza-diaz-de-ilarraza/>Arantza Diaz de Ilarraza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2716><div class="card-body p-3 small">The DISPRT 2019 workshop has organized a shared task aiming to identify cross-formalism and multilingual discourse segments. Elementary Discourse Units (EDUs) are quite similar across different theories. Segmentation is the very first stage on the way of rhetorical annotation. Still, each annotation project adopted several decisions with consequences not only on the annotation of the relational discourse structure but also at the segmentation stage. In this shared task, we have employed pre-trained word embeddings, neural networks (BiLSTM+CRF) to perform the segmentation. We report F1 results for 6 languages : <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> (0.853), <a href=https://en.wikipedia.org/wiki/English_language>English</a> (0.919), <a href=https://en.wikipedia.org/wiki/French_language>French</a> (0.907), <a href=https://en.wikipedia.org/wiki/German_language>German</a> (0.913), <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> (0.926) and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> (0.868 and 0.769). Finally, we also pursued an error analysis based on clause typology for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, in order to understand the performance of the segmenter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2719.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2719 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2719 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2719.Presentation.pptx data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2719/>Using <a href=https://en.wikipedia.org/wiki/Rhetorical_structure_theory>Rhetorical Structure Theory</a> to Assess Discourse Coherence for Non-native Spontaneous Speech<span class=acl-fixed-case>R</span>hetorical <span class=acl-fixed-case>S</span>tructure <span class=acl-fixed-case>T</span>heory to Assess Discourse Coherence for Non-native Spontaneous Speech</a></strong><br><a href=/people/x/xinhao-wang/>Xinhao Wang</a>
|
<a href=/people/b/binod-gyawali/>Binod Gyawali</a>
|
<a href=/people/j/james-v-bruno/>James V. Bruno</a>
|
<a href=/people/h/hillary-r-molloy/>Hillary R. Molloy</a>
|
<a href=/people/k/keelan-evanini/>Keelan Evanini</a>
|
<a href=/people/k/klaus-zechner/>Klaus Zechner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2719><div class="card-body p-3 small">This study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of English speaking proficiency for <a href=https://en.wikipedia.org/wiki/Foreign_language>non-native speakers</a>. Rhetorical Structure Theory (RST) has been commonly used in the analysis of discourse organization of written texts ; however, limited research has been conducted to date on RST annotation and parsing of spoken language, in particular, non-native spontaneous speech. Due to the fact that the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>, we conducted research to obtain RST annotations on non-native spoken responses from a standardized assessment of academic English proficiency. Subsequently, <a href=https://en.wikipedia.org/wiki/Parsing>automatic parsers</a> were trained on these <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to process non-native spontaneous speech. Finally, a set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were extracted from automatically generated RST trees to evaluate the discourse structure of non-native spontaneous speech, which were then employed to further improve the validity of an automated speech scoring system.</div></div></div><hr><div id=w19-28><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-28.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-28/>Proceedings of the Second Workshop on Computational Models of Reference, Anaphora and Coreference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2800/>Proceedings of the Second Workshop on Computational Models of Reference, Anaphora and Coreference</a></strong><br><a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2805/>Cross-lingual Incongruences in the Annotation of Coreference</a></strong><br><a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/p/pauline-krielke/>Pauline Krielke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2805><div class="card-body p-3 small">In the present paper, we deal with incongruences in English-German multilingual coreference annotation and present automated methods to discover them. More specifically, we automatically detect full coreference chains in parallel texts and analyse discrepancies in their annotations. In doing so, we wish to find out whether the discrepancies rather derive from <a href=https://en.wikipedia.org/wiki/Linguistic_typology>language typological constraints</a>, from the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> or the actual <a href=https://en.wikipedia.org/wiki/Annotation>annotation process</a>. The results of our study contribute to the referential analysis of similarities and differences across languages and support evaluation of cross-lingual coreference annotation. They are also useful for cross-lingual coreference resolution systems and <a href=https://en.wikipedia.org/wiki/Contrastive_linguistics>contrastive linguistic studies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2806/>Deep Cross-Lingual Coreference Resolution for Less-Resourced Languages : The Case of <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a><span class=acl-fixed-case>B</span>asque</a></strong><br><a href=/people/g/gorka-urbizu/>Gorka Urbizu</a>
|
<a href=/people/a/ander-soraluze/>Ander Soraluze</a>
|
<a href=/people/o/olatz-arregi/>Olatz Arregi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2806><div class="card-body p-3 small">In this paper, we present a cross-lingual neural coreference resolution system for a less-resourced language such as <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>. To begin with, we build the first neural coreference resolution system for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, training it with the relatively small EPEC-KORREF corpus (45,000 words). Next, a cross-lingual coreference resolution system is designed. With this approach, the system learns from a bigger <a href=https://en.wikipedia.org/wiki/English_language>English corpus</a>, using cross-lingual embeddings, to perform the <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>. The cross-lingual system obtains slightly better results (40.93 F1 CoNLL) than the monolingual system (39.12 F1 CoNLL), without using any Basque language corpus to train it.</div></div></div><hr><div id=w19-29><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-29.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-29/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2900/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra Jacobs</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/l/laurent-prevot/>Laurent Prévot</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2902/>Priming vs. Inhibition of Optional Infinitival to</a></strong><br><a href=/people/r/robin-melnick/>Robin Melnick</a>
|
<a href=/people/t/thomas-wasow/>Thomas Wasow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2902><div class="card-body p-3 small">The word to that precedes verbs in English infinitives is optional in at least two environments : in what Wasow et al. (2015) previously called the do-be construction, and in the complement of help, which we explore in the present work. In the do-be construction, Wasow et al. found that a preceding infinitival to increases the use of following optional to, but the use of to in the complement of help is reduced following to help. We examine two hypotheses regarding why the same <a href=https://en.wikipedia.org/wiki/Function_word>function word</a> is primed by prior use in one construction and inhibited in another. We then test predictions made by the two hypotheses, finding support for one of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2903/>Simulating Spanish-English Code-Switching : El Modelo Est Generating Code-Switches<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>E</span>nglish Code-Switching: El Modelo Está Generating Code-Switches</a></strong><br><a href=/people/c/chara-tsoukala/>Chara Tsoukala</a>
|
<a href=/people/s/stefan-l-frank/>Stefan L. Frank</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a>
|
<a href=/people/j/jorge-valdes-kroff/>Jorge Valdés Kroff</a>
|
<a href=/people/m/mirjam-broersma/>Mirjam Broersma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2903><div class="card-body p-3 small">Multilingual speakers are able to switch from one language to the other (code-switch) between or within sentences. Because the underlying cognitive mechanisms are not well understood, in this study we use computational cognitive modeling to shed light on the process of <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. We employed the Bilingual Dual-path model, a Recurrent Neural Network of bilingual sentence production (Tsoukala et al., 2017), and simulated <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence production</a> in simultaneous Spanish-English bilinguals. Our first goal was to investigate whether the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> would code-switch without being exposed to code-switched training input. The model indeed produced <a href=https://en.wikipedia.org/wiki/Code-switching>code-switches</a> even without any exposure to such <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>input</a> and the patterns of code-switches are in line with earlier linguistic work (Poplack,1980). The second goal of this study was to investigate an auxiliary phrase asymmetry that exists in Spanish-English code-switched production. Using this <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive model</a>, we examined a possible cause for this <a href=https://en.wikipedia.org/wiki/Asymmetry>asymmetry</a>. To our knowledge, this is the first computational cognitive model that aims to simulate code-switched sentence production.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2906/>A Modeling Study of the Effects of <a href=https://en.wikipedia.org/wiki/Surprisal>Surprisal</a> and <a href=https://en.wikipedia.org/wiki/Entropy>Entropy</a> in Perceptual Decision Making of an Adaptive Agent</a></strong><br><a href=/people/p/pyeong-whan-cho/>Pyeong Whan Cho</a>
|
<a href=/people/r/richard-l-lewis/>Richard Lewis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2906><div class="card-body p-3 small">Processing difficulty in online language comprehension has been explained in terms of surprisal and entropy reduction. Although both hypotheses have been supported by experimental data, we do not fully understand their relative contributions on processing difficulty. To develop a better understanding, we propose a mechanistic model of perceptual decision making that interacts with a simulated task environment with temporal dynamics. The proposed model collects noisy bottom-up evidence over multiple timesteps, integrates it with its <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down expectation</a>, and makes perceptual decisions, producing processing time data directly without relying on any linking hypothesis. Temporal dynamics in the task environment was determined by a simple finite-state grammar, which was designed to create the situations where the surprisal and entropy reduction hypotheses predict different patterns. After the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was trained to maximize rewards, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> developed an adaptive policy and both surprisal and entropy effects were observed especially in a <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> reflecting earlier processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2909/>Dependency Parsing with your Eyes : Dependency Structure Predicts Eye Regressions During Reading</a></strong><br><a href=/people/a/alessandro-lopopolo/>Alessandro Lopopolo</a>
|
<a href=/people/s/stefan-l-frank/>Stefan L. Frank</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a>
|
<a href=/people/r/roel-willems/>Roel Willems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2909><div class="card-body p-3 small">Backward saccades during <a href=https://en.wikipedia.org/wiki/Reading>reading</a> have been hypothesized to be involved in structural reanalysis, or to be related to the level of text difficulty. We test the hypothesis that backward saccades are involved in online syntactic analysis. If this is the case we expect that <a href=https://en.wikipedia.org/wiki/Saccade>saccades</a> will coincide, at least partially, with the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> of the relations computed by a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a>. In order to test this, we analyzed a large eye-tracking dataset collected while 102 participants read three short narrative texts. Our results show a relation between backward saccades and the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> of sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2911/>Testing a Minimalist Grammar Parser on Italian Relative Clause Asymmetries<span class=acl-fixed-case>M</span>inimalist <span class=acl-fixed-case>G</span>rammar Parser on <span class=acl-fixed-case>I</span>talian Relative Clause Asymmetries</a></strong><br><a href=/people/a/aniello-de-santo/>Aniello De Santo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2911><div class="card-body p-3 small">Stabler&#8217;s (2013) <a href=https://en.wikipedia.org/wiki/Top-down_parsing>top-down parser</a> for Minimalist grammars has been used to account for off-line processing preferences across a variety of seemingly unrelated phenomena cross-linguistically, via complexity metrics measuring memory burden. This paper extends the empirical coverage of the model by looking at the processing asymmetries of Italian relative clauses, as I discuss the relevance of these constructions in evaluating plausible structure-driven models of processing difficulty.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2914/>The Development of Abstract Concepts in Children’s Early Lexical Networks</a></strong><br><a href=/people/a/abdellah-fourtassi/>Abdellah Fourtassi</a>
|
<a href=/people/i/isaac-scheinfeld/>Isaac Scheinfeld</a>
|
<a href=/people/m/michael-c-frank/>Michael Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2914><div class="card-body p-3 small">How do children learn <a href=https://en.wikipedia.org/wiki/Abstraction>abstract concepts</a> such as animal vs. artifact? Previous research has suggested that such <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> can partly be derived using cues from the language children hear around them. Following this suggestion, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> where we represent the children&#8217; developing lexicon as an evolving network. The <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> of this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> are based on vocabulary knowledge as reported by parents, and the edges between pairs of <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> are based on the probability of their co-occurrence in a corpus of child-directed speech. We found that several <a href=https://en.wikipedia.org/wiki/Category_(mathematics)>abstract categories</a> can be identified as the dense regions in such <a href=https://en.wikipedia.org/wiki/Flow_network>networks</a>. In addition, our simulations suggest that these <a href=https://en.wikipedia.org/wiki/Categorization>categories</a> develop simultaneously, rather than sequentially, thanks to the children&#8217;s word learning trajectory which favors the exploration of the global conceptual space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2915/>Verb-Second Effect on Quantifier Scope Interpretation</a></strong><br><a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2915><div class="card-body p-3 small">Sentences like Every child climbed a tree have at least two interpretations depending on the precedence order of the <a href=https://en.wikipedia.org/wiki/Universal_quantifier>universal quantifier</a> and the indefinite. Previous experimental work explores the role that different <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> such as semantic reanalysis and <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> may have in enabling each interpretation. This paper discusses a web-based task that uses the verb-second characteristic of German main clauses to estimate the influence of word order variation over <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2916" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2916/>Neural Models of the Psychosemantics of ‘Most’</a></strong><br><a href=/people/l/lewis-osullivan/>Lewis O’Sullivan</a>
|
<a href=/people/s/shane-steinert-threlkeld/>Shane Steinert-Threlkeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2916><div class="card-body p-3 small">How are the meanings of linguistic expressions related to their use in concrete <a href=https://en.wikipedia.org/wiki/Cognition>cognitive tasks</a>? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain <a href=https://en.wikipedia.org/wiki/Quantifier_(linguistics)>quantifiers</a>. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) on the most verification task from Pietroski2009, manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive models</a> of this and other psychosemantic tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2917/>The Role of Utterance Boundaries and Word Frequencies for Part-of-speech Learning in <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> Through Distributional Analysis<span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese Through Distributional Analysis</a></strong><br><a href=/people/p/pablo-picasso-feliciano-de-faria/>Pablo Picasso Feliciano de Faria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2917><div class="card-body p-3 small">In this study, we address the problem of part-of-speech (or syntactic category) learning during <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> through distributional analysis of utterances. A <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on Redington et al.&#8217;s (1998) distributional learner is used to investigate the informativeness of distributional information in Brazilian Portuguese (BP). The data provided to the learner comes from two publicly available corpora of child directed speech. We present preliminary results from two experiments. The first one investigates the effects of different assumptions about utterance boundaries when presenting the input data to the <a href=https://en.wikipedia.org/wiki/Learning>learner</a>. The second experiment compares the learner&#8217;s performance when counting contextual words&#8217; frequencies versus just acknowledging their co-occurrence with a given target word. In general, our results indicate that explicit boundaries are more informative, <a href=https://en.wikipedia.org/wiki/Frequency>frequencies</a> are important, and that distributional information is useful to the child as a source of categorial information. These results are in accordance with Redington et al.&#8217;s findings for <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2918/>Using Grounded Word Representations to Study Theories of Lexical Concepts</a></strong><br><a href=/people/d/dylan-ebert/>Dylan Ebert</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2918><div class="card-body p-3 small">The fields of <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a> and <a href=https://en.wikipedia.org/wiki/Philosophy>philosophy</a> have proposed many different <a href=https://en.wikipedia.org/wiki/Theory>theories</a> for how humans represent concepts. Multiple such <a href=https://en.wikipedia.org/wiki/Theory>theories</a> are compatible with state-of-the-art NLP methods, and could in principle be operationalized using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We focus on two particularly prominent theoriesClassical Theory and Prototype Theoryin the context of visually-grounded lexical representations. We compare when and how the behavior of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> based on these theories differs in terms of <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> and <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment tasks</a>. Our preliminary results suggest that Classical-based representations perform better for <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a> and Prototype-based representations perform better for <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a>. We discuss plans for additional experiments needed to confirm these initial observations.</div></div></div><hr><div id=w19-30><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-30.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-30/>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3000/>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</a></strong><br><a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/k/kristy-hollingshead/>Kristy Hollingshead</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a>
|
<a href=/people/r/rebecca-resnik/>Rebecca Resnik</a>
|
<a href=/people/k/kate-loveys/>Kate Loveys</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3002/>Identifying therapist conversational actions across diverse <a href=https://en.wikipedia.org/wiki/Psychotherapy>psychotherapeutic approaches</a></a></strong><br><a href=/people/f/fei-tzin-lee/>Fei-Tzin Lee</a>
|
<a href=/people/d/derrick-hull/>Derrick Hull</a>
|
<a href=/people/j/jacob-levine/>Jacob Levine</a>
|
<a href=/people/b/bonnie-ray/>Bonnie Ray</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3002><div class="card-body p-3 small">While conversation in therapy sessions can vary widely in both topic and style, an understanding of the underlying techniques used by therapists can provide valuable insights into how therapists best help clients of different types. Dialogue act classification aims to identify the conversational action each speaker takes at each utterance, such as <a href=https://en.wikipedia.org/wiki/Sympathy>sympathizing</a>, <a href=https://en.wikipedia.org/wiki/Problem_solving>problem-solving</a> or assumption checking. We propose to apply dialogue act classification to therapy transcripts, using a therapy-specific labeling scheme, in order to gain a high-level understanding of the flow of conversation in therapy sessions. We present a novel annotation scheme that spans multiple <a href=https://en.wikipedia.org/wiki/Psychotherapy>psychotherapeutic approaches</a>, apply it to a large and diverse corpus of psychotherapy transcripts, and present and discuss <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results obtained using both SVM and neural network-based models. The results indicate that identifying the structure and flow of therapeutic actions is an obtainable goal, opening up the opportunity in the future to provide therapeutic recommendations tailored to specific client situations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3004/>CLaC at CLPsych 2019 : Fusion of Neural Features and Predicted Class Probabilities for Suicide Risk Assessment Based on Online Posts<span class=acl-fixed-case>CL</span>a<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>CLP</span>sych 2019: Fusion of Neural Features and Predicted Class Probabilities for Suicide Risk Assessment Based on Online Posts</a></strong><br><a href=/people/e/elham-mohammadi/>Elham Mohammadi</a>
|
<a href=/people/h/hessam-amini/>Hessam Amini</a>
|
<a href=/people/l/leila-kosseim/>Leila Kosseim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3004><div class="card-body p-3 small">This paper summarizes our participation to the CLPsych 2019 shared task, under the name CLaC. The goal of the shared task was to detect and assess suicide risk based on a collection of online posts. For our participation, we used an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> which utilizes 8 neural sub-models to extract neural features and predict class probabilities, which are then used by an SVM classifier. Our team ranked first in 2 out of the 3 <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (tasks A and C).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3005/>Suicide Risk Assessment with Multi-level Dual-Context Language and BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/m/matthew-matero/>Matthew Matero</a>
|
<a href=/people/a/akash-idnani/>Akash Idnani</a>
|
<a href=/people/y/youngseo-son/>Youngseo Son</a>
|
<a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a>
|
<a href=/people/h/huy-vu/>Huy Vu</a>
|
<a href=/people/m/mohammad-zamani/>Mohammad Zamani</a>
|
<a href=/people/p/parth-limbachiya/>Parth Limbachiya</a>
|
<a href=/people/s/sharath-chandra-guntuku/>Sharath Chandra Guntuku</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3005><div class="card-body p-3 small">Mental health predictive systems typically model language as if from a single context (e.g. Twitter posts, <a href=https://en.wikipedia.org/wiki/Twitter>status updates</a>, or forum posts) and often limited to a single level of analysis (e.g. either the message-level or user-level). Here, we bring these pieces together to explore the use of open-vocabulary (BERT embeddings, topics) and theoretical features (emotional expression lexica, personality) for the task of suicide risk assessment on support forums (the CLPsych-2019 Shared Task). We used dual context based approaches (modeling content from suicide forums separate from other content), built over both traditional ML models as well as a novel dual RNN architecture with user-factor adaptation. We find that while <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> from the suicide context distinguishes with no-risk from those with any-risk, personality factors from the non-suicide contexts provide distinction of the levels of risk : low, medium, and high risk. Within the shared task, our dual-context approach (listed as SBU-HLAB in the official results) achieved state-of-the-art performance predicting suicide risk using a combination of suicide-context and non-suicide posts (Task B), achieving an F1 score of 0.50 over hidden test set labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3006/>Using natural conversations to classify autism with limited data : Age matters</a></strong><br><a href=/people/m/michael-hauser/>Michael Hauser</a>
|
<a href=/people/e/evangelos-sariyanidi/>Evangelos Sariyanidi</a>
|
<a href=/people/b/birkan-tunc/>Birkan Tunc</a>
|
<a href=/people/c/casey-zampella/>Casey Zampella</a>
|
<a href=/people/e/edward-brodkin/>Edward Brodkin</a>
|
<a href=/people/r/robert-t-schultz/>Robert Schultz</a>
|
<a href=/people/j/julia-parish-morris/>Julia Parish-Morris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3006><div class="card-body p-3 small">Spoken language ability is highly heterogeneous in Autism Spectrum Disorder (ASD), which complicates efforts to identify linguistic markers for use in diagnostic classification, clinical characterization, and for research and clinical outcome measurement. Machine learning techniques that harness the power of <a href=https://en.wikipedia.org/wiki/Multivariate_statistics>multivariate statistics</a> and non-linear data analysis hold promise for modeling this <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>heterogeneity</a>, but many models require enormous datasets, which are unavailable for most psychiatric conditions (including ASD). In lieu of such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, good <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can still be built by leveraging <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. In this study, we compare two machine learning approaches : the first approach incorporates prior knowledge about language variation across middle childhood, adolescence, and adulthood to classify 6-minute naturalistic conversation samples from 140 age- and IQ-matched participants (81 with ASD), while the other approach treats all ages the same. We found that individual age-informed models were significantly more accurate than a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> tasked with building a common <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> across age groups. Furthermore, predictive linguistic features differed significantly by age group, confirming the importance of considering age-related changes in language use when classifying ASD. Our results suggest that limitations imposed by <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>heterogeneity</a> inherent to ASD and from developmental change with age can be (at least partially) overcome using <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>, such as understanding spoken language development from childhood through adulthood.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3007/>The importance of sharing patient-generated clinical speech and language data</a></strong><br><a href=/people/k/kathleen-c-fraser/>Kathleen C. Fraser</a>
|
<a href=/people/n/nicklas-linz/>Nicklas Linz</a>
|
<a href=/people/h/hali-lindsay/>Hali Lindsay</a>
|
<a href=/people/a/alexandra-konig/>Alexandra König</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3007><div class="card-body p-3 small">Increased access to large datasets has driven progress in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. However, most computational studies of clinically-validated, patient-generated speech and language involve very few datapoints, as such <a href=https://en.wikipedia.org/wiki/Data>data</a> are difficult (and expensive) to collect. In this position paper, we argue that we must find ways to promote <a href=https://en.wikipedia.org/wiki/Data_sharing>data sharing</a> across research groups, in order to build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of a more appropriate size for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning analysis</a>. We review the benefits and challenges of sharing clinical language data, and suggest several concrete actions by both clinical and NLP researchers to encourage multi-site and multi-disciplinary data sharing. We also propose the creation of a collaborative data sharing platform, to allow NLP researchers to take a more active responsibility for <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>data transcription</a>, <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, and <a href=https://en.wikipedia.org/wiki/Data_curation>curation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3008/>Depressed Individuals Use Negative Self-Focused Language When Recalling Recent Interactions with Close Romantic Partners but Not Family or Friends<span class=acl-fixed-case>F</span>riends</a></strong><br><a href=/people/t/taleen-nalabandian/>Taleen Nalabandian</a>
|
<a href=/people/m/molly-ireland/>Molly Ireland</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3008><div class="card-body p-3 small">Depression is characterized by a self-focused negative attentional bias, which is often reflected in everyday language use. In a prospective writing study, we explored whether the association between <a href=https://en.wikipedia.org/wiki/Major_depressive_disorder>depressive symptoms</a> and negative, self-focused language varies across social contexts. College students (N = 243) wrote about a recent interaction with a person they care deeply about. Depression symptoms positively correlated with negative emotion words and first-person singular pronouns (or negative self-focus) when writing about a recent interaction with romantic partners or, to a lesser extent, friends, but not family members. The pattern of results was more pronounced when participants perceived greater self-other overlap (i.e., interpersonal closeness) with their romantic partner. Findings regarding how the linguistic profile of <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a> differs by type of relationship may inform more effective methods of <a href=https://en.wikipedia.org/wiki/Medical_diagnosis>clinical diagnosis</a> and <a href=https://en.wikipedia.org/wiki/Therapy>treatment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3010/>Semantic Characteristics of Schizophrenic Speech</a></strong><br><a href=/people/k/kfir-bar/>Kfir Bar</a>
|
<a href=/people/v/vered-zilberstein/>Vered Zilberstein</a>
|
<a href=/people/i/ido-ziv/>Ido Ziv</a>
|
<a href=/people/h/heli-baram/>Heli Baram</a>
|
<a href=/people/n/nachum-dershowitz/>Nachum Dershowitz</a>
|
<a href=/people/s/samuel-itzikowitz/>Samuel Itzikowitz</a>
|
<a href=/people/e/eiran-vadim-harel/>Eiran Vadim Harel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3010><div class="card-body p-3 small">Natural language processing tools are used to automatically detect disturbances in <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed speech</a> of schizophrenia inpatients who speak <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>. We measure topic mutation over time and show that controls maintain more <a href=https://en.wikipedia.org/wiki/Cohesion_(linguistics)>cohesive speech</a> than inpatients. We also examine differences in how inpatients and controls use <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a> to describe <a href=https://en.wikipedia.org/wiki/Content_word>content words</a> and show that the ones used by controls are more common than the those of inpatients. We provide experimental results and show their potential for automatically detecting schizophrenia in patients by means only of their speech patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3013/>Mental Health Surveillance over <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> with Digital Cohorts</a></strong><br><a href=/people/s/silvio-amir/>Silvio Amir</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/j/john-w-ayers/>John W. Ayers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3013><div class="card-body p-3 small">The ability to track <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a> via <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> opened the doors for large-scale, automated, mental health surveillance. However, inferring accurate population-level trends requires representative samples of the underlying population, which can be challenging given the biases inherent in social media data. While previous work has adjusted samples based on <a href=https://en.wikipedia.org/wiki/Demography>demographic estimates</a>, the populations were selected based on specific outcomes, e.g. specific <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a>. We depart from these methods, by conducting analyses over demographically representative digital cohorts of social media users. To validated this approach, we constructed a cohort of US based Twitter users to measure the prevalence of depression and <a href=https://en.wikipedia.org/wiki/Posttraumatic_stress_disorder>PTSD</a>, and investigate how these illnesses manifest across demographic subpopulations. The analysis demonstrates that <a href=https://en.wikipedia.org/wiki/Cohort_study>cohort-based studies</a> can help control for <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling biases</a>, contextualize outcomes, and provide deeper insights into the data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3017/>Analyzing the use of existing <a href=https://en.wikipedia.org/wiki/System>systems</a> for the CLPsych 2019 Shared Task<span class=acl-fixed-case>CLP</span>sych 2019 Shared Task</a></strong><br><a href=/people/a/alejandro-gonzalez-hevia/>Alejandro González Hevia</a>
|
<a href=/people/r/rebeca-cerezo-menendez/>Rebeca Cerezo Menéndez</a>
|
<a href=/people/d/daniel-gayo-avello/>Daniel Gayo-Avello</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3017><div class="card-body p-3 small">In this paper we describe the UniOvi-WESO classification systems proposed for the 2019 Computational Linguistics and Clinical Psychology (CLPsych) Shared Task. We explore the use of two systems trained with ReachOut data from the 2016 CLPsych task, and compare them to a baseline system trained with the data provided for this task. All the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> were trained with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted just from the text of each post, without using any other <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. We found out that the baseline system performs slightly better than the pretrained systems, mainly due to the differences in labeling between the two tasks. However, they still work reasonably well and can detect if a user is at risk of suicide or not.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3018/>Similar Minds Post Alike : Assessment of Suicide Risk Using a Hybrid Model</a></strong><br><a href=/people/l/lushi-chen/>Lushi Chen</a>
|
<a href=/people/a/abeer-aldayel/>Abeer Aldayel</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/t/tao-gong/>Tao Gong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3018><div class="card-body p-3 small">This paper describes our system submission for the CLPsych 2019 shared task B on <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>. We approached the problem with three separate <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> : a behaviour model ; a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and a hybrid model. For the behavioral model approach, we model each user&#8217;s behaviour and thoughts with four groups of features : posting behaviour, <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Motivation>motivation</a>, and content of the user&#8217;s posting. We use these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> as an input in a support vector machine (SVM). For the <a href=https://en.wikipedia.org/wiki/Language_model>language model approach</a>, we trained a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> for each risk level using all the posts from the users as the training corpora. Then, we computed the perplexity of each user&#8217;s posts to determine how likely his / her posts were to belong to each risk level. Finally, we built a hybrid model that combines both the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and the <a href=https://en.wikipedia.org/wiki/Behavioral_model>behavioral model</a>, which demonstrates the best performance in detecting the suicide risk level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3021/>Suicide Risk Assessment on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : USI-UPF at the CLPsych 2019 Shared Task<span class=acl-fixed-case>USI</span>-<span class=acl-fixed-case>UPF</span> at the <span class=acl-fixed-case>CLP</span>sych 2019 Shared Task</a></strong><br><a href=/people/e/esteban-rissola/>Esteban Ríssola</a>
|
<a href=/people/d/diana-ramirez-cifuentes/>Diana Ramírez-Cifuentes</a>
|
<a href=/people/a/ana-freire/>Ana Freire</a>
|
<a href=/people/f/fabio-crestani/>Fabio Crestani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3021><div class="card-body p-3 small">This paper describes the participation of the USI-UPF team at the shared task of the 2019 Computational Linguistics and Clinical Psychology Workshop (CLPsych2019). The goal is to assess the degree of suicide risk of social media users given a labelled dataset with their posts. An appropriate <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>, with the usage of automated methods, can assist experts on the detection of people at risk and eventually contribute to prevent suicide. We propose a set of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> based on <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, word level n-grams, and statistics extracted from users&#8217; posts. The results show that the most effective <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> for the tasks are obtained integrating lexicon-based features, a selected set of <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a>, and <a href=https://en.wikipedia.org/wiki/Statistics>statistical measures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3023/>An Investigation of <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning Systems</a> for Suicide Risk Assessment</a></strong><br><a href=/people/m/michelle-morales/>Michelle Morales</a>
|
<a href=/people/p/prajjalita-dey/>Prajjalita Dey</a>
|
<a href=/people/t/thomas-theisen/>Thomas Theisen</a>
|
<a href=/people/d/danny-belitz/>Danny Belitz</a>
|
<a href=/people/n/natalia-chernova/>Natalia Chernova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3023><div class="card-body p-3 small">This work presents the <a href=https://en.wikipedia.org/wiki/System>systems</a> explored as part of the CLPsych 2019 Shared Task. More specifically, this work explores the promise of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning systems</a> for <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>