<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Noisy User-generated Text (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Noisy User-generated Text (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#d19-55>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a>
<span class="badge badge-info align-middle ml-1">30&nbsp;papers</span></li></ul></div></div><div id=d19-55><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-55.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-55/>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5500/>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5502 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5502.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5502/>Formality Style Transfer for Noisy, User-generated Conversations : Extracting Labeled, Parallel Data from Unlabeled Corpora</a></strong><br><a href=/people/i/isak-czeresnia-etinger/>Isak Czeresnia Etinger</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5502><div class="card-body p-3 small">Typical <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> used for style transfer in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> contain aligned pairs of two opposite extremes of a style. As each existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is sourced from a specific domain and context, most use cases will have a sizable mismatch from the vocabulary and sentence structures of any <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> available. This reduces the performance of the style transfer, and is particularly significant for noisy, user-generated text. To solve this problem, we show a technique to derive a dataset of aligned pairs (style-agnostic vs stylistic sentences) from an unlabeled corpus by using an auxiliary dataset, allowing for in-domain training. We test the technique with the Yahoo Formality Dataset and 6 novel datasets we produced, which consist of scripts from 5 popular TV-shows (Friends, Futurama, Seinfeld, Southpark, Stargate SG-1) and the Slate Star Codex online forum. We gather 1080 human evaluations, which show that our method produces a sizable change in formality while maintaining fluency and context ; and that it considerably outperforms OpenNMT&#8217;s Seq2Seq model directly trained on the Yahoo Formality Dataset. Additionally, we publish the full pipeline code and our novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5503 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5503/>Multilingual Whispers : Generating Paraphrases with Translation</a></strong><br><a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/o/oussama-elachqar/>Oussama Elachqar</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5503><div class="card-body p-3 small">Naturally occurring paraphrase data, such as multiple <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news stories</a> about the same event, is a useful but rare resource. This paper compares translation-based paraphrase gathering using human, automatic, or hybrid techniques to monolingual paraphrasing by experts and non-experts. We gather <a href=https://en.wikipedia.org/wiki/Translation>translations</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>, and empirical human quality assessments of these approaches. Neural machine translation techniques, especially when pivoting through related languages, provide a relatively robust source of <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with diversity comparable to expert human paraphrases. Surprisingly, human translators do not reliably outperform <a href=https://en.wikipedia.org/wiki/Nervous_system>neural systems</a>. The resulting data release will not only be a useful test set, but will also allow additional explorations in translation and paraphrase quality assessments and relationships.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5504 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5504.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5504/>Personalizing Grammatical Error Correction : Adaptation to Proficiency Level and L1<span class=acl-fixed-case>L</span>1</a></strong><br><a href=/people/m/maria-nadejde/>Maria Nadejde</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5504><div class="card-body p-3 small">Grammar error correction (GEC) systems have become ubiquitous in a variety of <a href=https://en.wikipedia.org/wiki/Application_software>software applications</a>, and have started to approach human-level performance for some datasets. However, very little is known about how to efficiently personalize these <a href=https://en.wikipedia.org/wiki/System>systems</a> to the user&#8217;s characteristics, such as their proficiency level and first language, or to emerging domains of text. We present the first results on adapting a general purpose neural GEC system to both the proficiency level and the first language of a writer, using only a few thousand annotated sentences. Our study is the broadest of its kind, covering five proficiency levels and twelve different languages, and comparing three different adaptation scenarios : adapting to the proficiency level only, to the first language only, or to both aspects simultaneously. We show that tailoring to both scenarios achieves the largest performance improvement (3.6 F0.5) relative to a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5506/>Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation</a></strong><br><a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5506><div class="card-body p-3 small">Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> to these variations, without diminishing performance on clean text. We focus on <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance on natural typos, and show that robustness to such <a href=https://en.wikipedia.org/wiki/Noise>noise</a> can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5508 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5508/>Tkol, Httt, and r / radiohead : High Affinity Terms in Reddit Communities<span class=acl-fixed-case>R</span>eddit Communities</a></strong><br><a href=/people/a/abhinav-bhandari/>Abhinav Bhandari</a>
|
<a href=/people/c/caitrin-armstrong/>Caitrin Armstrong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5508><div class="card-body p-3 small">Language is an important marker of a <a href=https://en.wikipedia.org/wiki/Cultural_group>cultural group</a>, large or small. One aspect of language variation between communities is the employment of highly specialized terms with unique significance to the group. We study these high affinity terms across a wide variety of communities by leveraging the rich diversity of <a href=https://en.wikipedia.org/wiki/Reddit>Reddit.com</a>. We provide a systematic exploration of high affinity terms, the often rapid semantic shifts they undergo, and their relationship to subreddit characteristics across 2600 diverse subreddits. Our results show that high affinity terms are effective signals of loyal communities, they undergo more semantic shift than low affinity terms, and that they are partial barrier to entry for new users. We conclude that <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> is a robust and valuable data source for testing further theories about high affinity terms across communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5511 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5511/>Predicting Algorithm Classes for Programming Word Problems</a></strong><br><a href=/people/v/vinayak-athavale/>Vinayak Athavale</a>
|
<a href=/people/a/aayush-naik/>Aayush Naik</a>
|
<a href=/people/r/rajas-vanjape/>Rajas Vanjape</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5511><div class="card-body p-3 small">We introduce the task of algorithm class prediction for programming word problems. A programming word problem is a problem written in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, which can be solved using an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> or a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a>. We define <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>classes</a> of various programming word problems which correspond to the class of algorithms required to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We present four new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task, two multiclass datasets with 550 and 1159 problems each and two multilabel datasets having 3737 and 3960 problems each. We pose the problem as a text classification problem and train <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and non-neural network based models on this task. Our best performing <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> gets an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 62.7 percent for the multiclass case on the five class classification dataset, Codeforces Multiclass-5 (CFMC5). We also do some human-level analysis and compare human performance with that of our text classification models. Our best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> has an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> only 9 percent lower than that of a human on this task. To the best of our knowledge, these are the first reported results on such a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We make our code and datasets publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5512 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5512.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5512/>Automatic identification of writers’ intentions : Comparing different methods for predicting relationship goals in online dating profile texts</a></strong><br><a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/t/tess-van-der-zanden/>Tess van der Zanden</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/m/maria-mos/>Maria Mos</a>
|
<a href=/people/a/alexander-schouten/>Alexander Schouten</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5512><div class="card-body p-3 small">Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and predictive ability of LIWC on a relationship goal classification task. In this paper, we compared the outcomes of (1) LIWC, (2) <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, and (3) a human baseline. A newly collected corpus of online dating profile texts (a genre not explored before in the ACL anthology) was used, accompanied by the profile writers&#8217; self-selected relationship goal (long-term versus date). These three approaches were tested by comparing their performance on identifying both the intended relationship goal and content-related text labels. Results show that LIWC and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> correlate with <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluations</a> in terms of content-related labels. LIWC&#8217;s content-related labels corresponded more strongly to humans than those of the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. Moreover, all <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> were similarly accurate in predicting the <a href=https://en.wikipedia.org/wiki/Goal>relationship goal</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5513/>Contextualized Word Representations from Distant Supervision with and for NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5513><div class="card-body p-3 small">We describe a special type of deep contextualized word representation that is learned from distant supervision annotations and dedicated to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our extensive experiments on 7 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> show systematic gains across all domains over strong baselines, and demonstrate that our representation is complementary to previously proposed <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. We report new state-of-the-art results on CONLL and ONTONOTES datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5515 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5515.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5515/>An In-depth Analysis of the Effect of Lexical Normalization on the Dependency Parsing of <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5515><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a> have often been designed with standard texts in mind. However, when these <a href=https://en.wikipedia.org/wiki/Tool>tools</a> are used on the substantially different texts from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, their performance drops dramatically. One solution is to translate social media data to standard language before processing, this is also called normalization. It is well-known that this improves performance for many natural language processing tasks on social media data. However, little is known about which types of <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization replacements</a> have the most effect. Furthermore, it is unknown what the weaknesses of existing lexical normalization systems are in an extrinsic setting. In this paper, we analyze the effect of manual as well as automatic lexical normalization for dependency parsing. After our analysis, we conclude that for most categories, automatic normalization scores close to manually annotated normalization and that small annotation differences are important to take into consideration when exploiting <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> in a pipeline setup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5518 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5518/>Normalising Non-standardised Orthography in Algerian Code-switched User-generated Data<span class=acl-fixed-case>A</span>lgerian Code-switched User-generated Data</a></strong><br><a href=/people/w/wafia-adouane/>Wafia Adouane</a>
|
<a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/s/simon-dobnik/>Simon Dobnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5518><div class="card-body p-3 small">We work with <a href=https://en.wikipedia.org/wiki/Algerian_language>Algerian</a>, an under-resourced non-standardised Arabic variety, for which we compile a new parallel corpus consisting of user-generated textual data matched with normalised and corrected human annotations following data-driven and our linguistically motivated standard. We use an end-to-end deep neural model designed to deal with context-dependent spelling correction and <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalisation</a>. Results indicate that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with two CNN sub-network encoders and an LSTM decoder performs the best, and that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>word context</a> matters. Additionally, pre-processing data token-by-token with an edit-distance based aligner significantly improves the performance. We get promising results for the spelling correction and normalisation, as a pre-processing step for downstream tasks, on detecting binary Semantic Textual Similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5519 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5519" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5519/>Dialect Text Normalization to Normative Standard Finnish<span class=acl-fixed-case>F</span>innish</a></strong><br><a href=/people/n/niko-partanen/>Niko Partanen</a>
|
<a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5519><div class="card-body p-3 small">We compare different LSTMs and transformer models in terms of their effectiveness in normalizing <a href=https://en.wikipedia.org/wiki/Finnish_dialects>dialectal Finnish</a> into the normative standard Finnish. As <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> is the common way of communication for people online in Finnish, such a normalization is a necessary step to improve the accuracy of the existing Finnish NLP tools that are tailored for normative Finnish text. We work on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of <a href=https://en.wikipedia.org/wiki/Dialect_continuum>dialectal data</a> of 23 distinct <a href=https://en.wikipedia.org/wiki/Finnish_dialects>Finnish dialects</a>. The best functioning BRNN approach lowers the initial <a href=https://en.wikipedia.org/wiki/Word_error_rate>word error rate</a> of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> from 52.89 to 5.73.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5521 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5521" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5521/>Exploring Multilingual Syntactic Sentence Representations</a></strong><br><a href=/people/c/chen-liu/>Chen Liu</a>
|
<a href=/people/a/anderson-de-andrade/>Anderson De Andrade</a>
|
<a href=/people/m/muhammad-osama/>Muhammad Osama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5521><div class="card-body p-3 small">We study <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for learning <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> with <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. We focus on methods of learning syntactic sentence-embeddings by using a multilingual parallel-corpus augmented by Universal Parts-of-Speech tags. We evaluate the quality of the learned embeddings by examining sentence-level nearest neighbours and functional dissimilarity in the <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a>. We also evaluate the ability of the method to learn syntactic sentence-embeddings for low-resource languages and demonstrate strong evidence for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. Our results show that syntactic sentence-embeddings can be learned while using less training data, fewer model parameters, and resulting in better evaluation metrics than state-of-the-art language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5523 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5523/>Latent semantic network induction in the context of linked example senses</a></strong><br><a href=/people/h/hunter-heidenreich/>Hunter Heidenreich</a>
|
<a href=/people/j/jake-williams/>Jake Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5523><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Princeton_WordNet>Princeton WordNet</a> is a powerful tool for studying <a href=https://en.wikipedia.org/wiki/Language>language</a> and developing natural language processing algorithms. With significant work developing it further, one line considers its extension through aligning its expert-annotated structure with other lexical resources. In contrast, this work explores a completely data-driven approach to network construction, forming a <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a> using the entirety of the open-source, noisy, user-annotated dictionary, <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>. Comparing baselines to <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, we find compelling evidence that our <a href=https://en.wikipedia.org/wiki/Social_network>network induction process</a> constructs a <a href=https://en.wikipedia.org/wiki/Social_network>network</a> with useful semantic structure. With thousands of semantically-linked examples that demonstrate sense usage from basic lemmas to multiword expressions (MWEs), we believe this work motivates future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5525 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5525/>Modelling Uncertainty in Collaborative Document Quality Assessment</a></strong><br><a href=/people/a/aili-shen/>Aili Shen</a>
|
<a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/b/bahar-salehi/>Bahar Salehi</a>
|
<a href=/people/j/jianzhong-qi/>Jianzhong Qi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5525><div class="card-body p-3 small">In the context of document quality assessment, previous work has mainly focused on predicting the quality of a document relative to a putative gold standard, without paying attention to the subjectivity of this task. To imitate people&#8217;s disagreement over inherently subjective tasks such as rating the quality of a Wikipedia article, a document quality assessment system should provide not only a prediction of the article quality but also the uncertainty over its predictions. This motivates us to measure the uncertainty in document quality predictions, in addition to making the label prediction. Experimental results show that both Gaussian processes (GPs) and random forests (RFs) can yield competitive results in predicting the quality of Wikipedia articles, while providing an estimate of uncertainty when there is inconsistency in the quality labels from the Wikipedia contributors. We additionally evaluate our methods in the context of a semi-automated document quality class assignment decision-making process, where there is asymmetric risk associated with overestimates and underestimates of document quality. Our experiments suggest that GPs provide more reliable estimates in this context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5526 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5526/>Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge Extraction from Patient-Generated Texts</a></strong><br><a href=/people/a/anja-belz/>Anja Belz</a>
|
<a href=/people/r/richard-hoile/>Richard Hoile</a>
|
<a href=/people/e/elizabeth-ford/>Elizabeth Ford</a>
|
<a href=/people/a/azam-mullick/>Azam Mullick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5526><div class="card-body p-3 small">Approaches to knowledge extraction (KE) in the health domain often start by annotating text to indicate the knowledge to be extracted, and then use the annotated text to train systems to perform the KE. This may work for annotat- ing named entities or other contiguous noun phrases (drugs, some drug effects), but be- comes increasingly difficult when items tend to be expressed across multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions of drug effects in user-generated text). Other issues include that it is not al- ways clear how <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> map to actionable insights, or how they scale up to, or can form part of, more complex KE tasks. This paper reports our efforts in developing an approach to extracting knowledge about drug nonadher- ence from health forums which led us to con- clude that development can not proceed in sep- arate steps but that all aspectsfrom concep- tualisation to annotation scheme development, annotation, KE system training and knowl- edge graph instantiationare interdependent and need to be co-developed. Our aim in this paper is two-fold : we describe a generally ap- plicable framework for developing a KE ap- proach, and present a specific KE approach, developed with the framework, for the task of gathering information about antidepressant drug nonadherence. We report the conceptual- isation, the annotation scheme, the annotated corpus, and an analysis of annotated texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5527 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5527/>What A Sunny Day : Toward Emoji-Sensitive Irony Detection</a></strong><br><a href=/people/s/shirley-anugrah-hayati/>Shirley Anugrah Hayati</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/n/naoki-otani/>Naoki Otani</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5527><div class="card-body p-3 small">Irony detection is an important task with applications in identification of online abuse and <a href=https://en.wikipedia.org/wiki/Harassment>harassment</a>. With the ubiquitous use of <a href=https://en.wikipedia.org/wiki/Nonverbal_communication>non-verbal cues</a> such as <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, in this work we aim to study the role of these structures in irony detection. Since the existing irony detection datasets have 10 % ironic tweets with <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a>, <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on them are insensitive to <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. We propose an <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>automated pipeline</a> for creating a more balanced dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5529 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5529/>Dense Node Representation for <a href=https://en.wikipedia.org/wiki/Geolocation>Geolocation</a></a></strong><br><a href=/people/t/tommaso-fornaciari/>Tommaso Fornaciari</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5529><div class="card-body p-3 small">Prior research has shown that <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> can be substantially improved by including user network information. While effective, it suffers from the <a href=https://en.wikipedia.org/wiki/Curse_of_dimensionality>curse of dimensionality</a>, since networks are usually represented as sparse adjacency matrices of connections, which grow exponentially with the number of users. In order to incorporate this <a href=https://en.wikipedia.org/wiki/Information>information</a>, we therefore need to limit the network size, in turn limiting performance and risking sample bias. In this paper, we address these limitations by instead using dense network representations. We explore two methods to learn continuous node representations from either 1) the network structure with node2vec (Grover and Leskovec, 2016), or 2) textual user mentions via doc2vec (Le and Mikolov, 2014). We combine both methods with input from <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a> in an attention-based convolutional neural network and evaluate the contribution of each component on <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> performance. Our method enables us to incorporate arbitrarily large networks in a fixed-length vector, without limiting the network size. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve competitive results with similar state-of-the-art methods, but with much fewer model parameters, while being applicable to <a href=https://en.wikipedia.org/wiki/Complex_network>networks</a> of virtually any size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5533 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5533/>Distant Supervised Relation Extraction with Separate Head-Tail CNN<span class=acl-fixed-case>CNN</span></a></strong><br><a href=/people/r/rui-xing/>Rui Xing</a>
|
<a href=/people/j/jie-luo/>Jie Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5533><div class="card-body p-3 small">Distant supervised relation extraction is an efficient and effective strategy to find relations between entities in texts. However, it inevitably suffers from mislabeling problem and the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy data</a> will hinder the performance. In this paper, we propose the Separate Head-Tail Convolution Neural Network (SHTCNN), a novel neural relation extraction framework to alleviate this issue. In this method, we apply separate convolution and pooling to the head and tail entity respectively for extracting better semantic features of sentences, and coarse-to-fine strategy to filter out instances which do not have actual relations in order to alleviate noisy data issues. Experiments on a widely used dataset show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant and consistent improvements in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> compared to statistical and vanilla CNN-based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5536 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5536/>Benefits of Data Augmentation for NMT-based Text Normalization of <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Content</a><span class=acl-fixed-case>NMT</span>-based Text Normalization of User-Generated Content</a></strong><br><a href=/people/c/claudia-matos-veliz/>Claudia Matos Veliz</a>
|
<a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5536><div class="card-body p-3 small">One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficulty to automatically process and analyze UGC. Text normalization is the task of transforming lexical variants to their canonical forms and is often used as a pre-processing step for conventional NLP tasks in order to overcome the performance drop that NLP systems experience when applied to UGC. In this work, we follow a Neural Machine Translation approach to <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>. To train such an encoder-decoder model, large parallel training corpora of sentence pairs are required. However, obtaining large data sets with UGC and their normalized version is not trivial, especially for languages other than <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In this paper, we explore how to overcome this data bottleneck for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, a low-resource language. We start off with a small publicly available parallel Dutch data set comprising three UGC genres and compare two different approaches. The <a href=https://en.wikipedia.org/wiki/First_law_of_thermodynamics>first</a> is to manually normalize and add training data, a money and time-consuming task. The second approach is a set of data augmentation techniques which increase data size by converting existing resources into synthesized non-standard forms. Our results reveal that, while the different approaches yield similar results regarding the normalization issues in the test set, they also introduce a large amount of over-normalizations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5540 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5540/>No, you’re not alone : A better way to find people with similar experiences on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/z/zhilin-wang/>Zhilin Wang</a>
|
<a href=/people/e/elena-rastorgueva/>Elena Rastorgueva</a>
|
<a href=/people/w/weizhe-lin/>Weizhe Lin</a>
|
<a href=/people/x/xiaodong-wu/>Xiaodong Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5540><div class="card-body p-3 small">We present a probabilistic clustering algorithm that can help Reddit users to find posts that discuss experiences similar to their own. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is built upon the BERT Next Sentence Prediction model and reduces the <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> for clustering all posts in a corpus from O(n2) to O(n) with respect to the number of posts. We demonstrate that such probabilistic clustering can yield a performance better than baseline clustering methods based on <a href=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>Latent Dirichlet Allocation</a> (Blei et al., 2003) and Word2Vec (Mikolov et al., 2013). Furthermore, there is a high degree of coherence between our probabilistic clustering and the exhaustive comparison O(n2) algorithm in which the similarity between every pair of posts is found. This makes the use of the BERT Next Sentence Prediction model more practical for unsupervised clustering tasks due to the high runtime overhead of each BERT computation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5541 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5541/>Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge</a></strong><br><a href=/people/w/wenhao-ying/>Wenhao Ying</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5541><div class="card-body p-3 small">Deep learning based general language models have achieved state-of-the-art results in many popular <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and QA tasks. Text in domains like <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has its own salient characteristics. Domain knowledge should be helpful in domain relevant tasks. In this work, we devise a simple method to obtain <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and further propose a method to integrate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> with <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> based on deep language models to improve performance of <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. Experiments on Twitter data show that even though a deep language model fine-tuned by a target domain data has attained comparable results to that of previous state-of-the-art models, this fine-tuned model can still benefit from our extracted <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to obtain more improvement. This highlights the importance of making use of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> in <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5542 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5542/>Adapting Deep Learning Methods for Mental Health Prediction on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/i/ivan-sekulic/>Ivan Sekulic</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5542><div class="card-body p-3 small">Mental health poses a significant challenge for an individual&#8217;s well-being. Text analysis of rich resources, like <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, can contribute to deeper understanding of illnesses and provide means for their early detection. We tackle a challenge of detecting social media users&#8217; mental status through deep learning-based models, moving away from traditional approaches to the task. In a binary classification task on predicting if a user suffers from one of nine different disorders, a hierarchical attention network outperforms previously set benchmarks for four of the disorders. Furthermore, we explore the limitations of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> and analyze phrases relevant for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> by inspecting the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s word-level attention weights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5544 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5544.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5544/>An Ensemble of <a href=https://en.wikipedia.org/wiki/Humour>Humour</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a>, and Hate Speechfor Sentiment Classification in Online Reviews</a></strong><br><a href=/people/r/rohan-badlani/>Rohan Badlani</a>
|
<a href=/people/n/nishit-asnani/>Nishit Asnani</a>
|
<a href=/people/m/manan-rai/>Manan Rai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5544><div class="card-body p-3 small">Due to the nature of online user reviews, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on such <a href=https://en.wikipedia.org/wiki/Data>data</a> requires a deep semantic understanding of the text. Many <a href=https://en.wikipedia.org/wiki/Review>online reviews</a> are sarcastic, humorous, or hateful. Signals from such language nuances may reinforce or completely alter the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> of a review as predicted by a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> that attempts to detect <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> alone. Thus, having a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that is explicitly aware of these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> should help <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> perform better on reviews that are characterized by them. We propose a composite two-step model that extracts features pertaining to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Humour>humour</a>, <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, as well as <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, in the first step, feeding them in conjunction to inform <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment classification</a> in the second step. We show that this multi-step approach leads to a better empirical performance for sentiment classification than a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that predicts sentiment alone. A qualitative analysis reveals that the conjunctive approach can better capture the nuances of sentiment as expressed in online reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5547 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5547/>A Social Opinion Gold Standard for the Malta Government Budget 2018<span class=acl-fixed-case>M</span>alta Government Budget 2018</a></strong><br><a href=/people/k/keith-cortis/>Keith Cortis</a>
|
<a href=/people/b/brian-davis/>Brian Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5547><div class="card-body p-3 small">We present a gold standard of annotated social opinion for the Malta Government Budget 2018. It consists of over 500 online posts in English and/or the Maltese less-resourced language, gathered from social media platforms, specifically, social networking services and newswires, which have been annotated with information about opinions expressed by the general public and other entities, in terms of sentiment polarity, emotion, sarcasm / irony, and negation. This <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is a resource for <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> based on social data, within the context of <a href=https://en.wikipedia.org/wiki/Politics>politics</a>. It is the first opinion annotated social dataset from Malta, which has very limited language resources available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5549 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5549/>Y’all should read this ! Identifying Plurality in Second-Person Personal Pronouns in English Texts<span class=acl-fixed-case>Y</span>’all should read this! Identifying Plurality in Second-Person Personal Pronouns in <span class=acl-fixed-case>E</span>nglish Texts</a></strong><br><a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/r/ronen-tamari/>Ronen Tamari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5549><div class="card-body p-3 small">Distinguishing between singular and plural you in English is a challenging task which has potential for downstream applications, such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> or <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. While formal written English does not distinguish between these cases, other languages (such as Spanish), as well as other dialects of English (via phrases such as y&#8217; all), do make this distinction. We make use of this to obtain distantly-supervised labels for the task on a large-scale in two domains. Following, we train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to distinguish between the single / plural &#8216;you&#8217;, finding that although in-domain training achieves reasonable <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> (77 %), there is still a lot of room for improvement, especially in the domain-transfer scenario, which proves extremely challenging. Our code and data are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5552.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5552 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5552 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5552/>Contextualized context2vec</a></strong><br><a href=/people/k/kazuki-ashihara/>Kazuki Ashihara</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/s/satoru-uchida/>Satoru Uchida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5552><div class="card-body p-3 small">Lexical substitution ranks substitution candidates from the viewpoint of paraphrasability for a target word in a given sentence. There are two major approaches for <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> : (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. Herein we propose a method that combines these two approaches to contextualize word embeddings for <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>. Experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the current state-of-the-art method. We also create CEFR-LP, a new evaluation dataset for the lexical substitution task. It has a wider coverage of substitution candidates than previous <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and assigns <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English proficiency levels</a> to all target words and substitution candidates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5555.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5555 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5555 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5555/>Unsupervised Neologism Normalization Using Embedding Space Mapping</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/k/kapil-thadani/>Kapil Thadani</a>
|
<a href=/people/a/aasish-pappu/>Aasish Pappu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5555><div class="card-body p-3 small">This paper presents an approach for detecting and normalizing neologisms in <a href=https://en.wikipedia.org/wiki/Social_media>social media content</a>. Neologisms refer to recent expressions that are specific to certain entities or events and are being increasingly used by the public, but have not yet been accepted in mainstream language. Automated methods for handling <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> are important for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and normalization, especially for informal genres with <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated content</a>. We present an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for detecting <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> and then normalizing them to canonical words without relying on parallel training data. Our approach builds on the text normalization literature and introduces <a href=https://en.wikipedia.org/wiki/Adaptation>adaptations</a> to fit the specificities of this task, including <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic and etymological considerations</a>. We evaluate the proposed techniques on a dataset of Reddit comments, with detected neologisms and corresponding <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalizations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5557 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5557/>Towards Actual (Not Operational) Textual Style Transfer Auto-Evaluation</a></strong><br><a href=/people/r/richard-yuanzhe-pang/>Richard Yuanzhe Pang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5557><div class="card-body p-3 small">Regarding the problem of automatically generating paraphrases with modified styles or attributes, the difficulty lies in the lack of parallel corpora. Numerous advances have been proposed for the <a href=https://en.wikipedia.org/wiki/Electricity_generation>generation</a>. However, significant problems remain with the auto-evaluation of style transfer tasks. Based on the summary of Pang and Gimpel (2018) and Mir et al. (2019), style transfer evaluations rely on three metrics : post-transfer style classification accuracy, content or semantic similarity, and naturalness or fluency. We elucidate the dangerous current state of style transfer auto-evaluation research. Moreover, we propose ways to aggregate the three <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> into one <a href=https://en.wikipedia.org/wiki/Evaluation>evaluator</a>. This abstract aims to bring researchers to think about the future of style transfer and style transfer evaluation research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5558 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5558/>CodeSwitch-Reddit : Exploration of Written Multilingual Discourse in Online Discussion Forums<span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>S</span>witch-<span class=acl-fixed-case>R</span>eddit: Exploration of Written Multilingual Discourse in Online Discussion Forums</a></strong><br><a href=/people/e/ella-rabinovich/>Ella Rabinovich</a>
|
<a href=/people/m/masih-sultani/>Masih Sultani</a>
|
<a href=/people/s/suzanne-stevenson/>Suzanne Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5558><div class="card-body p-3 small">In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into <a href=https://en.wikipedia.org/wiki/Code-switching>written code-switching</a> in <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion forums</a>. The released <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can further facilitate a range of research and practical activities.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>