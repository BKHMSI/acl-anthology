<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Storytelling (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Storytelling (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w19-34>Proceedings of the Second Workshop on Storytelling</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li></ul></div></div><div id=w19-34><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-34.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-34/>Proceedings of the Second Workshop on Storytelling</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3400/>Proceedings of the Second Workshop on Storytelling</a></strong><br><a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao ‘Kenneth’ Huang</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3404/>A Hybrid Model for Globally Coherent Story Generation</a></strong><br><a href=/people/f/fangzhou-zhai/>Fangzhou Zhai</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/p/pavel-shkadzko/>Pavel Shkadzko</a>
|
<a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3404><div class="card-body p-3 small">Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data ; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3405/>Guided Neural Language Generation for Automated Storytelling</a></strong><br><a href=/people/p/prithviraj-ammanabrolu/>Prithviraj Ammanabrolu</a>
|
<a href=/people/e/ethan-tien/>Ethan Tien</a>
|
<a href=/people/w/wesley-cheung/>Wesley Cheung</a>
|
<a href=/people/z/zhaochen-luo/>Zhaochen Luo</a>
|
<a href=/people/w/william-ma/>William Ma</a>
|
<a href=/people/l/lara-martin/>Lara Martin</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3405><div class="card-body p-3 small">Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries. Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into : (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence). However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences. We present an ensemble-based model that generates <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> guided by <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>. Our <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> outperforms the baseline sequence-to-sequence model. Additionally, we provide results for a full end-to-end automated story generation system, demonstrating how our model works with existing systems designed for the event-to-event problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3407" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3407/>Narrative Generation in the Wild : Methods from NaNoGenMo<span class=acl-fixed-case>N</span>arrative <span class=acl-fixed-case>G</span>eneration in the <span class=acl-fixed-case>W</span>ild: <span class=acl-fixed-case>M</span>ethods from <span class=acl-fixed-case>N</span>a<span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>G</span>en<span class=acl-fixed-case>M</span>o</a></strong><br><a href=/people/j/judith-van-stegeren/>Judith van Stegeren</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3407><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text generation</a>, generating long stories is still a challenge. Coherence tends to decrease rapidly as the output length increases. Especially for generated stories, coherence of the narrative is an important quality aspect of the output text. In this paper we examine how narrative coherence is attained in the submissions of NaNoGenMo 2018, an online text generation event where participants are challenged to generate a 50,000 word novel. We list the main approaches that were used to generate coherent narratives and link them to <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>. Finally, we give recommendations on when to use which <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3408/>Lexical concreteness in narrative</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3408><div class="card-body p-3 small">This study explores the relation between lexical concreteness and narrative text quality. We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to quantitatively measure lexical concreteness of a text. We apply <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to a corpus of student stories, scored according to writing evaluation rubrics. Lexical concreteness is weakly-to-moderately related to story quality, depending on story-type. The relation is mostly borne by <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>, but also found for <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3411/>Personality Traits Recognition in <a href=https://en.wikipedia.org/wiki/Literary_language>Literary Texts</a></a></strong><br><a href=/people/d/daniele-pizzolli/>Daniele Pizzolli</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3411><div class="card-body p-3 small">Interesting stories often are built around interesting characters. Finding and detailing what makes an interesting character is a real challenge, but certainly a significant cue is the character personality traits. Our exploratory work tests the adaptability of the current <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits theories</a> to literal characters, focusing on the analysis of utterances in <a href=https://en.wikipedia.org/wiki/Play_(theatre)>theatre scripts</a>. And, at the opposite, we try to find significant <a href=https://en.wikipedia.org/wiki/Trait_theory>traits</a> for interesting characters. The preliminary results demonstrate that our approach is reasonable. Using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> for gaining insight into the <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits</a> of <a href=https://en.wikipedia.org/wiki/Character_(arts)>fictional characters</a> can make sense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3413" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3413/>WriterForcing : Generating more interesting story endings<span class=acl-fixed-case>W</span>riter<span class=acl-fixed-case>F</span>orcing: Generating more interesting story endings</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/v/vinayshekhar-bannihatti-kumar/>Vinayshekhar Bannihatti Kumar</a>
|
<a href=/people/m/mukul-bhutani/>Mukul Bhutani</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3413><div class="card-body p-3 small">We study the problem of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>generating interesting endings</a> for stories. Neural generative models have shown promising results for various text generation problems. Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2Seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same <a href=https://en.wikipedia.org/wiki/Context_(language_use)>story context</a>. In this paper, we propose <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which generate more diverse and interesting outputs by 1) training <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>