<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Asian Chapter of the Association for Computational Linguistics (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Asian Chapter of the Association for Computational Linguistics (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020aacl-main>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">34&nbsp;papers</span></li><li><a class=align-middle href=#2020aacl-srw>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2020aacl-demo>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020iwdp-1>Proceedings of the Second International Workshop of Discourse Processing</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020knlp-1>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020lifelongnlp-1>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020loresmt-1>Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2020nlptea-1>Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2020wat-1>Proceedings of the 7th Workshop on Asian Translation</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li></ul></div></div><div id=2020aacl-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.aacl-main/>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.0/>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</a></strong><br><a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.1/>Touch Editing : A Flexible One-Time Interaction Approach for <a href=https://en.wikipedia.org/wiki/Translation>Translation</a></a></strong><br><a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/g/guoping-huang/>Guoping Huang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--1><div class="card-body p-3 small">We propose a touch-based editing method for <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on <a href=https://en.wikipedia.org/wiki/Somatosensory_system>touch actions</a> that users perform to indicate translation errors. We present a dual-encoder model to handle the actions and generate refined translations. To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions for training data construction. Experiments on translation datasets with simulated editing actions show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.2/>Can Monolingual Pretrained Models Help Cross-Lingual Classification?</a></strong><br><a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/x/xianling-mao/>Xianling Mao</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--2><div class="card-body p-3 small">Multilingual pretrained language models (such as multilingual BERT) have achieved impressive results for cross-lingual transfer. However, due to the constant model capacity, multilingual pre-training usually lags behind the monolingual competitors. In this work, we present two approaches to improve zero-shot cross-lingual classification, by transferring the knowledge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.5/>FERNet : Fine-grained Extraction and Reasoning Network for Emotion Recognition in Dialogues<span class=acl-fixed-case>FERN</span>et: Fine-grained Extraction and Reasoning Network for Emotion Recognition in Dialogues</a></strong><br><a href=/people/y/yingmei-guo/>Yingmei Guo</a>
|
<a href=/people/z/zhiyong-wu/>Zhiyong Wu</a>
|
<a href=/people/m/mingxing-xu/>Mingxing Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--5><div class="card-body p-3 small">Unlike non-conversation scenes, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> in dialogues (ERD) poses more complicated challenges due to its interactive nature and intricate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. All present methods model historical utterances without considering the content of the target utterance. However, different parts of a historical utterance may contribute differently to emotion inference of different target utterances. Therefore we propose Fine-grained Extraction and Reasoning Network (FERNet) to generate target-specific historical utterance representations. The reasoning module effectively handles both local and global sequential dependencies to reason over context, and updates target utterance representations to more informed vectors. Experiments on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves competitive performance compared with previous <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.11/>Investigating Learning Dynamics of BERT Fine-Tuning<span class=acl-fixed-case>BERT</span> Fine-Tuning</a></strong><br><a href=/people/y/yaru-hao/>Yaru Hao</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--11><div class="card-body p-3 small">The recently introduced pre-trained language model BERT advances the state-of-the-art on many NLP tasks through the fine-tuning approach, but few studies investigate how the fine-tuning process improves the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on downstream tasks. In this paper, we inspect the learning dynamics of BERT fine-tuning with two indicators. We use JS divergence to detect the change of the attention mode and use SVCCA distance to examine the change to the feature extraction mode during BERT fine-tuning. We conclude that BERT fine-tuning mainly changes the attention mode of the last layers and modifies the feature extraction mode of the intermediate and last layers. Moreover, we analyze the consistency of BERT fine-tuning between different <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a> and different datasets. In summary, we provide a distinctive understanding of the learning dynamics of BERT fine-tuning, which sheds some light on improving the <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.15/>A Simple Text-based Relevant Location Prediction Method using Knowledge Base</a></strong><br><a href=/people/m/mei-sasaki/>Mei Sasaki</a>
|
<a href=/people/s/shumpei-okura/>Shumpei Okura</a>
|
<a href=/people/s/shingo-ono/>Shingo Ono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--15><div class="card-body p-3 small">In this paper, we propose a simple method to predict salient locations from <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news article text</a> using a knowledge base (KB). The proposed method uses a dictionary of locations created from the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> to identify occurrences of locations in the text and uses the hierarchical information between entities in the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> for assigning appropriate saliency scores to regions. It allows prediction at arbitrary region units and has only a few hyperparameters that need to be tuned. We show using manually annotated news articles that the proposed method improves the <a href=https://en.wikipedia.org/wiki/F-measure>f-measure</a> by 0.12 compared to multiple baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.aacl-main.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.17/>An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks<span class=acl-fixed-case>K</span>orean <span class=acl-fixed-case>NLP</span> Tasks</a></strong><br><a href=/people/k/kyubyong-park/>Kyubyong Park</a>
|
<a href=/people/j/joohong-lee/>Joohong Lee</a>
|
<a href=/people/s/seongbo-jang/>Seongbo Jang</a>
|
<a href=/people/d/dawoon-jung/>Dawoon Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--17><div class="card-body p-3 small">Typically, <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a> is the very first step in most <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a> works. As a token serves as an atomic unit that embeds the contextual information of text, how to define a token plays a decisive role in the performance of a model. Even though Byte Pair Encoding (BPE) has been considered the de facto standard tokenization method due to its simplicity and universality, it still remains unclear whether BPE works best across all languages and tasks. In this paper, we test several tokenization strategies in order to answer our primary research question, that is, What is the best tokenization strategy for Korean NLP tasks? Experimental results demonstrate that a hybrid approach of morphological segmentation followed by BPE works best in <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> to / from English machine translation and natural language understanding tasks such as KorNLI, KorSTS, NSMC, and PAWS-X. As an exception, for KorQuAD, the Korean extension of SQuAD, BPE segmentation turns out to be the most effective. Our code and pre-trained models are publicly available at https://github.com/kakaobrain/kortok.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.22/>Named Entity Recognition in Multi-level Contexts</a></strong><br><a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--22><div class="card-body p-3 small">Named entity recognition is a critical task in the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing field</a>. Most existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can only exploit <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> within a sentence. However, their performance on recognizing entities in limited or ambiguous sentence-level contexts is usually unsatisfactory. Fortunately, other sentences in the same document can provide supplementary document-level contexts to help recognize these entities. In addition, words themselves contain word-level contextual information since they usually have different preferences of entity type and relative position from named entities. In this paper, we propose a unified framework to incorporate <a href=https://en.wikipedia.org/wiki/Context_(computing)>multi-level contexts</a> for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We use TagLM as our basic <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to capture sentence-level contexts. To incorporate document-level contexts, we propose to capture interactions between sentences via a multi-head self attention network. To mine word-level contexts, we propose an auxiliary task to predict the type of each word to capture its type preference. We jointly train our model in entity recognition and the auxiliary classification task via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. The experimental results on several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.28/>Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths</a></strong><br><a href=/people/h/haozhe-ji/>Haozhe Ji</a>
|
<a href=/people/p/pei-ke/>Pei Ke</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--28><div class="card-body p-3 small">Commonsense explanation generation aims to empower the machine&#8217;s sense-making capability by generating plausible explanations to statements against commonsense. While this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is easy to human, the machine still struggles to generate reasonable and informative explanations. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> that first extracts the underlying concepts which are served as bridges in the reasoning chain and then integrates these <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> to generate the final explanation. To facilitate the reasoning process, we utilize external commonsense knowledge to build the connection between a statement and the bridge concepts by extracting and pruning multi-hop paths to build a subgraph. We design a bridge concept extraction model that first scores the triples, routes the paths in the subgraph, and further selects bridge concepts with weak supervision at both the triple level and the concept level. We conduct experiments on the commonsense explanation generation task and our model outperforms the state-of-the-art baselines in both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.31/>All-in-One : A Deep Attentive Multi-task Learning Framework for <a href=https://en.wikipedia.org/wiki/Humour>Humour</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a>, Offensive, <a href=https://en.wikipedia.org/wiki/Motivation>Motivation</a>, and Sentiment on Memes</a></strong><br><a href=/people/d/dushyant-singh-chauhan/>Dushyant Singh Chauhan</a>
|
<a href=/people/d/dhanush-s-r/>Dhanush S R</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--31><div class="card-body p-3 small">In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on a somewhat complicated form of information, i.e., <a href=https://en.wikipedia.org/wiki/Meme>memes</a>. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For <a href=https://en.wikipedia.org/wiki/Computer_multitasking>multi-tasking</a>, we propose two attention-like mechanisms viz., Inter-task Relationship Module (iTRM) and Inter-class Relationship Module (iCRM). The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, <a href=https://en.wikipedia.org/wiki/ICRM>iCRM</a> develops relations between the different classes of tasks. Finally, <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> from both the attentions are concatenated and shared across the five <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Humour>humour</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, offensive, motivational, and sentiment) for <a href=https://en.wikipedia.org/wiki/Computer_multitasking>multi-tasking</a>. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of <a href=https://en.wikipedia.org/wiki/Meme>memes</a> annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-the-art systems (Baseline and SemEval 2020 winner). The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.32/>Identifying Implicit Quotes for Unsupervised Extractive Summarization of Conversations</a></strong><br><a href=/people/r/ryuji-kano/>Ryuji Kano</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--32><div class="card-body p-3 small">We propose Implicit Quote Extractor, an end-to-end unsupervised extractive neural summarization model for conversational texts. When we reply to posts, <a href=https://en.wikipedia.org/wiki/Quotation>quotes</a> are used to highlight important part of texts. We aim to extract quoted sentences as summaries. Most replies do not explicitly include <a href=https://en.wikipedia.org/wiki/Quotation>quotes</a>, so it is difficult to use <a href=https://en.wikipedia.org/wiki/Quotation>quotes</a> as <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. However, even if it is not explicitly shown, replies always refer to certain parts of texts ; we call them implicit quotes. Implicit Quote Extractor aims to extract implicit quotes as summaries. The training task of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is to predict whether a reply candidate is a true reply to a post. For <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has to choose a few sentences from the post. To predict accurately, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns to extract sentences that replies frequently refer to. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two email datasets and one social media dataset, and confirm that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is useful for extractive summarization. We further discuss two topics ; one is whether quote extraction is an important factor for summarization, and the other is whether our model can capture salient sentences that conventional methods can not.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.aacl-main.37.Dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.aacl-main.37.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.aacl-main.37/>Chinese Content Scoring : Open-Access Datasets and Features on Different Segmentation Levels<span class=acl-fixed-case>C</span>hinese Content Scoring: Open-Access Datasets and Features on Different Segmentation Levels</a></strong><br><a href=/people/y/yuning-ding/>Yuning Ding</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--37><div class="card-body p-3 small">In this paper, we analyse the challenges of Chinese content scoring in comparison to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. As a review of prior work for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese content scoring</a> shows a lack of <a href=https://en.wikipedia.org/wiki/Open_access>open-access data</a> in the field, we present two short-answer data sets for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. The Chinese Educational Short Answers data set (CESA) contains 1800 student answers for five science-related questions. As a second <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, we collected ASAP-ZH with 942 answers by re-using three existing prompts from the ASAP data set. We adapt a state-of-the-art content scoring system for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and evaluate it in several settings on these <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>. Results show that <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> on lower segmentation levels such as character n-grams tend to have better performance than <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> on token level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.39/>An Exploratory Study on Multilingual Quality Estimation</a></strong><br><a href=/people/s/shuo-sun/>Shuo Sun</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/a/adithya-renduchintala/>Adithya Renduchintala</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--39><div class="card-body p-3 small">Predicting the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has traditionally been addressed with language-specific models, under the assumption that the quality label distribution or <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> exhibit traits that are not shared across languages. An obvious disadvantage of this approach is the need for <a href=https://en.wikipedia.org/wiki/Data_(computing)>labelled data</a> for each given language pair. We challenge this assumption by exploring different approaches to multilingual Quality Estimation (QE), including using scores from translation models. We show that these outperform single-language models, particularly in less balanced quality label distributions and low-resource settings. In the extreme case of zero-shot QE, we show that it is possible to accurately predict <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> for any given new language from <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on other languages. Our findings indicate that state-of-the-art neural QE models based on powerful pre-trained representations generalise well across languages, making them more applicable in real-world settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.40/>English-to-Chinese Transliteration with Phonetic Auxiliary Task<span class=acl-fixed-case>E</span>nglish-to-<span class=acl-fixed-case>C</span>hinese Transliteration with Phonetic Auxiliary Task</a></strong><br><a href=/people/y/yuan-he/>Yuan He</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--40><div class="card-body p-3 small">Approaching named entities transliteration as a Neural Machine Translation (NMT) problem is common practice. While many have applied various NMT techniques to enhance machine transliteration models, few focus on the linguistic features particular to the relevant languages. In this paper, we investigate the effect of incorporating phonetic features for English-to-Chinese transliteration under the multi-task learning (MTL) settingwhere we define a phonetic auxiliary task aimed to improve the generalization performance of the main transliteration task. In addition to our system, we also release a new English-to-Chinese dataset and propose a novel evaluation metric which considers multiple possible <a href=https://en.wikipedia.org/wiki/Transliteration>transliterations</a> given a source name. Our results show that the multi-task model achieves similar performance as the previous state of the art with a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of a much smaller size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.41/>Predicting and Using Target Length in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/z/zijian-yang/>Zijian Yang</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--41><div class="card-body p-3 small">Attention-based encoder-decoder models have achieved great success in neural machine translation tasks. However, the lengths of the target sequences are not explicitly predicted in these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. This work proposes length prediction as an auxiliary task and set up a <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> to obtain the length information from the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Experimental results show that the length prediction sub-network brings improvements over the strong baseline system and that the predicted length can be used as an alternative to length normalization during decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.43/>Heads-up ! Unsupervised Constituency Parsing via Self-Attention Heads</a></strong><br><a href=/people/b/bowen-li/>Bowen Li</a>
|
<a href=/people/t/taeuk-kim/>Taeuk Kim</a>
|
<a href=/people/r/reinald-kim-amplayo/>Reinald Kim Amplayo</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--43><div class="card-body p-3 small">Transformer-based pre-trained language models (PLMs) have dramatically improved the state of the art in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> across many tasks. This has led to substantial interest in analyzing the syntactic knowledge PLMs learn. Previous approaches to this question have been limited, mostly using test suites or probes. Here, we propose a novel fully unsupervised parsing approach that extracts constituency trees from PLM attention heads. We rank transformer attention heads based on their inherent properties, and create an ensemble of high-ranking heads to produce the final <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a>. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is adaptable to low-resource languages, as it does not rely on development sets, which can be expensive to annotate. Our experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> often outperform existing approaches if there is no development set present. Our unsupervised parser can also be used as a tool to analyze the grammars PLMs learn implicitly. For this, we use the parse trees induced by our method to train a neural PCFG and compare it to a <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar</a> derived from a human-annotated treebank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.45/>Self-Supervised Learning for Pairwise Data Refinement</a></strong><br><a href=/people/g/gustavo-hernandez-abrego/>Gustavo Hernandez Abrego</a>
|
<a href=/people/b/bowen-liang/>Bowen Liang</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/z/zarana-parekh/>Zarana Parekh</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/y/yunhsuan-sung/>Yunhsuan Sung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--45><div class="card-body p-3 small">Pairwise data automatically constructed from weakly supervised signals has been widely used for training <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. Pairwise datasets such as parallel texts can have uneven quality levels overall, but usually contain data subsets that are more useful as learning examples. We present two methods to refine data that are aimed to obtain that kind of subsets in a self-supervised way. Our methods are based on iteratively training dual-encoder models to compute <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity scores</a>. We evaluate our methods on de-noising <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel texts</a> and training neural machine translation models. We find that : (i) The self-supervised refinement achieves most machine translation gains in the first iteration, but following iterations further improve its intrinsic evaluation. (ii) <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine translations</a> can improve the de-noising performance when combined with selection steps. (iii) Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> are able to reach the performance of a supervised method. Being entirely self-supervised, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> are well-suited to handle pairwise data without the need of prior knowledge or human annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.46/>A Survey of the State of Explainable AI for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a><span class=acl-fixed-case>AI</span> for Natural Language Processing</a></strong><br><a href=/people/m/marina-danilevsky/>Marina Danilevsky</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/y/yannis-katsis/>Yannis Katsis</a>
|
<a href=/people/b/ban-kawas/>Ban Kawas</a>
|
<a href=/people/p/prithviraj-sen/>Prithviraj Sen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--46><div class="card-body p-3 small">Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.47/>Beyond Fine-tuning : Few-Sample Sentence Embedding Transfer</a></strong><br><a href=/people/s/siddhant-garg/>Siddhant Garg</a>
|
<a href=/people/r/rohit-kumar-sharma/>Rohit Kumar Sharma</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--47><div class="card-body p-3 small">Fine-tuning (FT) pre-trained sentence embedding models on small datasets has been shown to have limitations. In this paper we show that concatenating the embeddings from the pre-trained model with those from a simple sentence embedding model trained only on the target data, can improve over the performance of FT for few-sample tasks. To this end, a linear classifier is trained on the combined embeddings, either by freezing the embedding model weights or training the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> and embedding models end-to-end. We perform evaluation on seven small datasets from NLP tasks and show that our approach with end-to-end training outperforms FT with negligible <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a>. Further, we also show that sophisticated combination techniques like <a href=https://en.wikipedia.org/wiki/Concatenation>CCA</a> and <a href=https://en.wikipedia.org/wiki/Concatenation>KCCA</a> do not work as well in practice as <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a>. We provide <a href=https://en.wikipedia.org/wiki/Mathematical_model>theoretical analysis</a> to explain this empirical observation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.aacl-main.48" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.48/>Multimodal Pretraining for Dense Video Captioning</a></strong><br><a href=/people/g/gabriel-huang/>Gabriel Huang</a>
|
<a href=/people/b/bo-pang/>Bo Pang</a>
|
<a href=/people/z/zhenhai-zhu/>Zhenhai Zhu</a>
|
<a href=/people/c/clara-rivera/>Clara Rivera</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--48><div class="card-body p-3 small">Learning specific hands-on skills such as <a href=https://en.wikipedia.org/wiki/Cooking>cooking</a>, <a href=https://en.wikipedia.org/wiki/Service_(motor_vehicle)>car maintenance</a>, and <a href=https://en.wikipedia.org/wiki/Home_repair>home repairs</a> increasingly happens via instructional videos. The user experience with such videos is known to be improved by meta-information such as time-stamped annotations for the main steps involved. Generating such <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> automatically is challenging, and we describe here two relevant contributions. First, we construct and release a new dense video captioning dataset, Video Timeline Tags (ViTT), featuring a variety of instructional videos together with time-stamped annotations. Second, we explore several multimodal sequence-to-sequence pretraining strategies that leverage large unsupervised datasets of videos and caption-like texts. We pretrain and subsequently finetune dense video captioning models using both YouCook2 and ViTT. We show that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> generalize well and are robust over a wide variety of instructional videos.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.54/>Point-of-Interest Oriented Question Answering with Joint Inference of Semantic Matching and Distance Correlation</a></strong><br><a href=/people/y/yifei-yuan/>Yifei Yuan</a>
|
<a href=/people/j/jingbo-zhou/>Jingbo Zhou</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--54><div class="card-body p-3 small">Point-of-Interest (POI) oriented question answering (QA) aims to return a list of POIs given a question issued by a user. Recent advances in intelligent virtual assistants have opened the possibility of engaging the client software more actively in the provision of location-based services, thereby showing great promise for automatic POI retrieval. Some existing <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA methods</a> can be adopted on this task such as <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA similarity calculation</a> and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> using pre-defined rules. The returned results, however, are subject to inherent limitations due to the lack of the ability for handling some important POI related information, including <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a>, location entities, and proximity-related terms (e.g. nearby, close). In this paper, we present a novel deep learning framework integrated with joint inference to capture both tag semantic and geographic correlation between question and POIs. One characteristic of our model is to propose a special cross attention question embedding neural network structure to obtain question-to-POI and POI-to-question information. Besides, we utilize a <a href=https://en.wikipedia.org/wiki/Skewness>skewed distribution</a> to simulate the spatial relationship between questions and POIs. By measuring the results offered by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> against existing methods, we demonstrate its robustness and practicability, and supplement our conclusions with empirical evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.55/>Leveraging Structured Metadata for Improving <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> on the Web</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/a/ahmed-hassan/>Ahmed Hassan Awadallah</a>
|
<a href=/people/a/adam-fourney/>Adam Fourney</a>
|
<a href=/people/r/robert-sim/>Robert Sim</a>
|
<a href=/people/p/paul-bennett/>Paul Bennett</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--55><div class="card-body p-3 small">We show that leveraging <a href=https://en.wikipedia.org/wiki/Metadata>metadata information</a> from <a href=https://en.wikipedia.org/wiki/Web_page>web pages</a> can improve the performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for answer passage selection / reranking. We propose a neural passage selection model that leverages <a href=https://en.wikipedia.org/wiki/Metadata>metadata information</a> with a fine-grained encoding strategy, which learns the representation for <a href=https://en.wikipedia.org/wiki/Metadata>metadata predicates</a> in a hierarchical way. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are evaluated on the MS MARCO (Nguyen et al., 2016) and Recipe-MARCO datasets. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> significantly outperform baseline models, which do not incorporate <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. We also show that the fine-grained encoding&#8217;s advantage over other strategies for encoding the <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.59/>Cue Me In : Content-Inducing Approaches to Interactive Story Generation</a></strong><br><a href=/people/f/faeze-brahman/>Faeze Brahman</a>
|
<a href=/people/a/alexandru-petrusca/>Alexandru Petrusca</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--59><div class="card-body p-3 small">Automatically generating stories is a challenging problem that requires producing causally related and logical sequences of events about a topic. Previous approaches in this domain have focused largely on one-shot generation, where a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> outputs a complete story based on limited initial input from a user. Here, we instead focus on the task of interactive story generation, where the user provides the model mid-level sentence abstractions in the form of cue phrases during the generation process. This provides an interface for human users to guide the <a href=https://en.wikipedia.org/wiki/Storytelling>story generation</a>. We present two content-inducing approaches to effectively incorporate this additional information. Experimental results from both automatic and human evaluations show that these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> produce more topically coherent and personalized stories compared to baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.aacl-main.60" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.60/>Liputan6 : A Large-scale Indonesian Dataset for Text Summarization<span class=acl-fixed-case>I</span>ndonesian Dataset for Text Summarization</a></strong><br><a href=/people/f/fajri-koto/>Fajri Koto</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--60><div class="card-body p-3 small">In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from Liputan6.com, an online news portal, and obtain 215,827 documentsummary pairs. We leverage pre-trained language models to develop benchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual BERT-based models. We include a thorough error analysis by examining machine-generated summaries that have low ROUGE scores, and expose both issues with ROUGE itself, as well as with extractive and abstractive summarization models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.aacl-main.61" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.61/>Generating Sports News from Live Commentary : A Chinese Dataset for Sports Game Summarization<span class=acl-fixed-case>C</span>hinese Dataset for Sports Game Summarization</a></strong><br><a href=/people/k/kuan-hao-huang/>Kuan-Hao Huang</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--61><div class="card-body p-3 small">Sports game summarization focuses on generating news articles from <a href=https://en.wikipedia.org/wiki/Sports_commentator>live commentaries</a>. Unlike traditional summarization tasks, the source documents and the target summaries for sports game summarization tasks are written in quite different writing styles. In addition, live commentaries usually contain many <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>, which makes summarizing sports games precisely very challenging. To deeply study this task, we present SportsSum, a Chinese sports game summarization dataset which contains 5,428 soccer games of live commentaries and the corresponding <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. Additionally, we propose a two-step summarization model consisting of a selector and a rewriter for SportsSum. To evaluate the correctness of generated sports summaries, we design two novel score metrics : name matching score and event matching score. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than other summarization baselines on ROUGE scores as well as the two designed scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.63/>Improving <a href=https://en.wikipedia.org/wiki/Context_model>Context Modeling</a> in Neural Topic Segmentation</a></strong><br><a href=/people/l/linzi-xing/>Linzi Xing</a>
|
<a href=/people/b/brad-hackinen/>Brad Hackinen</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/francesco-trebbi/>Francesco Trebbi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--63><div class="card-body p-3 small">Topic segmentation is critical in key NLP tasks and recent works favor highly effective neural supervised approaches. However, current neural solutions are arguably limited in how they model context. In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in domain transfer setting by training a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> to two other languages (German and Chinese), and show its effectiveness in multilingual scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.66/>Event Coreference Resolution with Non-Local Information</a></strong><br><a href=/people/j/jing-lu/>Jing Lu</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--66><div class="card-body p-3 small">We present two extensions to a state-of-theart joint model for event coreference resolution, which involve incorporating (1) a supervised topic model for improving trigger detection by providing global context, and (2) a preprocessing module that seeks to improve event coreference by discarding unlikely candidate antecedents of an event mention using discourse contexts computed based on salient entities. The resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields the best results reported to date on the KBP 2017 English and Chinese datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.aacl-main.68" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.68/>Asking Crowdworkers to Write Entailment Examples : The Best of Bad Options<span class=acl-fixed-case>A</span>sking <span class=acl-fixed-case>C</span>rowdworkers to <span class=acl-fixed-case>W</span>rite <span class=acl-fixed-case>E</span>ntailment <span class=acl-fixed-case>E</span>xamples: <span class=acl-fixed-case>T</span>he <span class=acl-fixed-case>B</span>est of <span class=acl-fixed-case>B</span>ad Options</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/r/ruijie-chen/>Ruijie Chen</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--68><div class="card-body p-3 small">Large-scale natural language inference (NLI) datasets such as SNLI or MNLI have been created by asking crowdworkers to read a premise and write three new hypotheses, one for each possible semantic relationships (entailment, contradiction, and neutral). While this <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> has been used to create useful benchmark data, it remains unclear whether the writing-based annotation protocol is optimal for any purpose, since it has not been evaluated directly. Furthermore, there is ample evidence that crowdworker writing can introduce artifacts in the data. We investigate two alternative <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocols</a> which automatically create candidate (premise, hypothesis) pairs for annotators to label. Using these protocols and a writing-based baseline, we collect several new English NLI datasets of over 3k examples each, each using a fixed amount of annotator time, but a varying number of examples to fit that time budget. Our experiments on NLI and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> show negative results : None of the alternative protocols outperforms the baseline in evaluations of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> within NLI or on transfer to outside target tasks. We conclude that crowdworker writing still the best known option for entailment data, highlighting the need for further data collection work to focus on improving writing-based annotation processes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.70/>Answering Product-related Questions with Heterogeneous Information</a></strong><br><a href=/people/w/wenxuan-zhang/>Wenxuan Zhang</a>
|
<a href=/people/q/qian-yu/>Qian Yu</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--70><div class="card-body p-3 small">Providing <a href=https://en.wikipedia.org/wiki/Instant_messaging>instant response</a> for product-related questions in E-commerce question answering platforms can greatly improve users&#8217; online shopping experience. However, existing product question answering (PQA) methods only consider a single information source such as user reviews and/or require large amounts of labeled data. In this paper, we propose a novel framework to tackle the PQA task via exploiting heterogeneous information including natural language text and attribute-value pairs from two information sources of the concerned product, namely product details and user reviews. A heterogeneous information encoding component is then designed for obtaining unified representations of information with different formats. The sources of the candidate snippets are also incorporated when measuring the question-snippet relevance. Moreover, the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> is trained with a specifically designed weak supervision paradigm making use of available answers in the training phase. Experiments on a real-world dataset show that our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves superior performance over state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.73/>Multi-view Classification Model for Knowledge Graph Completion</a></strong><br><a href=/people/w/wenbin-jiang/>Wenbin Jiang</a>
|
<a href=/people/m/mengfei-guo/>Mengfei Guo</a>
|
<a href=/people/y/yufeng-chen/>Yufeng Chen</a>
|
<a href=/people/y/ying-li/>Ying Li</a>
|
<a href=/people/j/jinan-xu/>Jinan Xu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/y/yong-zhu/>Yong Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--73><div class="card-body p-3 small">Most previous work on knowledge graph completion conducted single-view prediction or calculation for candidate triple evaluation, based only on the content information of the candidate triples. This paper describes a novel multi-view classification model for knowledge graph completion, where multiple classification views are performed based on both content and context information for candidate triple evaluation. Each classification view evaluates the validity of a candidate triple from a specific viewpoint, based on the content information inside the candidate triple and the context information nearby the triple. These classification views are implemented by a unified neural network and the classification predictions are weightedly integrated to obtain the final evaluation. Experiments show that, the multi-view model brings very significant improvements over previous methods, and achieves the new state-of-the-art on two representative datasets. We believe that, the flexibility and the scalability of the multi-view classification model facilitates the introduction of additional information and resources for better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.77/>ExpanRL : Hierarchical Reinforcement Learning for Course Concept Expansion in MOOCs<span class=acl-fixed-case>E</span>xpan<span class=acl-fixed-case>RL</span>: Hierarchical Reinforcement Learning for Course Concept Expansion in <span class=acl-fixed-case>MOOC</span>s</a></strong><br><a href=/people/j/jifan-yu/>Jifan Yu</a>
|
<a href=/people/c/chenyu-wang/>Chenyu Wang</a>
|
<a href=/people/g/gan-luo/>Gan Luo</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--77><div class="card-body p-3 small">Within the prosperity of Massive Open Online Courses (MOOCs), the education applications that automatically provide extracurricular knowledge for MOOC users become rising research topics. However, MOOC courses&#8217; diversity and rapid updates make it more challenging to find suitable new knowledge for students. In this paper, we present ExpanRL, an end-to-end hierarchical reinforcement learning (HRL) model for concept expansion in <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a>. Employing a two-level HRL mechanism of seed selection and concept expansion, ExpanRL is more feasible to adjust the expansion strategy to find new concepts based on the students&#8217; feedback on expansion results. Our experiments on nine novel datasets from real MOOCs show that ExpanRL achieves significant improvements over existing methods and maintain competitive performance under different settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.89/>You May Like This Hotel Because... : Identifying Evidence for Explainable Recommendations</a></strong><br><a href=/people/s/shin-kanouchi/>Shin Kanouchi</a>
|
<a href=/people/m/masato-neishi/>Masato Neishi</a>
|
<a href=/people/y/yuta-hayashibe/>Yuta Hayashibe</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--89><div class="card-body p-3 small">Explainable recommendation is a good way to improve user satisfaction. However, explainable recommendation in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> is challenging since it has to handle <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> as both input and output. To tackle the challenge, this paper proposes a novel and practical task to explain evidences in recommending hotels given vague requests expressed freely in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We decompose the process into two subtasks on hotel reviews : Evidence Identification and Evidence Explanation. The former predicts whether or not a sentence contains evidence that expresses why a given request is satisfied. The latter generates a <a href=https://en.wikipedia.org/wiki/Sentence_(law)>recommendation sentence</a> given a request and an <a href=https://en.wikipedia.org/wiki/Sentence_(law)>evidence sentence</a>. In order to address these subtasks, we build an Evidence-based Explanation dataset, which is the largest <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for explaining evidences in recommending hotels for vague requests. The experimental results demonstrate that the BERT model can find evidence sentences with respect to various vague requests and that the LSTM-based model can generate recommendation sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.90/>A Unified Framework for Multilingual and Code-Mixed Visual Question Answering</a></strong><br><a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/p/pabitra-lenka/>Pabitra Lenka</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--90><div class="card-body p-3 small">In this paper, we propose an effective deep learning framework for multilingual and code- mixed visual question answering. The pro- posed model is capable of predicting answers from the questions in Hindi, English or Code- mixed (Hinglish : Hindi-English) languages. The majority of the existing techniques on Vi- sual Question Answering (VQA) focus on En- glish questions only. However, many applica- tions such as <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical imaging</a>, <a href=https://en.wikipedia.org/wiki/Tourism>tourism</a>, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust tech- nique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the dif- ferent languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image fea- tures. We perform extensive evaluation and ablation studies for English, Hindi and Code- mixed VQA. The evaluation shows that the proposed multilingual model achieves state-of- the-art performance in all these settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-main.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-main--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-main.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-main.92/>Measuring What Counts : The Case of Rumour Stance Classification</a></strong><br><a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/d/diego-silva/>Diego Silva</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-main--92><div class="card-body p-3 small">Stance classification can be a powerful tool for understanding whether and which users believe in <a href=https://en.wikipedia.org/wiki/Rumor>online rumours</a>. The task aims to automatically predict the stance of replies towards a given <a href=https://en.wikipedia.org/wiki/Rumor>rumour</a>, namely support, deny, question, or comment. Numerous methods have been proposed and their performance compared in the RumourEval shared tasks in 2017 and 2019. Results demonstrated that this is a challenging problem since naturally occurring rumour stance data is highly imbalanced. This paper specifically questions the evaluation metrics used in these shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We re-evaluate the systems submitted to the two RumourEval tasks and show that the two widely adopted metrics <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and macro-F1 are not robust for the four-class imbalanced task of rumour stance classification, as they wrongly favour systems with highly skewed accuracy towards the majority class. To overcome this problem, we propose new evaluation metrics for rumour stance detection. These are not only robust to imbalanced data but also score higher systems that are capable of recognising the two most informative minority classes (support and deny).</div></div></div><hr><div id=2020aacl-srw><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.aacl-srw/>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-srw.0/>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop</a></strong><br><a href=/people/b/boaz-shmueli/>Boaz Shmueli</a>
|
<a href=/people/y/yin-jou-huang/>Yin Jou Huang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-srw.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-srw--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-srw.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-srw.5/>Automatic Classification of Students on Twitter Using Simple Profile Information<span class=acl-fixed-case>T</span>witter Using Simple Profile Information</a></strong><br><a href=/people/l/lili-michal-wilson/>Lili-Michal Wilson</a>
|
<a href=/people/c/christopher-wun/>Christopher Wun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-srw--5><div class="card-body p-3 small">Obtaining social media demographic information using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> is important for efficient computational social science research. Automatic age classification has been accomplished with relative success and allows for the study of youth populations, but student classificationdetermining which users are currently attending an academic institutionhas not been thoroughly studied. Previous work (He et al., 2016) proposes a model which utilizes 3 tweet-content features to classify users as students or non-students. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 84 %, but is restrictive and time intensive because it requires accessing and processing many user tweets. In this study, we propose <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification models</a> which use 7 <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>numerical features</a> and 10 <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>text-based features</a> drawn from simple profile information. These profile-based features allow for faster, more accessible data collection and enable the classification of users without needing access to their tweets. Compared to previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> identify students with greater <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ; our best model obtains an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 88.1 % and an F1 score of.704. This improved student identification tool has the potential to facilitate research on topics ranging from <a href=https://en.wikipedia.org/wiki/Professional_network>professional networking</a> to the impact of <a href=https://en.wikipedia.org/wiki/Education>education</a> on Twitter behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-srw.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-srw--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-srw.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-srw.16/>A Review of Cross-Domain Text-to-SQL Models<span class=acl-fixed-case>SQL</span> Models</a></strong><br><a href=/people/y/yujian-gan/>Yujian Gan</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a>
|
<a href=/people/j/john-r-woodward/>John R. Woodward</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-srw--16><div class="card-body p-3 small">WikiSQL and Spider, the large-scale cross-domain text-to-SQL datasets, have attracted much attention from the research community. The leaderboards of WikiSQL and Spider show that many researchers propose their models trying to solve the text-to-SQL problem. This paper first divides the top <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in these two leaderboards into two paradigms. We then present details not mentioned in their original paper by evaluating the key components, including schema linking, pretrained word embeddings, and reasoning assistance modules. Based on the analysis of these models, we want to promote understanding of the text-to-SQL field and find out some interesting future works, for example, it is worth studying the text-to-SQL problem in an environment where it is more challenging to build schema linking and also worth studying combing the advantage of each model toward text-to-SQL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-srw.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-srw--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-srw.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-srw.20/>Exploring Statistical and Neural Models for Noun Ellipsis Detection and Resolution in English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/p/payal-khullar/>Payal Khullar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-srw--20><div class="card-body p-3 small">Computational approaches to noun ellipsis resolution has been sparse, with only a naive rule-based approach that uses syntactic feature constraints for marking noun ellipsis licensors and selecting their antecedents. In this paper, we further the ellipsis research by exploring several statistical and neural models for both the subtasks involved in the ellipsis resolution process and addressing the representation and contribution of manual features proposed in previous research. Using the best performing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, we build an end-to-end supervised Machine Learning (ML) framework for this task that improves the existing <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> by 16.55 % for the <a href=https://en.wikipedia.org/wiki/Detection>detection</a> and 14.97 % for the resolution subtask. Our experiments demonstrate robust scores through pretrained BERT (Bidirectional Encoder Representations from Transformers) embeddings for word representation, and more so the importance of manual features once again highlighting the syntactic and semantic characteristics of the ellipsis phenomenon. For the classification decision, we notice that a simple Multilayar Perceptron (MLP) works well for the detection of ellipsis ; however, Recurrent Neural Networks (RNN) are a better choice for the much harder resolution step.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-srw.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-srw--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-srw.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-srw.22/>Text Simplification with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> Using <a href=https://en.wikipedia.org/wiki/Supervised_learning>Supervised Rewards</a> on <a href=https://en.wikipedia.org/wiki/Grammaticality>Grammaticality</a>, Meaning Preservation, and <a href=https://en.wikipedia.org/wiki/Simplicity>Simplicity</a></a></strong><br><a href=/people/a/akifumi-nakamachi/>Akifumi Nakamachi</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-srw--22><div class="card-body p-3 small">We optimize rewards of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> using <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that are highly correlated with human-perspectives. To address problems of <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> and loss-evaluation mismatch, text-to-text generation tasks employ <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> that rewards task-specific metrics. Previous studies in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> employ the weighted sum of sub-rewards from three perspectives : <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, meaning preservation, and <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a>. However, the previous rewards do not align with <a href=https://en.wikipedia.org/wiki/Human_psychology>human-perspectives</a> for these <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspectives</a>. In this study, we propose to use BERT regressors fine-tuned for <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, meaning preservation, and <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a> as reward estimators to achieve text simplification conforming to human-perspectives. Experimental results show that <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with our <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>rewards</a> balances meaning preservation and <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a>. Additionally, human evaluation confirmed that simplified texts by our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> are preferred by humans compared to previous studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-srw.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-srw--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-srw.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-srw.23/>Label Representations in Modeling Classification as Text Generation</a></strong><br><a href=/people/x/xinyi-chen/>Xinyi Chen</a>
|
<a href=/people/j/jingxian-xu/>Jingxian Xu</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-srw--23><div class="card-body p-3 small">Several recent state-of-the-art transfer learning methods model classification tasks as text generation, where labels are represented as strings for the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to generate. We investigate the effect that the choice of strings used to represent labels has on how effectively the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. For four standard text classification tasks, we design a diverse set of possible <a href=https://en.wikipedia.org/wiki/String_(computer_science)>string representations</a> for labels, ranging from canonical label definitions to random strings. We experiment with T5 on these tasks, varying the label representations as well as the amount of training data. We find that, in the low data setting, label representation impacts task performance on some tasks, with task-related labels being most effective, but fails to have an impact on others. In the full data setting, our results are largely negative : Different label representations do not affect overall task performance.</div></div></div><hr><div id=2020aacl-demo><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.aacl-demo/>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-demo.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-demo.0/>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/d/derek-wong/>Derek Wong</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-demo.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-demo--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-demo.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-demo.2/>AutoNLU : An On-demand Cloud-based Natural Language Understanding System for Enterprises<span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>NLU</span>: An On-demand Cloud-based Natural Language Understanding System for Enterprises</a></strong><br><a href=/people/n/nham-le/>Nham Le</a>
|
<a href=/people/t/tuan-lai/>Tuan Lai</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-demo--2><div class="card-body p-3 small">With the renaissance of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> have achieved promising results on many natural language understanding (NLU) tasks. Even though the source codes of many neural network models are publicly available, there is still a large gap from open-sourced models to solving real-world problems in enterprises. Therefore, to fill this gap, we introduce AutoNLU, an on-demand cloud-based system with an easy-to-use interface that covers all common use-cases and steps in developing an NLU model. AutoNLU has supported many product teams within Adobe with different use-cases and datasets, quickly delivering them working models. To demonstrate the effectiveness of AutoNLU, we present two case studies. i) We build a practical NLU model for handling various image-editing requests in <a href=https://en.wikipedia.org/wiki/Adobe_Photoshop>Photoshop</a>. ii) We build powerful keyphrase extraction models that achieve state-of-the-art results on two public benchmarks. In both cases, end users only need to write a small amount of code to convert their datasets into a common format used by AutoNLU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.aacl-demo.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--aacl-demo--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.aacl-demo.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.aacl-demo.3/>ISA : An Intelligent Shopping Assistant<span class=acl-fixed-case>ISA</span>: An Intelligent Shopping Assistant</a></strong><br><a href=/people/t/tuan-lai/>Tuan Lai</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/n/nedim-lipka/>Nedim Lipka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--aacl-demo--3><div class="card-body p-3 small">Despite the growth of <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a>, brick-and-mortar stores are still the preferred destinations for many people. In this paper, we present ISA, a mobile-based intelligent shopping assistant that is designed to improve shopping experience in physical stores. ISA assists users by leveraging advanced techniques in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, <a href=https://en.wikipedia.org/wiki/Speech_processing>speech processing</a>, and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. An in-store user only needs to take a picture or scan the barcode of the product of interest, and then the user can talk to the assistant about the product. The assistant can also guide the user through the purchase process or recommend other similar products to the user. We take a data-driven approach in building the engines of <a href=https://en.wikipedia.org/wiki/Industry_Standard_Architecture>ISA</a>&#8217;s natural language processing component, and the <a href=https://en.wikipedia.org/wiki/Engine>engines</a> achieve good performance.</div></div></div><hr><div id=2020iwdp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.iwdp-1/>Proceedings of the Second International Workshop of Discourse Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwdp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwdp-1.0/>Proceedings of the Second International Workshop of Discourse Processing</a></strong><br><a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/s/shili-ge/>Shili Ge</a>
|
<a href=/people/x/xiaojun-zhang/>Xiaojun Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwdp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwdp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwdp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.iwdp-1.5.Dataset.rar data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwdp-1.5/>Context-Aware Word Segmentation for Chinese Real-World Discourse<span class=acl-fixed-case>C</span>hinese Real-World Discourse</a></strong><br><a href=/people/k/kaiyu-huang/>Kaiyu Huang</a>
|
<a href=/people/j/junpeng-liu/>Junpeng Liu</a>
|
<a href=/people/j/jingxiang-cao/>Jingxiang Cao</a>
|
<a href=/people/d/degen-huang/>Degen Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwdp-1--5><div class="card-body p-3 small">Previous neural approaches achieve significant progress for Chinese word segmentation (CWS) as a sentence-level task, but it suffers from limitations on real-world scenario. In this paper, we address this issue with a context-aware method and optimize the <a href=https://en.wikipedia.org/wiki/Solution>solution</a> at document-level. This paper proposes a three-step strategy to improve the performance for discourse CWS. First, the method utilizes an auxiliary segmenter to remedy the limitation on pre-segmenter. Then the context-aware algorithm computes the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence</a> of each split. The maximum probability path is reconstructed via this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. Besides, in order to evaluate the performance in <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>, we build a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> consisting of the latest news and Chinese medical articles. Extensive experiments on this <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> show that our proposed method achieves a competitive performance on a document-level real-world scenario for CWS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwdp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwdp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwdp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwdp-1.6/>Neural Abstractive Multi-Document Summarization : Hierarchical or Flat Structure?</a></strong><br><a href=/people/y/ye-ma/>Ye Ma</a>
|
<a href=/people/l/lu-zong/>Lu Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwdp-1--6><div class="card-body p-3 small">With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT & VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively. A comprehensive evaluation is conducted on WikiSum to compare PHT & VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models. Moreover, we recommend PHT given its practical value of higher <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speed</a> and greater memory-saving capacity.</div></div></div><hr><div id=2020knlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.knlp-1/>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.knlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.knlp-1.0/>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</a></strong><br><a href=/people/o/oren-sar-shalom/>Oren Sar Shalom</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero dos Santos</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.knlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--knlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.knlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.knlp-1.3/>Social Media Medical Concept Normalization using RoBERTa in Ontology Enriched Text Similarity Framework<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a in Ontology Enriched Text Similarity Framework</a></strong><br><a href=/people/k/katikapalli-subramanyam-kalyan/>Katikapalli Subramanyam Kalyan</a>
|
<a href=/people/s/sivanesan-sangeetha/>Sivanesan Sangeetha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--knlp-1--3><div class="card-body p-3 small">Pattisapu et al. (2020) formulate medical concept normalization (MCN) as text similarity problem and propose a model based on RoBERTa and graph embedding based target concept vectors. However, graph embedding techniques ignore valuable information available in the clinical ontology like <a href=https://en.wikipedia.org/wiki/Conceptual_model>concept description</a> and synonyms. In this work, we enhance the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> of Pattisapu et al. (2020) with two novel changes. First, we use retrofitted target concept vectors instead of graph embedding based vectors. It is the first work to leverage both concept description and synonyms to represent concepts in the form of retrofitted target concept vectors in text similarity framework based social media MCN. Second, we generate both concept and concept mention vectors with same size which eliminates the need of dense layers to project concept mention vectors into the target concept embedding space. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> with improvements up to 3.75 % on two standard datasets. Further when trained only on mapping lexicon synonyms, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> with significant improvements up to 14.61 %. We attribute these significant improvements to the two novel changes introduced.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.knlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--knlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.knlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.knlp-1.4/>BERTChem-DDI : Improved Drug-Drug Interaction Prediction from text using Chemical Structure Information<span class=acl-fixed-case>BERTC</span>hem-<span class=acl-fixed-case>DDI</span> : Improved Drug-Drug Interaction Prediction from text using Chemical Structure Information</a></strong><br><a href=/people/i/ishani-mondal/>Ishani Mondal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--knlp-1--4><div class="card-body p-3 small">Traditional biomedical version of embeddings obtained from pre-trained language models have recently shown state-of-the-art results for relation extraction (RE) tasks in the medical domain. In this paper, we explore how to incorporate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>, available in the form of molecular structure of drugs, for predicting Drug-Drug Interaction from <a href=https://en.wikipedia.org/wiki/Text_corpus>textual corpus</a>. We propose a method, BERTChem-DDI, to efficiently combine drug embeddings obtained from the rich chemical structure of drugs (encoded in SMILES) along with off-the-shelf domain-specific BioBERT embedding-based RE architecture. Experiments conducted on the DDIExtraction 2013 corpus clearly indicate that this <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> improves other <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>strong baselines architectures</a> by 3.4 % <a href=https://en.wikipedia.org/wiki/Macro_(computer_science)>macro F1-score</a>.</div></div></div><hr><div id=2020lifelongnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.lifelongnlp-1/>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lifelongnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lifelongnlp-1.0/>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</a></strong><br><a href=/people/w/william-m-campbell/>William M. Campbell</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/t/timothy-j-hazen/>Timothy J. Hazen</a>
|
<a href=/people/k/kevin-kilgour/>Kevin Kilgour</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/v/varun-kumar/>Varun Kumar</a>
|
<a href=/people/h/hadrien-glaude/>Hadrien Glaude</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.lifelongnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--lifelongnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.lifelongnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.lifelongnlp-1.2/>Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting</a></strong><br><a href=/people/c/christian-huber/>Christian Huber</a>
|
<a href=/people/j/juan-hussain/>Juan Hussain</a>
|
<a href=/people/t/tuan-nam-nguyen/>Tuan-Nam Nguyen</a>
|
<a href=/people/k/kaihang-song/>Kaihang Song</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--lifelongnlp-1--2><div class="card-body p-3 small">When training <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition systems</a>, one often faces the situation that sufficient amounts of training data for the language in question are available but only small amounts of data for the domain in question. This problem is even bigger for end-to-end speech recognition systems that only accept transcribed speech as training data, which is harder and more expensive to obtain than text data. In this paper we present experiments in adapting end-to-end speech recognition systems by a method which is called batch-weighting and which we contrast against <a href=https://en.wikipedia.org/wiki/Fine-tuning>regular fine-tuning</a>, i.e., to continue to train existing neural speech recognition models on <a href=https://en.wikipedia.org/wiki/Adaptation_data>adaptation data</a>. We perform experiments using these s techniques in adapting to topic, accent and <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, showing that batch-weighting consistently outperforms <a href=https://en.wikipedia.org/wiki/Musical_tuning>fine-tuning</a>. In order to show the generalization capabilities of batch-weighting we perform experiments in several languages, i.e., <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Due to its relatively small computational requirements <a href=https://en.wikipedia.org/wiki/Batch_processing>batch-weighting</a> is a suitable technique for <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised life-long learning</a> during the life-time of a <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition system</a>, e.g., from user corrections.</div></div></div><hr><div id=2020loresmt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.loresmt-1/>Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.0/>Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages</a></strong><br><a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a>
|
<a href=/people/j/jade-abbott/>Jade Abbott</a>
|
<a href=/people/j/john-ortega/>John Ortega</a>
|
<a href=/people/j/jonathan-washington/>Jonathan Washington</a>
|
<a href=/people/n/nathaniel-oco/>Nathaniel Oco</a>
|
<a href=/people/s/surafel-melaku-lakew/>Surafel Melaku Lakew</a>
|
<a href=/people/t/tommi-a-pirinen/>Tommi A Pirinen</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/x/xiaobing-zhao/>Xiaobing Zhao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--loresmt-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.loresmt-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.2/>Bridging Philippine Languages With Multilingual Neural Machine Translation<span class=acl-fixed-case>P</span>hilippine Languages With Multilingual Neural Machine Translation</a></strong><br><a href=/people/r/renz-iver-baliber/>Renz Iver Baliber</a>
|
<a href=/people/c/charibeth-cheng/>Charibeth Cheng</a>
|
<a href=/people/k/kristine-mae-adlaon/>Kristine Mae Adlaon</a>
|
<a href=/people/v/virgion-mamonong/>Virgion Mamonong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--loresmt-1--2><div class="card-body p-3 small">The Philippines is home to more than 150 languages that is considered to be low-resourced even on its major languages. This results into a lack of pursuit in developing a <a href=https://en.wikipedia.org/wiki/Translation>translation system</a> for the underrepresented languages. To simplify the process of developing translation system for multiple languages, and to aid in improving the translation quality of zero to low-resource languages, multilingual NMT became an active area of research. However, existing works in multilingual NMT disregards the analysis of a multilingual model on a closely related and low-resource language group in the context of pivot-based translation and zero-shot translation. In this paper, we benchmarked <a href=https://en.wikipedia.org/wiki/Translation>translation</a> for several <a href=https://en.wikipedia.org/wiki/Languages_of_the_Philippines>Philippine Languages</a>, provided an analysis of a multilingual NMT system for morphologically rich and low-resource languages in terms of its effectiveness in translating zero-resource languages with zero-shot translations. To further evaluate the capability of the multilingual NMT model in translating unseen language pairs in training, we tested the model to translate between <a href=https://en.wikipedia.org/wiki/Tagalog_language>Tagalog</a> and <a href=https://en.wikipedia.org/wiki/Cebuano_language>Cebuano</a> and compared its performance with a simple NMT model that is directly trained on a parallel Tagalog and Cebuano data in which we showed that zero-shot translation outperforms a directly trained model in some instances, while utilizing English as a pivot language in translating outperform both approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--loresmt-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.loresmt-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.4/>Findings of the LoResMT 2020 Shared Task on Zero-Shot for Low-Resource languages<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>R</span>es<span class=acl-fixed-case>MT</span> 2020 Shared Task on Zero-Shot for Low-Resource languages</a></strong><br><a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a>
|
<a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--loresmt-1--4><div class="card-body p-3 small">This paper presents the findings of the LoResMT 2020 Shared Task on zero-shot translation for low resource languages. This task was organised as part of the 3rd Workshop on Technologies for MT of Low Resource Languages (LoResMT) at AACL-IJCNLP 2020. The focus was on the zero-shot approach as a notable development in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> to build MT systems for language pairs where parallel corpora are small or even non-existent. The shared task experience suggests that back-translation and domain adaptation methods result in better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for small-size datasets. We further noted that, although <a href=https://en.wikipedia.org/wiki/Translation>translation</a> between similar languages is no cakewalk, linguistically distinct languages require more data to give better results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--loresmt-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.loresmt-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.5/>Zero-Shot Neural Machine Translation : Russian-Hindi @LoResMT 2020<span class=acl-fixed-case>R</span>ussian-<span class=acl-fixed-case>H</span>indi @<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>R</span>es<span class=acl-fixed-case>MT</span> 2020</a></strong><br><a href=/people/s/sahinur-rahman-laskar/>Sahinur Rahman Laskar</a>
|
<a href=/people/a/abdullah-faiz-ur-rahman-khilji/>Abdullah Faiz Ur Rahman Khilji</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--loresmt-1--5><div class="card-body p-3 small">Neural machine translation (NMT) is a widely accepted approach in the machine translation (MT) community, translating from one natural language to another natural language. Although, NMT shows remarkable performance in both high and low resource languages, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> needs sufficient training corpus. The availability of a <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> in low resource language pairs is one of the challenging tasks in MT. To mitigate this issue, NMT attempts to utilize a monolingual corpus to get better at <a href=https://en.wikipedia.org/wiki/Translation>translation</a> for low resource language pairs. Workshop on Technologies for MT of Low Resource Languages (LoResMT 2020) organized shared tasks of low resource language pair translation using zero-shot NMT. Here, the <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> is not used and only monolingual corpora is allowed. We have participated in the same shared task with our team name CNLP-NITS for the Russian-Hindi language pair. We have used masked sequence to sequence pre-training for language generation (MASS) with only monolingual corpus following the unsupervised NMT architecture. The evaluated results are declared at the LoResMT 2020 shared task, which reports that our system achieves the bilingual evaluation understudy (BLEU) score of 0.59, precision score of 3.43, recall score of 5.48, F-measure score of 4.22, and rank-based intuitive bilingual evaluation score (RIBES) of 0.180147 in Russian to Hindi translation. And for Hindi to Russian translation, we have achieved <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, <a href=https://en.wikipedia.org/wiki/Precision_and_recall>recall</a>, <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>, and <a href=https://en.wikipedia.org/wiki/International_Bureau_of_Weights_and_Measures>RIBES score</a> of 1.11, 4.72, 4.41, 4.56, and 0.026842 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--loresmt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.loresmt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.6/>Unsupervised Approach for Zero-Shot Experiments : BhojpuriHindi and MagahiHindi@LoResMT 2020<span class=acl-fixed-case>B</span>hojpuri–<span class=acl-fixed-case>H</span>indi and <span class=acl-fixed-case>M</span>agahi–<span class=acl-fixed-case>H</span>indi@<span class=acl-fixed-case>L</span>o<span class=acl-fixed-case>R</span>es<span class=acl-fixed-case>MT</span> 2020</a></strong><br><a href=/people/a/amit-kumar/>Amit Kumar</a>
|
<a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--loresmt-1--6><div class="card-body p-3 small">This paper reports a Machine Translation (MT) system submitted by the NLPRL team for the BhojpuriHindi and MagahiHindi language pairs at LoResMT 2020 shared task. We used an unsupervised domain adaptation approach that gives promising results for zero or extremely low resource languages. Task organizers provide the development and the test sets for evaluation and the monolingual data for training. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is a hybrid approach of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> and <a href=https://en.wikipedia.org/wiki/Translation_(biology)>back-translation</a>. Metrics used to evaluate the trained model are BLEU, RIBES, <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision</a>, Recall and <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>. Our approach gives relatively promising results, with a wide range, of 19.5, 13.71, 2.54, and 3.16 BLEU points for <a href=https://en.wikipedia.org/wiki/Bhojpuri_language>Bhojpuri</a> to <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, Magahi to Hindi, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to Bhojpuri and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi to Magahi language pairs</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--loresmt-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.loresmt-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.loresmt-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.12/>Towards Machine Translation for the <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish Language</a><span class=acl-fixed-case>K</span>urdish Language</a></strong><br><a href=/people/s/sina-ahmadi/>Sina Ahmadi</a>
|
<a href=/people/m/maraim-masoud/>Maraim Masoud</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--loresmt-1--12><div class="card-body p-3 small">Machine translation is the task of translating texts from one language to another using <a href=https://en.wikipedia.org/wiki/Computer>computers</a>. It has been one of the major tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and has been motivating to facilitate <a href=https://en.wikipedia.org/wiki/Human_communication>human communication</a>. Kurdish, an <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language</a>, has received little attention in this realm due to the language being less-resourced. Therefore, in this paper, we are addressing the main issues in creating a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> for the <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish language</a>, with a focus on the <a href=https://en.wikipedia.org/wiki/Sorani>Sorani dialect</a>. We describe the available scarce <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> suitable for training a neural machine translation model for Sorani Kurdish-English translation. We also discuss some of the major challenges in Kurdish language translation and demonstrate how fundamental text processing tasks, such as <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>, can improve <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.loresmt-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--loresmt-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.loresmt-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.loresmt-1.15/>Investigating Low-resource Machine Translation for English-to-Tamil<span class=acl-fixed-case>E</span>nglish-to-<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/a/akshai-ramesh/>Akshai Ramesh</a>
|
<a href=/people/v/venkatesh-balavadhani-parthasa/>Venkatesh Balavadhani parthasa</a>
|
<a href=/people/r/rejwanul-haque/>Rejwanul Haque</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--loresmt-1--15><div class="card-body p-3 small">Statistical machine translation (SMT) which was the dominant paradigm in machine translation (MT) research for nearly three decades has recently been superseded by the end-to-end deep learning approaches to MT. Although <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a> produce state-of-the-art results in many translation tasks, they are found to under-perform on resource-poor scenarios. Despite some success, none of the present-day benchmarks that have tried to overcome this problem can be regarded as a universal solution to the problem of <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of many low-resource languages. In this work, we investigate the performance of phrase-based SMT (PB-SMT) and neural MT (NMT) on a rarely-tested low-resource language-pair, English-to-Tamil, taking a specialised data domain (software localisation) into consideration. In particular, we produce rankings of our MT systems via a social media platform-based human evaluation scheme, and demonstrate our findings in the low-resource domain-specific text translation task.</div></div></div><hr><div id=2020nlptea-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.nlptea-1/>Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlptea-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlptea-1.0/>Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications</a></strong><br><a href=/people/e/erhong-yang/>Erhong YANG</a>
|
<a href=/people/e/endong-xun/>Endong XUN</a>
|
<a href=/people/b/baolin-zhang/>Baolin ZHANG</a>
|
<a href=/people/g/gaoqi-rao/>Gaoqi RAO</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlptea-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlptea-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlptea-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlptea-1.7/>Integrating BERT and Score-based Feature Gates for Chinese Grammatical Error Diagnosis<span class=acl-fixed-case>BERT</span> and Score-based Feature Gates for <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis</a></strong><br><a href=/people/y/yongchang-cao/>Yongchang Cao</a>
|
<a href=/people/l/liang-he/>Liang He</a>
|
<a href=/people/r/robert-ridley/>Robert Ridley</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlptea-1--7><div class="card-body p-3 small">This paper describes our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the Chinese Grammatical Error Diagnosis (CGED) task in NLPTEA2020. The goal of CGED is to use <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing techniques</a> to automatically diagnose <a href=https://en.wikipedia.org/wiki/Chinese_grammar>Chinese grammatical errors</a> in sentences. To this end, we design and implement a CGED model named BERT with Score-feature Gates Error Diagnoser (BSGED), which is based on the BERT model, Bidirectional Long Short-Term Memory (BiLSTM) and conditional random field (CRF). In order to address the problem of losing partial-order relationships when embedding continuous feature items as with previous works, we propose a gating mechanism for integrating continuous feature items, which effectively retains the <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>partial-order relationships</a> between feature items. We perform LSTM processing on the encoding result of the BERT model, and further extract the sequence features. In the final test-set evaluation, we obtained the highest F1 score at the detection level and are among the top 3 F1 scores at the identification level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlptea-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlptea-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlptea-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlptea-1.12/>CYUT Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2020 CGED Shared Task<span class=acl-fixed-case>CYUT</span> Team <span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis System Report in <span class=acl-fixed-case>NLPTEA</span>-2020 <span class=acl-fixed-case>CGED</span> Shared Task</a></strong><br><a href=/people/s/shih-hung-wu/>Shih-Hung Wu</a>
|
<a href=/people/j/junwei-wang/>Junwei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlptea-1--12><div class="card-body p-3 small">This paper reports our Chinese Grammatical Error Diagnosis system in the NLPTEA-2020 CGED shared task. In 2020, we sent two runs with two <a href=https://en.wikipedia.org/wiki/Glossary_of_baseball_(S)>approaches</a>. The first one is a combination of conditional random fields (CRF) and a BERT model deep-learning approach. The second one is a BERT model deep-learning approach. The official results shows that our run1 achieved the highest precision rate 0.9875 with the lowest <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positive rate</a> 0.0163 on detection, while run2 gives a more balanced performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlptea-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlptea-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlptea-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlptea-1.14/>Chinese Grammatical Errors Diagnosis System Based on BERT at NLPTEA-2020 CGED Shared Task<span class=acl-fixed-case>C</span>hinese Grammatical Errors Diagnosis System Based on <span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>NLPTEA</span>-2020 <span class=acl-fixed-case>CGED</span> Shared Task</a></strong><br><a href=/people/h/hongying-zan/>Hongying Zan</a>
|
<a href=/people/y/yangchao-han/>Yangchao Han</a>
|
<a href=/people/h/haotian-huang/>Haotian Huang</a>
|
<a href=/people/y/yingjie-yan/>Yingjie Yan</a>
|
<a href=/people/y/yuke-wang/>Yuke Wang</a>
|
<a href=/people/y/yingjie-han/>Yingjie Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlptea-1--14><div class="card-body p-3 small">In the process of learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Second-language_acquisition>second language learners</a> may have various <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> due to the negative transfer of native language. This paper describes our submission to the NLPTEA 2020 shared task on CGED. We present a <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a> that utilizes both detection and correction stages. The detection stage is a sequential labelling model based on BiLSTM-CRF and BERT contextual word representation. The correction stage is a hybrid model based on the <a href=https://en.wikipedia.org/wiki/N-gram>n-gram</a> and Seq2Seq. Without adding additional features and external data, the BERT contextual word representation can effectively improve the performance metrics of Chinese grammatical error detection and correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nlptea-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nlptea-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nlptea-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nlptea-1.17/>SEMA : Text Simplification Evaluation through Semantic Alignment<span class=acl-fixed-case>SEMA</span>: Text Simplification Evaluation through Semantic Alignment</a></strong><br><a href=/people/x/xuan-zhang/>Xuan Zhang</a>
|
<a href=/people/h/huizhou-zhao/>Huizhou Zhao</a>
|
<a href=/people/k/kexin-zhang/>KeXin Zhang</a>
|
<a href=/people/y/yiyang-zhang/>Yiyang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nlptea-1--17><div class="card-body p-3 small">Text simplification is an important branch of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. At present, methods used to evaluate the semantic retention of <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> are mostly based on <a href=https://en.wikipedia.org/wiki/String_matching>string matching</a>. We propose the SEMA (text Simplification Evaluation Measure through Semantic Alignment), which is based on semantic alignment. Semantic alignments include complete alignment, partial alignment and <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy alignment</a>. Our experiments show that the evaluation results of SEMA have a high consistency with human evaluation for the simplified corpus of Chinese and English news texts.</div></div></div><hr><div id=2020wat-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wat-1/>Proceedings of the 7th Workshop on Asian Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.0/>Proceedings of the 7th Workshop on Asian Translation</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hidaya-mino/>Hidaya Mino</a>
|
<a href=/people/h/hiroshi-manabe/>Hiroshi Manabe</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.5/>Meta Ensemble for Japanese-Chinese Neural Machine Translation : Kyoto-U+ECNU Participation to WAT 2020<span class=acl-fixed-case>J</span>apanese-<span class=acl-fixed-case>C</span>hinese Neural Machine Translation: <span class=acl-fixed-case>K</span>yoto-<span class=acl-fixed-case>U</span>+<span class=acl-fixed-case>ECNU</span> Participation to <span class=acl-fixed-case>WAT</span> 2020</a></strong><br><a href=/people/z/zhuoyuan-mao/>Zhuoyuan Mao</a>
|
<a href=/people/y/yibin-shen/>Yibin Shen</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/c/cheqing-jin/>Cheqing Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--5><div class="card-body p-3 small">This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> into a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.8/>HW-TSC’s Participation in the WAT 2020 Indic Languages Multilingual Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WAT</span> 2020 Indic Languages Multilingual Task</a></strong><br><a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--8><div class="card-body p-3 small">This paper describes our work in the WAT 2020 Indic Multilingual Translation Task. We participated in all 7 language pairs (En-Bn / Hi / Gu / Ml / Mr / Ta / Te) in both directions under the constrained conditionusing only the officially provided data. Using <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> as a baseline, our Multi-En and En-Multi translation systems achieve the best performances. Detailed data filtering and data domain selection are the keys to performance enhancement in our experiment, with an average improvement of 2.6 BLEU scores for each language pair in the En-Multi system and an average improvement of 4.6 BLEU scores regarding the Multi-En. In addition, we employed language independent adapter to further improve the <a href=https://en.wikipedia.org/wiki/System>system</a> performances. Our submission obtains competitive results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.11/>Multimodal Neural Machine Translation for English to Hindi<span class=acl-fixed-case>E</span>nglish to <span class=acl-fixed-case>H</span>indi</a></strong><br><a href=/people/s/sahinur-rahman-laskar/>Sahinur Rahman Laskar</a>
|
<a href=/people/a/abdullah-faiz-ur-rahman-khilji/>Abdullah Faiz Ur Rahman Khilji</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--11><div class="card-body p-3 small">Machine translation (MT) focuses on the <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translation</a> of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-the-art results in the task of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for <a href=https://en.wikipedia.org/wiki/Linguistic_conservatism>low resource languages</a>. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the <a href=https://en.wikipedia.org/wiki/System>system</a> for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multi-modal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves <a href=https://en.wikipedia.org/wiki/Bilingual_Evaluation_Understudy>Bilingual Evaluation Understudy (BLEU) score</a> of 33.57, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.14/>WT : Wipro AI Submissions to the WAT 2020<span class=acl-fixed-case>WT</span>: Wipro <span class=acl-fixed-case>AI</span> Submissions to the <span class=acl-fixed-case>WAT</span> 2020</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--14><div class="card-body p-3 small">In this paper we present an EnglishHindi and HindiEnglish neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show : (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> 20.80 and 29.59 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.17/>The ADAPT Centre’s Neural MT Systems for the WAT 2020 Document-Level Translation Task<span class=acl-fixed-case>ADAPT</span> Centre’s Neural <span class=acl-fixed-case>MT</span> Systems for the <span class=acl-fixed-case>WAT</span> 2020 Document-Level Translation Task</a></strong><br><a href=/people/w/wandri-jooste/>Wandri Jooste</a>
|
<a href=/people/r/rejwanul-haque/>Rejwanul Haque</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--17><div class="card-body p-3 small">In this paper we describe the ADAPT Centre&#8217;s submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) Translation task. We only consider translating from <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> for this task and we use the MarianNMT toolkit to train Transformer models. In order to improve the translation quality, we made use of both in-domain and out-of-domain data for training our Machine Translation (MT) systems, as well as various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we ran to train our <a href=https://en.wikipedia.org/wiki/System>systems</a> and report the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> achieved through these various experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.19/>Improving NMT via Filtered Back Translation<span class=acl-fixed-case>NMT</span> via Filtered Back Translation</a></strong><br><a href=/people/n/nikhil-jaiswal/>Nikhil Jaiswal</a>
|
<a href=/people/m/mayur-patidar/>Mayur Patidar</a>
|
<a href=/people/s/surabhi-kumari/>Surabhi Kumari</a>
|
<a href=/people/m/manasi-patwardhan/>Manasi Patwardhan</a>
|
<a href=/people/s/shirish-karande/>Shirish Karande</a>
|
<a href=/people/p/puneet-agarwal/>Puneet Agarwal</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--19><div class="card-body p-3 small">Document-Level Machine Translation (MT) has become an active research area among the NLP community in recent years. Unlike sentence-level MT, which translates the sentences independently, document-level MT aims to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> while translating a given source sentence. This paper demonstrates our submission (Team ID-DEEPNLP) to the Document-Level Translation task organized by WAT 2020. This task focuses on translating texts from a business dialog corpus while optionally utilizing the context present in the dialog. In our proposed approach, we utilize publicly available <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> from different domains to train an open domain base NMT model. We then use monolingual target data to create filtered pseudo parallel data and employ <a href=https://en.wikipedia.org/wiki/Back-translation>Back-Translation</a> to fine-tune the base model. This is further followed by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on the domain-specific corpus. We also ensemble various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to improvise the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve a <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> of 26.59 and 22.83 in an <a href=https://en.wikipedia.org/wiki/BLEU>unconstrained setting</a> and 15.10 and 10.91 in the <a href=https://en.wikipedia.org/wiki/BLEU>constrained settings</a> for <a href=https://en.wikipedia.org/wiki/BLEU>En-Ja & Ja-En direction</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wat-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wat-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wat-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wat-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wat-1.20/>A Parallel Evaluation Data Set of <a href=https://en.wikipedia.org/wiki/Software_documentation>Software Documentation</a> with Document Structure Annotation</a></strong><br><a href=/people/b/bianka-buschbeck/>Bianka Buschbeck</a>
|
<a href=/people/m/miriam-exel/>Miriam Exel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wat-1--20><div class="card-body p-3 small">This paper accompanies the software documentation data set for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, a parallel evaluation data set of data originating from the SAP Help Portal, that we released to the machine translation community for research purposes. It offers the possibility to tune and evaluate <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> in the domain of corporate software documentation and contributes to the availability of a wider range of evaluation scenarios. The data set comprises of the language pairs English to Hindi, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>, <a href=https://en.wikipedia.org/wiki/Malay_language>Malay</a> and <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a>, and thus also increases the test coverage for the many low-resource language pairs. Unlike most evaluation data sets that consist of plain parallel text, the segments in this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> come with additional metadata that describes structural information of the document context. We provide insights into the origin and creation, the particularities and characteristics of the <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> as well as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> results.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>