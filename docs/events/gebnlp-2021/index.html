<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Gender Bias in Natural Language Processing (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Gender Bias in Natural Language Processing (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021gebnlp-1>Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li></ul></div></div><div id=2021gebnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.gebnlp-1/>Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.0/>Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</a></strong><br><a href=/people/m/marta-costa-jussa/>Marta Costa-jussa</a>
|
<a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.2/>Gender Bias Hidden Behind Chinese Word Embeddings : The Case of Chinese Adjectives<span class=acl-fixed-case>C</span>hinese Word Embeddings: The Case of <span class=acl-fixed-case>C</span>hinese Adjectives</a></strong><br><a href=/people/m/meichun-jiao/>Meichun Jiao</a>
|
<a href=/people/z/ziyang-luo/>Ziyang Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--2><div class="card-body p-3 small">Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with <a href=https://en.wikipedia.org/wiki/English_language>English</a> as the target language. This paper investigates <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in static word embeddings from a unique perspective, <a href=https://en.wikipedia.org/wiki/Chinese_adjectives>Chinese adjectives</a>. By training word representations with different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, the <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people&#8217;s attitudes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.gebnlp-1.3.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.3/>Evaluating Gender Bias in Hindi-English Machine Translation<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Machine Translation</a></strong><br><a href=/people/k/krithika-ramesh/>Krithika Ramesh</a>
|
<a href=/people/g/gauri-gupta/>Gauri Gupta</a>
|
<a href=/people/s/sanjay-singh/>Sanjay Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--3><div class="card-body p-3 small">With <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indic languages</a>. In our work, we attempt to evaluate and quantify the <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We also compare and contrast the resulting bias measurements across multiple <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for pre-trained embeddings and the ones learned by our machine translation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gebnlp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.4/>Alexa, Google, Siri : What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants<span class=acl-fixed-case>A</span>lexa, <span class=acl-fixed-case>G</span>oogle, <span class=acl-fixed-case>S</span>iri: What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants</a></strong><br><a href=/people/g/gavin-abercrombie/>Gavin Abercrombie</a>
|
<a href=/people/a/amanda-cercas-curry/>Amanda Cercas Curry</a>
|
<a href=/people/m/mugdha-pandya/>Mugdha Pandya</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--4><div class="card-body p-3 small">Technology companies have produced varied responses to concerns about the effects of the design of their conversational AI systems. Some have claimed that their voice assistants are in fact not gendered or human-likedespite design features suggesting the contrary. We compare these claims to user perceptions by analysing the pronouns they use when referring to AI assistants. We also examine systems&#8217; responses and the extent to which they generate output which is gendered and anthropomorphic. We find that, while some companies appear to be addressing the ethical concerns raised, in some cases, their claims do not seem to hold true. In particular, our results show that system outputs are ambiguous as to the humanness of the systems, and that users tend to personify and gender them as a result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.5/>Gender Bias in Text : Origin, Taxonomy, and Implications</a></strong><br><a href=/people/j/jad-doughman/>Jad Doughman</a>
|
<a href=/people/w/wael-khreich/>Wael Khreich</a>
|
<a href=/people/m/maya-el-gharib/>Maya El Gharib</a>
|
<a href=/people/m/maha-wiss/>Maha Wiss</a>
|
<a href=/people/z/zahraa-berjawi/>Zahraa Berjawi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--5><div class="card-body p-3 small">Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning (ML)</a> have been shown to reflect and even amplify <a href=https://en.wikipedia.org/wiki/Gender_bias>gender biases</a> and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in English text, we develop a comprehensive taxonomy that relies on the following <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias types</a> : Generic Pronouns, <a href=https://en.wikipedia.org/wiki/Sexism>Sexism</a>, Occupational Bias, Exclusionary Bias, and <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a>. We also provide a bottom-up overview of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a>, from its societal origin to its spillover onto language. Finally, we link the societal implications of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> to their corresponding type(s) in the proposed <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a>. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.6/>Sexism in the Judiciary : The Importance of Bias Definition in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> and In Our Courts<span class=acl-fixed-case>NLP</span> and In Our Courts</a></strong><br><a href=/people/n/noa-baker-gillis/>Noa Baker Gillis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--6><div class="card-body p-3 small">We analyze 6.7 million case law documents to determine the presence of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> within our <a href=https://en.wikipedia.org/wiki/Judiciary>judicial system</a>. We find that current bias detection methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms&#8217; inconsistent results are consequences of prior research&#8217;s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent <a href=https://en.wikipedia.org/wiki/Bias>bias</a> (e.g., &#8216;salary,&#8217; &#8216;job,&#8217; and &#8216;boss&#8217; to represent <a href=https://en.wikipedia.org/wiki/Employment>employment</a> as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers&#8217; own intuitions. We suggest two new methods of automating the creation of word lists to represent <a href=https://en.wikipedia.org/wiki/Bias>biases</a>. We find that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> outperform current NLP bias detection methods. Our research improves the capabilities of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP technology</a> to detect <a href=https://en.wikipedia.org/wiki/Bias>bias</a> and highlights <a href=https://en.wikipedia.org/wiki/Sexism>gender biases</a> present in influential case law. In order to test our NLP bias detection method&#8217;s performance, we regress our results of bias in case law against <a href=https://en.wikipedia.org/wiki/United_States_Census>U.S census data</a> of women&#8217;s participation in the workforce in the last 100 years.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.10/>Investigating the Impact of <a href=https://en.wikipedia.org/wiki/Gender_representation>Gender Representation</a> in ASR Training Data : a Case Study on Librispeech<span class=acl-fixed-case>ASR</span> Training Data: a Case Study on Librispeech</a></strong><br><a href=/people/m/mahault-garnerin/>Mahault Garnerin</a>
|
<a href=/people/s/solange-rossato/>Solange Rossato</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--10><div class="card-body p-3 small">In this paper we question the impact of <a href=https://en.wikipedia.org/wiki/Gender_representation>gender representation</a> in training data on the performance of an end-to-end ASR system. We create an experiment based on the Librispeech corpus and build 3 different training corpora varying only the proportion of data produced by each gender category. We observe that if our system is overall robust to the gender balance or imbalance in training data, it is nonetheless dependant of the adequacy between the individuals present in the training and testing sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.gebnlp-1.11.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gebnlp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.11/>Generating Gender Augmented Data for NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/n/nishtha-jain/>Nishtha Jain</a>
|
<a href=/people/m/maja-popovic/>Maja Popović</a>
|
<a href=/people/d/declan-groves/>Declan Groves</a>
|
<a href=/people/e/eva-vanmassenhove/>Eva Vanmassenhove</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--11><div class="card-body p-3 small">Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The <a href=https://en.wikipedia.org/wiki/Rewriting>rewriting method</a> can be applied to sentences that, without <a href=https://en.wikipedia.org/wiki/Context_(language_use)>extra-sentential context</a>, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to &#8216;translate&#8217; from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.12/>Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution<span class=acl-fixed-case>W</span>ino<span class=acl-fixed-case>B</span>ias (<span class=acl-fixed-case>S</span>o<span class=acl-fixed-case>W</span>ino<span class=acl-fixed-case>B</span>ias) Test Set for Latent Gender Bias Detection in Coreference Resolution</a></strong><br><a href=/people/h/hillary-dawkins/>Hillary Dawkins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--12><div class="card-body p-3 small">We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWinoBias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWinoBias test set, especially in reference to the method&#8217;s design and altered embedding space properties. See https://github.com/hillary-dawkins/SoWinoBias.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>