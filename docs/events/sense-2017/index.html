<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Sense, Concept and Entity Representations and their Applications (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Sense, Concept and Entity Representations and their Applications (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w17-19>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li></ul></div></div><div id=w17-19><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-19.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-19/>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1900/>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1901/>Compositional Semantics using Feature-Based Models from <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a><span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a>
|
<a href=/people/m/martin-pereira-farina/>Martín Pereira-Fariña</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1901><div class="card-body p-3 small">This article describes a method to build semantic representations of composite expressions in a compositional way by using WordNet relations to represent the meaning of words. The meaning of a target word is modelled as a vector in which its semantically related words are assigned weights according to both the type of the relationship and the distance to the target word. Word vectors are compositionally combined by syntactic dependencies. Each syntactic dependency triggers two complementary compositional functions : the named head function and dependent function. The experiments show that the proposed compositional method outperforms the state-of-the-art for both intransitive subject-verb and transitive subject-verb-object constructions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1902/>Automated WordNet Construction Using Word Embeddings<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Construction Using Word Embeddings</a></strong><br><a href=/people/m/mikhail-khodak/>Mikhail Khodak</a>
|
<a href=/people/a/andrej-risteski/>Andrej Risteski</a>
|
<a href=/people/c/christiane-fellbaum/>Christiane Fellbaum</a>
|
<a href=/people/s/sanjeev-arora/>Sanjeev Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1902><div class="card-body p-3 small">We present a fully unsupervised method for automated construction of WordNets based upon recent advances in distributional representations of sentences and word-senses combined with readily available machine translation tools. The approach requires very few linguistic resources and is thus extensible to multiple target languages. To evaluate our method we construct two 600-word testsets for word-to-synset matching in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> using native speakers and evaluate the performance of our method along with several other recent approaches. Our method exceeds the best language-specific and multi-lingual automated WordNets in <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> for both languages. The databases we construct for <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, both languages without large publicly available manually constructed WordNets, will be publicly released along with the testsets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1903/>Improving Verb Metaphor Detection by Propagating Abstractness to Words, Phrases and Individual Senses</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1903><div class="card-body p-3 small">Abstract words refer to things that can not be seen, heard, felt, smelled, or tasted as opposed to concrete words. Among other <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, the degree of <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness</a> has been shown to be a useful information for metaphor detection. Our contribution to this topic are as follows : i) we compare supervised techniques to learn and extend <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness ratings</a> for huge vocabularies ii) we learn and investigate norms for larger units by propagating <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness</a> to verb-noun pairs which lead to better metaphor detection iii) we overcome the limitation of learning a single rating per word and show that multi-sense abstractness ratings are potentially useful for metaphor detection. Finally, with this paper we publish automatically created abstractness norms for 3million English words and multi-words as well as automatically created sense specific abstractness ratings</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1904/>Improving Clinical Diagnosis Inference through Integration of Structured and Unstructured Knowledge</a></strong><br><a href=/people/y/yuan-ling/>Yuan Ling</a>
|
<a href=/people/y/yuan-an/>Yuan An</a>
|
<a href=/people/s/sadid-a-hasan/>Sadid Hasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1904><div class="card-body p-3 small">This paper presents a novel approach to the task of automatically inferring the most probable diagnosis from a given clinical narrative. Structured Knowledge Bases (KBs) can be useful for such complex <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> but not sufficient. Hence, we leverage a vast amount of unstructured free text to integrate with structured KBs. The key innovative ideas include building a concept graph from both structured and unstructured knowledge sources and ranking the diagnosis concepts using the enhanced word embedding vectors learned from integrated sources. Experiments on the TREC CDS and HumanDx datasets showed that our methods improved the results of clinical diagnosis inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1905/>Classifying Lexical-semantic Relationships by Exploiting Sense / Concept Representations</a></strong><br><a href=/people/k/kentaro-kanada/>Kentaro Kanada</a>
|
<a href=/people/t/tetsunori-kobayashi/>Tetsunori Kobayashi</a>
|
<a href=/people/y/yoshihiko-hayashi/>Yoshihiko Hayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1905><div class="card-body p-3 small">This paper proposes a method for classifying the type of lexical-semantic relation between a given pair of words. Given an inventory of target relationships, this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> can be seen as a multi-class classification problem. We train a supervised classifier by assuming : (1) a specific type of lexical-semantic relation between a pair of words would be indicated by a carefully designed set of relation-specific similarities associated with the words ; and (2) the similarities could be effectively computed by sense representations (sense / concept embeddings). The experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> clearly outperforms an existing state-of-the-art method that does not utilize sense / concept embeddings, thereby demonstrating the effectiveness of the sense representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1906/>Supervised and unsupervised approaches to measuring usage similarity</a></strong><br><a href=/people/m/milton-king/>Milton King</a>
|
<a href=/people/p/paul-cook/>Paul Cook</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1906><div class="card-body p-3 small">Usage similarity (USim) is an approach to determining word meaning in context that does not rely on a sense inventory. Instead, pairs of usages of a target lemma are rated on a scale. In this paper we propose unsupervised approaches to USim based on embeddings for words, contexts, and sentences, and achieve state-of-the-art results over two USim datasets. We further consider supervised approaches to USim, and find that although they outperform unsupervised approaches, they are unable to generalize to lemmas that are unseen in the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1908/>Creating and Validating Multilingual Semantic Representations for Six Languages : Expert versus Non-Expert Crowds</a></strong><br><a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a>
|
<a href=/people/s/scott-s-l-piao/>Scott Piao</a>
|
<a href=/people/s/stephen-wattam/>Stephen Wattam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1908><div class="card-body p-3 small">Creating high-quality wide-coverage multilingual semantic lexicons to support knowledge-based approaches is a challenging time-consuming manual task. This has traditionally been performed by <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic experts</a> : a slow and expensive process. We present an experiment in which we adapt and evaluate crowdsourcing methods employing native speakers to generate a list of coarse-grained senses under a common multilingual semantic taxonomy for sets of words in six languages. 451 non-experts (including 427 Mechanical Turk workers) and 15 expert participants semantically annotated 250 words manually for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, Portuguese and Urdu lexicons. In order to avoid erroneous (spam) crowdsourced results, we used a novel task-specific two-phase filtering process where users were asked to identify synonyms in the target language, and remove erroneous senses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1910" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1910/>One Representation per Word-Does it make Sense for Composition?</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/j/julie-weeds/>Julie Weeds</a>
|
<a href=/people/j/john-wilkie/>John Wilkie</a>
|
<a href=/people/j/jeremy-reffin/>Jeremy Reffin</a>
|
<a href=/people/d/david-weir/>David Weir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1910><div class="card-body p-3 small">In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through <a href=https://en.wikipedia.org/wiki/Composition_(language)>composition</a> alone. We evaluate the performance of off-the-shelf single-vector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as <a href=https://en.wikipedia.org/wiki/Pointwise_addition>pointwise addition</a> are able to recover sense specific information from a single-sense vector model remarkably well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1911/>Elucidating Conceptual Properties from Word Embeddings</a></strong><br><a href=/people/k/kyoung-rok-jang/>Kyoung-Rok Jang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1911><div class="card-body p-3 small">In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> of identifying the components (i.e. dimensions) of word embeddings that strongly signifies properties of a word. By elucidating such properties hidden in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, we could make <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> more interpretable, and also could perform property-based meaning comparison. With the capability, we can answer questions like To what degree a given word has the property cuteness? or In what perspective two words are similar?. We verify our method by examining how the strength of property-signifying components correlates with the degree of prototypicality of a target word.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1912/>TTCS^ : a Vectorial Resource for Computing Conceptual Similarity<span class=acl-fixed-case>TTCS</span><span class=tex-math><sup>ℰ</sup></span>: a Vectorial Resource for Computing Conceptual Similarity</a></strong><br><a href=/people/e/enrico-mensa/>Enrico Mensa</a>
|
<a href=/people/d/daniele-p-radicioni/>Daniele P. Radicioni</a>
|
<a href=/people/a/antonio-lieto/>Antonio Lieto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1912><div class="card-body p-3 small">In this paper we introduce the TTCS^, a linguistic resource that relies on <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a>, NASARI and <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a>, that has now been used to compute the conceptual similarity between concept pairs. The conceptual representation herein provides uniform access to concepts based on BabelNet synset IDs, and consists of a vector-based semantic representation which is compliant with the Conceptual Spaces, a geometric framework for common-sense knowledge representation and reasoning. The TTCS^ has been evaluated in a preliminary experimentation on a conceptual similarity task.<tex-math>^{\\mathcal{E}}</tex-math>, a linguistic resource that relies on BabelNet, NASARI and ConceptNet, that has now been used to compute the conceptual similarity between concept pairs. The conceptual representation herein provides uniform access to concepts based on BabelNet synset IDs, and consists of a vector-based semantic representation which is compliant with the Conceptual Spaces, a geometric framework for common-sense knowledge representation and reasoning. The TTCS<tex-math>^{\\mathcal{E}}</tex-math> has been evaluated in a preliminary experimentation on a conceptual similarity task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1913/>Measuring the Italian-English lexical gap for <a href=https://en.wikipedia.org/wiki/Action_verb>action verbs</a> and its impact on translation<span class=acl-fixed-case>I</span>talian-<span class=acl-fixed-case>E</span>nglish lexical gap for action verbs and its impact on translation</a></strong><br><a href=/people/l/lorenzo-gregori/>Lorenzo Gregori</a>
|
<a href=/people/a/alessandro-panunzi/>Alessandro Panunzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1913><div class="card-body p-3 small">This paper describes a method to measure the lexical gap of action verbs in <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> by using the IMAGACT ontology of action. The fine-grained categorization of action concepts of the data source allowed to have wide overview of the relation between concepts in the two languages. The calculated <a href=https://en.wikipedia.org/wiki/Lexical_gap>lexical gap</a> for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> is about 30 % of the action concepts, much higher than previous results. Beyond this general numbers a deeper analysis has been performed in order to evaluate the impact that <a href=https://en.wikipedia.org/wiki/Lexical_gap>lexical gaps</a> can have on <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In particular a distinction has been made between the cases in which the presence of a <a href=https://en.wikipedia.org/wiki/Lexical_gap>lexical gap</a> affects translation correctness and <a href=https://en.wikipedia.org/wiki/Completeness_(logic)>completeness</a> at a <a href=https://en.wikipedia.org/wiki/Semantics>semantic level</a>. The results highlight a high percentage of concepts that can be considered hard to translate (about 18 % from English to <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and 20 % from <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> to English) and confirms that <a href=https://en.wikipedia.org/wiki/Action_verb>action verbs</a> are a critical lexical class for translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1914/>Word Sense Filtering Improves Embedding-Based Lexical Substitution</a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1914><div class="card-body p-3 small">The role of <a href=https://en.wikipedia.org/wiki/Word_sense_disambiguation>word sense disambiguation</a> in <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for <a href=https://en.wikipedia.org/wiki/Word_sense>sense</a>. We show that a filtering mechanism based on a sense inventory optimized for <a href=https://en.wikipedia.org/wiki/Substituent>substitutability</a> can improve the results of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution annotations in a development set. The results show that <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> can still benefit from <a href=https://en.wikipedia.org/wiki/Word_sense>senses</a> which can improve the output of vector space paraphrase ranking models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1915/>Supervised and Unsupervised Word Sense Disambiguation on Word Embedding Vectors of Unambigous Synonyms</a></strong><br><a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1915><div class="card-body p-3 small">This paper compares two approaches to <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> trained on unambiguous synonyms. The first is <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> based on computing log probability from sequences of word embedding vectors, taking into account ambiguous word senses and guessing correct sense from context. The second method is supervised. We use a multilayer neural network model to learn a context-sensitive transformation that maps an input vector of ambiguous word into an output vector representing its sense. We evaluate both methods on corpora with manual annotations of word senses from the Polish wordnet (plWordnet).</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>