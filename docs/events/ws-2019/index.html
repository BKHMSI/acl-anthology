<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Other Workshops and Events (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Other Workshops and Events (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w19-01>Proceedings of the Society for Computation in Linguistics (SCiL) 2019</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-03>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-04>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w19-05>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w19-06>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w19-08>RELATIONS - Workshop on meaning relations between phrases and sentences</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-09>Proceedings of the IWCS Workshop Vector Semantics for Discourse and Dialogue</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-10>Proceedings of the IWCS 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-11>Proceedings of the Sixth Workshop on Natural Language and Computer Science</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w19-12>Proceedings of the IWCS Shared Task on Semantic Parsing</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w19-13>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-14>Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w19-15>Proceedings of the Third Workshop on Structured Prediction for NLP</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-16>Proceedings of the Combined Workshop on Spatial Language Understanding (SpLU) and Grounded Communication for Robotics (RoboNLP)</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-17>Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-18>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w19-19>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-20>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-21>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-22>Proceedings of the Natural Legal Language Processing Workshop 2019</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-23>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-24>Proceedings of the First Workshop on Narrative Understanding</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w19-25>Proceedings of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w19-26>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-27>Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-28>Proceedings of the Second Workshop on Computational Models of Reference, Anaphora and Coreference</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-29>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w19-30>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w19-31>Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w19-32>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w19-33>Proceedings of the First International Workshop on Designing Meaning Representations</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w19-34>Proceedings of the Second Workshop on Storytelling</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w19-35>Proceedings of the Third Workshop on Abusive Language Online</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w19-36>Proceedings of the 2019 Workshop on Widening NLP</a>
<span class="badge badge-info align-middle ml-1">57&nbsp;papers</span></li><li><a class=align-middle href=#w19-37>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-38>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-39>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w19-40>Proceedings of the 13th Linguistic Annotation Workshop</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w19-41>Proceedings of the First Workshop on NLP for Conversational AI</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w19-42>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w19-43>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class=align-middle href=#w19-44>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a>
<span class="badge badge-info align-middle ml-1">28&nbsp;papers</span></li><li><a class=align-middle href=#w19-45>Proceedings of the 6th Workshop on Argument Mining</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-46>Proceedings of the Fourth Arabic Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#w19-47>Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#w19-48>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#w19-49>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-50>Proceedings of the 18th BioNLP Workshop and Shared Task</a>
<span class="badge badge-info align-middle ml-1">24&nbsp;papers</span></li><li><a class=align-middle href=#w19-51>Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w19-52>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w19-53>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a>
<span class="badge badge-info align-middle ml-1">37&nbsp;papers</span></li><li><a class=align-middle href=#w19-54>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a>
<span class="badge badge-info align-middle ml-1">25&nbsp;papers</span></li><li><a class=align-middle href=#w19-55>Proceedings of the First Workshop on Financial Technology and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-56>Proceedings of the 3rd Workshop on Arabic Corpus Linguistics</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-57>Proceedings of the 16th Meeting on the Mathematics of Language</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-58>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-59>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a>
<span class="badge badge-info align-middle ml-1">28&nbsp;papers</span></li><li><a class=align-middle href=#w19-60>Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-61>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w19-62>Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-63>Proceedings of the 8th Workshop on NLP for Computer Assisted Language Learning</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-64>Proceedings of the Second Financial Narrative Processing Workshop (FNP 2019)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-65>Proceedings of the Workshop on NLP and Pseudonymisation</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-66>Proceedings of Machine Translation Summit XVII: Research Track</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-67>Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-68>Proceedings of the 2nd Workshop on Technologies for MT of Low Resource Languages</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-69>Proceedings of the Celtic Language Technology Workshop</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-70>Proceedings of the Second MEMENTO workshop on Modelling Parameters of Cognitive Effort in Translation Production</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-71>Proceedings of the Second Workshop on Multilingualism at the Intersection of Knowledge Bases and Machine Translation</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-72>Proceedings of The 8th Workshop on Patent and Scientific Literature Translation</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-73>Proceedings of the Qualities of Literary Machine Translation</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-74>Proceedings of the 3rd International Conference on Natural Language and Speech Processing</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-75>Proceedings of the 6th International Sanskrit Computational Linguistics Symposium</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-76>Proceedings of Machine Translation Summit XVII: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w19-77>Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-78>Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-79>Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-80>Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-81>Proceedings of the 1st Workshop on Discourse Structure in Neural NLG</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w19-83>Proceedings of the 1st International Workshop of AI Werewolf and Dialog System (AIWolfDial2019)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-84>Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-85>Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-86>Proceedings of the 12th International Conference on Natural Language Generation</a>
<span class="badge badge-info align-middle ml-1">31&nbsp;papers</span></li><li><a class=align-middle href=#w19-87>Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w19-89>Proceedings of the Workshop MultiLing 2019: Summarization Across Languages, Genres and Sources</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w19-90>Proceedings of the Workshop on Language Technology for Digital Historical Archives</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#d19-50>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#d19-51>Proceedings of the Second Workshop on Economics and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#d19-52>Proceedings of the 6th Workshop on Asian Translation</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#d19-53>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#d19-54>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#d19-55>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a>
<span class="badge badge-info align-middle ml-1">30&nbsp;papers</span></li><li><a class=align-middle href=#d19-56>Proceedings of the 3rd Workshop on Neural Generation and Translation</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#d19-57>Proceedings of The 5th Workshop on BioNLP Open Shared Tasks</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#d19-58>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#d19-59>Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#d19-60>Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#d19-61>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#d19-62>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#d19-63>Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#d19-64>Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#d19-65>Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#d19-66>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li></ul></div></div><div id=w19-01><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W19-01/>Proceedings of the Society for Computation in Linguistics (SCiL) 2019</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0100/>Proceedings of the Society for Computation in Linguistics (<span class=acl-fixed-case>SC</span>i<span class=acl-fixed-case>L</span>) 2019</a></strong><br><a href=/people/g/gaja-jarosz/>Gaja Jarosz</a>
|
<a href=/people/m/max-nelson/>Max Nelson</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/j/joe-pater/>Joe Pater</a></span></p></div><hr><div id=w19-03><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-03.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-03/>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0300/>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</a></strong><br><a href=/people/t/tommi-a-pirinen/>Tommi A. Pirinen</a>
|
<a href=/people/h/heiki-jaan-kaalep/>Heiki-Jaan Kaalep</a>
|
<a href=/people/f/francis-tyers/>Francis M. Tyers</a></span></p></div><hr><div id=w19-04><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-04.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-04/>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0400/>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0401/>Projecting Temporal Properties, Events and Actions</a></strong><br><a href=/people/t/tim-fernando/>Tim Fernando</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0401><div class="card-body p-3 small">Temporal notions based on a finite set A of properties are represented in strings, on which <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projections</a> are defined that vary the granularity A. The structure of properties in A is elaborated to describe statives, events and actions, subject to a distinction in meaning (advocated by Levin and Rappaport Hovav) between what the lexicon prescribes and what a context of use supplies. The <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projections</a> proposed are deployed as labels for records and record types amenable to <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite-state methods</a>.<i>A</i> of properties are represented in strings, on which projections are defined that vary the granularity <i>A</i>. The structure of properties in <i>A</i> is elaborated to describe statives, events and actions, subject to a distinction in meaning (advocated by Levin and Rappaport Hovav) between what the lexicon prescribes and what a context of use supplies. The projections proposed are deployed as labels for records and record types amenable to finite-state methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0402/>A Type-coherent, Expressive Representation as an Initial Step to <a href=https://en.wikipedia.org/wiki/Language_understanding>Language Understanding</a></a></strong><br><a href=/people/g/gene-louis-kim/>Gene Louis Kim</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0402><div class="card-body p-3 small">A growing interest in tasks involving <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> by the NLP community has led to the need for effective <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> and <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> : adequately modeling <a href=https://en.wikipedia.org/wiki/Semantics>language semantics</a>, enabling general inferences, and being accurately recoverable. This document describes underspecified logical forms (ULF) for Episodic Logic (EL), which is an initial form for a semantic representation that balances these needs. ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a>, and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> unresolved ; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution. This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> starting with derivation of ULFs will lead to <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analyses</a> that do justice to subtle aspects of <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>linguistic meaning</a>, and will enable construction of more accurate semantic parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0405/>An Improved Approach for Semantic Graph Composition with CCG<span class=acl-fixed-case>CCG</span></a></strong><br><a href=/people/a/austin-blodgett/>Austin Blodgett</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0405><div class="card-body p-3 small">This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators : these require that the two constituents being combined overlap in one AMR relation. We also provide a new <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> for type raising, which is necessary for certain <a href=https://en.wikipedia.org/wiki/Constructor_(object-oriented_programming)>constructions</a>. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0407/>Towards a Compositional Analysis of German Light Verb Constructions (LVCs) Combining Lexicalized Tree Adjoining Grammar (LTAG) with Frame Semantics<span class=acl-fixed-case>G</span>erman Light Verb Constructions (<span class=acl-fixed-case>LVC</span>s) Combining <span class=acl-fixed-case>L</span>exicalized <span class=acl-fixed-case>T</span>ree <span class=acl-fixed-case>A</span>djoining <span class=acl-fixed-case>G</span>rammar (<span class=acl-fixed-case>LTAG</span>) with Frame Semantics</a></strong><br><a href=/people/j/jens-fleischhauer/>Jens Fleischhauer</a>
|
<a href=/people/t/thomas-gamerschlag/>Thomas Gamerschlag</a>
|
<a href=/people/l/laura-kallmeyer/>Laura Kallmeyer</a>
|
<a href=/people/s/simon-petitjean/>Simon Petitjean</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0407><div class="card-body p-3 small">Complex predicates formed of a semantically &#8216;light&#8217; verbal head and a noun or verb which contributes the major part of the meaning are frequently referred to as &#8216;light verb constructions&#8217; (LVCs). In the paper, we present a case study of LVCs with the German posture verb stehen &#8216;stand&#8217;. In our account, we model the syntactic as well as semantic composition of such LVCs by combining Lexicalized Tree Adjoining Grammar (LTAG) with <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>frames</a>. Starting from the analysis of the literal uses of posture verbs, we show how the meaning components of the literal uses are systematically exploited in the interpretation of stehen-LVCs. The paper constitutes an important step towards a compositional and computational analysis of LVCs. We show that LTAG allows us to separate constructional from lexical meaning components and that frames enable elegant generalizations over event types and related constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0408/>Words are Vectors, Dependencies are Matrices : Learning Word Embeddings from Dependency Graphs</a></strong><br><a href=/people/p/paula-czarnowska/>Paula Czarnowska</a>
|
<a href=/people/g/guy-emerson/>Guy Emerson</a>
|
<a href=/people/a/ann-copestake/>Ann Copestake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0408><div class="card-body p-3 small">Distributional Semantic Models (DSMs) construct vector representations of word meanings based on their contexts. Typically, the contexts of a word are defined as its closest neighbours, but they can also be retrieved from its syntactic dependency relations. In this work, we propose a new dependency-based DSM. The novelty of our model lies in associating an independent meaning representation, a <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a>, with each dependency-label. This allows <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to capture specifics of the relations between words and contexts, leading to good performance on both intrinsic and extrinsic evaluation tasks. In addition to that, our model has an inherent ability to represent dependency chains as products of matrices which provides a straightforward way of handling further contexts of a word.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0409/>Temporal and Aspectual Entailment</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/s/sander-bijl-de-vroe/>Sander Bijl de Vroe</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0409><div class="card-body p-3 small">Inferences regarding Jane&#8217;s arrival in London from <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predications</a> such as Jane is going to London or Jane has gone to London depend on <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of going to London is completed and whether its consequences hold at that time or not. While <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a> are among the most important factors for determining <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, there has been very little work to show whether modern <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform <a href=https://en.wikipedia.org/wiki/Inference>inference</a> on <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predications</a> across aspectual types and tenses. We show that they encode a substantial amount of information relating to <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, but fail to consistently model inferences that require reasoning with these semantic properties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0412/>Aligning Open IE Relations and KB Relations using a Siamese Network Based on Word Embedding<span class=acl-fixed-case>IE</span> Relations and <span class=acl-fixed-case>KB</span> Relations using a <span class=acl-fixed-case>S</span>iamese Network Based on Word Embedding</a></strong><br><a href=/people/r/rifki-afina-putri/>Rifki Afina Putri</a>
|
<a href=/people/g/giwon-hong/>Giwon Hong</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0412><div class="card-body p-3 small">Open Information Extraction (Open IE) aims at generating entity-relation-entity triples from a large amount of text, aiming at capturing key semantics of the text. Given a triple, the <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> expresses the type of semantic relation between the entities. Although relations from an Open IE system are more extensible than those used in a traditional Information Extraction system and a Knowledge Base (KB) such as Knowledge Graphs, the former lacks in semantics ; an Open IE relation is simply a sequence of words, whereas a KB relation has a predefined meaning. As a way to provide a meaning to an Open IE relation, we attempt to align it with one of the predefined set of relations used in a KB. Our approach is to use a <a href=https://en.wikipedia.org/wiki/Siamese_network>Siamese network</a> that compares two sequences of word embeddings representing an Open IE relation and a predefined KB relation. In order to make the approach practical, we automatically generate a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training dataset</a> using a distant supervision approach instead of relying on a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>hand-labeled dataset</a>. Our experiment shows that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can capture the <a href=https://en.wikipedia.org/wiki/Relational_semantics>relational semantics</a> better than the recent <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0414/>The Effect of Context on Metaphor Paraphrase Aptness Judgments</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/s/shalom-lappin/>Shalom Lappin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0414><div class="card-body p-3 small">We conduct two experiments to study the effect of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> on metaphor paraphrase aptness judgments. The first is an AMT crowd source task in which speakers rank metaphor-paraphrase candidate sentence pairs in short document contexts for paraphrase aptness. In the second we train a composite DNN to predict these human judgments, first in binary classifier mode, and then as gradient ratings. We found that for both mean human judgments and our DNN&#8217;s predictions, adding document context compresses the aptness scores towards the center of the scale, raising low out-of-context ratings and decreasing high out-of-context scores. We offer a provisional explanation for this compression effect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0415/>Predicting Word Concreteness and Imagery</a></strong><br><a href=/people/j/jean-charbonnier/>Jean Charbonnier</a>
|
<a href=/people/c/christian-wartena/>Christian Wartena</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0415><div class="card-body p-3 small">Concreteness of words has been studied extensively in psycholinguistic literature. A number of <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have been created with average values for perceived concreteness of words. We show that we can train a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> on these data, using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and morphological features, that can predict these concreteness values with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 7 publicly available datasets. Only for a few small subsets of these datasets prediction of concreteness values are found in the literature. Our results clearly outperform the reported results for these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0416/>Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Network for Implicit Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0416><div class="card-body p-3 small">Implicit discourse relation classification is one of the most difficult steps in discourse parsing. The difficulty stems from the fact that the coherence relation must be inferred based on the content of the discourse relational arguments. Therefore, an effective encoding of the relational arguments is of crucial importance. We here propose a new model for implicit discourse relation classification, which consists of a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>, and a sequence-to-sequence model which is trained to generate a representation of the discourse relational arguments by trying to predict the relational arguments including a suitable implicit connective. Training is possible because such implicit connectives have been annotated as part of the PDTB corpus. Along with a memory network, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could generate more refined representations for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. And on the now standard 11-way classification, our method outperforms the previous state of the art systems on the PDTB benchmark on multiple settings including cross validation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-0421" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-0421/>Using Multi-Sense Vector Embeddings for Reverse Dictionaries</a></strong><br><a href=/people/m/michael-a-hedderich/>Michael A. Hedderich</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0421><div class="card-body p-3 small">Popular word embedding methods such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically can not serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of <a href=https://en.wikipedia.org/wiki/Reverse_dictionary>reverse dictionaries</a>. We propose a <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> to easily integrate them into an existing <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0425/>Frame Identification as Categorization : Exemplars vs Prototypes in Embeddingland</a></strong><br><a href=/people/j/jennifer-sikos/>Jennifer Sikos</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0425><div class="card-body p-3 small">Categorization is a central capability of <a href=https://en.wikipedia.org/wiki/Cognition>human cognition</a>, and a number of <a href=https://en.wikipedia.org/wiki/Theory>theories</a> have been developed to account for properties of <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a>. Even though many tasks in <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> also involve <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> of some kind, theories of <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> do not play a major role in contemporary research in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories. The benefit is a space of model families that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions. We instantiate this idea on the task of frame-semantic frame identification. We define four models that cross two <a href=https://en.wikipedia.org/wiki/Design_of_experiments>design variables</a> : (a) the choice of <a href=https://en.wikipedia.org/wiki/Design_of_experiments>prototype vs. exemplar categorization</a>, corresponding to different degrees of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> applied to the input ; and (b) the presence vs. absence of a <a href=https://en.wikipedia.org/wiki/Design_of_experiments>fine-tuning step</a>, corresponding to <a href=https://en.wikipedia.org/wiki/Design_of_experiments>generic vs. task-adaptive categorization</a>. We find that for frame identification, <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and task-adaptive categorization both yield substantial benefits. Our prototype-based, fine-tuned model, which combines the best choices for these variables, establishes a new state of the art in frame identification.</div></div></div><hr><div id=w19-05><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-05.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-05/>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0500/>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0503/>Distributional Semantics in the Real World : Building Word Vector Representations from a Truth-Theoretic Model</a></strong><br><a href=/people/e/elizaveta-kuzmenko/>Elizaveta Kuzmenko</a>
|
<a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0503><div class="card-body p-3 small">Distributional semantics models (DSMs) are known to produce excellent representations of word meaning, which correlate with a range of behavioural data. As lexical representations, they have been said to be fundamentally different from truth-theoretic models of semantics, where <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a> is defined as a correspondence relation to the world. There are two main aspects to this difference : a) DSMs are built over corpus data which may or may not reflect &#8216;what is in the world&#8217; ; b) they are built from word co-occurrences, that is, from lexical types rather than entities and sets. In this paper, we inspect the properties of a <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional model</a> built over a set-theoretic approximation of &#8216;the real world&#8217;. To achieve this, we take the annotation a large database of images marked with objects, attributes and relations, convert the <a href=https://en.wikipedia.org/wiki/Data>data</a> into a representation akin to <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic</a> and build several distributional models using various combinations of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. We evaluate those <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> over both relatedness and similarity datasets, demonstrating their effectiveness in standard evaluations. This allows us to conclude that, despite prior claims, truth-theoretic models are good candidates for building graded lexical representations of meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0505/>Making Sense of Conflicting (Defeasible) Rules in the Controlled Natural Language ACE : Design of a System with Support for Existential Quantification Using Skolemization<span class=acl-fixed-case>ACE</span>: Design of a System with Support for Existential Quantification Using Skolemization</a></strong><br><a href=/people/m/martin-diller/>Martin Diller</a>
|
<a href=/people/a/adam-wyner/>Adam Wyner</a>
|
<a href=/people/h/hannes-strass/>Hannes Strass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0505><div class="card-body p-3 small">We present the design of a system for making sense of conflicting rules expressed in a fragment of the prominent controlled natural language ACE, yet extended with means of expressing defeasible rules in the form of normality assumptions. The approach we describe is ultimately based on answer-set-programming (ASP) ; simulating <a href=https://en.wikipedia.org/wiki/Existential_quantification>existential quantification</a> by using <a href=https://en.wikipedia.org/wiki/Skolemization>skolemization</a> in a manner resembling a translation for ASP recently formalized in the context of -ASP. We discuss the advantages of this approach to building on the existing ACE interface to rule-systems, ACERules.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0506/>Distributional Interaction of Concreteness and Abstractness in VerbNoun Subcategorisation</a></strong><br><a href=/people/d/diego-frassinelli/>Diego Frassinelli</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0506><div class="card-body p-3 small">In recent years, both cognitive and computational research has provided empirical analyses of contextual co-occurrence of concrete and abstract words, partially resulting in inconsistent pictures. In this work we provide a more fine-grained description of the distributional nature in the corpus-based interaction of verbs and nouns within <a href=https://en.wikipedia.org/wiki/Subcategorization>subcategorisation</a>, by investigating the concreteness of verbs and nouns that are in a specific syntactic relationship with each other, i.e., subject, direct object, and <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>prepositional object</a>. Overall, our experiments show consistent patterns in the distributional representation of subcategorising and subcategorised concrete and abstract words. At the same time, the studies reveal empirical evidence why contextual abstractness represents a valuable indicator for automatic non-literal language identification.</div></div></div><hr><div id=w19-06><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-06.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-06/>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0600/>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</a></strong><br><a href=/people/s/simon-dobnik/>Simon Dobnik</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/k/kathrein-abu-kwaik/>Kathrein Abu Kwaik</a>
|
<a href=/people/v/vladislav-maraev/>Vladislav Maraev</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0601/>A <a href=https://en.wikipedia.org/wiki/Dynamic_semantics>Dynamic Semantics</a> for Causal Counterfactuals</a></strong><br><a href=/people/k/kenneth-lai/>Kenneth Lai</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0601><div class="card-body p-3 small">Under the standard approach to counterfactuals, to determine the meaning of a counterfactual sentence, we consider the closest possible world(s) where the antecedent is true, and evaluate the consequent. Building on the standard approach, some researchers have found that the set of worlds to be considered is dependent on context ; it evolves with the <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. Others have focused on how to define the distance between possible worlds, using ideas from <a href=https://en.wikipedia.org/wiki/Causal_model>causal modeling</a>. This paper integrates the two ideas. We present a <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> for <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> that uses a distance measure based on <a href=https://en.wikipedia.org/wiki/Causality>causal laws</a>, that can also change over time. We show how our <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> can be implemented in the <a href=https://en.wikipedia.org/wiki/Haskell_(programming_language)>Haskell programming language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0602/>Visual TTR-Modelling Visual Question Answering in Type Theory with Records<span class=acl-fixed-case>TTR</span> - Modelling Visual Question Answering in Type Theory with Records</a></strong><br><a href=/people/r/ronja-utescher/>Ronja Utescher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0602><div class="card-body p-3 small">In this paper, I will describe a <a href=https://en.wikipedia.org/wiki/System>system</a> that was developed for the task of Visual Question Answering. The system uses the rich type universe of Type Theory with Records (TTR) to model both the utterances about the <a href=https://en.wikipedia.org/wiki/Image>image</a>, the <a href=https://en.wikipedia.org/wiki/Image>image</a> itself and classifications made related to the two. At its most basic, the decision of whether any given predicate can be assigned to an object in the image is delegated to a CNN. Consequently, <a href=https://en.wikipedia.org/wiki/Image>images</a> can be judged as evidence for propositions. The end result is a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> whose application of perceptual classifiers to a given <a href=https://en.wikipedia.org/wiki/Image>image</a> is guided by the accompanying utterance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0603/>The Lexical Gap : An Improved Measure of Automated Image Description Quality</a></strong><br><a href=/people/a/austin-kershaw/>Austin Kershaw</a>
|
<a href=/people/m/miroslaw-bober/>Miroslaw Bober</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0603><div class="card-body p-3 small">The challenge of automatically describing images and videos has stimulated much research in <a href=https://en.wikipedia.org/wiki/Computer_vision>Computer Vision</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. In order to test the semantic abilities of new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>, we need reliable and objective ways of measuring progress. We show that standard evaluation measures do not take into account the semantic richness of a description, and give the impression that sparse machine descriptions outperform rich human descriptions. We introduce and test a new <a href=https://en.wikipedia.org/wiki/Measurement>measure</a> of semantic ability based on relative lexical diversity. We show how our <a href=https://en.wikipedia.org/wiki/Measurement>measure</a> can work alongside existing measures to achieve state of the art correlation with human judgement of quality. We also introduce a new dataset : Rich-Sparse Descriptions, which provides 2 K human and machine descriptions to stimulate interest into the semantic evaluation of machine descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0604/>Modeling language constructs with <a href=https://en.wikipedia.org/wiki/Fuzzy_set>fuzzy sets</a> : some approaches, examples and interpretations</a></strong><br><a href=/people/p/pavlo-kapustin/>Pavlo Kapustin</a>
|
<a href=/people/m/michael-kapustin/>Michael Kapustin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0604><div class="card-body p-3 small">We present and discuss a couple of approaches, including different types of <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projections</a>, and some examples, discussing the use of <a href=https://en.wikipedia.org/wiki/Fuzzy_set>fuzzy sets</a> for modeling meaning of certain types of <a href=https://en.wikipedia.org/wiki/Construct_(philosophy)>language constructs</a>. We are mostly focusing on words other than <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Hedge_(linguistics)>linguistic hedges</a> as these <a href=https://en.wikipedia.org/wiki/Categorization>categories</a> are the most studied from before. We discuss logical and linguistic interpretations of membership functions. We argue that using fuzzy sets for modeling meaning of words and other <a href=https://en.wikipedia.org/wiki/Natural_language>natural language constructs</a>, along with situations described with <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> is interesting both from purely linguistic perspective, and also as a <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> for problems of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0605/>Topological Data Analysis for Discourse Semantics?</a></strong><br><a href=/people/k/ketki-savle/>Ketki Savle</a>
|
<a href=/people/w/wlodek-zadrozny/>Wlodek Zadrozny</a>
|
<a href=/people/m/minwoo-lee/>Minwoo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0605><div class="card-body p-3 small">In this paper we present new results on applying <a href=https://en.wikipedia.org/wiki/Topological_data_analysis>topological data analysis</a> to discourse structures. We show that topological information, extracted from the relationships between sentences can be used in <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, namely it can be applied to the very difficult <a href=https://en.wikipedia.org/wiki/Logical_consequence>legal entailment</a> given in the COLIEE 2018 data set. Previous results of Doshi and Zadrozny (2018) and Gholizadeh et al. (2018) show that topological features are useful for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. The applications of <a href=https://en.wikipedia.org/wiki/Computational_topology>computational topology</a> to <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a> are novel in our view provide a new set of tools for discourse semantics : <a href=https://en.wikipedia.org/wiki/Computational_topology>computational topology</a> can perhaps provide a bridge between the brittleness of logic and the regression of neural networks. We discuss the advantages and disadvantages of using topological information, and some open problems such as explainability of the classifier decisions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0606/>Semantic Frame Embeddings for Detecting Relations between Software Requirements</a></strong><br><a href=/people/w/waad-alhoshan/>Waad Alhoshan</a>
|
<a href=/people/r/riza-theresa-batista-navarro/>Riza Batista-Navarro</a>
|
<a href=/people/l/liping-zhao/>Liping Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0606><div class="card-body p-3 small">The early phases of <a href=https://en.wikipedia.org/wiki/Requirements_engineering>requirements engineering (RE)</a> deal with a vast amount of <a href=https://en.wikipedia.org/wiki/Software_requirements>software requirements</a> (i.e., requirements that define characteristics of software systems), which are typically expressed in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Analysing such unstructured requirements, usually obtained from users&#8217; inputs, is considered a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> due to the inherent ambiguity and inconsistency of natural language. To support such a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> based on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> can be employed. One of the more recent advances in NLP is the use of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for capturing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>, which can then be applied in word analogy tasks. In this paper, we describe a new <a href=https://en.wikipedia.org/wiki/Resource>resource</a>, i.e., embedding-based representations of semantic frames in <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a>, which was developed to support the detection of relations between software requirements. Our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>, which encapsulate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> at the semantic frame level, were trained on a large corpus of requirements (i.e., a collection of more than three million mobile application reviews). The similarity between these frame embeddings is then used as a basis for detecting <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> between software requirements. Compared with existing resources underpinned by word-level embeddings alone, and frame embeddings built upon pre-trained vectors, our proposed frame embeddings obtained better performance against judgements of an RE expert. These encouraging results demonstrate the strong potential of the <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resource</a> in supporting <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>RE analysis tasks</a> (e.g., <a href=https://en.wikipedia.org/wiki/Trace_(disambiguation)>traceability</a>), which we plan to investigate as part of our future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-0607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-0607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0607/>R-grams : Unsupervised Learning of Semantic Units in Natural Language<span class=acl-fixed-case>R</span>-grams: Unsupervised Learning of Semantic Units in Natural Language</a></strong><br><a href=/people/a/amaru-cuba-gyllensten/>Amaru Cuba Gyllensten</a>
|
<a href=/people/a/ariel-ekgren/>Ariel Ekgren</a>
|
<a href=/people/m/magnus-sahlgren/>Magnus Sahlgren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-0607><div class="card-body p-3 small">This paper investigates data-driven segmentation using Re-Pair or Byte Pair Encoding-techniques. In contrast to previous work which has primarily been focused on subword units for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, we are interested in the general properties of such <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segments</a> above the word level. We call these segments r-grams, and discuss their properties and the effect they have on the token frequency distribution. The proposed approach is evaluated by demonstrating its viability in embedding techniques, both in monolingual and multilingual test settings. We also provide a number of qualitative examples of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>, demonstrating its viability as a language-invariant segmentation procedure.</div></div></div><hr><div id=w19-08><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-08.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-08/>RELATIONS - Workshop on meaning relations between phrases and sentences</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0800/><span class=acl-fixed-case>RELATIONS</span> - Workshop on meaning relations between phrases and sentences</a></strong><br><a href=/people/v/venelin-kovatchev/>Venelin Kovatchev</a>
|
<a href=/people/d/darina-gold/>Darina Gold</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p></div><hr><div id=w19-09><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-09.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-09/>Proceedings of the IWCS Workshop Vector Semantics for Discourse and Dialogue</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-0900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-0900/>Proceedings of the <span class=acl-fixed-case>IWCS</span> Workshop Vector Semantics for Discourse and Dialogue</a></strong><br><a href=/people/m/mehrnoosh-sadrzadeh/>Mehrnoosh Sadrzadeh</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a>
|
<a href=/people/j/julian-hough/>Julian Hough</a>
|
<a href=/people/r/ruth-kempson/>Ruth Kempson</a>
|
<a href=/people/p/patrick-healey/>Patrick G. T. Healey</a></span></p></div><hr><div id=w19-10><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-10.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-10/>Proceedings of the IWCS 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1000/>Proceedings of the <span class=acl-fixed-case>IWCS</span> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</a></strong><br><a href=/people/r/rainer-osswald/>Rainer Osswald</a>
|
<a href=/people/c/christian-retore/>Christian Retoré</a>
|
<a href=/people/p/peter-sutton/>Peter Sutton</a></span></p></div><hr><div id=w19-11><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-11.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-11/>Proceedings of the Sixth Workshop on Natural Language and Computer Science</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1100/>Proceedings of the Sixth Workshop on Natural Language and Computer Science</a></strong><br><a href=/people/r/robin-cooper/>Robin Cooper</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a>
|
<a href=/people/l/lawrence-s-moss/>Lawrence S. Moss</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1101/>Distribution is not enough : going Firther</a></strong><br><a href=/people/a/andy-lucking/>Andy Lücking</a>
|
<a href=/people/r/robin-cooper/>Robin Cooper</a>
|
<a href=/people/s/staffan-larsson/>Staffan Larsson</a>
|
<a href=/people/j/jonathan-ginzburg/>Jonathan Ginzburg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1101><div class="card-body p-3 small">Much work in contemporary <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a> follows the distributional hypothesis (DH), which is understood as an approach to <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> according to which the meaning of a word is a function of its distribution over contexts which is represented as vectors (word embeddings) within a multi-dimensional semantic space. In practice, use is identified with occurrence in <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>, though there are some efforts to use corpora containing multi-modal information. In this paper we argue that the <a href=https://en.wikipedia.org/wiki/Distributional_hypothesis>distributional hypothesis</a> is intrinsically misguided as a self-supporting basis for <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, as Firth was entirely aware. We mention philosophical arguments concerning the lack of <a href=https://en.wikipedia.org/wiki/Norm_(philosophy)>normativity</a> within DH data. Furthermore, we point out the shortcomings of DH as a model of learning, by discussing a variety of linguistic classes that can not be learnt on a distributional basis, including <a href=https://en.wikipedia.org/wiki/Indexicality>indexicals</a>, proper names, and wh-phrases. Instead of pursuing DH, we sketch an account of the problematic learning cases by integrating a rich, Firthian notion of dialogue context with interactive learning in signalling games backed by in probabilistic Type Theory with Records. We conclude that the success of the DH in <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a> rests on a post hoc effect : DS presupposes a referential semantics on the basis of which utterances can be produced, comprehended and analysed in the first place.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1102/>Towards Natural Language Story Understanding with Rich Logical Schemas</a></strong><br><a href=/people/l/lane-lawley/>Lane Lawley</a>
|
<a href=/people/g/gene-louis-kim/>Gene Louis Kim</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1102><div class="card-body p-3 small">Generating commonsense&#8217;&#8217; knowledge for intelligent understanding and reasoning is a difficult, long-standing problem, whose scale challenges the capacity of any approach driven primarily by human input. Furthermore, approaches based on mining statistically repetitive patterns fail to produce the rich representations humans acquire, and fall far short of human efficiency in inducing knowledge from text. The idea of our approach to this problem is to provide a learning system with a <a href=https://en.wikipedia.org/wiki/Head_start_(positioning)>head start</a> consisting of a semantic parser, some basic ontological knowledge, and most importantly, a small set of very general schemas about the kinds of patterns of events (often purposive, causal, or socially conventional) that even a one- or two-year-old could reasonably be presumed to possess. We match these initial <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schemas</a> to simple children&#8217;s stories, obtaining concrete instances, and combining and abstracting these into new candidate <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schemas</a>. Both the initial and generated schemas are specified using a rich, expressive <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a>. While modern approaches to schema reasoning often only use slot-and-filler structures, this <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> allows us to specify complex relations and constraints over the slots. Though formal, the <a href=https://en.wikipedia.org/wiki/Representation_(systemics)>representations</a> are language-like, and as such readily relatable to NL text. The <a href=https://en.wikipedia.org/wiki/Agency_(philosophy)>agents</a>, <a href=https://en.wikipedia.org/wiki/Object_(philosophy)>objects</a>, and other roles in the schemas are represented by typed variables, and the <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event variables</a> can be related through partial temporal ordering and causal relations. To match natural language stories with existing <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schemas</a>, we first parse the stories into an underspecified variant of the <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> used by the <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schemas</a>, which is suitable for most concrete stories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1103/>Questions in Dependent Type Semantics</a></strong><br><a href=/people/k/kazuki-watanabe/>Kazuki Watanabe</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1103><div class="card-body p-3 small">Dependent Type Semantics (DTS ; Bekki and Mineshima, 2017) is a proof-theoretic compositional dynamic semantics based on <a href=https://en.wikipedia.org/wiki/Dependent_type>Dependent Type Theory</a>. The semantic representations for <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>declarative sentences</a> in DTS are types, based on the propositions-as-types paradigm. While <a href=https://en.wikipedia.org/wiki/Type_theory>type-theoretic semantics</a> for <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> based on <a href=https://en.wikipedia.org/wiki/Dependent_type>dependent type theory</a> has been developed by many authors, how to assign semantic representations to interrogative sentences has been a non-trivial problem. In this study, we show how to provide the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of interrogative sentences in DTS. The basic idea is to assign the same type to both <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>declarative sentences</a> and <a href=https://en.wikipedia.org/wiki/Interrogative>interrogative sentences</a>, partly building on the recent proposal in <a href=https://en.wikipedia.org/wiki/Inquisitive_semantics>Inquisitive Semantics</a>. We use Combinatory Categorial Grammar (CCG) as a syntactic component of DTS and implement our compositional semantics for interrogative sentences using ccg2lambda, a semantic parsing platform based on CCG. Based on the idea that the relationship between questions and answers can be formulated as the task of Recognizing Textual Entailment (RTE), we implement our inference system using proof assistant Coq and show that our system can deal with a wide range of question-answer relationships discussed in the formal semantics literature, including those with polar questions, alternative questions, and wh-questions.</div></div></div><hr><div id=w19-12><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-12.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-12/>Proceedings of the IWCS Shared Task on Semantic Parsing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1200/>Proceedings of the <span class=acl-fixed-case>IWCS</span> Shared Task on Semantic Parsing</a></strong><br><a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a>
|
<a href=/people/r/rik-van-noord/>Rik van Noord</a>
|
<a href=/people/h/hessel-haagsma/>Hessel Haagsma</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1204/>Neural Boxer at the IWCS Shared Task on DRS Parsing<span class=acl-fixed-case>IWCS</span> Shared Task on <span class=acl-fixed-case>DRS</span> Parsing</a></strong><br><a href=/people/r/rik-van-noord/>Rik van Noord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1204><div class="card-body p-3 small">This paper describes our participation in the shared task of Discourse Representation Structure parsing. It follows the work of Van Noord et al. (2018), who employed a neural sequence-to-sequence model to produce DRSs, also exploiting linguistic information with multiple encoders. We provide a detailed look in the performance of this <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> and show that (i) the benefit of the linguistic features is evident across a number of experiments which vary the amount of training data and (ii) the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can be improved by applying a number of postprocessing methods to fix ill-formed output. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> ended up in second place in the competition, with an <a href=https://en.wikipedia.org/wiki/Grading_in_education>F-score</a> of 84.5.</div></div></div><hr><div id=w19-13><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-13.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-13/>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1300/>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></strong><br><a href=/people/a/alexandra-balahur/>Alexandra Balahur</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1302/>A Soft Label Strategy for Target-Level Sentiment Classification</a></strong><br><a href=/people/d/da-yin/>Da Yin</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/x/xiuyu-wu/>Xiuyu Wu</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1302><div class="card-body p-3 small">In this paper, we propose a soft label approach to target-level sentiment classification task, in which a history-based soft labeling model is proposed to measure the possibility of a context word as an opinion word. We also apply a <a href=https://en.wikipedia.org/wiki/Convolution>convolution layer</a> to extract local active features, and introduce positional weights to take relative distance information into consideration. In addition, we obtain more informative target representation by training with context tokens together to make deeper interaction between target and context tokens. We conduct experiments on SemEval 2014 datasets and the experimental results show that our approach significantly outperforms previous models and gives state-of-the-art results on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-1303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-1303/>Online abuse detection : the value of <a href=https://en.wikipedia.org/wiki/Data_preprocessing>preprocessing</a> and neural attention models</a></strong><br><a href=/people/d/dhruv-kumar/>Dhruv Kumar</a>
|
<a href=/people/r/robin-cohen/>Robin Cohen</a>
|
<a href=/people/l/lukasz-golab/>Lukasz Golab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1303><div class="card-body p-3 small">We propose an attention-based neural network approach to detect abusive speech in online social networks. Our approach enables more effective modeling of context and the semantic relationships between words. We also empirically evaluate the value of text pre-processing techniques in addressing the challenge of out-of-vocabulary words in toxic content. Finally, we conduct extensive experiments on the Wikipedia Talk page datasets, showing improved <a href=https://en.wikipedia.org/wiki/Predictive_power>predictive power</a> over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1306/>Using Structured Representation and Data : A Hybrid Model for Negation and Sentiment in Customer Service Conversations</a></strong><br><a href=/people/a/amita-misra/>Amita Misra</a>
|
<a href=/people/m/mansurul-bhuiyan/>Mansurul Bhuiyan</a>
|
<a href=/people/j/jalal-mahmud/>Jalal Mahmud</a>
|
<a href=/people/s/saurabh-tripathy/>Saurabh Tripathy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1306><div class="card-body p-3 small">Twitter customer service interactions have recently emerged as an effective platform to respond and engage with customers. In this work, we explore the role of <a href=https://en.wikipedia.org/wiki/Negation>negation</a> in customer service interactions, particularly applied to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We define rules to identify true negation cues and scope more suited to conversational data than existing general review data. Using semantic knowledge and syntactic structure from constituency parse trees, we propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for scope detection that performs comparable to state of the art BiLSTM. We further investigate the results of negation scope detection for the sentiment prediction task on customer service conversation data using both a traditional SVM and a <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a>. We propose an antonym dictionary based method for <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> applied to a combination CNN-LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Experimental results show that the antonym-based method outperforms the previous lexicon-based and Neural Network methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1309/>When Numbers Matter ! : Detecting Sarcasm in Numerical Portions of Text</a></strong><br><a href=/people/a/abhijeet-dubey/>Abhijeet Dubey</a>
|
<a href=/people/l/lakshya-kumar/>Lakshya Kumar</a>
|
<a href=/people/a/arpan-somani/>Arpan Somani</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1309><div class="card-body p-3 small">Research in sarcasm detection spans almost a decade. However a particular form of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> remains unexplored : <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> expressed through numbers, which we estimate, forms about 11 % of the sarcastic tweets in our dataset. The sentence &#8216;Love waking up at 3 am&#8217; is sarcastic because of the number. In this paper, we focus on detecting <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> in tweets arising out of numbers. Initially, to get an insight into the problem, we implement a rule-based and a statistical machine learning-based (ML) classifier. The rule-based classifier conveys the crux of the numerical sarcasm problem, namely, incongruity arising out of numbers. The statistical ML classifier uncovers the indicators i.e., features of such <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>. The actual system in place, however, are two deep learning (DL) models, CNN and attention network that obtains an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.93 and 0.91 on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets containing numbers. To the best of our knowledge, this is the first line of research investigating the phenomenon of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> arising out of numbers, culminating in a detector thereof.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1310/>Cross-lingual Subjectivity Detection for Resource Lean Languages</a></strong><br><a href=/people/i/ida-amini/>Ida Amini</a>
|
<a href=/people/s/samane-karimi/>Samane Karimi</a>
|
<a href=/people/a/azadeh-shakery/>Azadeh Shakery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1310><div class="card-body p-3 small">Wide and universal changes in the <a href=https://en.wikipedia.org/wiki/Web_content>web content</a> due to the growth of <a href=https://en.wikipedia.org/wiki/Web_2.0>web 2 applications</a> increase the importance of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. Therefore, the related research areas such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> and subjectivity detection receives much attention from the research community. Due to the diverse languages that web-users use to express their opinions and sentiments, research areas like subjectivity detection should present methods which are practicable on all languages. An important prerequisite to effectively achieve this aim is considering the limitations in resource-lean languages. In this paper, cross-lingual subjectivity detection on resource lean languages is investigated using two different approaches : a language-model based and a learning-to-rank approach. Experimental results show the impact of different factors on the performance of subjectivity detection methods using <a href=https://en.wikipedia.org/wiki/English_language>English resources</a> to detect the subjectivity score of <a href=https://en.wikipedia.org/wiki/Persian_language>Persian documents</a>. The experiments demonstrate that the proposed learning-to-rank method outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline method</a> in ranking documents based on their <a href=https://en.wikipedia.org/wiki/Subjectivity>subjectivity degree</a>.</div></div></div><hr><div id=w19-14><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-14.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-14/>Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1400/>Proceedings of the Sixth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-1405.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-1405/>Modeling Global Syntactic Variation in English Using Dialect Classification<span class=acl-fixed-case>E</span>nglish Using Dialect Classification</a></strong><br><a href=/people/j/jonathan-dunn/>Jonathan Dunn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1405><div class="card-body p-3 small">This paper evaluates global-scale dialect identification for 14 national varieties of English on both <a href=https://en.wikipedia.org/wiki/Web_crawler>web-crawled data</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter data</a>. The paper makes three main contributions : (i) introducing data-driven language mapping as a method for selecting the inventory of national varieties to include in the task ; (ii) producing a large and dynamic set of syntactic features using <a href=https://en.wikipedia.org/wiki/Grammar_induction>grammar induction</a> rather than focusing on a few hand-selected features such as <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> ; and (iii) comparing models across both web corpora and social media corpora in order to measure the robustness of syntactic variation across registers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1406/>Language Discrimination and Transfer Learning for Similar Languages : Experiments with Feature Combinations and Adaptation</a></strong><br><a href=/people/n/nianheng-wu/>Nianheng Wu</a>
|
<a href=/people/e/eric-demattos/>Eric DeMattos</a>
|
<a href=/people/k/kwok-him-so/>Kwok Him So</a>
|
<a href=/people/p/pin-zhen-chen/>Pin-zhen Chen</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1406><div class="card-body p-3 small">This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines : <a href=https://en.wikipedia.org/wiki/Support_Vector_Machine>SVM</a> with a flat combination of features and <a href=https://en.wikipedia.org/wiki/Support_Vector_Machine>SVM ensembles</a>. We participated in all language / dialect identification tasks, as well as the Moldavian vs. Romanian cross-dialect topic identification (MRC) task. Our team achieved first place in German Dialect identification (GDI) and MRC subtasks 2 and 3, second place in the simplified variant of Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT) as well as Cuneiform Language Identification (CLI), and third and fifth place in DMT traditional and MRC subtask 1 respectively. In most cases, the <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM</a> with a flat combination of features performed better than <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM ensembles</a>. Besides describing the systems and the results obtained by them, we provide a tentative comparison between the feature combination methods, and present additional experiments with a method of adaptation to the test set, which may indicate potential pitfalls with some of the data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1411/>Toward a deep dialectological representation of Indo-Aryan<span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>A</span>ryan</a></strong><br><a href=/people/c/chundra-cathcart/>Chundra Cathcart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1411><div class="card-body p-3 small">This paper presents a new approach to disentangling inter-dialectal and intra-dialectal relationships within one such group, the Indo-Aryan subgroup of Indo-European. We draw upon admixture models and deep generative models to tease apart historic language contact and language-specific behavior in the overall patterns of sound change displayed by <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indo-Aryan languages</a>. We show that a deep model of Indo-Aryan dialectology sheds some light on questions regarding inter-relationships among the Indo-Aryan languages, and performs better than a shallow model in terms of certain qualities of the posterior distribution (e.g., entropy of posterior distributions), and outline future pathways for model development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1412/>Naive Bayes and BiLSTM Ensemble for Discriminating between Mainland and Taiwan Variation of Mandarin Chinese<span class=acl-fixed-case>B</span>ayes and <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> Ensemble for Discriminating between Mainland and <span class=acl-fixed-case>T</span>aiwan Variation of <span class=acl-fixed-case>M</span>andarin <span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/l/li-yang/>Li Yang</a>
|
<a href=/people/y/yang-xiang/>Yang Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1412><div class="card-body p-3 small">Automatic dialect identification is a more challengingctask than <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a>, as it requires the ability to discriminate between varieties of one language. In this paper, we propose an ensemble based system, which combines traditional machine learning models trained on bag of n-gram fetures, with deep learning models trained on word embeddings, to solve the Discriminating between Mainland and Taiwan Variation of Mandarin Chinese (DMT) shared task at VarDial 2019. Our experiments show that a character bigram-trigram combination based Naive Bayes is a very strong model for identifying varieties of <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin Chinense</a>. Through further ensemble of Navie Bayes and BiLSTM, our system (team : itsalexyang) achived an macro-averaged F1 score of 0.8530 and 0.8687 in two tracks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1413/>BAM : A combination of deep and shallow models for German Dialect Identification.<span class=acl-fixed-case>BAM</span>: A combination of deep and shallow models for <span class=acl-fixed-case>G</span>erman Dialect Identification.</a></strong><br><a href=/people/a/andrei-butnaru/>Andrei M. Butnaru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1413><div class="card-body p-3 small">* This is a submission for the Third VarDial Evaluation Campaign * In this paper, we present a machine learning approach for the German Dialect Identification (GDI) Closed Shared Task of the DSL 2019 Challenge. The proposed approach combines deep and shallow models, by applying a voting scheme on the outputs resulted from a Character-level Convolutional Neural Networks (Char-CNN), a Long Short-Term Memory (LSTM) network, and a model based on String Kernels. The first <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used is the Char-CNN model that merges multiple convolutions computed with <a href=https://en.wikipedia.org/wiki/Kernel_(statistics)>kernels</a> of different sizes. The second model is the LSTM network which applies a global max pooling over the returned sequences over time. Both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> pass the activation maps to two <a href=https://en.wikipedia.org/wiki/Connected_space>fully-connected layers</a>. The final model is based on <a href=https://en.wikipedia.org/wiki/String_kernel>String Kernels</a>, computed on character p-grams extracted from <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech transcripts</a>. The model combines two blended kernel functions, one is the presence bits kernel, and the other is the intersection kernel. The empirical results obtained in the shared task prove that the <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> can achieve good results. The <a href=https://en.wikipedia.org/wiki/System>system</a> proposed in this paper obtained the fourth place with a macro-F1 score of 62.55 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1415/>Initial Experiments In Cross-Lingual Morphological Analysis Using Morpheme Segmentation</a></strong><br><a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/l/lorenzo-tosi/>Lorenzo Tosi</a>
|
<a href=/people/a/anastasia-khorosheva/>Anastasia Khorosheva</a>
|
<a href=/people/o/oleg-serikov/>Oleg Serikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1415><div class="card-body p-3 small">The paper describes initial experiments in data-driven cross-lingual morphological analysis of open-category words using a combination of unsupervised morpheme segmentation, annotation projection and an LSTM encoder-decoder model with attention. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> provides <a href=https://en.wikipedia.org/wiki/Lemmatisation>lemmatisation</a> and morphological analysis generation for previously unseen low-resource language surface forms with only annotated data on the related languages given. Despite the inherently lossy annotation projection, we achieved the best lemmatisation F1-score in the VarDial 2019 Shared Task on Cross-Lingual Morphological Analysis for both Karachay-Balkar (Turkic languages, agglutinative morphology) and Sardinian (Romance languages, fusional morphology).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1416/>Neural and Linear Pipeline Approaches to Cross-lingual Morphological Analysis</a></strong><br><a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/j/jeremy-barnes/>Jeremy Barnes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1416><div class="card-body p-3 small">This paper describes Tbingen-Oslo team&#8217;s participation in the cross-lingual morphological analysis task in the VarDial 2019 evaluation campaign. We participated in the shared task with a standard <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a>. Our model achieved analysis F1-scores of 31.48 and 23.67 on test languages <a href=https://en.wikipedia.org/wiki/Karachay-Balkar_language>Karachay-Balkar (Turkic)</a> and <a href=https://en.wikipedia.org/wiki/Sardinian_language>Sardinian (Romance)</a> respectively. The scores are comparable to the scores obtained by the other participants in both <a href=https://en.wikipedia.org/wiki/Language_family>language families</a>, and the <a href=https://en.wikipedia.org/wiki/Score_(statistics)>analysis score</a> on the Romance data set was also the best result obtained in the shared task. Besides describing the <a href=https://en.wikipedia.org/wiki/System>system</a> used in our shared task participation, we describe another, simpler, <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifiers</a>, and present further analyses using both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our analyses, besides revealing some of the difficult cases, also confirm that the usefulness of a source language in this task is highly correlated with the similarity of source and target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1418/>SC-UPB at the VarDial 2019 Evaluation Campaign : Moldavian vs. Romanian Cross-Dialect Topic Identification<span class=acl-fixed-case>SC</span>-<span class=acl-fixed-case>UPB</span> at the <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2019 Evaluation Campaign: <span class=acl-fixed-case>M</span>oldavian vs. <span class=acl-fixed-case>R</span>omanian Cross-Dialect Topic Identification</a></strong><br><a href=/people/c/cristian-onose/>Cristian Onose</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/s/stefan-trausan-matu/>Stefan Trausan-Matu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1418><div class="card-body p-3 small">This paper describes our models for the Moldavian vs. Romanian Cross-Topic Identification (MRC) evaluation campaign, part of the VarDial 2019 workshop. We focus on the three subtasks for MRC : binary classification between the Moldavian (MD) and the Romanian (RO) dialects and two cross-dialect multi-class classification between six news topics, MD to RO and RO to MD. We propose several deep learning models based on long short-term memory cells, Bidirectional Gated Recurrent Unit (BiGRU) and Hierarchical Attention Networks (HAN). We also employ three word embedding models to represent the text as a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>low dimensional vector</a>. Our official submission includes two runs of the BiGRU and HAN models for each of the three subtasks. The best submitted <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtained the following macro-averaged F1 scores : 0.708 for subtask 1, 0.481 for subtask 2 and 0.480 for the last one. Due to a read error caused by the quoting behaviour over the test file, our final submissions contained a smaller number of items than expected. More than 50 % of the submission files were corrupted. Thus, we also present the results obtained with the corrected labels for which the HAN model achieves the following results : 0.930 for subtask 1, 0.590 for subtask 2 and 0.687 for the third one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1420/>Investigating Machine Learning Methods for Language and Dialect Identification of Cuneiform Texts</a></strong><br><a href=/people/e/ehsan-doostmohammadi/>Ehsan Doostmohammadi</a>
|
<a href=/people/m/minoo-nassajian/>Minoo Nassajian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1420><div class="card-body p-3 small">Identification of the languages written using <a href=https://en.wikipedia.org/wiki/Cuneiform>cuneiform symbols</a> is a difficult task due to the lack of resources and the problem of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a>. The Cuneiform Language Identification task in VarDial 2019 addresses the problem of identifying seven languages and dialects written in cuneiform ; Sumerian and six dialects of <a href=https://en.wikipedia.org/wiki/Akkadian_language>Akkadian language</a> : Old Babylonian, Middle Babylonian Peripheral, Standard Babylonian, <a href=https://en.wikipedia.org/wiki/Neo-Babylonian_language>Neo-Babylonian</a>, Late Babylonian, and <a href=https://en.wikipedia.org/wiki/Neo-Assyrian_language>Neo-Assyrian</a>. This paper describes the approaches taken by SharifCL team to this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> in VarDial 2019. The best result belongs to an ensemble of Support Vector Machines and a <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>naive Bayes classifier</a>, both working on <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>character-level features</a>, with macro-averaged F1-score of 72.10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1422/>DTeam @ VarDial 2019 : Ensemble based on skip-gram and triplet loss neural networks for Moldavian vs. Romanian cross-dialect topic identification<span class=acl-fixed-case>DT</span>eam @ <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2019: Ensemble based on skip-gram and triplet loss neural networks for <span class=acl-fixed-case>M</span>oldavian vs. <span class=acl-fixed-case>R</span>omanian cross-dialect topic identification</a></strong><br><a href=/people/d/diana-tudoreanu/>Diana Tudoreanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1422><div class="card-body p-3 small">This paper presents the solution proposed by DTeam in the VarDial 2019 Evaluation Campaign for the Moldavian vs. Romanian cross-topic identification task. The solution proposed is a Support Vector Machines (SVM) ensemble composed of a two character-level neural networks. The first network is a skip-gram classification model formed of an embedding layer, three convolutional layers and two fully-connected layers. The second <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> has a similar architecture, but is trained using the triplet loss function.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1424/>Comparing Pipelined and Integrated Approaches to Dialectal Arabic Neural Machine Translation<span class=acl-fixed-case>A</span>rabic Neural Machine Translation</a></strong><br><a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1424><div class="card-body p-3 small">When translating diglossic languages such as <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, situations may arise where we would like to translate a text but do not know which dialect it is. A traditional approach to this problem is to design dialect identification systems and dialect-specific machine translation systems. However, under the recent paradigm of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, shared multi-dialectal systems have become a natural alternative. Here we explore under which conditions it is beneficial to perform dialect identification for Arabic neural machine translation versus using a general system for all dialects.</div></div></div><hr><div id=w19-15><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-15.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-15/>Proceedings of the Third Workshop on Structured Prediction for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1500/>Proceedings of the Third Workshop on Structured Prediction for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/a/andre-f-t-martins/>Andre Martins</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/g/gerasimos-lampouras/>Gerasimos Lampouras</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1504/>Lightly-supervised Representation Learning with Global Interpretability</a></strong><br><a href=/people/a/andrew-zupon/>Andrew Zupon</a>
|
<a href=/people/m/maria-alexeeva/>Maria Alexeeva</a>
|
<a href=/people/m/marco-valenzuela-escarcega/>Marco Valenzuela-Escárcega</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1504><div class="card-body p-3 small">We propose a lightly-supervised approach for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, in particular named entity classification, which combines the benefits of traditional <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping</a>, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets : CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a>, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> can be used to produce interpretable models with small loss in performance. This <a href=https://en.wikipedia.org/wiki/Decision_list>decision list</a> can be edited by human experts to mitigate some of that loss and in some cases outperform the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1505/>Semi-Supervised Teacher-Student Architecture for Relation Extraction</a></strong><br><a href=/people/f/fan-luo/>Fan Luo</a>
|
<a href=/people/a/ajay-nagesh/>Ajay Nagesh</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1505><div class="card-body p-3 small">Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi-supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt <a href=https://en.wikipedia.org/wiki/Mean_Teacher>Mean Teacher</a> (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Additionally, different syntax representations are incorporated into our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10 % of the labeled corpus. Further, the syntax-aware model outperforms other syntax-free approaches across all levels of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>.</div></div></div><hr><div id=w19-16><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-16.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-16/>Proceedings of the Combined Workshop on Spatial Language Understanding (SpLU) and Grounded Communication for Robotics (RoboNLP)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1600/>Proceedings of the Combined Workshop on Spatial Language Understanding (<span class=acl-fixed-case>S</span>p<span class=acl-fixed-case>LU</span>) and Grounded Communication for Robotics (<span class=acl-fixed-case>R</span>obo<span class=acl-fixed-case>NLP</span>)</a></strong><br><a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a>
|
<a href=/people/j/jesse-thomason/>Jesse Thomason</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1603/>From Virtual to Real : A Framework for Verbal Interaction with Robots</a></strong><br><a href=/people/e/eugene-joseph/>Eugene Joseph</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1603><div class="card-body p-3 small">A Natural Language Understanding (NLU) pipeline integrated with a 3D physics-based scene is a flexible way to develop and test language-based human-robot interaction, by virtualizing people, robot hardware and the target 3D environment. Here, <a href=https://en.wikipedia.org/wiki/Interaction>interaction</a> means both controlling robots using language and conversing with them about the user&#8217;s physical environment and her daily life. Such a virtual development framework was initially developed for the Bot Colony videogame launched on Steam in June 2014, and has been undergoing improvements since. The <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> is focused of developing intuitive verbal interaction with various types of <a href=https://en.wikipedia.org/wiki/Robot>robots</a>. Key robot functions (robot vision and object recognition, path planning and obstacle avoidance, task planning and constraints, grabbing and inverse kinematics), the human participants in the interaction, and the impact of gravity and other forces on the environment are all simulated using commercial 3D tools. The framework can be used as a robotics testbed : the results of our simulations can be compared with the output of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> in real robots, to validate such <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. A novelty of our framework is support for social interaction with robots-enabling robots to converse about people and objects in the user&#8217;s environment, as well as learning about human needs and everyday life topics from their owner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1605/>Multi-modal Discriminative Model for Vision-and-Language Navigation</a></strong><br><a href=/people/h/haoshuo-huang/>Haoshuo Huang</a>
|
<a href=/people/v/vihan-jain/>Vihan Jain</a>
|
<a href=/people/h/harsh-mehta/>Harsh Mehta</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a>
|
<a href=/people/e/eugene-ie/>Eugene Ie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1605><div class="card-body p-3 small">Vision-and-Language Navigation (VLN) is a natural language grounding task where agents have to interpret natural language instructions in the context of visual scenes in a dynamic environment to achieve prescribed navigation goals. Successful agents must have the ability to parse natural language of varying linguistic styles, ground them in potentially unfamiliar scenes, plan and react with ambiguous environmental feedback. Generalization ability is limited by the amount of human annotated data. In particular, paired vision-language sequence data is expensive to collect. We develop a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> that evaluates how well an instruction explains a given path in VLN task using multi-modal alignment. Our study reveals that only a small fraction of the high-quality augmented data from Fried et al., as scored by our <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a>, is useful for training VLN agents with similar performance. We also show that a VLN agent warm-started with pre-trained components from the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> outperforms the benchmark success rates of 35.5 by 10 % relative measure.</div></div></div><hr><div id=w19-17><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-17.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-17/>Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1700/>Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies</a></strong><br><a href=/people/u/university-of-sheffield-heidi-christensen/>University of Sheffield Heidi Christensen</a>
|
<a href=/people/f/florida-institute-for-human-kristy-hollingshead/>Florida Institute for Human Kristy Hollingshead</a>
|
<a href=/people/m/machine-cognition/>Machine Cognition</a>
|
<a href=/people/b/boston-college-emily-prudhommeaux/>Boston College Emily Prud’hommeaux</a>
|
<a href=/people/u/university-of-toronto-frank-rudzicz/>University of Toronto Frank Rudzicz</a>
|
<a href=/people/m/michigan-technological-university-keith-vertanen/>Michigan Technological University Keith Vertanen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1703/>Permanent Magnetic Articulograph (PMA) vs Electromagnetic Articulograph (EMA) in Articulation-to-Speech Synthesis for Silent Speech Interface<span class=acl-fixed-case>PMA</span>) vs Electromagnetic Articulograph (<span class=acl-fixed-case>EMA</span>) in Articulation-to-Speech Synthesis for Silent Speech Interface</a></strong><br><a href=/people/b/beiming-cao/>Beiming Cao</a>
|
<a href=/people/n/nordine-sebkhi/>Nordine Sebkhi</a>
|
<a href=/people/t/ted-mau/>Ted Mau</a>
|
<a href=/people/o/omer-t-inan/>Omer T. Inan</a>
|
<a href=/people/j/jun-wang/>Jun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1703><div class="card-body p-3 small">Silent speech interfaces (SSIs) are devices that enable <a href=https://en.wikipedia.org/wiki/Speech>speech communication</a> when <a href=https://en.wikipedia.org/wiki/Speech>audible speech</a> is unavailable. Articulation-to-speech (ATS) synthesis is a software design in <a href=https://en.wikipedia.org/wiki/Speech_synthesis>SSI</a> that directly converts articulatory movement information into audible speech signals. Permanent magnetic articulograph (PMA) is a wireless articulator motion tracking technology that is similar to commercial, wired Electromagnetic Articulograph (EMA). PMA has shown great potential for practical SSI applications, because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is wireless. The <a href=https://en.wikipedia.org/wiki/Autostereoscopy>ATS</a> performance of <a href=https://en.wikipedia.org/wiki/Poly(methyl_methacrylate)>PMA</a>, however, is unknown when compared with current <a href=https://en.wikipedia.org/wiki/Electromagnetic_spectrum>EMA</a>. In this study, we compared the performance of ATS using a PMA we recently developed and a commercially available EMA (NDI Wave system). Datasets with same stimuli and size that were collected from tongue tip were used in the comparison. The experimental results indicated the performance of PMA was close to, although not as equally good as that of EMA. Furthermore, in PMA, converting the raw magnetic signals to positional signals did not significantly affect the performance of <a href=https://en.wikipedia.org/wiki/Autostereoscopy>ATS</a>, which support the future direction in PMA-based ATS can be focused on the use of positional signals to maximize the benefit of <a href=https://en.wikipedia.org/wiki/Spatial_analysis>spatial analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1706/>Investigating <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a> for Improving Predictive AAC<span class=acl-fixed-case>AAC</span></a></strong><br><a href=/people/j/jiban-adhikary/>Jiban Adhikary</a>
|
<a href=/people/r/robbie-watling/>Robbie Watling</a>
|
<a href=/people/c/crystal-fletcher/>Crystal Fletcher</a>
|
<a href=/people/a/alex-stanage/>Alex Stanage</a>
|
<a href=/people/k/keith-vertanen/>Keith Vertanen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1706><div class="card-body p-3 small">Making good letter or word predictions can help accelerate the communication of users of high-tech AAC devices. This is particularly important for real-time person-to-person conversations. We investigate whether per forming speech recognition on the speaking-side of a conversation can improve language model based predictions. We compare the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of three plausible microphone deployment options and the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of two commercial speech recognition engines (Google and IBM Watson). We found that despite recognition word error rates of 7-16 %, our ensemble of N-gram and recurrent neural network language models made predictions nearly as good as when they used the reference transcripts.</div></div></div><hr><div id=w19-18><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-18.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-18/>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1800/>Proceedings of the Second Workshop on Shortcomings in Vision and Language</a></strong><br><a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernandez</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/k/kushal-kafle/>Kushal Kafle</a>
|
<a href=/people/c/christopher-kanan/>Christopher Kanan</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-1802.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-1802/>Referring to Objects in Videos Using Spatio-Temporal Identifying Descriptions</a></strong><br><a href=/people/p/peratham-wiriyathammabhum/>Peratham Wiriyathammabhum</a>
|
<a href=/people/a/abhinav-shrivastava/>Abhinav Shrivastava</a>
|
<a href=/people/v/vlad-morariu/>Vlad Morariu</a>
|
<a href=/people/l/larry-davis/>Larry Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1802><div class="card-body p-3 small">This paper presents a new task, the grounding of spatio-temporal identifying descriptions in videos. Previous work suggests potential bias in existing datasets and emphasizes the need for a new data creation schema to better model linguistic structure. We introduce a new data collection scheme based on grammatical constraints for surface realization to enable us to investigate the problem of grounding spatio-temporal identifying descriptions in videos. We then propose a two-stream modular attention network that learns and grounds spatio-temporal identifying descriptions based on appearance and motion. We show that motion modules help to ground motion-related words and also help to learn in appearance modules because modular neural networks resolve task interference between modules. Finally, we propose a future challenge and a need for a robust system arising from replacing ground truth visual annotations with automatic video object detector and temporal event localization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-1803" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-1803/>A Survey on Biomedical Image Captioning</a></strong><br><a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/v/vasiliki-kougia/>Vasiliki Kougia</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1803><div class="card-body p-3 small">Image captioning applied to biomedical images can assist and accelerate the diagnosis process followed by clinicians. This article is the first survey of biomedical image captioning, discussing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation measures</a>, and <a href=https://en.wikipedia.org/wiki/Scientific_method>state of the art methods</a>. Additionally, we suggest two <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, a weak and a stronger one ; the latter outperforms all current state of the art systems on one of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1807/>Learning Multilingual Word Embeddings Using Image-Text Data</a></strong><br><a href=/people/k/karan-singhal/>Karan Singhal</a>
|
<a href=/people/k/karthik-raman/>Karthik Raman</a>
|
<a href=/people/b/balder-ten-cate/>Balder ten Cate</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1807><div class="card-body p-3 small">There has been significant interest recently in learning multilingual word embeddings in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.</div></div></div><hr><div id=w19-19><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-19.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-19/>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1900/>Proceedings of the 2nd Clinical Natural Language Processing Workshop</a></strong><br><a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/t/tristan-naumann/>Tristan Naumann</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1902/>An Analysis of Attention over Clinical Notes for Predictive Tasks</a></strong><br><a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/r/ramin-mohammadi/>Ramin Mohammadi</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1902><div class="card-body p-3 small">The shift to electronic medical records (EMRs) has engendered research into machine learning and natural language technologies to analyze patient records, and to predict from these clinical outcomes of interest. Two observations motivate our aims here. First, <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured notes</a> contained within EMR often contain key information, and hence should be exploited by <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Second, while strong predictive performance is important, interpretability of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is perhaps equally so for applications in this domain. Together, these points suggest that neural models for <a href=https://en.wikipedia.org/wiki/Electroencephalography>EMR</a> may benefit from incorporation of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> over notes, which one may hope will both yield performance gains and afford transparency in predictions. In this work we perform experiments to explore this question using two EMR corpora and four different predictive tasks, that : (i) inclusion of attention mechanisms is critical for neural encoder modules that operate over notes fields in order to yield competitive performance, but, (ii) unfortunately, while these boost predictive performance, it is decidedly less clear whether they provide meaningful support for predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-1904.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-1904/>Hierarchical Nested Named Entity Recognition</a></strong><br><a href=/people/z/zita-marinho/>Zita Marinho</a>
|
<a href=/people/a/alfonso-mendes/>Afonso Mendes</a>
|
<a href=/people/s/sebastiao-miranda/>Sebastião Miranda</a>
|
<a href=/people/d/david-nogueira/>David Nogueira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1904><div class="card-body p-3 small">In the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a> and other scientific areas, it is often important to recognize different levels of hierarchy in mentions, such as those related to specific symptoms or diseases associated with different <a href=https://en.wikipedia.org/wiki/Anatomical_terms_of_location>anatomical regions</a>. Unlike previous approaches, we build a transition-based parser that explicitly models an arbitrary number of hierarchical and nested mentions, and propose a <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> that encourages correct predictions of higher-level mentions. We further introduce a set of modifier classes which introduces certain concepts that change the meaning of an entity, such as absence, or uncertainty about a given disease. Our proposed model achieves state-of-the-art results in medical entity recognition datasets, using both nested and hierarchical mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-1905" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-1905/>Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models</a></strong><br><a href=/people/o/oren-melamud/>Oren Melamud</a>
|
<a href=/people/c/chaitanya-shivade/>Chaitanya Shivade</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1905><div class="card-body p-3 small">Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding <a href=https://en.wikipedia.org/wiki/Patient_privacy>patient privacy</a> hinder the open dissemination of such <a href=https://en.wikipedia.org/wiki/Data>data</a> and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in <a href=https://en.wikipedia.org/wiki/Medical_record>clinical notes</a> and propose to automatically generate synthetic clinical notes that are more amenable to sharing using <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as <a href=https://en.wikipedia.org/wiki/Utility>utility</a> in training clinical NLP models. Experiments using neural language models yield notes whose <a href=https://en.wikipedia.org/wiki/Utility>utility</a> is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1906/>A Novel System for Extractive Clinical Note Summarization using <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHR Data</a><span class=acl-fixed-case>EHR</span> Data</a></strong><br><a href=/people/j/jennifer-liang/>Jennifer Liang</a>
|
<a href=/people/c/ching-huei-tsou/>Ching-Huei Tsou</a>
|
<a href=/people/a/ananya-poddar/>Ananya Poddar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1906><div class="card-body p-3 small">While much data within a patient&#8217;s electronic health record (EHR) is coded, crucial information concerning the patient&#8217;s care and management remain buried in unstructured clinical notes, making it difficult and time-consuming for physicians to review during their usual clinical workflow. In this paper, we present our clinical note processing pipeline, which extends beyond basic medical natural language processing (NLP) with concept recognition and relation detection to also include components specific to EHR data, such as structured data associated with the encounter, sentence-level clinical aspects, and structures of the clinical notes. We report on the use of this <a href=https://en.wikipedia.org/wiki/Drug_pipeline>pipeline</a> in a disease-specific extractive text summarization task on <a href=https://en.wikipedia.org/wiki/Medical_record>clinical notes</a>, focusing primarily on progress notes by physicians and nurse practitioners. We show how the addition of EHR-specific components to the pipeline resulted in an improvement in our overall system performance and discuss the potential impact of EHR-specific components on other higher-level clinical NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1912/>Medical Entity Linking using Triplet Network</a></strong><br><a href=/people/i/ishani-mondal/>Ishani Mondal</a>
|
<a href=/people/s/sukannya-purkayastha/>Sukannya Purkayastha</a>
|
<a href=/people/s/sudeshna-sarkar/>Sudeshna Sarkar</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/j/jitesh-pillai/>Jitesh Pillai</a>
|
<a href=/people/a/amitava-bhattacharyya/>Amitava Bhattacharyya</a>
|
<a href=/people/m/mahanandeeshwar-gattu/>Mahanandeeshwar Gattu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1912><div class="card-body p-3 small">Entity linking (or Normalization) is an essential task in <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a> that maps the entity mentions in the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical text</a> to standard entities in a given Knowledge Base (KB). This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is of great importance in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>. It can also be used for merging different <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>medical and clinical ontologies</a>. In this paper, we center around the problem of disease linking or normalization. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is executed in two phases : candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for <a href=https://en.wikipedia.org/wiki/Ranked_voting>candidate ranking</a>. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1914/>Extracting Factual Min / Max Age Information from Clinical Trial Studies<span class=acl-fixed-case>M</span>in/Max Age Information from Clinical Trial Studies</a></strong><br><a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/d/debasis-ganguly/>Debasis Ganguly</a>
|
<a href=/people/l/lea-deleris/>Léa Deleris</a>
|
<a href=/people/f/francesca-bonin/>Francesca Bonin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1914><div class="card-body p-3 small">Population age information is an essential characteristic of <a href=https://en.wikipedia.org/wiki/Clinical_trial>clinical trials</a>. In this paper, we focus on extracting minimum and maximum (min / max) age values for the study samples from clinical research articles. Specifically, we investigate the use of a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> to address this information extraction task. The min / max age QA model is trained on the massive structured clinical study records from <a href=https://en.wikipedia.org/wiki/ClinicalTrials.gov>ClinicalTrials.gov</a>. For each article, based on multiple min and max age values extracted from the QA model, we predict both actual min / max age values for the study samples and filter out non-factual age expressions. Our system improves the results over (i) a passage retrieval based IE system and (ii) a CRF-based system by a large margin when evaluated on an annotated dataset consisting of 50 research papers on <a href=https://en.wikipedia.org/wiki/Smoking_cessation>smoking cessation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1915/>Distinguishing Clinical Sentiment : The Importance of Domain Adaptation in Psychiatric Patient Health Records</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/p/philip-cawkwell/>Philip Cawkwell</a>
|
<a href=/people/k/kirsten-bolton/>Kirsten Bolton</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/mei-hua-hall/>Mei-Hua Hall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1915><div class="card-body p-3 small">Recently <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tools</a> have been developed to identify and extract salient risk indicators in <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health records (EHRs)</a>. Sentiment analysis, although widely used in non-medical areas for improving <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to psychiatric EHRs by defining psychiatric clinical sentiment, performing an annotation project, and evaluating multiple sentence-level sentiment machine learning (ML) models. Results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity, and that the definition of clinical sentiment that we provide is learnable with relatively small amounts of training data. This project is an initial step towards further refining <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis methods</a> for clinical use. Our long-term objective is to incorporate the results of this project as part of a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> that predicts inpatient readmission risk. We hope that this work will initiate a discussion concerning domain adaptation of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to the clinical setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-1917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-1917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-1917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-1917/>Attention Neural Model for Temporal Relation Extraction</a></strong><br><a href=/people/s/sijia-liu/>Sijia Liu</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a>
|
<a href=/people/v/vipin-chaudhary/>Vipin Chaudhary</a>
|
<a href=/people/h/hongfang-liu/>Hongfang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-1917><div class="card-body p-3 small">Neural network models have shown promise in the temporal relation extraction task. In this paper, we present the attention based neural network model to extract the containment relations within sentences from clinical narratives. The attention mechanism used on top of GRU model outperforms the existing state-of-the-art neural network models on THYME corpus in intra-sentence temporal relation extraction.</div></div></div><hr><div id=w19-20><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-20.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-20/>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2000/>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2002.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2002.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2002/>Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance</a></strong><br><a href=/people/b/brendan-whitaker/>Brendan Whitaker</a>
|
<a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/aparajita-haldar/>Aparajita Haldar</a>
|
<a href=/people/h/hakan-ferhatosmanoglu/>Hakan Ferhatosmanoglu</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2002><div class="card-body p-3 small">Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbors</a>. However, <a href=https://en.wikipedia.org/wiki/Geometry>geometric properties</a> of the continuous feature space contribute directly to the use of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>embedding features</a> in downstream models, and are largely unexplored. We consider four properties of word embedding geometry, namely : position relative to the origin, distribution of features in the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, global pairwise distances, and local pairwise distances. We define a sequence of <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> to generate new embeddings that expose subsets of these <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>properties</a> to downstream models and evaluate change in task performance to understand the contribution of each property to NLP models. We transform publicly available pretrained embeddings from three popular toolkits (word2vec, GloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model linguistic information in the vector space, and extrinsic tasks, which use vectors as input to machine learning models. We find that intrinsic evaluations are highly sensitive to absolute position, while extrinsic tasks rely primarily on local similarity. Our findings suggest that future embedding models and post-processing techniques should focus primarily on <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> to nearby points in <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2003/>The Influence of Down-Sampling Strategies on SVD Word Embedding Stability<span class=acl-fixed-case>SVD</span> Word Embedding Stability</a></strong><br><a href=/people/j/johannes-hellrich/>Johannes Hellrich</a>
|
<a href=/people/b/bernd-kampe/>Bernd Kampe</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2003><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Consistency>stability</a> of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD-PPMI-type embeddings. This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior <a href=https://en.wikipedia.org/wiki/BIBO_stability>stability</a> as well as <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on par with skip-gram embedding</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2004/>How Well Do Embedding Models Capture Non-compositionality? A View from Multiword Expressions</a></strong><br><a href=/people/n/navnita-nandakumar/>Navnita Nandakumar</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/b/bahar-salehi/>Bahar Salehi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2004><div class="card-body p-3 small">In this paper, we apply various <a href=https://en.wikipedia.org/wiki/Embedding>embedding methods</a> on multiword expressions to study how well they capture the nuances of non-compositional data. Our results from a pool of word-, character-, and document-level embbedings suggest that <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a> performs the best, followed by <a href=https://en.wikipedia.org/wiki/FastText>FastText</a> and Infersent. Moreover, we find that recently-proposed contextualised embedding models such as Bert and ELMo are not adept at handling non-compositionality in multiword expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2005.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2005/>Measuring Semantic Abstraction of Multilingual NMT with Paraphrase Recognition and Generation Tasks<span class=acl-fixed-case>NMT</span> with Paraphrase Recognition and Generation Tasks</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2005><div class="card-body p-3 small">In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> when applied to <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> of the source language. The intuition is that an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> produces better representations if a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is capable of recognizing synonymous sentences in the same language even though the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> is significantly reduced in each of the cases, indicating that meaning can be grounded in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. This is further supported by a study on <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> that we also include at the end of the paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2008/>CODAH : An Adversarially-Authored Question Answering Dataset for Common Sense<span class=acl-fixed-case>CODAH</span>: An Adversarially-Authored Question Answering Dataset for Common Sense</a></strong><br><a href=/people/m/michael-chen/>Michael Chen</a>
|
<a href=/people/m/mike-darcy/>Mike D’Arcy</a>
|
<a href=/people/a/alisa-liu/>Alisa Liu</a>
|
<a href=/people/j/jared-fernandez/>Jared Fernandez</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2008><div class="card-body p-3 small">Commonsense reasoning is a critical AI capability, but it is difficult to construct challenging <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that test <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to achieve human-level scores. We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>. CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedure</a> and evaluate the performance of multiple state-of-the-art <a href=https://en.wikipedia.org/wiki/Question_answering>question answering systems</a> on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We observe a significant gap between <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a>, which is 95.3 %, and the performance of the best baseline accuracy of 65.3 % by the OpenAI GPT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2009.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2009.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2009/>Syntactic Interchangeability in Word Embedding Models</a></strong><br><a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/a/assaf-toledo/>Assaf Toledo</a>
|
<a href=/people/a/alon-halfon/>Alon Halfon</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2009><div class="card-body p-3 small">Nearest neighbors in word embedding models are commonly observed to be semantically similar, but the relations between them can vary greatly. We investigate the extent to which word embedding models preserve syntactic interchangeability, as reflected by distances between word vectors, and the effect of hyper-parameterscontext window size in particular. We use part of speech (POS) as a proxy for syntactic interchangeability, as generally speaking, words with the same POS are syntactically valid in the same contexts. We also investigate the relationship between <a href=https://en.wikipedia.org/wiki/Interchangeability>interchangeability</a> and <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> as judged by commonly-used word similarity benchmarks, and correlate the result with the performance of word embedding models on these benchmarks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2011/>Probing Biomedical Embeddings from Language Models</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/x/xinghua-lu/>Xinghua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2011><div class="card-body p-3 small">Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained <a href=https://en.wikipedia.org/wiki/Linear_model>LMs</a> as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT (Devlin et al. 2018), ELMo (Peters et al., 2018), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10 M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a> and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2012/>Dyr Bul Shchyl. Proxying Sound Symbolism With Word Embeddings</a></strong><br><a href=/people/i/ivan-p-yamshchikov/>Ivan P. Yamshchikov</a>
|
<a href=/people/v/viascheslav-shibaev/>Viascheslav Shibaev</a>
|
<a href=/people/a/alexey-tikhonov/>Alexey Tikhonov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2012><div class="card-body p-3 small">This paper explores modern <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in the context of <a href=https://en.wikipedia.org/wiki/Sound_symbolism>sound symbolism</a>. Using basic properties of the representations space one can construct semantic axes. A method is proposed to measure if the presence of individual sounds in a given word shifts its <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of that word along a specific axis. It is shown that, in accordance with several experimental and statistical results, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> capture <a href=https://en.wikipedia.org/wiki/Symbol>symbolism</a> for certain sounds.</div></div></div><hr><div id=w19-21><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-21.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-21/>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2100/>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</a></strong><br><a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/d/david-bamman/>David Bamman</a>
|
<a href=/people/o/oren-tsur/>Oren Tsur</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2104/>Geolocating Political Events in Text</a></strong><br><a href=/people/a/andrew-halterman/>Andrew Halterman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2104><div class="card-body p-3 small">This work introduces a general <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for automatically finding the locations where <a href=https://en.wikipedia.org/wiki/Politics>political events</a> in text occurred. Using a novel set of 8,000 labeled sentences, I create a method to link automatically extracted events and locations in text. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves human level performance on the annotation task and outperforms previous event geolocation systems. It can be applied to most <a href=https://en.wikipedia.org/wiki/Event_(computing)>event extraction systems</a> across <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geographic contexts</a>. I formalize the eventlocation linking task, describe the neural network model, describe the potential uses of such a system in <a href=https://en.wikipedia.org/wiki/Political_science>political science</a>, and demonstrate a workflow to answer an open question on the role of conventional military offensives in causing civilian casualties in the Syrian civil war.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2105/>Neural Network Prediction of Censorable Language</a></strong><br><a href=/people/k/kei-yin-ng/>Kei Yin Ng</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/j/jing-peng/>Jing Peng</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2105><div class="card-body p-3 small">Internet censorship imposes restrictions on what information can be publicized or viewed on the Internet. According to Freedom House&#8217;s annual Freedom on the Net report, more than half the world&#8217;s Internet users now live in a place where the Internet is censored or restricted. China has built the world&#8217;s most extensive and sophisticated <a href=https://en.wikipedia.org/wiki/Internet_censorship_in_China>online censorship system</a>. In this paper, we describe a new corpus of censored and uncensored social media tweets from a Chinese microblogging website, <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a>, collected by tracking posts that mention &#8216;sensitive&#8217; topics or authored by &#8216;sensitive&#8217; users. We use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to build a neural network classifier to predict <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs with a 88.50 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> using only <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. We discuss these features in detail and hypothesize that they could potentially be used for <a href=https://en.wikipedia.org/wiki/Censorship_circumvention>censorship circumvention</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2107/>Using <a href=https://en.wikipedia.org/wiki/Time_series>time series and natural language processing</a> to identify <a href=https://en.wikipedia.org/wiki/Viral_phenomenon>viral moments</a> in the 2016 U.S. Presidential Debate<span class=acl-fixed-case>U</span>.<span class=acl-fixed-case>S</span>. Presidential Debate</a></strong><br><a href=/people/j/josephine-lukito/>Josephine Lukito</a>
|
<a href=/people/p/prathusha-kameswara-sarma/>Prathusha K Sarma</a>
|
<a href=/people/j/jordan-foley/>Jordan Foley</a>
|
<a href=/people/a/aman-abhishek/>Aman Abhishek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2107><div class="card-body p-3 small">This paper proposes a method for identifying and studying viral moments or highlights during a <a href=https://en.wikipedia.org/wiki/Political_debate>political debate</a>. Using a combined strategy of <a href=https://en.wikipedia.org/wiki/Time_series>time series analysis</a> and domain adapted word embeddings, this study provides an in-depth analysis of several key moments during the 2016 U.S. Presidential election. First, a time series outlier analysis is used to identify key moments during the debate. These <a href=https://en.wikipedia.org/wiki/Moment_(mathematics)>moments</a> had to result in a long-term shift in attention towards either <a href=https://en.wikipedia.org/wiki/Hillary_Clinton>Hillary Clinton</a> or Donald Trump (i.e., a transient change outlier or an intervention, resulting in a permanent change in the time series). To assess whether these moments also resulted in a discursive shift, two corpora are produced for each potential viral moment (a pre-viral corpus and post-viral corpus). A domain adaptation layer learns weights to combine a generic and domain-specific (DS) word embedding into a domain adapted (DA) embedding. Words are then classified using a generic encoder+ classifier framework that relies on these <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as inputs. Results suggest that both <a href=https://en.wikipedia.org/wiki/Hillary_Clinton>Clinton</a> and <a href=https://en.wikipedia.org/wiki/Donald_Trump>Trump</a> were able to induce discourse-shifting viral moments, though the former is much better at producing a topically-specific discursive shift.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2108/>Stance Classification, Outcome Prediction, and <a href=https://en.wikipedia.org/wiki/Impact_assessment>Impact Assessment</a> : NLP Tasks for Studying Group Decision-Making<span class=acl-fixed-case>NLP</span> Tasks for Studying Group Decision-Making</a></strong><br><a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/a/alan-w-black/>Alan Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2108><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Group_decision-making>group decision-making</a>, the nuanced process of conflict and resolution that leads to <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>consensus formation</a> is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>process variables</a>, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> alongside a large new corpus of over 400,000 <a href=https://en.wikipedia.org/wiki/Internet_forum>group debates</a> on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2112/>Modeling Behavioral Aspects of Social Media Discourse for Moral Classification</a></strong><br><a href=/people/k/kristen-johnson/>Kristen Johnson</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2112><div class="card-body p-3 small">Political discourse on <a href=https://en.wikipedia.org/wiki/Microblogging>social media microblogs</a>, specifically <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, has become an undeniable part of mainstream U.S. politics. Given the length constraint of tweets, politicians must carefully word their statements to ensure their message is understood by their intended audience. This constraint often eliminates the context of the tweet, making automatic analysis of social media political discourse a difficult task. To overcome this challenge, we propose simultaneous modeling of high-level abstractions of political language, such as political slogans and framing strategies, with <a href=https://en.wikipedia.org/wiki/Abstraction>abstractions</a> of how politicians behave on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. These behavioral abstractions can be further leveraged as forms of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> in order to increase prediction accuracy, while reducing the burden of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. In this work, we use Probabilistic Soft Logic (PSL) to build <a href=https://en.wikipedia.org/wiki/Relational_model>relational models</a> to capture the similarities in language and behavior that obfuscate political messages on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. When combined, these descriptors reveal the moral foundations underlying the discourse of U.S. politicians online, across differing governing administrations, showing how <a href=https://en.wikipedia.org/wiki/List_of_political_parties_in_the_United_States>party talking points</a> remain cohesive or change over time.<i>across</i> differing governing administrations, showing how party talking points remain cohesive or change over time.</div></div></div><hr><div id=w19-22><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-22.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-22/>Proceedings of the Natural Legal Language Processing Workshop 2019</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2200/>Proceedings of the Natural Legal Language Processing Workshop 2019</a></strong><br><a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a>
|
<a href=/people/e/elliott-ash/>Elliott Ash</a>
|
<a href=/people/l/leslie-barrett/>Leslie Barrett</a>
|
<a href=/people/d/daniel-chen/>Daniel Chen</a>
|
<a href=/people/a/adam-meyers/>Adam Meyers</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preotiuc-Pietro</a>
|
<a href=/people/d/david-rosenberg/>David Rosenberg</a>
|
<a href=/people/a/amanda-stent/>Amanda Stent</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2202/>Scalable Methods for Annotating Legal-Decision Corpora</a></strong><br><a href=/people/l/lisa-ferro/>Lisa Ferro</a>
|
<a href=/people/j/john-aberdeen/>John Aberdeen</a>
|
<a href=/people/k/karl-branting/>Karl Branting</a>
|
<a href=/people/c/craig-pfeifer/>Craig Pfeifer</a>
|
<a href=/people/a/alexander-yeh/>Alexander Yeh</a>
|
<a href=/people/a/amartya-chakraborty/>Amartya Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2202><div class="card-body p-3 small">Recent research has demonstrated that judicial and administrative decisions can be predicted by <a href=https://en.wikipedia.org/wiki/Machine_learning>machine-learning models</a> trained on prior decisions. However, to have any practical application, these predictions must be explainable, which in turn requires modeling a rich set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Such approaches face a roadblock if the <a href=https://en.wikipedia.org/wiki/Knowledge_engineering>knowledge engineering</a> required to create these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> is not scalable. We present an approach to developing a feature-rich corpus of administrative rulings about domain name disputes, an approach which leverages a small amount of manual annotation and prototypical patterns present in the case documents to automatically extend feature labels to the entire corpus. To demonstrate the feasibility of this approach, we report results from <a href=https://en.wikipedia.org/wiki/Computer>systems</a> trained on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2203/>The Extent of Repetition in Contract Language</a></strong><br><a href=/people/d/dan-simonson/>Dan Simonson</a>
|
<a href=/people/d/daniel-broderick/>Daniel Broderick</a>
|
<a href=/people/j/jonathan-herr/>Jonathan Herr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2203><div class="card-body p-3 small">Contract language is repetitive (Anderson and Manns, 2017), but so is all language (Zipf, 1949). In this paper, we measure the extent to which contract language in <a href=https://en.wikipedia.org/wiki/English_language>English</a> is repetitive compared with the language of other <a href=https://en.wikipedia.org/wiki/English_language>English language corpora</a>. Contracts have much smaller vocabulary sizes compared with similarly sized non-contract corpora across multiple contract types, contain 1/5th as many <a href=https://en.wikipedia.org/wiki/Hapax_legomena>hapax legomena</a>, pattern differently on a log-log plot, use fewer pronouns, and contain sentences that are about 20 % more similar to one another than in other corpora. These suggest that the study of <a href=https://en.wikipedia.org/wiki/Contract>contracts</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> controls for some linguistic phenomena and allows for more in depth study of others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2204/>Sentence Boundary Detection in Legal Text</a></strong><br><a href=/people/g/george-sanchez/>George Sanchez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2204><div class="card-body p-3 small">In this paper, we examined several <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to detect <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence boundaries</a> in <a href=https://en.wikipedia.org/wiki/Legal_writing>legal text</a>. Legal text presents challenges for sentence tokenizers because of the variety of <a href=https://en.wikipedia.org/wiki/Punctuation>punctuations</a> and syntax of legal text. Out-of-the-box algorithms perform poorly on <a href=https://en.wikipedia.org/wiki/Legal_writing>legal text</a> affecting further analysis of the text. A novel and domain-specific approach is needed to detect <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence boundaries</a> to further analyze <a href=https://en.wikipedia.org/wiki/Legal_text>legal text</a>. We present the results of our investigation in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2206/>Litigation Analytics : Case Outcomes Extracted from US Federal Court Dockets<span class=acl-fixed-case>US</span> Federal Court Dockets</a></strong><br><a href=/people/t/thomas-vacek/>Thomas Vacek</a>
|
<a href=/people/r/ronald-teo/>Ronald Teo</a>
|
<a href=/people/d/dezhao-song/>Dezhao Song</a>
|
<a href=/people/t/timothy-nugent/>Timothy Nugent</a>
|
<a href=/people/c/conner-cowling/>Conner Cowling</a>
|
<a href=/people/f/frank-schilder/>Frank Schilder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2206><div class="card-body p-3 small">Dockets contain a wealth of information for planning a litigation strategy, but the information is locked up in semi-structured text. Manually deriving the outcomes for each party (e.g., settlement, verdict) would be very labor intensive. Having such information available for every past court case, however, would be very useful for developing a <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> because it potentially reveals tendencies and trends of judges and courts and the opposing counsel. We used Natural Language Processing (NLP) techniques and deep learning methods allowing us to scale the automatic analysis of millions of US federal court dockets. The automatically extracted information is fed into a Litigation Analytics tool that is used by lawyers to plan how they approach concrete litigations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2208/>Legal Area Classification : A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments<span class=acl-fixed-case>S</span>ingapore <span class=acl-fixed-case>S</span>upreme <span class=acl-fixed-case>C</span>ourt Judgments</a></strong><br><a href=/people/j/jerrold-soh/>Jerrold Soh</a>
|
<a href=/people/h/how-khang-lim/>How Khang Lim</a>
|
<a href=/people/i/ian-ernst-chai/>Ian Ernst Chai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2208><div class="card-body p-3 small">This paper conducts a comparative study on the performance of various machine learning approaches for classifying judgments into legal areas. Using a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> for the legal domain.</div></div></div><hr><div id=w19-23><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-23.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-23/>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2300/>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</a></strong><br><a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/u/urvashi-khandelwal/>Urvashi Khandelwal</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2301/>An Adversarial Learning Framework For A Persona-Based Multi-Turn Dialogue Model</a></strong><br><a href=/people/o/oluwatobi-olabiyi/>Oluwatobi Olabiyi</a>
|
<a href=/people/a/anish-khazane/>Anish Khazane</a>
|
<a href=/people/a/alan-salimov/>Alan Salimov</a>
|
<a href=/people/e/erik-mueller/>Erik Mueller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2301><div class="card-body p-3 small">In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator : (1) phredGANa, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGANd, a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona Seq2Seq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of <a href=https://en.wikipedia.org/wiki/Quantitative_research>quantitative measures</a> as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with <a href=https://en.wikipedia.org/wiki/Big_Bang>Big Bang Theory and Friends</a>) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2303/>How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature</a></strong><br><a href=/people/s/simeng-sun/>Simeng Sun</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2303><div class="card-body p-3 small">We show that plain ROUGE F1 scores are not ideal for comparing current neural systems which on average produce different lengths. This is due to a <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linear pattern</a> between ROUGE F1 and summary length. To alleviate the effect of <a href=https://en.wikipedia.org/wiki/Length>length</a> during evaluation, we have proposed a new method which normalizes the ROUGE F1 scores of a <a href=https://en.wikipedia.org/wiki/System>system</a> by that of a random system with same average output length. A pilot human evaluation has shown that humans prefer short summaries in terms of the verbosity of a summary but overall consider longer summaries to be of higher quality. While human evaluations are more expensive in time and resources, it is clear that <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a>, such as the one we proposed for automatic evaluation, will make human evaluations more meaningful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2304" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2304/>BERT has a Mouth, and It Must Speak : BERT as a Markov Random Field Language Model<span class=acl-fixed-case>BERT</span> has a Mouth, and It Must Speak: <span class=acl-fixed-case>BERT</span> as a <span class=acl-fixed-case>M</span>arkov Random Field Language Model</a></strong><br><a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2304><div class="card-body p-3 small">We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from <a href=https://en.wikipedia.org/wiki/Boolean_satisfiability_problem>BERT</a>. We generate from <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and find that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2307/>Bilingual-GAN : A Step Towards Parallel Text Generation<span class=acl-fixed-case>GAN</span>: A Step Towards Parallel Text Generation</a></strong><br><a href=/people/a/ahmad-rashid/>Ahmad Rashid</a>
|
<a href=/people/a/alan-do-omri/>Alan Do-Omri</a>
|
<a href=/people/m/md-akmal-haidar/>Md. Akmal Haidar</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2307><div class="card-body p-3 small">Latent space based GAN methods and attention based sequence to sequence models have achieved impressive results in text generation and unsupervised machine translation respectively. Leveraging the two domains, we propose an adversarial latent space based model capable of generating parallel sentences in two languages concurrently and translating bidirectionally. The bilingual generation goal is achieved by sampling from the latent space that is shared between both languages. First two denoising autoencoders are trained, with shared encoders and back-translation to enforce a shared latent state between the two languages. The <a href=https://en.wikipedia.org/wiki/Code>decoder</a> is shared for the two translation directions. Next, a GAN is trained to generate synthetic &#8216;code&#8217; mimicking the languages&#8217; shared latent space. This <a href=https://en.wikipedia.org/wiki/Code>code</a> is then fed into the <a href=https://en.wikipedia.org/wiki/Code_generation_(compiler)>decoder</a> to generate text in either language. We perform our experiments on Europarl and Multi30k datasets, on the English-French language pair, and document our performance using both supervised and unsupervised machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2310/>Better Automatic Evaluation of Open-Domain Dialogue Systems with Contextualized Embeddings</a></strong><br><a href=/people/s/sarik-ghazarian/>Sarik Ghazarian</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2310><div class="card-body p-3 small">Despite advances in open-domain dialogue systems, automatic evaluation of such <a href=https://en.wikipedia.org/wiki/System>systems</a> is still a challenging problem. Traditional reference-based metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> are ineffective because there could be many valid responses for a given context that share no common words with reference responses. A recent work proposed Referenced metric and Unreferenced metric Blended Evaluation Routine (RUBER) to combine a learning-based metric, which predicts relatedness between a generated response and a given query, with reference-based metric ; it showed high correlation with human judgments. In this paper, we explore using contextualized word embeddings to compute more accurate <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness scores</a>, thus better evaluation metrics. Experiments show that our evaluation metrics outperform RUBER, which is trained on static embeddings.</div></div></div><hr><div id=w19-24><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-24.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-24/>Proceedings of the First Workshop on Narrative Understanding</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2400/>Proceedings of the First Workshop on Narrative Understanding</a></strong><br><a href=/people/d/david-bamman/>David Bamman</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/m/madalina-fiterau/>Madalina Fiterau</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2404/>Extraction of Message Sequence Charts from Narrative History Text</a></strong><br><a href=/people/g/girish-palshikar/>Girish Palshikar</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/h/harsimran-bedi/>Harsimran Bedi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2404><div class="card-body p-3 small">In this paper, we advocate the use of Message Sequence Chart (MSC) as a <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> to capture and visualize multi-actor interactions and their temporal ordering. We propose <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to automatically extract an <a href=https://en.wikipedia.org/wiki/Most_recent_common_ancestor>MSC</a> from a <a href=https://en.wikipedia.org/wiki/Narrative>history narrative</a>. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extraction, we employ a state-of-the art algorithm to temporally re-order these interactions. Our evaluation on multiple publicly available narratives shows improvements over four <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div></div><hr><div id=w19-25><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-25.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-25/>Proceedings of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2500/>Proceedings of the 3rd Joint <span class=acl-fixed-case>SIGHUM</span> Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/b/beatrice-alex/>Beatrice Alex</a>
|
<a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2501/>Modeling Word Emotion in Historical Language : Quantity Beats Supposed Stability in Seed Word Selection</a></strong><br><a href=/people/j/johannes-hellrich/>Johannes Hellrich</a>
|
<a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2501><div class="card-body p-3 small">To understand historical texts, we must be aware that languageincluding the emotional connotation attached to wordschanges over time. In this paper, we aim at estimating the <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> which is associated with a given word in former language stages of <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Emotion is represented following the popular Valence-Arousal-Dominance (VAD) annotation scheme. While being more expressive than <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>polarity</a> alone, existing word emotion induction methods are typically not suited for addressing it. To overcome this limitation, we present adaptations of two popular <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> to VAD. To measure their effectiveness in diachronic settings, we present the first <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a> for historical word emotions, which was created by scholars with proficiency in the respective language stages and covers both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. In contrast to claims in previous work, our findings indicate that hand-selecting small sets of seed words with supposedly stable emotional meaning is actually harm- rather than helpful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2504/>Are Fictional Voices Distinguishable? Classifying Character Voices in Modern Drama</a></strong><br><a href=/people/k/krishnapriya-vishnubhotla/>Krishnapriya Vishnubhotla</a>
|
<a href=/people/a/adam-hammond/>Adam Hammond</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2504><div class="card-body p-3 small">According to the <a href=https://en.wikipedia.org/wiki/Literary_theory>literary theory</a> of Mikhail Bakhtin, a dialogic novel is one in which characters speak in their own distinct voices, rather than serving as mouthpieces for their authors. We use <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> to determine which authors best achieve <a href=https://en.wikipedia.org/wiki/Dialogism>dialogism</a>, looking at a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of plays</a> from the late nineteenth and early twentieth centuries. We find that the SAGE model of text generation, which highlights deviations from a background lexical distribution, is an effective method of weighting the words of characters&#8217; utterances. Our results show that it is indeed possible to distinguish characters by their speech in the plays of canonical writers such as George Bernard Shaw, whereas characters are clustered more closely in the works of lesser-known playwrights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2505/>Automatic Alignment and Annotation Projection for Literary Texts</a></strong><br><a href=/people/u/uli-steinbach/>Uli Steinbach</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2505><div class="card-body p-3 small">This paper presents a modular NLP pipeline for the creation of a parallel literature corpus, followed by annotation transfer from the source to the target language. The test case we use to evaluate our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> is the automatic transfer of quote and speaker mention annotations from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We evaluate the different components of the <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> and discuss challenges specific to <a href=https://en.wikipedia.org/wiki/Literature>literary texts</a>. Our experiments show that after applying a reasonable amount of semi-automatic postprocessing we can obtain high-quality aligned and annotated resources for a new language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2506/>Inferring missing metadata from environmental policy texts</a></strong><br><a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/e/egoitz-laparra/>Egoitz Laparra</a>
|
<a href=/people/s/sophia-wang/>Sophia Wang</a>
|
<a href=/people/y/yiyun-zhao/>Yiyun Zhao</a>
|
<a href=/people/r/ragheb-al-ghezi/>Ragheb Al-Ghezi</a>
|
<a href=/people/a/aaron-lien/>Aaron Lien</a>
|
<a href=/people/l/laura-lopez-hoffman/>Laura López-Hoffman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2506><div class="card-body p-3 small">The National Environmental Policy Act (NEPA) provides a trove of data on how environmental policy decisions have been made in the United States over the last 50 years. Unfortunately, there is no central database for this information and it is too voluminous to assess manually. We describe our efforts to enable systematic research over <a href=https://en.wikipedia.org/wiki/Environmental_policy_of_the_United_States>US environmental policy</a> by extracting and organizing <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> from the text of NEPA documents. Our contributions include collecting more than 40,000 NEPA-related documents, and evaluating rule-based baselines that establish the difficulty of three important tasks : identifying lead agencies, aligning document versions, and detecting reused text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2508/>A framework for streamlined statistical prediction using <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a></a></strong><br><a href=/people/v/vanessa-glenny/>Vanessa Glenny</a>
|
<a href=/people/j/jonathan-tuke/>Jonathan Tuke</a>
|
<a href=/people/n/nigel-bean/>Nigel Bean</a>
|
<a href=/people/l/lewis-mitchell/>Lewis Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2508><div class="card-body p-3 small">In the Humanities and Social Sciences, there is increasing interest in approaches to <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>, intelligent linkage, and <a href=https://en.wikipedia.org/wiki/Dimension_reduction>dimension reduction</a> applicable to large text corpora. With approaches in these fields being grounded in traditional <a href=https://en.wikipedia.org/wiki/Statistics>statistical techniques</a>, the need arises for frameworks whereby advanced NLP techniques such as topic modelling may be incorporated within classical methodologies. This paper provides a classical, supervised, statistical learning framework for prediction from text, using topic models as a data reduction method and the topics themselves as predictors, alongside typical statistical tools for predictive modelling. We apply this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> in a Social Sciences context (applied animal behaviour) as well as a Humanities context (narrative analysis) as examples of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>. The results show that topic regression models perform comparably to their much less efficient equivalents that use individual words as predictors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2510/>Graph convolutional networks for exploring authorship hypotheses</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2510><div class="card-body p-3 small">This work considers a task from traditional <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary criticism</a> : annotating a structured, composite document with information about its sources. We take the <a href=https://en.wikipedia.org/wiki/Documentary_hypothesis>Documentary Hypothesis</a>, a prominent theory regarding the composition of the first five books of the <a href=https://en.wikipedia.org/wiki/Hebrew_Bible>Hebrew bible</a>, extract stylistic features designed to avoid bias or overfitting, and train several classification models. Our main result is that the recently-introduced graph convolutional network architecture outperforms structurally-uninformed models. We also find that including information about the granularity of text spans is a crucial ingredient when employing hidden layers, in contrast to simple <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>. We perform error analysis at several levels, noting how some characteristic limitations of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and simple <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> lead to <a href=https://en.wikipedia.org/wiki/Statistical_classification>misclassifications</a>, and conclude with an overview of future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2511/>Semantics and Homothetic Clustering of Hafez Poetry</a></strong><br><a href=/people/a/arya-rahgozar/>Arya Rahgozar</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2511><div class="card-body p-3 small">We have created two sets of labels for Hafez (1315-1390) poems, using <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a>. Our labels are the only semantic clustering alternative to the previously existing, hand-labeled, gold-standard classification of Hafez poems, to be used for <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary research</a>. We have cross-referenced, measured and analyzed the agreements of our clustering labels with Houman&#8217;s chronological classes. Our <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are based on <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We also introduced a similarity of similarities&#8217; features, we called homothetic clustering approach that proved effective, in case of Hafez&#8217;s small corpus of ghazals2. Although all our experiments showed different clusters when compared with Houman&#8217;s classes, we think they were valid in their own right to have provided further insights, and have proved useful as a contrasting alternative to Houman&#8217;s classes. Our homothetic clusterer and its feature design and engineering framework can be used for further semantic analysis of Hafez&#8217;s poetry and other similar literary research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2512/>Computational Linguistics Applications for Multimedia Services</a></strong><br><a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a>
|
<a href=/people/k/kelley-lynch/>Kelley Lynch</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2512><div class="card-body p-3 small">We present Computational Linguistics Applications for Multimedia Services (CLAMS), a platform that provides access to computational content analysis tools for archival multimedia material that appear in different media, such as <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, <a href=https://en.wikipedia.org/wiki/Sound>audio</a>, <a href=https://en.wikipedia.org/wiki/Image>image</a>, and <a href=https://en.wikipedia.org/wiki/Video>video</a>. The primary goal of CLAMS is : (1) to develop an interchange format between multimodal metadata generation tools to ensure interoperability between tools ; (2) to provide users with a portable, user-friendly workflow engine to chain selected tools to extract meaningful analyses ; and (3) to create a public software development kit (SDK) for developers that eases deployment of analysis tools within the CLAMS platform. CLAMS is designed to help archives and libraries enrich the <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a> associated with their mass-digitized multimedia collections, that would otherwise be largely unsearchable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2514/>On the Feasibility of Automated Detection of Allusive Text Reuse</a></strong><br><a href=/people/e/enrique-manjavacas/>Enrique Manjavacas</a>
|
<a href=/people/b/brian-long/>Brian Long</a>
|
<a href=/people/m/mike-kestemont/>Mike Kestemont</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2514><div class="card-body p-3 small">The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely commonly based on none or very few shared words. Arguably, <a href=https://en.wikipedia.org/wiki/Lexical_semantics>lexical semantics</a> can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> performance with respect to a windowing approach, and that (ii) <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> performance can be moderately boosted with <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2516/>Sign Clustering and Topic Extraction in Proto-Elamite<span class=acl-fixed-case>P</span>roto-<span class=acl-fixed-case>E</span>lamite</a></strong><br><a href=/people/l/logan-born/>Logan Born</a>
|
<a href=/people/k/kate-kelley/>Kate Kelley</a>
|
<a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/c/carolyn-chen/>Carolyn Chen</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2516><div class="card-body p-3 small">We describe a first attempt at using techniques from <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> to analyze the undeciphered proto-Elamite script. Using <a href=https://en.wikipedia.org/wiki/Hierarchical_clustering>hierarchical clustering</a>, n-gram frequencies, and LDA topic models, we both replicate results obtained by manual decipherment and reveal previously-unobserved relationships between signs. This demonstrates the utility of these techniques as an aid to manual decipherment.</div></div></div><hr><div id=w19-26><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-26.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-26/>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2600/>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</a></strong><br><a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a>
|
<a href=/people/l/laura-dietz/>Laura Dietz</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2601/>Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention</a></strong><br><a href=/people/q/qin-dai/>Qin Dai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/r/ryo-takahashi/>Ryo Takahashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2601><div class="card-body p-3 small">The increased demand for structured scientific knowledge has attracted considerable attention in extracting <a href=https://en.wikipedia.org/wiki/Scientific_method>scientific relation</a> from the ever growing <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a>. Distant supervision is widely applied approach to automatically generate large amounts of <a href=https://en.wikipedia.org/wiki/Data_type>labelled data</a> with low manual annotation cost. However, distant supervision inevitably accompanies the wrong labelling problem, which will negatively affect the performance of Relation Extraction (RE). To address this issue, (Han et al., 2018) proposes a novel framework for jointly training both RE model and Knowledge Graph Completion (KGC) model to extract structured knowledge from non-scientific dataset. In this work, we firstly investigate the feasibility of this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> on <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific dataset</a>, specifically on <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical dataset</a>. Secondly, to achieve better performance on the biomedical dataset, we extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> with other competitive KGC models. Moreover, we proposed a new end-to-end KGC model to extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Experimental results not only show the feasibility of the framework on the biomedical dataset, but also indicate the effectiveness of our extensions, because our extended model achieves significant and consistent improvements on distant supervised RE as compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2603/>Understanding the Polarity of Events in the Biomedical Literature : <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> vs. <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistically-informed Methods</a></a></strong><br><a href=/people/e/enrique-noriega-atala/>Enrique Noriega-Atala</a>
|
<a href=/people/z/zhengzhong-liang/>Zhengzhong Liang</a>
|
<a href=/people/j/john-bachman/>John Bachman</a>
|
<a href=/people/c/clayton-morrison/>Clayton Morrison</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2603><div class="card-body p-3 small">An important task in the machine reading of biochemical events expressed in biomedical texts is correctly reading the <a href=https://en.wikipedia.org/wiki/Chemical_polarity>polarity</a>, i.e., attributing whether the biochemical event is a promotion or an inhibition. Here we present a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for studying polarity attribution accuracy. We use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to train and evaluate several <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for polarity identification, and compare these to a linguistically-informed model. The best performing deep learning architecture achieves 0.968 average F1 performance in a five-fold cross-validation study, a considerable improvement over the linguistically informed model average F1 of 0.862.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2604/>Dataset Mention Extraction and Classification</a></strong><br><a href=/people/a/animesh-prasad/>Animesh Prasad</a>
|
<a href=/people/c/chenglei-si/>Chenglei Si</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2604><div class="card-body p-3 small">Datasets are integral artifacts of <a href=https://en.wikipedia.org/wiki/Empirical_research>empirical scientific research</a>. However, due to <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>natural language variation</a>, their recognition can be difficult and even when identified, can often be inconsistently referred across and within publications. We report our approach to the Coleridge Initiative&#8217;s Rich Context Competition, which tasks participants with identifying dataset surface forms (dataset mention extraction) and associating the extracted mention to its referred dataset (dataset classification). In this work, we propose various neural baselines and evaluate these <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on one-plus and zero-shot classification scenarios. We further explore various joint learning approaches-exploring the synergy between the tasks-and report the issues with such techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2605" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2605/>Annotating with Pros and Cons of Technologies in Computer Science Papers</a></strong><br><a href=/people/h/hono-shirai/>Hono Shirai</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2605><div class="card-body p-3 small">This paper explores a task for extracting a technological expression and its pros / cons from computer science papers. We report ongoing efforts on an annotated corpus of pros / cons and an analysis of the nature of the automatic extraction task. Specifically, we show how to adapt the targeted sentiment analysis task for pros / cons extraction in computer science papers and conduct an annotation study. In order to identify the challenges of the automatic extraction task, we construct a strong baseline model and conduct an error analysis. The experiments show that pros / cons can be consistently annotated by several annotators, and that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging due to domain-specific knowledge. The annotated dataset is made publicly available for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2607/>An Analysis of Deep Contextual Word Embeddings and Neural Architectures for Toponym Mention Detection in Scientific Publications</a></strong><br><a href=/people/m/matthew-magnusson/>Matthew Magnusson</a>
|
<a href=/people/l/laura-dietz/>Laura Dietz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2607><div class="card-body p-3 small">Toponym detection in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a> is an open task and a key first step in place entity enrichment of documents. We examine three common neural architectures in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> : 1) <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, 2) multi-layer perceptron (both applied in a sliding window context) and 3) bidirectional LSTM and apply contextual and non-contextual word embedding layers to these models. We find that deep contextual word embeddings improve the performance of the bi-LSTM with CRF neural architecture achieving the best performance when multiple layers of deep contextual embeddings are concatenated. Our best performing model achieves an average F1 of 0.910 when evaluated on overlap macro exceeding previous state-of-the-art models in the toponym detection task.</div></div></div><hr><div id=w19-27><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-27.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-27/>Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2700/>Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a>
|
<a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/e/erick-maziero-galani/>Erick Maziero Galani</a>
|
<a href=/people/j/juliano-desiderato-antonio/>Juliano Desiderato Antonio</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2705/>Nuclearity in RST and signals of coherence relations<span class=acl-fixed-case>RST</span> and signals of coherence relations</a></strong><br><a href=/people/d/debopam-das/>Debopam Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2705><div class="card-body p-3 small">We investigate the relationship between the notion of nuclearity as proposed in <a href=https://en.wikipedia.org/wiki/Rhetorical_structure_theory>Rhetorical Structure Theory (RST)</a> and the signalling of coherence relations. RST relations are categorized as either mononuclear (comprising a <a href=https://en.wikipedia.org/wiki/Atomic_nucleus>nucleus</a> and a satellite span) or multinuclear (comprising two or more nuclei spans). We examine how mononuclear relations (e.g., <a href=https://en.wikipedia.org/wiki/Antithesis>Antithesis</a>, Condition) and multinuclear relations (e.g., <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>Contrast</a>, List) are indicated by relational signals, more particularly by <a href=https://en.wikipedia.org/wiki/Discourse_marker>discourse markers</a> (e.g., because, however, if, therefore). We conduct a corpus study, examining the distribution of either type of relations in the RST Discourse Treebank (Carlson et al., 2002) and the distribution of discourse markers for those relations in the RST Signalling Corpus (Das et al., 2015). Our results show that <a href=https://en.wikipedia.org/wiki/Discourse_marker>discourse markers</a> are used more often to signal multinuclear relations than mononuclear relations. The findings also suggest a complex relationship between the relation types and syntactic categories of <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>discourse markers</a> (subordinating and coordinating conjunctions).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2707/>Annotating Shallow Discourse Relations in Twitter Conversations<span class=acl-fixed-case>T</span>witter Conversations</a></strong><br><a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/b/berfin-aktas/>Berfin Aktaş</a>
|
<a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2707><div class="card-body p-3 small">We introduce our pilot study applying PDTB-style annotation to Twitter conversations. Lexically grounded coherence annotation for Twitter threads will enable detailed investigations of the discourse structure of conversations on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Here, we present our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 185 threads and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, including an inter-annotator agreement study. We discuss our observations as to how Twitter discourses differ from written news text wrt. discourse connectives and <a href=https://en.wikipedia.org/wiki/Social_relation>relations</a>. We confirm our hypothesis that <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> in written social media conversations are expressed differently than in (news) text. We find that in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, connective arguments frequently are not full syntactic clauses, and that a few general connectives expressing EXPANSION and CONTINGENCY make up the majority of the explicit relations in our data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2708/>A Discourse Signal Annotation System for RST Trees<span class=acl-fixed-case>RST</span> Trees</a></strong><br><a href=/people/l/luke-gessler/>Luke Gessler</a>
|
<a href=/people/y/yang-liu-georgetown/>Yang Liu</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2708><div class="card-body p-3 small">This paper presents a new system for open-ended discourse relation signal annotation in the framework of Rhetorical Structure Theory (RST), implemented on top of an online tool for RST annotation. We discuss existing projects annotating textual signals of discourse relations, which have so far not allowed simultaneously structuring and annotating words signaling hierarchical discourse trees, and demonstrate the design and applications of our interface by extending existing RST annotations in the freely available GUM corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2709/>EusDisParser : improving an under-resourced discourse parser with cross-lingual data<span class=acl-fixed-case>E</span>us<span class=acl-fixed-case>D</span>is<span class=acl-fixed-case>P</span>arser: improving an under-resourced discourse parser with cross-lingual data</a></strong><br><a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a>
|
<a href=/people/c/chloe-braud/>Chloé Braud</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2709><div class="card-body p-3 small">Development of discourse parsers to annotate the relational discourse structure of a text is crucial for many downstream tasks. However, most of the existing work focuses on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, assuming a quite large dataset. Discourse data have been annotated for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, but training a system on these <a href=https://en.wikipedia.org/wiki/Data>data</a> is challenging since the corpus is very small. In this paper, we create the first demonstrator based on RST for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, and we investigate the use of data in another language to improve the performance of a <a href=https://en.wikipedia.org/wiki/Basque_language>Basque discourse parser</a>. More precisely, we build a monolingual system using the small set of data available and investigate the use of multilingual word embeddings to train a system for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> using data annotated for another language. We found that our approach to building a system limited to the small set of data available for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> allowed us to get an improvement over previous approaches making use of many data annotated in other languages. At best, we get 34.78 in F1 for the full discourse structure. More <a href=https://en.wikipedia.org/wiki/Annotation>data annotation</a> is necessary in order to improve the results obtained with these techniques. We also describe which relations match with the gold standard, in order to understand these results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2711.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2711/>Towards the Data-driven System for Rhetorical Parsing of Russian Texts<span class=acl-fixed-case>R</span>ussian Texts</a></strong><br><a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/d/dina-pisarevskaya/>Dina Pisarevskaya</a>
|
<a href=/people/e/elena-chistova/>Elena Chistova</a>
|
<a href=/people/s/svetlana-toldova/>Svetlana Toldova</a>
|
<a href=/people/m/maria-kobozeva/>Maria Kobozeva</a>
|
<a href=/people/i/ivan-smirnov/>Ivan Smirnov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2711><div class="card-body p-3 small">Results of the first experimental evaluation of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> trained on Ru-RSTreebank first Russian corpus annotated within RST framework are presented. Various lexical, quantitative, morphological, and semantic features were used. In rhetorical relation classification, ensemble of CatBoost model with selected <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and a linear SVM model provides the best score (macro F1 = 54.67 0.38). We discover that most of the important features for rhetorical relation classification are related to discourse connectives derived from the connectives lexicon for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and from other sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2713/>The DISRPT 2019 Shared Task on Elementary Discourse Unit Segmentation and Connective Detection<span class=acl-fixed-case>DISRPT</span> 2019 Shared Task on Elementary Discourse Unit Segmentation and Connective Detection</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a>
|
<a href=/people/d/debopam-das/>Debopam Das</a>
|
<a href=/people/e/erick-galani-maziero/>Erick Galani Maziero</a>
|
<a href=/people/j/juliano-antonio/>Juliano Antonio</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2713><div class="card-body p-3 small">In 2019, we organized the first iteration of a shared task dedicated to the underlying units used in discourse parsing across formalisms : the DISRPT Shared Task on Elementary Discourse Unit Segmentation and Connective Detection. In this paper we review the data included in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which cover 2.6 million manually annotated tokens from 15 datasets in 10 languages, survey and compare submitted systems and report on system performance on each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for both annotated and plain-tokenized versions of the <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2716.Software.pdf data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2716/>Multilingual segmentation based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and pre-trained word embeddings</a></strong><br><a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a>
|
<a href=/people/k/kepa-bengoetxea/>Kepa Bengoetxea</a>
|
<a href=/people/a/aitziber-atutxa-salazar/>Aitziber Atutxa Salazar</a>
|
<a href=/people/a/arantza-diaz-de-ilarraza/>Arantza Diaz de Ilarraza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2716><div class="card-body p-3 small">The DISPRT 2019 workshop has organized a shared task aiming to identify cross-formalism and multilingual discourse segments. Elementary Discourse Units (EDUs) are quite similar across different theories. Segmentation is the very first stage on the way of rhetorical annotation. Still, each annotation project adopted several decisions with consequences not only on the annotation of the relational discourse structure but also at the segmentation stage. In this shared task, we have employed pre-trained word embeddings, neural networks (BiLSTM+CRF) to perform the segmentation. We report F1 results for 6 languages : <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> (0.853), <a href=https://en.wikipedia.org/wiki/English_language>English</a> (0.919), <a href=https://en.wikipedia.org/wiki/French_language>French</a> (0.907), <a href=https://en.wikipedia.org/wiki/German_language>German</a> (0.913), <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> (0.926) and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> (0.868 and 0.769). Finally, we also pursued an error analysis based on clause typology for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, in order to understand the performance of the segmenter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2719.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2719 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2719 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-2719.Presentation.pptx data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-2719/>Using <a href=https://en.wikipedia.org/wiki/Rhetorical_structure_theory>Rhetorical Structure Theory</a> to Assess Discourse Coherence for Non-native Spontaneous Speech<span class=acl-fixed-case>R</span>hetorical <span class=acl-fixed-case>S</span>tructure <span class=acl-fixed-case>T</span>heory to Assess Discourse Coherence for Non-native Spontaneous Speech</a></strong><br><a href=/people/x/xinhao-wang/>Xinhao Wang</a>
|
<a href=/people/b/binod-gyawali/>Binod Gyawali</a>
|
<a href=/people/j/james-v-bruno/>James V. Bruno</a>
|
<a href=/people/h/hillary-r-molloy/>Hillary R. Molloy</a>
|
<a href=/people/k/keelan-evanini/>Keelan Evanini</a>
|
<a href=/people/k/klaus-zechner/>Klaus Zechner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2719><div class="card-body p-3 small">This study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of English speaking proficiency for <a href=https://en.wikipedia.org/wiki/Foreign_language>non-native speakers</a>. Rhetorical Structure Theory (RST) has been commonly used in the analysis of discourse organization of written texts ; however, limited research has been conducted to date on RST annotation and parsing of spoken language, in particular, non-native spontaneous speech. Due to the fact that the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>, we conducted research to obtain RST annotations on non-native spoken responses from a standardized assessment of academic English proficiency. Subsequently, <a href=https://en.wikipedia.org/wiki/Parsing>automatic parsers</a> were trained on these <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to process non-native spontaneous speech. Finally, a set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were extracted from automatically generated RST trees to evaluate the discourse structure of non-native spontaneous speech, which were then employed to further improve the validity of an automated speech scoring system.</div></div></div><hr><div id=w19-28><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-28.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-28/>Proceedings of the Second Workshop on Computational Models of Reference, Anaphora and Coreference</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2800/>Proceedings of the Second Workshop on Computational Models of Reference, Anaphora and Coreference</a></strong><br><a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2805/>Cross-lingual Incongruences in the Annotation of Coreference</a></strong><br><a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/p/pauline-krielke/>Pauline Krielke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2805><div class="card-body p-3 small">In the present paper, we deal with incongruences in English-German multilingual coreference annotation and present automated methods to discover them. More specifically, we automatically detect full coreference chains in parallel texts and analyse discrepancies in their annotations. In doing so, we wish to find out whether the discrepancies rather derive from <a href=https://en.wikipedia.org/wiki/Linguistic_typology>language typological constraints</a>, from the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> or the actual <a href=https://en.wikipedia.org/wiki/Annotation>annotation process</a>. The results of our study contribute to the referential analysis of similarities and differences across languages and support evaluation of cross-lingual coreference annotation. They are also useful for cross-lingual coreference resolution systems and <a href=https://en.wikipedia.org/wiki/Contrastive_linguistics>contrastive linguistic studies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2806/>Deep Cross-Lingual Coreference Resolution for Less-Resourced Languages : The Case of <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a><span class=acl-fixed-case>B</span>asque</a></strong><br><a href=/people/g/gorka-urbizu/>Gorka Urbizu</a>
|
<a href=/people/a/ander-soraluze/>Ander Soraluze</a>
|
<a href=/people/o/olatz-arregi/>Olatz Arregi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2806><div class="card-body p-3 small">In this paper, we present a cross-lingual neural coreference resolution system for a less-resourced language such as <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>. To begin with, we build the first neural coreference resolution system for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, training it with the relatively small EPEC-KORREF corpus (45,000 words). Next, a cross-lingual coreference resolution system is designed. With this approach, the system learns from a bigger <a href=https://en.wikipedia.org/wiki/English_language>English corpus</a>, using cross-lingual embeddings, to perform the <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> for <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>. The cross-lingual system obtains slightly better results (40.93 F1 CoNLL) than the monolingual system (39.12 F1 CoNLL), without using any Basque language corpus to train it.</div></div></div><hr><div id=w19-29><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-29.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-29/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2900/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra Jacobs</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/l/laurent-prevot/>Laurent Prévot</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2902/>Priming vs. Inhibition of Optional Infinitival to</a></strong><br><a href=/people/r/robin-melnick/>Robin Melnick</a>
|
<a href=/people/t/thomas-wasow/>Thomas Wasow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2902><div class="card-body p-3 small">The word to that precedes verbs in English infinitives is optional in at least two environments : in what Wasow et al. (2015) previously called the do-be construction, and in the complement of help, which we explore in the present work. In the do-be construction, Wasow et al. found that a preceding infinitival to increases the use of following optional to, but the use of to in the complement of help is reduced following to help. We examine two hypotheses regarding why the same <a href=https://en.wikipedia.org/wiki/Function_word>function word</a> is primed by prior use in one construction and inhibited in another. We then test predictions made by the two hypotheses, finding support for one of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2903/>Simulating Spanish-English Code-Switching : El Modelo Est Generating Code-Switches<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>E</span>nglish Code-Switching: El Modelo Está Generating Code-Switches</a></strong><br><a href=/people/c/chara-tsoukala/>Chara Tsoukala</a>
|
<a href=/people/s/stefan-l-frank/>Stefan L. Frank</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a>
|
<a href=/people/j/jorge-valdes-kroff/>Jorge Valdés Kroff</a>
|
<a href=/people/m/mirjam-broersma/>Mirjam Broersma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2903><div class="card-body p-3 small">Multilingual speakers are able to switch from one language to the other (code-switch) between or within sentences. Because the underlying cognitive mechanisms are not well understood, in this study we use computational cognitive modeling to shed light on the process of <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. We employed the Bilingual Dual-path model, a Recurrent Neural Network of bilingual sentence production (Tsoukala et al., 2017), and simulated <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence production</a> in simultaneous Spanish-English bilinguals. Our first goal was to investigate whether the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> would code-switch without being exposed to code-switched training input. The model indeed produced <a href=https://en.wikipedia.org/wiki/Code-switching>code-switches</a> even without any exposure to such <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>input</a> and the patterns of code-switches are in line with earlier linguistic work (Poplack,1980). The second goal of this study was to investigate an auxiliary phrase asymmetry that exists in Spanish-English code-switched production. Using this <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive model</a>, we examined a possible cause for this <a href=https://en.wikipedia.org/wiki/Asymmetry>asymmetry</a>. To our knowledge, this is the first computational cognitive model that aims to simulate code-switched sentence production.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2906/>A Modeling Study of the Effects of <a href=https://en.wikipedia.org/wiki/Surprisal>Surprisal</a> and <a href=https://en.wikipedia.org/wiki/Entropy>Entropy</a> in Perceptual Decision Making of an Adaptive Agent</a></strong><br><a href=/people/p/pyeong-whan-cho/>Pyeong Whan Cho</a>
|
<a href=/people/r/richard-l-lewis/>Richard Lewis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2906><div class="card-body p-3 small">Processing difficulty in online language comprehension has been explained in terms of surprisal and entropy reduction. Although both hypotheses have been supported by experimental data, we do not fully understand their relative contributions on processing difficulty. To develop a better understanding, we propose a mechanistic model of perceptual decision making that interacts with a simulated task environment with temporal dynamics. The proposed model collects noisy bottom-up evidence over multiple timesteps, integrates it with its <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down expectation</a>, and makes perceptual decisions, producing processing time data directly without relying on any linking hypothesis. Temporal dynamics in the task environment was determined by a simple finite-state grammar, which was designed to create the situations where the surprisal and entropy reduction hypotheses predict different patterns. After the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was trained to maximize rewards, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> developed an adaptive policy and both surprisal and entropy effects were observed especially in a <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> reflecting earlier processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2909/>Dependency Parsing with your Eyes : Dependency Structure Predicts Eye Regressions During Reading</a></strong><br><a href=/people/a/alessandro-lopopolo/>Alessandro Lopopolo</a>
|
<a href=/people/s/stefan-l-frank/>Stefan L. Frank</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a>
|
<a href=/people/r/roel-willems/>Roel Willems</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2909><div class="card-body p-3 small">Backward saccades during <a href=https://en.wikipedia.org/wiki/Reading>reading</a> have been hypothesized to be involved in structural reanalysis, or to be related to the level of text difficulty. We test the hypothesis that backward saccades are involved in online syntactic analysis. If this is the case we expect that <a href=https://en.wikipedia.org/wiki/Saccade>saccades</a> will coincide, at least partially, with the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> of the relations computed by a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a>. In order to test this, we analyzed a large eye-tracking dataset collected while 102 participants read three short narrative texts. Our results show a relation between backward saccades and the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> of sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2911/>Testing a Minimalist Grammar Parser on Italian Relative Clause Asymmetries<span class=acl-fixed-case>M</span>inimalist <span class=acl-fixed-case>G</span>rammar Parser on <span class=acl-fixed-case>I</span>talian Relative Clause Asymmetries</a></strong><br><a href=/people/a/aniello-de-santo/>Aniello De Santo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2911><div class="card-body p-3 small">Stabler&#8217;s (2013) <a href=https://en.wikipedia.org/wiki/Top-down_parsing>top-down parser</a> for Minimalist grammars has been used to account for off-line processing preferences across a variety of seemingly unrelated phenomena cross-linguistically, via complexity metrics measuring memory burden. This paper extends the empirical coverage of the model by looking at the processing asymmetries of Italian relative clauses, as I discuss the relevance of these constructions in evaluating plausible structure-driven models of processing difficulty.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2914/>The Development of Abstract Concepts in Children’s Early Lexical Networks</a></strong><br><a href=/people/a/abdellah-fourtassi/>Abdellah Fourtassi</a>
|
<a href=/people/i/isaac-scheinfeld/>Isaac Scheinfeld</a>
|
<a href=/people/m/michael-c-frank/>Michael Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2914><div class="card-body p-3 small">How do children learn <a href=https://en.wikipedia.org/wiki/Abstraction>abstract concepts</a> such as animal vs. artifact? Previous research has suggested that such <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> can partly be derived using cues from the language children hear around them. Following this suggestion, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> where we represent the children&#8217; developing lexicon as an evolving network. The <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> of this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> are based on vocabulary knowledge as reported by parents, and the edges between pairs of <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> are based on the probability of their co-occurrence in a corpus of child-directed speech. We found that several <a href=https://en.wikipedia.org/wiki/Category_(mathematics)>abstract categories</a> can be identified as the dense regions in such <a href=https://en.wikipedia.org/wiki/Flow_network>networks</a>. In addition, our simulations suggest that these <a href=https://en.wikipedia.org/wiki/Categorization>categories</a> develop simultaneously, rather than sequentially, thanks to the children&#8217;s word learning trajectory which favors the exploration of the global conceptual space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2915/>Verb-Second Effect on Quantifier Scope Interpretation</a></strong><br><a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2915><div class="card-body p-3 small">Sentences like Every child climbed a tree have at least two interpretations depending on the precedence order of the <a href=https://en.wikipedia.org/wiki/Universal_quantifier>universal quantifier</a> and the indefinite. Previous experimental work explores the role that different <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanisms</a> such as semantic reanalysis and <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> may have in enabling each interpretation. This paper discusses a web-based task that uses the verb-second characteristic of German main clauses to estimate the influence of word order variation over <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-2916" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-2916/>Neural Models of the Psychosemantics of ‘Most’</a></strong><br><a href=/people/l/lewis-osullivan/>Lewis O’Sullivan</a>
|
<a href=/people/s/shane-steinert-threlkeld/>Shane Steinert-Threlkeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2916><div class="card-body p-3 small">How are the meanings of linguistic expressions related to their use in concrete <a href=https://en.wikipedia.org/wiki/Cognition>cognitive tasks</a>? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain <a href=https://en.wikipedia.org/wiki/Quantifier_(linguistics)>quantifiers</a>. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) on the most verification task from Pietroski2009, manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive models</a> of this and other psychosemantic tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2917/>The Role of Utterance Boundaries and Word Frequencies for Part-of-speech Learning in <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> Through Distributional Analysis<span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese Through Distributional Analysis</a></strong><br><a href=/people/p/pablo-picasso-feliciano-de-faria/>Pablo Picasso Feliciano de Faria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2917><div class="card-body p-3 small">In this study, we address the problem of part-of-speech (or syntactic category) learning during <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> through distributional analysis of utterances. A <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on Redington et al.&#8217;s (1998) distributional learner is used to investigate the informativeness of distributional information in Brazilian Portuguese (BP). The data provided to the learner comes from two publicly available corpora of child directed speech. We present preliminary results from two experiments. The first one investigates the effects of different assumptions about utterance boundaries when presenting the input data to the <a href=https://en.wikipedia.org/wiki/Learning>learner</a>. The second experiment compares the learner&#8217;s performance when counting contextual words&#8217; frequencies versus just acknowledging their co-occurrence with a given target word. In general, our results indicate that explicit boundaries are more informative, <a href=https://en.wikipedia.org/wiki/Frequency>frequencies</a> are important, and that distributional information is useful to the child as a source of categorial information. These results are in accordance with Redington et al.&#8217;s findings for <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-2918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-2918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-2918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-2918/>Using Grounded Word Representations to Study Theories of Lexical Concepts</a></strong><br><a href=/people/d/dylan-ebert/>Dylan Ebert</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-2918><div class="card-body p-3 small">The fields of <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a> and <a href=https://en.wikipedia.org/wiki/Philosophy>philosophy</a> have proposed many different <a href=https://en.wikipedia.org/wiki/Theory>theories</a> for how humans represent concepts. Multiple such <a href=https://en.wikipedia.org/wiki/Theory>theories</a> are compatible with state-of-the-art NLP methods, and could in principle be operationalized using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We focus on two particularly prominent theoriesClassical Theory and Prototype Theoryin the context of visually-grounded lexical representations. We compare when and how the behavior of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> based on these theories differs in terms of <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> and <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment tasks</a>. Our preliminary results suggest that Classical-based representations perform better for <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a> and Prototype-based representations perform better for <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a>. We discuss plans for additional experiments needed to confirm these initial observations.</div></div></div><hr><div id=w19-30><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-30.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-30/>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3000/>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</a></strong><br><a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/k/kristy-hollingshead/>Kristy Hollingshead</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a>
|
<a href=/people/r/rebecca-resnik/>Rebecca Resnik</a>
|
<a href=/people/k/kate-loveys/>Kate Loveys</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3002/>Identifying therapist conversational actions across diverse <a href=https://en.wikipedia.org/wiki/Psychotherapy>psychotherapeutic approaches</a></a></strong><br><a href=/people/f/fei-tzin-lee/>Fei-Tzin Lee</a>
|
<a href=/people/d/derrick-hull/>Derrick Hull</a>
|
<a href=/people/j/jacob-levine/>Jacob Levine</a>
|
<a href=/people/b/bonnie-ray/>Bonnie Ray</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3002><div class="card-body p-3 small">While conversation in therapy sessions can vary widely in both topic and style, an understanding of the underlying techniques used by therapists can provide valuable insights into how therapists best help clients of different types. Dialogue act classification aims to identify the conversational action each speaker takes at each utterance, such as <a href=https://en.wikipedia.org/wiki/Sympathy>sympathizing</a>, <a href=https://en.wikipedia.org/wiki/Problem_solving>problem-solving</a> or assumption checking. We propose to apply dialogue act classification to therapy transcripts, using a therapy-specific labeling scheme, in order to gain a high-level understanding of the flow of conversation in therapy sessions. We present a novel annotation scheme that spans multiple <a href=https://en.wikipedia.org/wiki/Psychotherapy>psychotherapeutic approaches</a>, apply it to a large and diverse corpus of psychotherapy transcripts, and present and discuss <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results obtained using both SVM and neural network-based models. The results indicate that identifying the structure and flow of therapeutic actions is an obtainable goal, opening up the opportunity in the future to provide therapeutic recommendations tailored to specific client situations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3004/>CLaC at CLPsych 2019 : Fusion of Neural Features and Predicted Class Probabilities for Suicide Risk Assessment Based on Online Posts<span class=acl-fixed-case>CL</span>a<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>CLP</span>sych 2019: Fusion of Neural Features and Predicted Class Probabilities for Suicide Risk Assessment Based on Online Posts</a></strong><br><a href=/people/e/elham-mohammadi/>Elham Mohammadi</a>
|
<a href=/people/h/hessam-amini/>Hessam Amini</a>
|
<a href=/people/l/leila-kosseim/>Leila Kosseim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3004><div class="card-body p-3 small">This paper summarizes our participation to the CLPsych 2019 shared task, under the name CLaC. The goal of the shared task was to detect and assess suicide risk based on a collection of online posts. For our participation, we used an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> which utilizes 8 neural sub-models to extract neural features and predict class probabilities, which are then used by an SVM classifier. Our team ranked first in 2 out of the 3 <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (tasks A and C).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3005/>Suicide Risk Assessment with Multi-level Dual-Context Language and BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/m/matthew-matero/>Matthew Matero</a>
|
<a href=/people/a/akash-idnani/>Akash Idnani</a>
|
<a href=/people/y/youngseo-son/>Youngseo Son</a>
|
<a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a>
|
<a href=/people/h/huy-vu/>Huy Vu</a>
|
<a href=/people/m/mohammad-zamani/>Mohammad Zamani</a>
|
<a href=/people/p/parth-limbachiya/>Parth Limbachiya</a>
|
<a href=/people/s/sharath-chandra-guntuku/>Sharath Chandra Guntuku</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3005><div class="card-body p-3 small">Mental health predictive systems typically model language as if from a single context (e.g. Twitter posts, <a href=https://en.wikipedia.org/wiki/Twitter>status updates</a>, or forum posts) and often limited to a single level of analysis (e.g. either the message-level or user-level). Here, we bring these pieces together to explore the use of open-vocabulary (BERT embeddings, topics) and theoretical features (emotional expression lexica, personality) for the task of suicide risk assessment on support forums (the CLPsych-2019 Shared Task). We used dual context based approaches (modeling content from suicide forums separate from other content), built over both traditional ML models as well as a novel dual RNN architecture with user-factor adaptation. We find that while <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a> from the suicide context distinguishes with no-risk from those with any-risk, personality factors from the non-suicide contexts provide distinction of the levels of risk : low, medium, and high risk. Within the shared task, our dual-context approach (listed as SBU-HLAB in the official results) achieved state-of-the-art performance predicting suicide risk using a combination of suicide-context and non-suicide posts (Task B), achieving an F1 score of 0.50 over hidden test set labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3006/>Using natural conversations to classify autism with limited data : Age matters</a></strong><br><a href=/people/m/michael-hauser/>Michael Hauser</a>
|
<a href=/people/e/evangelos-sariyanidi/>Evangelos Sariyanidi</a>
|
<a href=/people/b/birkan-tunc/>Birkan Tunc</a>
|
<a href=/people/c/casey-zampella/>Casey Zampella</a>
|
<a href=/people/e/edward-brodkin/>Edward Brodkin</a>
|
<a href=/people/r/robert-t-schultz/>Robert Schultz</a>
|
<a href=/people/j/julia-parish-morris/>Julia Parish-Morris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3006><div class="card-body p-3 small">Spoken language ability is highly heterogeneous in Autism Spectrum Disorder (ASD), which complicates efforts to identify linguistic markers for use in diagnostic classification, clinical characterization, and for research and clinical outcome measurement. Machine learning techniques that harness the power of <a href=https://en.wikipedia.org/wiki/Multivariate_statistics>multivariate statistics</a> and non-linear data analysis hold promise for modeling this <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>heterogeneity</a>, but many models require enormous datasets, which are unavailable for most psychiatric conditions (including ASD). In lieu of such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, good <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can still be built by leveraging <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. In this study, we compare two machine learning approaches : the first approach incorporates prior knowledge about language variation across middle childhood, adolescence, and adulthood to classify 6-minute naturalistic conversation samples from 140 age- and IQ-matched participants (81 with ASD), while the other approach treats all ages the same. We found that individual age-informed models were significantly more accurate than a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> tasked with building a common <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> across age groups. Furthermore, predictive linguistic features differed significantly by age group, confirming the importance of considering age-related changes in language use when classifying ASD. Our results suggest that limitations imposed by <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>heterogeneity</a> inherent to ASD and from developmental change with age can be (at least partially) overcome using <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>, such as understanding spoken language development from childhood through adulthood.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3007/>The importance of sharing patient-generated clinical speech and language data</a></strong><br><a href=/people/k/kathleen-c-fraser/>Kathleen C. Fraser</a>
|
<a href=/people/n/nicklas-linz/>Nicklas Linz</a>
|
<a href=/people/h/hali-lindsay/>Hali Lindsay</a>
|
<a href=/people/a/alexandra-konig/>Alexandra König</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3007><div class="card-body p-3 small">Increased access to large datasets has driven progress in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. However, most computational studies of clinically-validated, patient-generated speech and language involve very few datapoints, as such <a href=https://en.wikipedia.org/wiki/Data>data</a> are difficult (and expensive) to collect. In this position paper, we argue that we must find ways to promote <a href=https://en.wikipedia.org/wiki/Data_sharing>data sharing</a> across research groups, in order to build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of a more appropriate size for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning analysis</a>. We review the benefits and challenges of sharing clinical language data, and suggest several concrete actions by both clinical and NLP researchers to encourage multi-site and multi-disciplinary data sharing. We also propose the creation of a collaborative data sharing platform, to allow NLP researchers to take a more active responsibility for <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>data transcription</a>, <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, and <a href=https://en.wikipedia.org/wiki/Data_curation>curation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3008/>Depressed Individuals Use Negative Self-Focused Language When Recalling Recent Interactions with Close Romantic Partners but Not Family or Friends<span class=acl-fixed-case>F</span>riends</a></strong><br><a href=/people/t/taleen-nalabandian/>Taleen Nalabandian</a>
|
<a href=/people/m/molly-ireland/>Molly Ireland</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3008><div class="card-body p-3 small">Depression is characterized by a self-focused negative attentional bias, which is often reflected in everyday language use. In a prospective writing study, we explored whether the association between <a href=https://en.wikipedia.org/wiki/Major_depressive_disorder>depressive symptoms</a> and negative, self-focused language varies across social contexts. College students (N = 243) wrote about a recent interaction with a person they care deeply about. Depression symptoms positively correlated with negative emotion words and first-person singular pronouns (or negative self-focus) when writing about a recent interaction with romantic partners or, to a lesser extent, friends, but not family members. The pattern of results was more pronounced when participants perceived greater self-other overlap (i.e., interpersonal closeness) with their romantic partner. Findings regarding how the linguistic profile of <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a> differs by type of relationship may inform more effective methods of <a href=https://en.wikipedia.org/wiki/Medical_diagnosis>clinical diagnosis</a> and <a href=https://en.wikipedia.org/wiki/Therapy>treatment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3010/>Semantic Characteristics of Schizophrenic Speech</a></strong><br><a href=/people/k/kfir-bar/>Kfir Bar</a>
|
<a href=/people/v/vered-zilberstein/>Vered Zilberstein</a>
|
<a href=/people/i/ido-ziv/>Ido Ziv</a>
|
<a href=/people/h/heli-baram/>Heli Baram</a>
|
<a href=/people/n/nachum-dershowitz/>Nachum Dershowitz</a>
|
<a href=/people/s/samuel-itzikowitz/>Samuel Itzikowitz</a>
|
<a href=/people/e/eiran-vadim-harel/>Eiran Vadim Harel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3010><div class="card-body p-3 small">Natural language processing tools are used to automatically detect disturbances in <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed speech</a> of schizophrenia inpatients who speak <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>. We measure topic mutation over time and show that controls maintain more <a href=https://en.wikipedia.org/wiki/Cohesion_(linguistics)>cohesive speech</a> than inpatients. We also examine differences in how inpatients and controls use <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a> to describe <a href=https://en.wikipedia.org/wiki/Content_word>content words</a> and show that the ones used by controls are more common than the those of inpatients. We provide experimental results and show their potential for automatically detecting schizophrenia in patients by means only of their speech patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3013/>Mental Health Surveillance over <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> with Digital Cohorts</a></strong><br><a href=/people/s/silvio-amir/>Silvio Amir</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/j/john-w-ayers/>John W. Ayers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3013><div class="card-body p-3 small">The ability to track <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a> via <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> opened the doors for large-scale, automated, mental health surveillance. However, inferring accurate population-level trends requires representative samples of the underlying population, which can be challenging given the biases inherent in social media data. While previous work has adjusted samples based on <a href=https://en.wikipedia.org/wiki/Demography>demographic estimates</a>, the populations were selected based on specific outcomes, e.g. specific <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a>. We depart from these methods, by conducting analyses over demographically representative digital cohorts of social media users. To validated this approach, we constructed a cohort of US based Twitter users to measure the prevalence of depression and <a href=https://en.wikipedia.org/wiki/Posttraumatic_stress_disorder>PTSD</a>, and investigate how these illnesses manifest across demographic subpopulations. The analysis demonstrates that <a href=https://en.wikipedia.org/wiki/Cohort_study>cohort-based studies</a> can help control for <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling biases</a>, contextualize outcomes, and provide deeper insights into the data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3017/>Analyzing the use of existing <a href=https://en.wikipedia.org/wiki/System>systems</a> for the CLPsych 2019 Shared Task<span class=acl-fixed-case>CLP</span>sych 2019 Shared Task</a></strong><br><a href=/people/a/alejandro-gonzalez-hevia/>Alejandro González Hevia</a>
|
<a href=/people/r/rebeca-cerezo-menendez/>Rebeca Cerezo Menéndez</a>
|
<a href=/people/d/daniel-gayo-avello/>Daniel Gayo-Avello</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3017><div class="card-body p-3 small">In this paper we describe the UniOvi-WESO classification systems proposed for the 2019 Computational Linguistics and Clinical Psychology (CLPsych) Shared Task. We explore the use of two systems trained with ReachOut data from the 2016 CLPsych task, and compare them to a baseline system trained with the data provided for this task. All the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> were trained with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted just from the text of each post, without using any other <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>. We found out that the baseline system performs slightly better than the pretrained systems, mainly due to the differences in labeling between the two tasks. However, they still work reasonably well and can detect if a user is at risk of suicide or not.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3018/>Similar Minds Post Alike : Assessment of Suicide Risk Using a Hybrid Model</a></strong><br><a href=/people/l/lushi-chen/>Lushi Chen</a>
|
<a href=/people/a/abeer-aldayel/>Abeer Aldayel</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/t/tao-gong/>Tao Gong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3018><div class="card-body p-3 small">This paper describes our system submission for the CLPsych 2019 shared task B on <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>. We approached the problem with three separate <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> : a behaviour model ; a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and a hybrid model. For the behavioral model approach, we model each user&#8217;s behaviour and thoughts with four groups of features : posting behaviour, <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Motivation>motivation</a>, and content of the user&#8217;s posting. We use these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> as an input in a support vector machine (SVM). For the <a href=https://en.wikipedia.org/wiki/Language_model>language model approach</a>, we trained a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> for each risk level using all the posts from the users as the training corpora. Then, we computed the perplexity of each user&#8217;s posts to determine how likely his / her posts were to belong to each risk level. Finally, we built a hybrid model that combines both the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and the <a href=https://en.wikipedia.org/wiki/Behavioral_model>behavioral model</a>, which demonstrates the best performance in detecting the suicide risk level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3021/>Suicide Risk Assessment on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : USI-UPF at the CLPsych 2019 Shared Task<span class=acl-fixed-case>USI</span>-<span class=acl-fixed-case>UPF</span> at the <span class=acl-fixed-case>CLP</span>sych 2019 Shared Task</a></strong><br><a href=/people/e/esteban-rissola/>Esteban Ríssola</a>
|
<a href=/people/d/diana-ramirez-cifuentes/>Diana Ramírez-Cifuentes</a>
|
<a href=/people/a/ana-freire/>Ana Freire</a>
|
<a href=/people/f/fabio-crestani/>Fabio Crestani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3021><div class="card-body p-3 small">This paper describes the participation of the USI-UPF team at the shared task of the 2019 Computational Linguistics and Clinical Psychology Workshop (CLPsych2019). The goal is to assess the degree of suicide risk of social media users given a labelled dataset with their posts. An appropriate <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>, with the usage of automated methods, can assist experts on the detection of people at risk and eventually contribute to prevent suicide. We propose a set of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> based on <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, word level n-grams, and statistics extracted from users&#8217; posts. The results show that the most effective <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> for the tasks are obtained integrating lexicon-based features, a selected set of <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a>, and <a href=https://en.wikipedia.org/wiki/Statistics>statistical measures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3023/>An Investigation of <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning Systems</a> for Suicide Risk Assessment</a></strong><br><a href=/people/m/michelle-morales/>Michelle Morales</a>
|
<a href=/people/p/prajjalita-dey/>Prajjalita Dey</a>
|
<a href=/people/t/thomas-theisen/>Thomas Theisen</a>
|
<a href=/people/d/danny-belitz/>Danny Belitz</a>
|
<a href=/people/n/natalia-chernova/>Natalia Chernova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3023><div class="card-body p-3 small">This work presents the <a href=https://en.wikipedia.org/wiki/System>systems</a> explored as part of the CLPsych 2019 Shared Task. More specifically, this work explores the promise of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning systems</a> for <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>.</div></div></div><hr><div id=w19-31><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-31.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-31/>Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3100/>Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing</a></strong><br><a href=/people/h/heiko-vogler/>Heiko Vogler</a>
|
<a href=/people/a/andreas-maletti/>Andreas Maletti</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3105/>On the Compression of Lexicon Transducers</a></strong><br><a href=/people/m/marco-cognetta/>Marco Cognetta</a>
|
<a href=/people/c/cyril-allauzen/>Cyril Allauzen</a>
|
<a href=/people/m/michael-riley/>Michael Riley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3105><div class="card-body p-3 small">In finite-state language processing pipelines, a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> is often a key component. It needs to be comprehensive to ensure <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, reducing out-of-vocabulary misses. However, in memory-constrained environments (e.g., <a href=https://en.wikipedia.org/wiki/Mobile_phone>mobile phones</a>), the size of the <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>component automata</a> must be kept small. Indeed, a delicate balance between comprehensiveness, speed, and memory must be struck to conform to device requirements while providing a good user experience. In this paper, we describe a compression scheme for <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> when represented as finite-state transducers. We efficiently encode the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> of the <a href=https://en.wikipedia.org/wiki/Transducer>transducer</a> while storing transition labels separately. The graph encoding scheme is based on the LOUDS (Level Order Unary Degree Sequence) tree representation, which has constant time tree traversal for queries while being information-theoretically optimal in space. We find that our encoding is near the theoretical lower bound for such <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> and substantially outperforms more traditional representations in space while remaining competitive in latency benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3107/>Finite State Transducer Calculus for Whole Word Morphology</a></strong><br><a href=/people/m/maciej-janicki/>Maciej Janicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3107><div class="card-body p-3 small">The research on machine learning of morphology often involves formulating <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological descriptions</a> directly on <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>surface forms of words</a>. As the established two-level morphology paradigm requires the knowledge of the underlying structure, it is not widely used in such settings. In this paper, we propose a formalism describing structural relationships between words based on theories of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> that reject the notions of internal word structure and <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme</a>. The formalism covers a wide variety of morphological phenomena (including non-concatenative ones like stem vowel alternation) without the need of workarounds and extensions. Furthermore, we show that morphological rules formulated in such way can be easily translated to FSTs, which enables us to derive performant approaches to morphological analysis, generation and automatic rule discovery.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-3108.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-3108/>Weighted parsing for grammar-based language models</a></strong><br><a href=/people/r/richard-morbitz/>Richard Mörbitz</a>
|
<a href=/people/h/heiko-vogler/>Heiko Vogler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3108><div class="card-body p-3 small">We develop a general <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for weighted parsing which is built on top of grammar-based language models and employs flexible weight algebras. It generalizes previous work in that area (semiring parsing, weighted deductive parsing) and also covers applications outside the classical scope of <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, e.g., algebraic dynamic programming. We show an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> which terminates and is correct for a large class of weighted grammar-based language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3111/>Using Meta-Morph Rules to develop Morphological Analysers : A case study concerning Tamil<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/k/kengatharaiyer-sarveswaran/>Kengatharaiyer Sarveswaran</a>
|
<a href=/people/g/gihan-dias/>Gihan Dias</a>
|
<a href=/people/m/miriam-butt/>Miriam Butt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3111><div class="card-body p-3 small">This paper describes a new and larger coverage Finite-State Morphological Analyser (FSM) and Generator for the Dravidian language Tamil. The FSM has been developed in the context of computational grammar engineering, adhering to the standards of the ParGram effort. Tamil is a morphologically rich language and the interaction between <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis</a> and <a href=https://en.wikipedia.org/wiki/Formal_methods>formal implementation</a> is complex, resulting in a challenging task. In order to allow the development of the <a href=https://en.wikipedia.org/wiki/Formal_semantics_(linguistics)>FSM</a> to focus more on the linguistic analysis and less on the formal details, we have developed a system of meta-morph(ology) rules along with a script which translates these rules into <a href=https://en.wikipedia.org/wiki/Formal_semantics_(linguistics)>FSM processable representations</a>. The introduction of meta-morph rules makes it possible for computationally naive linguists to interact with the <a href=https://en.wikipedia.org/wiki/System>system</a> and to expand <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> in future work. We found that the meta-morph rules help to express linguistic generalisations and reduce the manual effort of writing lexical classes for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>. Our Tamil FSM currently handles mainly the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>inflectional morphology</a> of 3,300 verb roots and their 260 forms. Further, it also has a lexicon of approximately 100,000 nouns along with a guesser to handle out-of-vocabulary items. Although the Tamil FSM was primarily developed to be part of a computational grammar, it can also be used as a web or stand-alone application for other NLP tasks, as per general ParGram practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3112/>Distilling weighted finite automata from arbitrary probabilistic models</a></strong><br><a href=/people/a/ananda-theertha-suresh/>Ananda Theertha Suresh</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/m/michael-riley/>Michael Riley</a>
|
<a href=/people/v/vlad-schogol/>Vlad Schogol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3112><div class="card-body p-3 small">Weighted finite automata (WFA) are often used to represent probabilistic models, such as n-gram language models, since they are efficient for recognition tasks in time and space. The probabilistic source to be represented as a WFA, however, may come in many forms. Given a generic probabilistic model over sequences, we propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to approximate it as a weighted finite automaton such that the <a href=https://en.wikipedia.org/wiki/Kullback&#8211;Leibler_divergence>Kullback-Leibler divergence</a> between the source model and the WFA target model is minimized. The proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> involves a counting step and a difference of convex optimization, both of which can be performed efficiently. We demonstrate the usefulness of our approach on some tasks including distilling <a href=https://en.wikipedia.org/wiki/N-gram_model>n-gram models</a> from neural models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3113/>Silent HMMs : Generalized Representation of Hidden Semi-Markov Models and Hierarchical HMMs<span class=acl-fixed-case>HMM</span>s: Generalized Representation of Hidden Semi-<span class=acl-fixed-case>M</span>arkov Models and Hierarchical <span class=acl-fixed-case>HMM</span>s</a></strong><br><a href=/people/k/kei-wakabayashi/>Kei Wakabayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3113><div class="card-body p-3 small">Modeling sequence data using probabilistic finite state machines (PFSMs) is a technique that analyzes the underlying dynamics in sequences of symbols. Hidden semi-Markov models (HSMMs) and hierarchical hidden Markov models (HHMMs) are PFSMs that have been successfully applied to a wide variety of applications by extending HMMs to make the extracted patterns easier to interpret. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are independently developed with their own <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithm</a>, so that we can not combine multiple kinds of structures to build a PFSM for a specific application. In this paper, we prove that silent hidden Markov models (silent HMMs) are flexible models that have more expressive power than <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>HSMMs</a> and <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>HHMMs</a>. Silent HMMs are HMMs that contain silent states, which do not emit any observations. We show that we can obtain silent HMM equivalent to given HSMMs and HHMMs. We believe that these results form a firm foundation to use silent HMMs as a unified representation for PFSM modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3115/>Transition-Based Coding and <a href=https://en.wikipedia.org/wiki/Formal_language_theory>Formal Language Theory</a> for Ordered Digraphs</a></strong><br><a href=/people/a/anssi-yli-jyra/>Anssi Yli-Jyrä</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3115><div class="card-body p-3 small">Transition-based parsing of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> uses transition systems to build directed annotation graphs (digraphs) for sentences. In this paper, we define, for an arbitrary ordered digraph, a unique decomposition and a corresponding linear encoding that are associated bijectively with each other via a new transition system. These results give us an efficient and succinct representation for <a href=https://en.wikipedia.org/wiki/Digraph_(orthography)>digraphs</a> and sets of <a href=https://en.wikipedia.org/wiki/Digraph_(orthography)>digraphs</a>. Based on the system and our analysis of its syntactic properties, we give structural bounds under which the set of encoded digraphs is restricted and becomes a context-free or a regular string language. The context-free restriction is essentially a superset of the encodings used previously to characterize properties of noncrossing digraphs and to solve maximal subgraphs problems. The regular restriction with a tight bound is shown to capture the Universal Dependencies v2.4 treebanks in linguistics.</div></div></div><hr><div id=w19-32><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-32.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-32/>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3200/>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task</a></strong><br><a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3201/>Extracting Kinship from Obituary to Enhance Electronic Health Records for Genetic Research</a></strong><br><a href=/people/k/kai-he/>Kai He</a>
|
<a href=/people/j/jialun-wu/>Jialun Wu</a>
|
<a href=/people/x/xiaoyong-ma/>Xiaoyong Ma</a>
|
<a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/m/ming-huang/>Ming Huang</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/l/lixia-yao/>Lixia Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3201><div class="card-body p-3 small">Claims database and electronic health records database do not usually capture kinship or family relationship information, which is imperative for <a href=https://en.wikipedia.org/wiki/Genetic_research>genetic research</a>. We identify online obituaries as a new data source and propose a special named entity recognition and relation extraction solution to extract names and kinships from online obituaries. Built on 1,809 annotated obituaries and a novel tagging scheme, our joint neural model achieved macro-averaged precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/F-number>F measure</a> of 72.69 %, 78.54 % and 74.93 %, and micro-averaged precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/F-number>F measure</a> of 95.74 %, 98.25 % and 96.98 % using 57 <a href=https://en.wikipedia.org/wiki/Kinship>kinships</a> with 10 or more examples in a 10-fold cross-validation experiment. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> performance improved dramatically when trained with 34 <a href=https://en.wikipedia.org/wiki/Kinship>kinships</a> with 50 or more examples. Leveraging additional information such as age, death date, birth date and residence mentioned by obituaries, we foresee a promising future of supplementing EHR databases with comprehensive and accurate kinship information for genetic research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3202/>Lexical Normalization of User-Generated Medical Text</a></strong><br><a href=/people/a/anne-dirkson/>Anne Dirkson</a>
|
<a href=/people/s/suzan-verberne/>Suzan Verberne</a>
|
<a href=/people/w/wessel-kraaij/>Wessel Kraaij</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3202><div class="card-body p-3 small">In the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>, <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated social media text</a> is increasingly used as a valuable complementary knowledge source to <a href=https://en.wikipedia.org/wiki/Medical_literature>scientific medical literature</a>. The extraction of this knowledge is complicated by <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial language use</a> and misspellings. Yet, lexical normalization of such <a href=https://en.wikipedia.org/wiki/Data>data</a> has not been addressed properly. This paper presents an unsupervised, data-driven spelling correction module for medical social media. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms state-of-the-art spelling correction and can detect mistakes with an <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>F0.5</a> of 0.888. Additionally, we present a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for spelling mistake detection and correction on a medical patient forum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3206/>HITSZ-ICRC : A Report for SMM4H Shared Task 2019-Automatic Classification and Extraction of Adverse Effect Mentions in Tweets<span class=acl-fixed-case>HITSZ</span>-<span class=acl-fixed-case>ICRC</span>: A Report for <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> Shared Task 2019-Automatic Classification and Extraction of Adverse Effect Mentions in Tweets</a></strong><br><a href=/people/s/shuai-chen/>Shuai Chen</a>
|
<a href=/people/y/yuanhang-huang/>Yuanhang Huang</a>
|
<a href=/people/x/xiaowei-huang/>Xiaowei Huang</a>
|
<a href=/people/h/haoming-qin/>Haoming Qin</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/b/buzhou-tang/>Buzhou Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3206><div class="card-body p-3 small">This is the system description of the Harbin Institute of Technology Shenzhen (HITSZ) team for the first and second subtasks of the fourth Social Media Mining for Health Applications (SMM4H) shared task in 2019. The two subtasks are automatic classification and extraction of adverse effect mentions in tweets. The systems for the two subtasks are based on bidirectional encoder representations from transformers (BERT), and achieves promising results. Among the systems we developed for subtask1, the best F1-score was 0.6457, for subtask2, the best relaxed F1-score and the best strict F1-score were 0.614 and 0.407 respectively. Our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks first among all <a href=https://en.wikipedia.org/wiki/System>systems</a> on subtask1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3208/>Approaching SMM4H with Merged Models and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task Learning</a><span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> with Merged Models and Multi-task Learning</a></strong><br><a href=/people/t/tilia-ellendorff/>Tilia Ellendorff</a>
|
<a href=/people/l/lenz-furrer/>Lenz Furrer</a>
|
<a href=/people/n/nicola-colic/>Nicola Colic</a>
|
<a href=/people/n/noemi-aepli/>Noëmi Aepli</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3208><div class="card-body p-3 small">We describe our submissions to the 4th edition of the Social Media Mining for Health Applications (SMM4H) shared task. Our team (UZH) participated in two sub-tasks : Automatic classifications of adverse effects mentions in tweets (Task 1) and Generalizable identification of personal health experience mentions (Task 4). For our submissions, we exploited ensembles based on a pre-trained language representation with a neural transformer architecture (BERT) (Tasks 1 and 4) and a CNN-BiLSTM(-CRF) network within a multi-task learning scenario (Task 1). These <a href=https://en.wikipedia.org/wiki/System>systems</a> are placed on top of a carefully crafted pipeline of domain-specific preprocessing steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3209/>Identifying Adverse Drug Events Mentions in Tweets Using Attentive, Collocated, and Aggregated Medical Representation</a></strong><br><a href=/people/x/xinyan-zhao/>Xinyan Zhao</a>
|
<a href=/people/d/deahan-yu/>Deahan Yu</a>
|
<a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3209><div class="card-body p-3 small">Identifying mentions of medical concepts in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is challenging because of high variability in free text. In this paper, we propose a novel neural network architecture, the Collocated LSTM with Attentive Pooling and Aggregated representation (CLAPA), that integrates a bidirectional LSTM model with attention and pooling strategy and utilizes the collocation information from training data to improve the representation of medical concepts. The collocation and aggregation layers improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on the task of identifying mentions of adverse drug events (ADE) in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. Using the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> made available as part of the workshop shared task, we show that careful selection of neighborhood contexts can help uncover useful local information and improve the overall medical concept representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3210/>Correlating Twitter Language with Community-Level Health Outcomes<span class=acl-fixed-case>T</span>witter Language with Community-Level Health Outcomes</a></strong><br><a href=/people/a/arno-schneuwly/>Arno Schneuwly</a>
|
<a href=/people/r/ralf-grubenmann/>Ralf Grubenmann</a>
|
<a href=/people/s/severine-rion-logean/>Séverine Rion Logean</a>
|
<a href=/people/m/mark-cieliebak/>Mark Cieliebak</a>
|
<a href=/people/m/martin-jaggi/>Martin Jaggi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3210><div class="card-body p-3 small">We study how language on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is linked to mortal diseases such as atherosclerotic heart disease (AHD), <a href=https://en.wikipedia.org/wiki/Diabetes>diabetes</a> and various types of <a href=https://en.wikipedia.org/wiki/Cancer>cancer</a>. Our proposed model leverages state-of-the-art <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>, followed by a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> and <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>, without the need of additional labelled data. It allows to predict community-level medical outcomes from <a href=https://en.wikipedia.org/wiki/Language>language</a>, and thereby potentially translate these to the individual level. The method is applicable to a wide range of target variables and allows us to discover known and potentially novel correlations of <a href=https://en.wikipedia.org/wiki/Outcome_(probability)>medical outcomes</a> with <a href=https://en.wikipedia.org/wiki/Quality_of_life>life-style aspects</a> and other socioeconomic risk factors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3211/>Affective Behaviour Analysis of On-line User Interactions : Are On-line Support Groups More Therapeutic than <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>?<span class=acl-fixed-case>T</span>witter?</a></strong><br><a href=/people/g/giuliano-tortoreto/>Giuliano Tortoreto</a>
|
<a href=/people/e/evgeny-stepanov/>Evgeny Stepanov</a>
|
<a href=/people/a/alessandra-cervone/>Alessandra Cervone</a>
|
<a href=/people/m/mateusz-dubiel/>Mateusz Dubiel</a>
|
<a href=/people/g/giuseppe-riccardi/>Giuseppe Riccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3211><div class="card-body p-3 small">The increase in the prevalence of mental health problems has coincided with a growing popularity of health related social networking sites. Regardless of their therapeutic potential, on-line support groups (OSGs) can also have negative effects on patients. In this work we propose a novel <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to automatically verify the presence of therapeutic factors in <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking websites</a> by using Natural Language Processing (NLP) techniques. The <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is evaluated on on-line asynchronous multi-party conversations collected from an <a href=https://en.wikipedia.org/wiki/Operations_research>OSG</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. The results of the analysis indicate that therapeutic factors occur more frequently in OSG conversations than in Twitter conversations. Moreover, the analysis of OSG conversations reveals that the users of that <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> are supportive, and interactions are likely to lead to the improvement of their <a href=https://en.wikipedia.org/wiki/Emotion>emotional state</a>. We believe that our method provides a stepping stone towards automatic analysis of emotional states of users of online platforms. Possible applications of the method include provision of guidelines that highlight potential implications of using such platforms on users&#8217; mental health, and/or support in the analysis of their impact on specific individuals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3213/>NLP@UNED at SMM4H 2019 : Neural Networks Applied to Automatic Classifications of Adverse Effects Mentions in Tweets<span class=acl-fixed-case>NLP</span>@<span class=acl-fixed-case>UNED</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2019: Neural Networks Applied to Automatic Classifications of Adverse Effects Mentions in Tweets</a></strong><br><a href=/people/j/javier-cortes-tejada/>Javier Cortes-Tejada</a>
|
<a href=/people/j/juan-martinez-romo/>Juan Martinez-Romo</a>
|
<a href=/people/l/lourdes-araujo/>Lourdes Araujo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3213><div class="card-body p-3 small">This paper describes a system for automatically classifying adverse effects mentions in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> developed for the task 1 at Social Media Mining for Health Applications (SMM4H) Shared Task 2019. We have developed a system based on LSTM neural networks inspired by the excellent results obtained by deep learning classifiers in the last edition of this task. The <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is trained along with Twitter GloVe pre-trained word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3214/>Detecting and Extracting of Adverse Drug Reaction Mentioning Tweets with Multi-Head Self Attention</a></strong><br><a href=/people/s/suyu-ge/>Suyu Ge</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3214><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> for the first and second shared tasks of the fourth Social Media Mining for Health Applications (SMM4H) workshop. We enhance tweet representation with a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and distinguish the importance of different words with Multi-Head Self-Attention. In addition, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is exploited to make up for the data shortage. Our system achieved competitive results on both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> with an F1-score of 0.5718 for task 1 and 0.653 (overlap) / 0.357 (strict) for task 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3215/>Deep Learning for Identification of Adverse Effect Mentions In Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/p/paul-barry/>Paul Barry</a>
|
<a href=/people/o/ozlem-uzuner/>Ozlem Uzuner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3215><div class="card-body p-3 small">Social Media Mining for Health Applications (SMM4H) Adverse Effect Mentions Shared Task challenges participants to accurately identify spans of text within a tweet that correspond to Adverse Effects (AEs) resulting from medication usage (Weissenbacher et al., 2019). This task features a training data set of 2,367 tweets, in addition to a 1,000 tweet evaluation data set. The solution presented here features a bidirectional Long Short-term Memory Network (bi-LSTM) for the generation of character-level embeddings. It uses a second bi-LSTM trained on both character and token level embeddings to feed a Conditional Random Field (CRF) which provides the final <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. This paper further discusses the deep learning algorithms used in our <a href=https://en.wikipedia.org/wiki/Solution>solution</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3216/>Using Machine Learning and Deep Learning Methods to Find Mentions of Adverse Drug Reactions in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/p/pilar-lopez-ubeda/>Pilar López Úbeda</a>
|
<a href=/people/m/manuel-carlos-diaz-galiano/>Manuel Carlos Díaz Galiano</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Maite Martin</a>
|
<a href=/people/l/l-alfonso-urena-lopez/>L. Alfonso Urena Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3216><div class="card-body p-3 small">Over time the use of <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> is becoming very popular platforms for <a href=https://en.wikipedia.org/wiki/Health_informatics>sharing health related information</a>. Social Media Mining for Health Applications (SMM4H) provides tasks such as those described in this document to help manage information in the health domain. This document shows the first participation of the SINAI group. We study approaches based on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to extract adverse drug reaction mentions from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. The results obtained in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are encouraging, we are close to the average of all participants and even above in some cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3217/>Towards Text Processing Pipelines to Identify Adverse Drug Events-related Tweets : University of Michigan @ SMM4H 2019 Task 1<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>ichigan @ <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2019 Task 1</a></strong><br><a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a>
|
<a href=/people/g/grace-ganzel/>Grace Ganzel</a>
|
<a href=/people/b/bryan-romas/>Bryan Romas</a>
|
<a href=/people/d/deahan-yu/>Deahan Yu</a>
|
<a href=/people/a/amy-austin/>Amy Austin</a>
|
<a href=/people/n/neha-bhomia/>Neha Bhomia</a>
|
<a href=/people/s/socheatha-chan/>Socheatha Chan</a>
|
<a href=/people/s/stephanie-hall/>Stephanie Hall</a>
|
<a href=/people/v/van-le/>Van Le</a>
|
<a href=/people/a/aaron-miller/>Aaron Miller</a>
|
<a href=/people/o/olawunmi-oduyebo/>Olawunmi Oduyebo</a>
|
<a href=/people/a/aulia-song/>Aulia Song</a>
|
<a href=/people/r/radhika-sondhi/>Radhika Sondhi</a>
|
<a href=/people/d/danny-teng/>Danny Teng</a>
|
<a href=/people/h/hao-tseng/>Hao Tseng</a>
|
<a href=/people/k/kim-vuong/>Kim Vuong</a>
|
<a href=/people/s/stephanie-zimmerman/>Stephanie Zimmerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3217><div class="card-body p-3 small">We participated in Task 1 of the Social Media Mining for Health Applications (SMM4H) 2019 Shared Tasks on detecting mentions of adverse drug events (ADEs) in tweets. Our approach relied on a text processing pipeline for <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, and training traditional <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning and deep learning models</a>. Our submitted runs performed above average for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3219/>Give It a Shot : Few-shot Learning to Normalize ADR Mentions in Social Media Posts<span class=acl-fixed-case>ADR</span> Mentions in Social Media Posts</a></strong><br><a href=/people/e/emmanouil-manousogiannis/>Emmanouil Manousogiannis</a>
|
<a href=/people/s/sepideh-mesbah/>Sepideh Mesbah</a>
|
<a href=/people/a/alessandro-bozzon/>Alessandro Bozzon</a>
|
<a href=/people/s/selene-baez/>Selene Baez</a>
|
<a href=/people/r/robert-jan-sips/>Robert Jan Sips</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3219><div class="card-body p-3 small">This paper describes the system that team MYTOMORROWS-TU DELFT developed for the 2019 Social Media Mining for Health Applications (SMM4H) Shared Task 3, for the end-to-end normalization of ADR tweet mentions to their corresponding MEDDRA codes. For the first two steps, we reuse a state-of-the art approach, focusing our contribution on the final entity-linking step. For that we propose a simple Few-Shot learning approach, based on pre-trained word embeddings and data from the <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a>, combined with the provided training data. Our system (relaxed F1 : 0.337-0.345) outperforms the average (relaxed F1 0.2972) of the participants in this task, demonstrating the potential feasibility of few-shot learning in the context of medical text normalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3221/>Detection of Adverse Drug Reaction Mentions in Tweets Using ELMo<span class=acl-fixed-case>ELM</span>o</a></strong><br><a href=/people/s/sarah-sarabadani/>Sarah Sarabadani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3221><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> used by our team in SMM4H 2019 shared task. We submitted results for subtasks 1 and 2. For task 1 which aims to detect tweets with Adverse Drug Reaction (ADR) mentions we used ELMo embeddings which is a deep contextualized word representation able to capture both syntactic and semantic characteristics. For task 2, which focuses on extraction of ADR mentions, first the same architecture as task 1 was used to identify whether or not a tweet contains ADR. Then, for tweets positively classified as mentioning ADR, the relevant text span was identified by similarity matching with 3 different lexicon sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3222/>Adverse Drug Effect and Personalized Health Mentions, CLaC at SMM4H 2019, Tasks 1 and 4<span class=acl-fixed-case>CL</span>a<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2019, Tasks 1 and 4</a></strong><br><a href=/people/p/parsa-bagherzadeh/>Parsa Bagherzadeh</a>
|
<a href=/people/n/nadia-sheikh/>Nadia Sheikh</a>
|
<a href=/people/s/sabine-bergler/>Sabine Bergler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3222><div class="card-body p-3 small">CLaC labs participated in Task 1 and 4 of SMM4H 2019. We pursed two main objectives in our submission. First we tried to use some textual features in a deep net framework, and second, the potential use of more than one <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> was tested. The results seem positively affected by the proposed <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3223 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3223/>MIDAS@SMM4H-2019 : Identifying Adverse Drug Reactions and Personal Health Experience Mentions from Twitter<span class=acl-fixed-case>MIDAS</span>@<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>-2019: Identifying Adverse Drug Reactions and Personal Health Experience Mentions from <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/d/debanjan-mahata/>Debanjan Mahata</a>
|
<a href=/people/s/sarthak-anand/>Sarthak Anand</a>
|
<a href=/people/h/haimin-zhang/>Haimin Zhang</a>
|
<a href=/people/s/simra-shahid/>Simra Shahid</a>
|
<a href=/people/l/laiba-mehnaz/>Laiba Mehnaz</a>
|
<a href=/people/y/yaman-kumar/>Yaman Kumar</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3223><div class="card-body p-3 small">In this paper, we present our approach and the system description for the Social Media Mining for Health Applications (SMM4H) Shared Task 1,2 and 4 (2019). Our main contribution is to show the effectiveness of Transfer Learning approaches like BERT and ULMFiT, and how they generalize for the classification tasks like identification of adverse drug reaction mentions and reporting of personal health problems in tweets. We show the use of stacked embeddings combined with BLSTM+CRF tagger for identifying spans mentioning adverse drug reactions in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. We also show that these approaches perform well even with imbalanced dataset in comparison to undersampling and oversampling.</div></div></div><hr><div id=w19-33><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-33.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-33/>Proceedings of the First International Workshop on Designing Meaning Representations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3300/>Proceedings of the First International Workshop on Designing Meaning Representations</a></strong><br><a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/j/james-pustejovksy/>James Pustejovksy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3303/>Modeling Quantification and Scope in Abstract Meaning Representations<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentations</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/k/ken-lai/>Ken Lai</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3303><div class="card-body p-3 small">In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification : <a href=https://en.wikipedia.org/wiki/Quantification_(science)>quantification</a>, <a href=https://en.wikipedia.org/wiki/Negation>negation</a> (generally), and <a href=https://en.wikipedia.org/wiki/Modal_logic>modality</a>. The resulting representation, which we call Uniform Meaning Representation (UMR), adopts the predicative core of AMR and embeds it under a scope graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways : (a) they are more transparent ; and (b) they specify default scope when possible. &#8216;</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3306/>Generating Discourse Inferences from Unscoped Episodic Logical Formulas</a></strong><br><a href=/people/g/gene-kim/>Gene Kim</a>
|
<a href=/people/b/benjamin-kane/>Benjamin Kane</a>
|
<a href=/people/v/viet-duong/>Viet Duong</a>
|
<a href=/people/m/muskaan-mendiratta/>Muskaan Mendiratta</a>
|
<a href=/people/g/graeme-mcguire/>Graeme McGuire</a>
|
<a href=/people/s/sophie-sackstein/>Sophie Sackstein</a>
|
<a href=/people/g/georgiy-platonov/>Georgiy Platonov</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3306><div class="card-body p-3 small">Abstract Unscoped episodic logical form (ULF) is a semantic representation capturing the predicate-argument structure of English within the episodic logic formalism in relation to the syntactic structure, while leaving scope, <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a>, and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> unresolved. We describe how ULF can be used to generate natural language inferences that are grounded in the semantic and syntactic structure through a small set of rules defined over interpretable predicates and transformations on ULFs. The semantic restrictions placed by ULF semantic types enables us to ensure that the inferred structures are semantically coherent while the nearness to <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> enables accurate mapping to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We demonstrate these <a href=https://en.wikipedia.org/wiki/Statistical_inference>inferences</a> on four classes of conversationally-oriented inferences in a <a href=https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance>mixed genre dataset</a> with 68.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> from human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3307/>A Plea for Information Structure as a Part of Meaning Representation</a></strong><br><a href=/people/e/eva-hajicova/>Eva Hajicova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3307><div class="card-body p-3 small">The view that the representation of information structure (IS) should be a part of (any type of) representation of meaning is based on the fact that IS is a semantically relevant phenomenon. In the contribution, three arguments supporting this view are briefly summarized, namely, the relation of IS to the interpretation of negation and presupposition, the relevance of IS to the understanding of discourse connectivity and for the establishment and interpretation of coreference relations. Afterwards, possible integration of the description of the main ingredient of <a href=https://en.wikipedia.org/wiki/Isoamyl_acetate>IS</a> into a meaning representation is illustrated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3308/>TCL-a Lexicon of Turkish Discourse Connectives<span class=acl-fixed-case>TCL</span> - a Lexicon of <span class=acl-fixed-case>T</span>urkish Discourse Connectives</a></strong><br><a href=/people/d/deniz-zeyrek/>Deniz Zeyrek</a>
|
<a href=/people/k/kezban-basibuyuk/>Kezban Başıbüyük</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3308><div class="card-body p-3 small">It is known that discourse connectives are the most salient indicators of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>. State-of-the-art parsers being developed to predict explicit discourse connectives exploit annotated discourse corpora but a lexicon of discourse connectives is also needed to enable further research in discourse structure and support the development of language technologies that use these structures for text understanding. This paper presents a lexicon of Turkish discourse connectives built by automatic means. The <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> has the format of the German connective lexicon, DiMLex, where for each discourse connective, information about the connective&#8216;s orthographic variants, syntactic category and senses are provided along with sample relations. In this paper, we describe the data sources we used and the development steps of the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3310/>Ellipsis in Chinese AMR Corpus<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>AMR</span> Corpus</a></strong><br><a href=/people/y/yihuan-liu/>Yihuan Liu</a>
|
<a href=/people/b/bin-li/>Bin Li</a>
|
<a href=/people/p/peiyi-yan/>Peiyi Yan</a>
|
<a href=/people/l/li-song/>Li Song</a>
|
<a href=/people/w/weiguang-qu/>Weiguang Qu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3310><div class="card-body p-3 small">Ellipsis is very common in language. It&#8217;s necessary for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> to restore the <a href=https://en.wikipedia.org/wiki/Elision>elided elements</a> in a sentence. However, there&#8217;s only a few corpora annotating the <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipsis</a>, which draws back the automatic detection and recovery of the ellipsis. This paper introduces the annotation of ellipsis in Chinese sentences, using a novel graph-based representation Abstract Meaning Representation (AMR), which has a good mechanism to restore the elided elements manually. We annotate 5,000 sentences selected from Chinese TreeBank (CTB). We find that 54.98 % of sentences have <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipses</a>. 92 % of the <a href=https://en.wikipedia.org/wiki/Ellipse>ellipses</a> are restored by copying the antecedents&#8217; concepts. and 12.9 % of them are the new added concepts. In addition, we find that the elided element is a word or phrase in most cases, but sometimes only the head of a phrase or parts of a phrase, which is rather hard for the automatic recovery of ellipsis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3313/>Meaning Representation of Null Instantiated Semantic Roles in FrameNet<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/m/miriam-r-l-petruck/>Miriam R L Petruck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3313><div class="card-body p-3 small">Humans have the unique ability to infer information about participants in a scene, even if they are not mentioned in a text about that scene. Computer systems can not do so without explicit information about those participants. This paper addresses the linguistic phenomenon of null-instantiated frame elements, i.e., implicit semantic roles, and their representation in FrameNet (FN). It motivates FN&#8217;s annotation practice, and illustrates three types of null-instantiated arguments that <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> tracks, noting that other lexical resources do not record such semantic-pragmatic information, despite its need in natural language understanding (NLU), and the elaborate efforts to create new datasets. It challenges the community to appeal to FN data to develop more sophisticated techniques for recognizing implicit semantic roles, and creating needed datasets. Although the annotation of null-instantiated roles was lexicographically motivated, FN provides useful information for <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>, and therefore must be considered in the design of any meaning representation for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3314/>Copula and Case-Stacking Annotations for Korean AMR<span class=acl-fixed-case>K</span>orean <span class=acl-fixed-case>AMR</span></a></strong><br><a href=/people/h/hyonsu-choe/>Hyonsu Choe</a>
|
<a href=/people/j/jiyoon-han/>Jiyoon Han</a>
|
<a href=/people/h/hyejin-park/>Hyejin Park</a>
|
<a href=/people/h/hansaem-kim/>Hansaem Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3314><div class="card-body p-3 small">This paper concerns the application of Abstract Meaning Representation (AMR) to <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. In this regard, it focuses on the <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>copula construction</a> and its negation and the case-stacking phenomenon thereof. To illustrate this clearly, we reviewed the : domain annotation scheme from various perspectives. In this process, the existing <a href=https://en.wikipedia.org/wiki/Annotation>annotation guidelines</a> were improved to devise <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a> for each issue under the principle of pursuing consistency and efficiency of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> without distorting the characteristics of <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3316 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-3316.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3316/>Preparing SNACS for Subjects and Objects<span class=acl-fixed-case>SNACS</span> for Subjects and Objects</a></strong><br><a href=/people/a/adi-shalev/>Adi Shalev</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3316><div class="card-body p-3 small">Research on adpositions and possessives in multiple languages has led to a small inventory of general-purpose meaning classes that disambiguate tokens. Importantly, that work has argued for a principled separation of the semantic role in a scene from the function coded by <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntax</a>. Here, we ask whether this approach can be generalized beyond adpositions and <a href=https://en.wikipedia.org/wiki/Possessive>possessives</a> to cover all scene participantsincluding subjects and objectsdirectly, without reference to a <a href=https://en.wikipedia.org/wiki/Frame_language>frame lexicon</a>. We present new guidelines for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the results of an interannotator agreement study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3317/>A Case Study on <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>Meaning Representation</a> for Vietnamese<span class=acl-fixed-case>V</span>ietnamese</a></strong><br><a href=/people/h/ha-linh/>Ha Linh</a>
|
<a href=/people/h/huyen-nguyen/>Huyen Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3317><div class="card-body p-3 small">This paper presents a case study on meaning representation for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>. Having introduced several existing semantic representation schemes for different languages, we select as basis for our work on Vietnamese AMR (Abstract Meaning Representation). From it, we define a meaning representation label set by adapting the English schema and taking into account the specific characteristics of <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3318/>VerbNet Representations : Subevent Semantics for Transfer Verbs<span class=acl-fixed-case>V</span>erb<span class=acl-fixed-case>N</span>et Representations: Subevent Semantics for Transfer Verbs</a></strong><br><a href=/people/s/susan-windisch-brown/>Susan Windisch Brown</a>
|
<a href=/people/j/julia-bonn/>Julia Bonn</a>
|
<a href=/people/j/james-gung/>James Gung</a>
|
<a href=/people/a/annie-zaenen/>Annie Zaenen</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3318><div class="card-body p-3 small">This paper announces the release of a new version of the English lexical resource VerbNet with substantially revised semantic representations designed to facilitate computer planning and reasoning based on human language. We use the transfer of possession and transfer of information event representations to illustrate both the general framework of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> and the types of nuances the new <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> can capture. These representations use a Generative Lexicon-inspired subevent structure to track attributes of event participants across time, highlighting oppositions and temporal and causal relations among the subevents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3319/>Semantically Constrained Multilayer Annotation : The Case of Coreference</a></strong><br><a href=/people/j/jakob-prange/>Jakob Prange</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3319><div class="card-body p-3 small">We propose a coreference annotation scheme as a layer on top of the Universal Conceptual Cognitive Annotation foundational layer, treating units in predicate-argument structure as a basis for entity and event mentions. We argue that this allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from other schemes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3320/>Towards Universal Semantic Representation</a></strong><br><a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/laura-chiticariu/>Laura Chiticariu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3320><div class="card-body p-3 small">Natural language understanding at the <a href=https://en.wikipedia.org/wiki/Semantics>semantic level</a> and independent of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variations</a> is of great practical value. Existing approaches such as semantic role labeling (SRL) and abstract meaning representation (AMR) still have features related to the peculiarities of the particular language. In this work we describe various challenges and possible solutions in designing a semantic representation that is universal across a variety of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3322/>Augmenting Abstract Meaning Representation for <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>Human-Robot Dialogue</a><span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation for Human-Robot Dialogue</a></strong><br><a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a>
|
<a href=/people/s/stephen-tratz/>Stephen Tratz</a>
|
<a href=/people/r/ron-artstein/>Ron Artstein</a>
|
<a href=/people/d/david-traum/>David Traum</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3322><div class="card-body p-3 small">We detail refinements made to Abstract Meaning Representation (AMR) that make the representation more suitable for supporting a situated dialogue system, where a human remotely controls a robot for purposes of <a href=https://en.wikipedia.org/wiki/Search_and_rescue>search and rescue</a> and reconnaissance. We propose 36 augmented AMRs that capture speech acts, <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and aspect, and <a href=https://en.wikipedia.org/wiki/Spatial_analysis>spatial information</a>. This linguistic information is vital for representing important distinctions, for example whether the robot has moved, is moving, or will move. We evaluate two existing AMR parsers for their performance on dialogue data. We also outline a model for graph-to-graph conversion, in which output from AMR parsers is converted into our refined AMRs. The design scheme presented here, though task-specific, is extendable for broad coverage of <a href=https://en.wikipedia.org/wiki/Speech_act>speech acts</a> using <a href=https://en.wikipedia.org/wiki/Adaptive_Multi-Rate_audio_codec>AMR</a> in future task-independent work.</div></div></div><hr><div id=w19-34><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-34.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-34/>Proceedings of the Second Workshop on Storytelling</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3400/>Proceedings of the Second Workshop on Storytelling</a></strong><br><a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao ‘Kenneth’ Huang</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3404/>A Hybrid Model for Globally Coherent Story Generation</a></strong><br><a href=/people/f/fangzhou-zhai/>Fangzhou Zhai</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/p/pavel-shkadzko/>Pavel Shkadzko</a>
|
<a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3404><div class="card-body p-3 small">Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data ; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3405/>Guided Neural Language Generation for Automated Storytelling</a></strong><br><a href=/people/p/prithviraj-ammanabrolu/>Prithviraj Ammanabrolu</a>
|
<a href=/people/e/ethan-tien/>Ethan Tien</a>
|
<a href=/people/w/wesley-cheung/>Wesley Cheung</a>
|
<a href=/people/z/zhaochen-luo/>Zhaochen Luo</a>
|
<a href=/people/w/william-ma/>William Ma</a>
|
<a href=/people/l/lara-martin/>Lara Martin</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3405><div class="card-body p-3 small">Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries. Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into : (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence). However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences. We present an ensemble-based model that generates <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> guided by <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>. Our <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> outperforms the baseline sequence-to-sequence model. Additionally, we provide results for a full end-to-end automated story generation system, demonstrating how our model works with existing systems designed for the event-to-event problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3407" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3407/>Narrative Generation in the Wild : Methods from NaNoGenMo<span class=acl-fixed-case>N</span>arrative <span class=acl-fixed-case>G</span>eneration in the <span class=acl-fixed-case>W</span>ild: <span class=acl-fixed-case>M</span>ethods from <span class=acl-fixed-case>N</span>a<span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>G</span>en<span class=acl-fixed-case>M</span>o</a></strong><br><a href=/people/j/judith-van-stegeren/>Judith van Stegeren</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3407><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text generation</a>, generating long stories is still a challenge. Coherence tends to decrease rapidly as the output length increases. Especially for generated stories, coherence of the narrative is an important quality aspect of the output text. In this paper we examine how narrative coherence is attained in the submissions of NaNoGenMo 2018, an online text generation event where participants are challenged to generate a 50,000 word novel. We list the main approaches that were used to generate coherent narratives and link them to <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>. Finally, we give recommendations on when to use which <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3408/>Lexical concreteness in narrative</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3408><div class="card-body p-3 small">This study explores the relation between lexical concreteness and narrative text quality. We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to quantitatively measure lexical concreteness of a text. We apply <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to a corpus of student stories, scored according to writing evaluation rubrics. Lexical concreteness is weakly-to-moderately related to story quality, depending on story-type. The relation is mostly borne by <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>, but also found for <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3411/>Personality Traits Recognition in <a href=https://en.wikipedia.org/wiki/Literary_language>Literary Texts</a></a></strong><br><a href=/people/d/daniele-pizzolli/>Daniele Pizzolli</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3411><div class="card-body p-3 small">Interesting stories often are built around interesting characters. Finding and detailing what makes an interesting character is a real challenge, but certainly a significant cue is the character personality traits. Our exploratory work tests the adaptability of the current <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits theories</a> to literal characters, focusing on the analysis of utterances in <a href=https://en.wikipedia.org/wiki/Play_(theatre)>theatre scripts</a>. And, at the opposite, we try to find significant <a href=https://en.wikipedia.org/wiki/Trait_theory>traits</a> for interesting characters. The preliminary results demonstrate that our approach is reasonable. Using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> for gaining insight into the <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits</a> of <a href=https://en.wikipedia.org/wiki/Character_(arts)>fictional characters</a> can make sense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3413" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3413/>WriterForcing : Generating more interesting story endings<span class=acl-fixed-case>W</span>riter<span class=acl-fixed-case>F</span>orcing: Generating more interesting story endings</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/v/vinayshekhar-bannihatti-kumar/>Vinayshekhar Bannihatti Kumar</a>
|
<a href=/people/m/mukul-bhutani/>Mukul Bhutani</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3413><div class="card-body p-3 small">We study the problem of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>generating interesting endings</a> for stories. Neural generative models have shown promising results for various text generation problems. Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2Seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same <a href=https://en.wikipedia.org/wiki/Context_(language_use)>story context</a>. In this paper, we propose <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which generate more diverse and interesting outputs by 1) training <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings.</div></div></div><hr><div id=w19-35><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-35.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-35/>Proceedings of the Third Workshop on Abusive Language Online</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3500/>Proceedings of the Third Workshop on Abusive Language Online</a></strong><br><a href=/people/s/sarah-t-roberts/>Sarah T. Roberts</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3501/>Subversive Toxicity Detection using Sentiment Information</a></strong><br><a href=/people/e/eloi-brassard-gourdeau/>Eloi Brassard-Gourdeau</a>
|
<a href=/people/r/richard-khoury/>Richard Khoury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3501><div class="card-body p-3 small">The presence of toxic content has become a major problem for many <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a>. Moderators try to limit this problem by implementing more and more refined comment filters, but toxic users are constantly finding new ways to circumvent them. Our hypothesis is that while modifying toxic content and <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> to fool filters can be easy, hiding sentiment is harder. In this paper, we explore various aspects of sentiment detection and their correlation to <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>, and use our results to implement a <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity detection tool</a>. We then test how adding the sentiment information helps detect <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in three different real-world datasets, and incorporate <a href=https://en.wikipedia.org/wiki/Subversion>subversion</a> to these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to simulate a user trying to circumvent the system. Our results show <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> has a positive impact on toxicity detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3504" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3504/>Racial Bias in <a href=https://en.wikipedia.org/wiki/Hate_speech>Hate Speech</a> and Abusive Language Detection Datasets</a></strong><br><a href=/people/t/thomas-davidson/>Thomas Davidson</a>
|
<a href=/people/d/debasmita-bhattacharya/>Debasmita Bhattacharya</a>
|
<a href=/people/i/ingmar-weber/>Ingmar Weber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3504><div class="card-body p-3 small">Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine <a href=https://en.wikipedia.org/wiki/Racism_in_the_United_States>racial bias</a> in five different sets of Twitter data annotated for hate speech and abusive language. We train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on these datasets and compare the predictions of these <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on tweets written in <a href=https://en.wikipedia.org/wiki/African-American_English>African-American English</a> with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on them tend to predict that tweets written in <a href=https://en.wikipedia.org/wiki/African-American_English>African-American English</a> are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these <a href=https://en.wikipedia.org/wiki/System>systems</a> may discriminate against the groups who are often the targets of the abuse we are trying to detect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3505/>Automated Identification of Verbally Abusive Behaviors in Online Discussions</a></strong><br><a href=/people/s/srecko-joksimovic/>Srecko Joksimovic</a>
|
<a href=/people/r/ryan-s-baker/>Ryan S. Baker</a>
|
<a href=/people/j/jaclyn-ocumpaugh/>Jaclyn Ocumpaugh</a>
|
<a href=/people/j/juan-miguel-l-andres/>Juan Miguel L. Andres</a>
|
<a href=/people/i/ivan-tot/>Ivan Tot</a>
|
<a href=/people/e/elle-yuan-wang/>Elle Yuan Wang</a>
|
<a href=/people/s/shane-dawson/>Shane Dawson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3505><div class="card-body p-3 small">Discussion forum participation represents one of the crucial factors for <a href=https://en.wikipedia.org/wiki/Learning>learning</a> and often the only way of supporting <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a> in <a href=https://en.wikipedia.org/wiki/Online_and_offline>online settings</a>. However, as much as sharing new ideas or asking thoughtful questions contributes <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, verbally abusive behaviors, such as expressing negative emotions in online discussions, could have disproportionate detrimental effects. To provide means for mitigating the potential negative effects on course participation and <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, we developed an automated classifier for identifying <a href=https://en.wikipedia.org/wiki/Communication>communication</a> that show linguistic patterns associated with <a href=https://en.wikipedia.org/wiki/Hostility>hostility</a> in online forums. In so doing, we employ several well-established automated text analysis tools and build on the common practices for handling highly imbalanced datasets and reducing the sensitivity to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. Although still in its infancy, our approach shows promising results (ROC AUC.73) towards establishing a robust detector of abusive behaviors. We further provide an overview of the classification (linguistic and contextual) features most indicative of online aggression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3508/>Pay Attention to your Context when Classifying Abusive Language</a></strong><br><a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a>
|
<a href=/people/k/kilol-gupta/>Kilol Gupta</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3508><div class="card-body p-3 small">The goal of any <a href=https://en.wikipedia.org/wiki/Social_media>social media platform</a> is to facilitate healthy and meaningful interactions among its users. But more often than not, it has been found that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> becomes an avenue for wanton attacks. We propose an experimental study that has three aims : 1) to provide us with a deeper understanding of current data sets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language, and personal attacks) ; 2) to investigate what type of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> (contextual vs. self-attention) is better for abusive language detection using deep learning architectures ; and 3) to investigate whether stacked architectures provide an advantage over simple architectures for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3509/>Challenges and frontiers in abusive content detection</a></strong><br><a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/a/alex-harris/>Alex Harris</a>
|
<a href=/people/d/dong-nguyen/>Dong Nguyen</a>
|
<a href=/people/r/rebekah-tromble/>Rebekah Tromble</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3509><div class="card-body p-3 small">Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which <a href=https://en.wikipedia.org/wiki/Social_science>social scientific insights</a> can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3511/>A System to Monitor Cyberbullying based on Message Classification and Social Network Analysis</a></strong><br><a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/g/giovanni-moretti/>Giovanni Moretti</a>
|
<a href=/people/m/michele-corazza/>Michele Corazza</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3511><div class="card-body p-3 small">Social media platforms like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Instagram>Instagram</a> face a surge in cyberbullying phenomena against young users and need to develop scalable computational methods to limit the negative consequences of this kind of abuse. Despite the number of approaches recently proposed in the Natural Language Processing (NLP) research area for detecting different forms of abusive language, the issue of identifying cyberbullying phenomena at scale is still an unsolved problem. This is because of the need to couple abusive language detection on textual message with network analysis, so that repeated attacks against the same person can be identified. In this paper, we present a system to monitor cyberbullying phenomena by combining message classification and <a href=https://en.wikipedia.org/wiki/Social_network_analysis>social network analysis</a>. We evaluate the classification module on a data set built on <a href=https://en.wikipedia.org/wiki/Instagram>Instagram messages</a>, and we describe the cyberbullying monitoring user interface.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3512/>L-HSAB : A Levantine Twitter Dataset for Hate Speech and Abusive Language<span class=acl-fixed-case>L</span>-<span class=acl-fixed-case>HSAB</span>: A <span class=acl-fixed-case>L</span>evantine <span class=acl-fixed-case>T</span>witter Dataset for Hate Speech and Abusive Language</a></strong><br><a href=/people/h/hala-mulki/>Hala Mulki</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/c/chedi-bechikh-ali/>Chedi Bechikh Ali</a>
|
<a href=/people/h/halima-alshabani/>Halima Alshabani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3512><div class="card-body p-3 small">Hate speech and abusive language have become a common phenomenon on Arabic social media. Automatic hate speech and abusive detection systems can facilitate the prohibition of toxic textual contents. The <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>, informality and ambiguity of the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> hindered the provision of the needed resources for Arabic abusive / hate speech detection research. In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents. We, further, provide a detailed review of the data collection steps and how we design the annotation guidelines such that a reliable dataset annotation is guaranteed. This has been later emphasized through the comprehensive evaluation of the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> as the annotation agreement metrics of Cohen&#8217;s Kappa (k) and Krippendorff&#8217;s alpha () indicated the consistency of the annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3514/>Preemptive Toxic Language Detection in Wikipedia Comments Using Thread-Level Context<span class=acl-fixed-case>W</span>ikipedia Comments Using Thread-Level Context</a></strong><br><a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3514><div class="card-body p-3 small">We address the task of automatically detecting toxic content in <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated texts</a>. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that jointly considers all comments in a <a href=https://en.wikipedia.org/wiki/Conversation_threading>conversation thread</a> outperforms a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that considers only individual comments. Using an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of conversations among <a href=https://en.wikipedia.org/wiki/Wikipedia_community>Wikipedia contributors</a> as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3516/>A Platform Agnostic Dual-Strand Hate Speech Detector</a></strong><br><a href=/people/j/johannes-skjeggestad-meyer/>Johannes Skjeggestad Meyer</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3516><div class="card-body p-3 small">Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text&#8217;s author may differ between services, and so using such data would reduce a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s general applicability. The paper thus focuses on using exclusively <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-based input</a> in the detection, in an optimised architecture combining <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and Long Short-Term Memory-networks. The hate speech detector merges two strands with <a href=https://en.wikipedia.org/wiki/N-gram>character n-grams</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to produce the final classification, and is shown to outperform comparable previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3520/>Online aggression from a sociological perspective : An integrative view on determinants and possible countermeasures</a></strong><br><a href=/people/s/sebastian-weingartner/>Sebastian Weingartner</a>
|
<a href=/people/l/lea-stahel/>Lea Stahel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3520><div class="card-body p-3 small">The present paper introduces a <a href=https://en.wikipedia.org/wiki/Mathematical_model>theoretical model</a> for explaining aggressive online comments from a sociological perspective. It is innovative as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> combines individual, situational, and social-structural determinants of online aggression and tries to theoretically derive their interplay. Moreover, the paper suggests an empirical strategy for testing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The main contribution will be to match online commenting data with survey data containing rich background data of non- /aggressive online commentators.</div></div></div><hr><div id=w19-36><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W19-36/>Proceedings of the 2019 Workshop on Widening NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3600/>Proceedings of the 2019 Workshop on Widening NLP</a></strong><br><a href=/people/a/amittai-axelrod/>Amittai Axelrod</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/r/rossana-cunha/>Rossana Cunha</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3601/>Development of a General Purpose Sentiment Lexicon for <span class=acl-fixed-case>I</span>gbo Language</a></strong><br><a href=/people/e/emeka-ogbuju/>Emeka Ogbuju</a>
|
<a href=/people/m/moses-onyesolu/>Moses Onyesolu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3601><div class="card-body p-3 small">There are publicly available general purpose sentiment lexicons in some high resource languages but very few exist in the low resource languages. This makes it difficult to directly perform sentiment analysis tasks in such languages. The objective of this work is to create a general purpose sentiment lexicon for Igbo language that can determine the sentiment of documents written in Igbo language without having to translate it to English language. The material used was an automatically translated Liu&#8217;s lexicon and manual addition of Igbo native words. The result of this work is a general purpose lexicon &#8211; IgboSentilex. The performance was tested on the BBC Igbo news channel. It returned an average polarity agreement of 95% with other general purpose sentiment lexicons.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3602/>Towards a Resource Grammar for <span class=acl-fixed-case>R</span>unyankore and Rukiga</a></strong><br><a href=/people/d/david-bamutura/>David Bamutura</a>
|
<a href=/people/p/peter-ljunglof/>Peter Ljunglöf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3602><div class="card-body p-3 small">Currently, there is a lack of computational grammar resources for many under-resourced languages which limits the ability to develop Natural Language Processing (NLP) tools and applications such as Multilingual Document Authoring, Computer-Assisted Language Learning (CALL) and Low-Coverage Machine Translation (MT) for these languages. In this paper, we present our attempt to formalise the grammar of two such languages: Runyankore and Rukiga. For this formalisation we use the Grammatical Framework (GF) and its Resource Grammar Library (GF-RGL).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3603/>Speech Recognition for <span class=acl-fixed-case>T</span>igrinya language Using Deep Neural Network Approach</a></strong><br><a href=/people/h/hafte-abera/>Hafte Abera</a>
|
<a href=/people/s/sebsibe-h-mariam/>Sebsibe H/mariam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3603><div class="card-body p-3 small">This work presents a speech recognition model for Tigrinya language .The Deep Neural Network is used to make the recognition model. The Long Short-Term Memory Network (LSTM), which is a special kind of Recurrent Neural Network composed of Long Short-Term Memory blocks, is the primary layer of our neural network model. The 40-dimensional features are MFCC-LDA-MLLT-fMLLR with CMN were used. The acoustic models are trained on features that are obtained by projecting down to 40 dimensions using linear discriminant analysis (LDA). Moreover, speaker adaptive training (SAT) is done using a single feature-space maximum likelihood linear regression (FMLLR) transform estimated per speaker. We train and compare LSTM and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. Finally, the accuracy of the model is evaluated based on the recognition rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3604/>Knowledge-Based Word Sense Disambiguation with Distributional Semantic Expansion</a></strong><br><a href=/people/h/hossein-rouhizadeh/>Hossein Rouhizadeh</a>
|
<a href=/people/m/mehrnoush-shamsfard/>Mehrnoush Shamsfard</a>
|
<a href=/people/m/masoud-rouhizadeh/>Masoud Rouhizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3604><div class="card-body p-3 small">In this paper, we presented a WSD system that uses LDA topics for semantic expansion of document words. Our system also uses sense frequency information from SemCor to give higher priority to the senses which are more probable to happen.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3605/><span class=acl-fixed-case>A</span>spe<span class=acl-fixed-case>R</span>a: Aspect-Based Rating Prediction Based on User Reviews</a></strong><br><a href=/people/e/elena-tutubalina/>Elena Tutubalina</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a>
|
<a href=/people/s/sergey-nikolenko/>Sergey Nikolenko</a>
|
<a href=/people/a/anton-alekseev/>Anton Alekseev</a>
|
<a href=/people/i/ilya-shenbin/>Ilya Shenbin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3605><div class="card-body p-3 small">We propose a novel Aspect-based Rating Prediction model (AspeRa) that estimates user rating based on review texts for the items. It is based on aspect extraction with neural networks and combines the advantages of deep learning and topic modeling. It is mainly designed for recommendations, but an important secondary goal of AspeRa is to discover coherent aspects of reviews that can be used to explain predictions or for user profiling. We conduct a comprehensive empirical study of AspeRa, showing that it outperforms state-of-the-art models in terms of recommendation quality and produces interpretable aspects. This paper is an abridged version of our work (Nikolenko et al., 2019)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3606/>Recognizing Arrow Of Time In The Short Stories</a></strong><br><a href=/people/f/fahimeh-hosseini/>Fahimeh Hosseini</a>
|
<a href=/people/h/hosein-fooladi/>Hosein Fooladi</a>
|
<a href=/people/m/mohammad-reza-samsami/>Mohammad Reza Samsami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3606><div class="card-body p-3 small">Recognizing the arrow of time in the context of paragraphs in short stories is a challenging task. i.e., given only two paragraphs (excerpted from a random position in a short story), determining which comes first and which comes next is a difficult task even for humans. In this paper, we have collected and curated a novel dataset for tackling this challenging task. We have shown that a pre-trained BERT architecture achieves reasonable accuracy on the task, and outperforms RNN-based architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3607/><span class=acl-fixed-case>A</span>mharic Word Sequence Prediction</a></strong><br><a href=/people/n/nuniyat-kifle/>Nuniyat Kifle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3607><div class="card-body p-3 small">The significance of computers and handheld devices are not deniable in the modern world of today. Texts are entered to these devices using word processing programs as well as other techniques and word prediction is one of the techniques. Word Prediction is the action of guessing or forecasting what word comes after, based on some current information, and it is the main focus of this study. Even though Amharic is used by a large number of populations, no significant work is done on the topic of word sequence prediction. In this study, Amharic word sequence prediction model is developed with statistical methods using Hidden Markov Model by incorporating detailed Part of speech tag. Evaluation of the model is performed using developed prototype and keystroke savings (KSS) as a metrics. According to our experiment, prediction result using a bi-gram with detailed Part of Speech tag model has higher KSS and it is better compared to tri-gram model and better than those without Part of Speech tag. Therefore, statistical approach with Detailed POS has quite good potential on word sequence prediction for Amharic language. This research deals with designing word sequence prediction model in Amharic language. It is a language that is spoken in eastern Africa. One of the needs for Amharic word sequence prediction for mobile use and other digital devices is in order to facilitate data entry and communication in our language. Word sequence prediction is a challenging task for inflected languages. (Arora, 2007) These kinds of languages are morphologically rich and have enormous word forms. i.e. one word can have different forms. As Amharic language is highly inflected language and morphologically rich it shares this problem. (prediction, 2008) This problem makes word prediction system much more difficult and results poor performance. Due to this reason storing all forms in dictionary won&#8217;t solve the problem as in English and other less inflected languages. But considering other techniques that could help the predictor to suggest the next word like a POS based prediction should be used. Previous researches used dictionary approach with no consideration of context information. Hence storing all forms of words in dictionary for inflected languages such as Amharic language has been less effective. The main goal of this thesis is to implement Amharic word prediction model that works with better prediction speed and with narrowed search space as much as possible. We introduced two models; tags and words and linear interpolation that use part of speech tag information in addition to word n-grams in order to maximize the likelihood of syntactic appropriateness of the suggestions. We believe the results found reflect this. Amharic word sequence prediction using bi-gram model with higher POS weight and detailed Part of speech tag gave better keystroke savings in all scenarios of our experiment. The study followed Design Science Research Methodology (DSRM). Since DSRM includes approaches, techniques, tools, algorithms and evaluation mechanisms in the process, we followed statistical approach with statistical language modeling and built Amharic prediction model based on information from Part of Speech tagger. The statistics included in the systems varies from single word frequencies to part-of-speech tag n-grams. That means it included the statistics of Word frequencies, Word sequence frequencies, Part-of-speech sequence frequencies and other important information. Later on the system was evaluated using Keystroke Savings. (Lindh, 011). Linux mint was used as the main Operation System during the frame work design. We used corpus of 680,000 tagged words that has 31 tag sets, python programming language and its libraries for both the part of speech tagger and the predictor module. Other Tool that was used is the SRILIM (The SRI language modeling toolkit) in order to generate unigram bigram and trigram count as an input for the language model. SRILIM is toolkit that uses to build and apply statistical language modeling. This thesis presented Amharic word sequence prediction model using the statistical approach. We described a combined statistical and lexical word prediction system for handling inflected languages by making use of POS tags to build the language model. We developed Amharic language models of bigram and trigram for the training purpose. We obtained 29% of KSS using bigram model with detailed part ofspeech tag. Hence, Based on the experiments carried out for this study and the results obtained, the following conclusions were made. We concluded that employing syntactic information in the form of Part-of-Speech (POS) n-grams promises more effective predictions. We also can conclude data quantity, performance of POS tagger and data quality highly affects the keystroke savings. Here in our study the tests were done on a small collection of 100 phrases. According to our evaluation better Keystroke saving (KSS) is achieved when using bi-gram model than the tri-gram models. We believe the results obtained using the experiment of detailed Part of speech tags were effective Since speed and search space are the basic issues in word sequence prediction</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3608/>A Framework for Relation Extraction Across Multiple Datasets in Multiple Domains</a></strong><br><a href=/people/g/geeticka-chauhan/>Geeticka Chauhan</a>
|
<a href=/people/m/matthew-mcdermott/>Matthew McDermott</a>
|
<a href=/people/p/peter-szolovits/>Peter Szolovits</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3608><div class="card-body p-3 small">In this work, we aim to build a unifying framework for relation extraction (RE), applying this on 3 highly used datasets with the ability to be extendable to new datasets. At the moment, the domain suffers from lack of reproducibility as well as a lack of consensus on generalizable techniques. Our framework will be open-sourced and will aid in performing systematic exploration on the effect of different modeling techniques, pre-processing, training methodologies and evaluation metrics on the 3 datasets to help establish a consensus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3609/>Learning and Understanding Different Categories of Sexism Using Convolutional Neural Network’s Filters</a></strong><br><a href=/people/s/sima-sharifirad/>Sima Sharifirad</a>
|
<a href=/people/a/alon-jacovi/>Alon Jacovi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3609><div class="card-body p-3 small">Sexism is very common in social media and makes the boundaries of free speech tighter for female users. Automatically flagging and removing sexist content requires niche identification and description of the categories. In this study, inspired by social science work, we propose three categories of sexism toward women as follows: &#8220;Indirect sexism&#8221;, &#8220;Sexual sexism&#8221; and &#8220;Physical sexism&#8221;. We build classifiers such as Convolutional Neural Network (CNN) to automatically detect different types of sexism and address problems of annotation. Even though inherent non-interpretability of CNN is a challenge for users who detect sexism, as the reason classifying a given speech instance with regard to sexism is difficult to glance from a CNN. However, recent research developed interpretable CNN filters for text data. In a CNN, filters followed by different activation patterns along with global max-pooling can help us tease apart the most important ngrams from the rest. In this paper, we interpret a CNN model trained to classify sexism in order to understand different categories of sexism by detecting semantic categories of ngrams and clustering them. Then, these ngrams in each category are used to improve the performance of the classification task. It is a preliminary work using machine learning and natural language techniques to learn the concept of sexism and distinguishes itself by looking at more precise categories of sexism in social media along with an in-depth investigation of CNN&#8217;s filters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3610/>Modeling Five Sentence Quality Representations by Finding Latent Spaces Produced with Deep Long Short-Memory Models</a></strong><br><a href=/people/p/pablo-rivas/>Pablo Rivas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3610><div class="card-body p-3 small">We present a study in which we train neural models that approximate rules that assess the quality of English sentences. We modeled five rules using deep LSTMs trained over a dataset of sentences whose quality is evaluated under such rules. Preliminary results suggest the neural architecture can model such rules to high accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3611/><span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>E</span>thiopian Languages Statistical Machine Translation</a></strong><br><a href=/people/s/solomon-teferra-abate/>Solomon Teferra Abate</a>
|
<a href=/people/m/michael-melese/>Michael Melese</a>
|
<a href=/people/m/martha-yifiru-tachbelie/>Martha Yifiru Tachbelie</a>
|
<a href=/people/m/million-meshesha/>Million Meshesha</a>
|
<a href=/people/s/solomon-atinafu/>Solomon Atinafu</a>
|
<a href=/people/w/wondwossen-mulugeta/>Wondwossen Mulugeta</a>
|
<a href=/people/y/yaregal-assabie/>Yaregal Assabie</a>
|
<a href=/people/h/hafte-abera/>Hafte Abera</a>
|
<a href=/people/b/biniyam-ephrem/>Biniyam Ephrem</a>
|
<a href=/people/t/tewodros-gebreselassie/>Tewodros Gebreselassie</a>
|
<a href=/people/w/wondimagegnhue-tsegaye-tufa/>Wondimagegnhue Tsegaye Tufa</a>
|
<a href=/people/a/amanuel-lemma/>Amanuel Lemma</a>
|
<a href=/people/t/tsegaye-andargie/>Tsegaye Andargie</a>
|
<a href=/people/s/seifedin-shifaw/>Seifedin Shifaw</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3611><div class="card-body p-3 small">In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge&#8217;ez. The corpora are used for conducting bi-directional SMT experiments. The BLEU scores of the bi-directional SMT systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT especially when the targets are Ethiopian languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3612 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3612/>An automatic discourse relation alignment experiment on <span class=acl-fixed-case>TED</span>-<span class=acl-fixed-case>MDB</span></a></strong><br><a href=/people/s/sibel-ozer/>Sibel Ozer</a>
|
<a href=/people/d/deniz-zeyrek/>Deniz Zeyrek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3612><div class="card-body p-3 small">This paper describes an automatic discourse relation alignment experiment as an empirical justification of the planned annotation projection approach to enlarge the 3600-word multilingual corpus of TED Multilingual Discourse Bank (TED-MDB). The experiment is carried out on a single language pair (English-Turkish) included in TED-MDB. The paper first describes the creation of a large corpus of English-Turkish bi-sentences, then it presents a sense-based experiment that automatically aligns the relations in the English sentences of TED-MDB with the Turkish sentences. The results are very close to the results obtained from an earlier semi-automatic post-annotation alignment experiment validated by human annotators and are encouraging for future annotation projection tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3613/>The Design and Construction of the Corpus of <span class=acl-fixed-case>C</span>hina <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/l/lixin-xia/>Lixin Xia</a>
|
<a href=/people/y/yun-xia/>Yun Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3613><div class="card-body p-3 small">The paper describes the development a corpus of an English variety, i.e. China English, in or-der to provide a linguistic resource for researchers in the field of China English. The Corpus of China English (CCE) was built with due consideration given to its representativeness and authenticity. It was composed of more than 13,962,102 tokens in 15,333 texts evenly divided between the following four genres: newspapers, magazines, fiction and academic writings. The texts cover a wide range of domains, such as news, financial, politics, environment, social, culture, technology, sports, education, philosophy, literary, etc. It is a helpful resource for research on China English, computational linguistics, natural language processing, corpus linguistics and English language education.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3614 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3614/>Learning Trilingual Dictionaries for <span class=acl-fixed-case>U</span>rdu – <span class=acl-fixed-case>R</span>oman <span class=acl-fixed-case>U</span>rdu – <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/moiz-rauf/>Moiz Rauf</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3614><div class="card-body p-3 small">In this paper, we present an effort to generate a joint Urdu, Roman Urdu and English trilingual lexicon using automated methods. We make a case for using statistical machine translation approaches and parallel corpora for dictionary creation. To this purpose, we use word alignment tools on the corpus and evaluate translations using human evaluators. Despite different writing script and considerable noise in the corpus our results show promise with over 85% accuracy of Roman Urdu&#8211;Urdu and 45% English&#8211;Urdu pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3615/>Joint Inference on Bilingual Parse Trees for <span class=acl-fixed-case>PP</span>-attachment Disambiguation</a></strong><br><a href=/people/g/geetanjali-rakshit/>Geetanjali Rakshit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3615><div class="card-body p-3 small">Prepositional Phrase (PP) attachment is a classical problem in NLP for languages like English, which suffer from structural ambiguity. In this work, we solve this problem with the help of another language free from such ambiguities, using the parse tree of the parallel sentence in the other language, and word alignments. We formulate an optimization framework that encourages agreement between the parse trees for two languages, and solve it using a novel Dual Decomposition (DD) based algorithm. Experiments on the English-Hindi language pair show promising improvements over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3616/>Using Attention-based Bidirectional <span class=acl-fixed-case>LSTM</span> to Identify Different Categories of Offensive Language Directed Toward Female Celebrities</a></strong><br><a href=/people/s/sima-sharifirad/>Sima Sharifirad</a>
|
<a href=/people/s/stan-matwin/>Stan Matwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3616><div class="card-body p-3 small">Social media posts reflect the emotions, intentions and mental state of the users. Twitter users who harass famous female figures may do so with different intentions and intensities. Recent studies have published datasets focusing on different types of online harassment, vulgar language, and emotional intensities. We trained, validate and test our proposed model, attention-based bidirectional neural network, on the three datasets:&#8221;online harassment&#8221;, &#8220;vulgar language&#8221; and &#8220;valance&#8221; and achieved state of the art performance in two of the datasets. We report F1 score for each dataset separately along with the final precision, recall and macro-averaged F1 score. In addition, we identify ten female figures from different professions and racial backgrounds who have experienced harassment on Twitter. We tested the trained models on ten collected corpuses each related to one famous female figure to predict the type of harassing language, the type of vulgar language and the degree of intensity of language occurring on their social platforms. Interestingly, the achieved results show different patterns of linguistic use targeting different racial background and occupations. The contribution of this study is two-fold. From the technical perspective, our proposed methodology is shown to be effective with a good margin in comparison to the previous state-of-the-art results on one of the two available datasets. From the social perspective, we introduce a methodology which can unlock facts about the nature of offensive language targeting women on online social platforms. The collected dataset will be shared publicly for further investigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3617/>Sentiment Analysis Model for Opinionated <span class=acl-fixed-case>A</span>wngi Text: Case of Music Reviews</a></strong><br><a href=/people/m/melese-mihret/>Melese Mihret</a>
|
<a href=/people/m/muluneh-atinaf/>Muluneh Atinaf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3617><div class="card-body p-3 small">Abstract The analysis of sentiments is imperative to make a decision for individuals, organizations, and governments. Due to the rapid growth of Awngi (Agew) text on the web, there is no available corpus annotated for sentiment analysis. In this paper, we present a SA model for the Awngi language spoken in Ethiopia, by using a supervised machine learning approach. We developed our corpus by collecting around 1500 posts from online sources. This research is begun to build and evaluate the model for opinionated Awngi music reviews. Thus, pre-processing techniques have been employed to clean the data, to convert transliterations to the native Ethiopic script for accessibility and convenience to typing and to change the words to their base form by removing the inflectional morphemes. After pre-processing, the corpus is manually annotated by three the language professional for giving polarity, and rate, their level of confidence in their selection and sentiment intensity scale values. To improve the calculation method of feature selection and weighting and proposed a more suitable SA algorithm for feature extraction named CHI and weight calculation named TF IDF, increasing the proportion and weight of sentiment words in the feature words. We employed Support Vector Machines (SVM), Na&#239;ve Bayes (NB) and Maximum Entropy (MxEn) machine learning algorithms. Generally, the results are encouraging, despite the morphological challenge in Awngi, the data cleanness and small size of data. We are believed that the results could improve further with a larger corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3618/>A compositional view of questions</a></strong><br><a href=/people/m/maria-boritchev/>Maria Boritchev</a>
|
<a href=/people/m/maxime-amblard/>Maxime Amblard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3618><div class="card-body p-3 small">We present a research on compositional treatment of questions in neo-davidsonian event semantics style. Our work is based on (Champollion, 2011) where only declarative sentences were considered. Our research is based on complex formal examples, paving the way towards further research in this domain and further testing on real-life corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3619 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3619/>Controlling the Specificity of Clarification Question Generation</a></strong><br><a href=/people/y/yang-trista-cao/>Yang Trista Cao</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3619><div class="card-body p-3 small">Unlike comprehension-style questions, clarification questions look for some missing information in a given context. However, without guidance, neural models for question generation, similar to dialog generation models, lead to generic and bland questions that cannot elicit useful information. We argue that controlling the level of specificity of the generated questions can have useful applications and propose a neural clarification question generation model for the same. We first train a classifier that annotates a clarification question with its level of specificity (generic or specific) to the given context. Our results on the Amazon questions dataset demonstrate that training a clarification question generation model on specificity annotated data can generate questions with varied levels of specificity to the given context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3620 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3620/>Non-Monotonic Sequential Text Generation</a></strong><br><a href=/people/k/kiante-brantley/>Kiante Brantley</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3620><div class="card-body p-3 small">Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy&#8217;s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order while achieving competitive performance with conventional left-to-right generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3621/>Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3621><div class="card-body p-3 small">Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between &#8220;gender-neutralized&#8221; words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3622/>How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3622><div class="card-body p-3 small">Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun&#8217;s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While &#8220;embedding debiasing&#8221; methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words&#8217; context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3623 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3623/>Automatic Product Categorization for Official Statistics</a></strong><br><a href=/people/a/andrea-roberson/>Andrea Roberson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3623><div class="card-body p-3 small">The North American Product Classification System (NAPCS) is a comprehensive, hierarchical classification system for products (goods and services) that is consistent across the three North American countries. Beginning in 2017, the Economic Census will use NAPCS to produce economy-wide product tabulations. Respondents are asked to report data from a long, pre-specified list of potential products in a given industry, with some lists containing more than 50 potential products. Businesses have expressed the desire to alternatively supply Universal Product Codes (UPC) to the U. S. Census Bureau. Much work has been done around the categorization of products using product descriptions. No study has applied these efforts for the calculation of official statistics (statistics published by government agencies) using only the text of UPC product descriptions. The question we address in this paper is: Given UPC codes and their associated product descriptions, can we accurately predict NAPCS? We tested the feasibility of businesses submitting a spreadsheet with Universal Product Codes and their associated text descriptions. This novel strategy classified text with very high accuracy rates, all of our algorithms surpassed over 90 percent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3624/>An Online Topic Modeling Framework with Topics Automatically Labeled</a></strong><br><a href=/people/j/jin-fenglei/>Jin Fenglei</a>
|
<a href=/people/g/gao-cuiyun/>Gao Cuiyun</a>
|
<a href=/people/l/lyu-michael-r/>Lyu Michael R.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3624><div class="card-body p-3 small">In this paper, we propose a novel online topic tracking framework, named IEDL, for tracking the topic changes related to deep learning techniques on Stack Exchange and automatically interpreting each identified topic. The proposed framework combines the prior topic distributions in a time window during inferring the topics in current time slice, and introduces a new ranking scheme to select most representative phrases and sentences for the inferred topics. Experiments on 7,076 Stack Exchange posts show the effectiveness of IEDL in tracking topic changes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3625 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3625/>Construction and Alignment of Multilingual Entailment Graphs for Semantic Inference</a></strong><br><a href=/people/s/sabine-weber/>Sabine Weber</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3625><div class="card-body p-3 small">This paper presents ongoing work on the construction and alignment of predicate entailment graphs in English and German. We extract predicate-argument pairs from large corpora of monolingual English and German news text and construct monolingual paraphrase clusters and entailment graphs. We use an aligned subset of entities to derive the bilingual alignment of entities and relations, and achieve better than baseline results on a translated subset of a predicate entailment data set (Levy and Dagan, 2016) and the German portion of XNLI (Conneau et al., 2018).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3626 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3626 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3626/><span class=acl-fixed-case>KB</span>-<span class=acl-fixed-case>NLG</span>: From Knowledge Base to Natural Language Generation</a></strong><br><a href=/people/w/wen-cui/>Wen Cui</a>
|
<a href=/people/m/minghui-zhou/>Minghui Zhou</a>
|
<a href=/people/r/rongwen-zhao/>Rongwen Zhao</a>
|
<a href=/people/n/narges-norouzi/>Narges Norouzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3626><div class="card-body p-3 small">We perform the natural language generation (NLG) task by mapping sets of Resource Description Framework (RDF) triples into text. First we investigate the impact of increasing the number of entity types in delexicalisaiton on the generation quality. Second we conduct different experiments to evaluate two widely applied language generation systems, encoder-decoder with attention and the Transformer model on a large benchmark dataset. We evaluate different models on automatic metrics, as well as the training time. To our knowledge, we are the first to apply Transformer model to this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3627 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3627/>Acoustic Characterization of Singaporean Children’s <span class=acl-fixed-case>E</span>nglish: Comparisons to <span class=acl-fixed-case>A</span>merican and <span class=acl-fixed-case>B</span>ritish Counterparts</a></strong><br><a href=/people/y/yuling-gu/>Yuling Gu</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3627><div class="card-body p-3 small">We investigate English pronunciation patterns in Singaporean children in relation to their American and British counterparts by conducting archetypal analysis on selected vowel pairs. Given that Singapore adopts British English as the institutional standard, one might expect Singaporean children to follow British pronunciation patterns, but we observe that Singaporean children also present similar patterns to Americans for TRAP-BATH spilt vowels: (1) British and Singaporean children both produce these vowels with a relatively lowered tongue height. (2) These vowels are more fronted for American and Singaporean children (p &lt; 0.001). In addition, when comparing /&#230;/ and /&#949;/ productions, British speakers show the clearest distinction between the two vowels; Singaporean and American speakers exhibit a higher and more fronted tongue position for /&#230;/ (p &lt; 0.001), causing /&#230;/ to be acoustically more similar to /&#949;/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3628 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3628/>Rethinking Phonotactic Complexity</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3628><div class="card-body p-3 small">In this work, we propose the use of phone-level language models to estimate phonotactic complexity&#8212;measured in bits per phoneme&#8212;which makes cross-linguistic comparison straightforward. We compare the entropy across languages using this simple measure, gaining insight on how complex different language&#8217;s phonotactics are. Finally, we show a very strong negative correlation between phonotactic complexity and the average length of words&#8212;Spearman rho=-0.744&#8212;when analysing a collection of 106 languages with 1016 basic concepts each.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3629 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3629/>Implementing a Multi-lingual Chatbot for Positive Reinforcement in Young Learners</a></strong><br><a href=/people/f/francisca-oladipo/>Francisca Oladipo</a>
|
<a href=/people/a/abdulmalik-rufai/>Abdulmalik Rufai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3629><div class="card-body p-3 small">This is a humanitarian work &#8211;a counter-terrorism effort. The presentation describes the experiences of developing a multi-lingua, interactive chatbot trained on the corpus of two Nigerian Languages (Hausa and Fulfude), with simultaneous translation to a third (Kanuri), to stimulate conversations, deliver tailored contents to the users thereby aiding in the detection of the probability and degree of radicalization in young learners through data analysis of the games moves and vocabularies. As chatbots have the ability to simulate a human conversation based on rhetorical behavior, the system is able to learn the need of individual user through constant interaction and deliver tailored contents that promote good behavior in Hausa, Fulfulde and Kanuri languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3630 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3630/>A Deep Learning Approach to Language-independent Gender Prediction on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/reyhaneh-hashempour/>Reyhaneh Hashempour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3630><div class="card-body p-3 small">This work presents a set of experiments conducted to predict the gender of Twitter users based on language-independent features extracted from the text of the users&#8217; tweets. The experiments were performed on a version of TwiSty dataset including tweets written by the users of six different languages: Portuguese, French, Dutch, English, German, and Italian. Logistic regression (LR), and feed-forward neural networks (FFNN) with back-propagation were used to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual (CL). In the IL setting, the training and testing were performed on the same language whereas in the CL, Italian and German datasets were set aside and only used as test sets and the rest were combined to compose training and development sets. In the IL, the highest accuracy score belongs to LR whereas, in the CL, FFNN with three hidden layers yields the highest score. The results show that neural network based models underperform traditional models when the size of the training set is small; however, they beat traditional models by a non-trivial margin, when they are fed with large enough data. Finally, the feature analysis confirms that men and women have different writing styles independent of their language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3631 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3631/>Isolating the Effects of Modeling Recursive Structures: A Case Study in Pronunciation Prediction of <span class=acl-fixed-case>C</span>hinese Characters</a></strong><br><a href=/people/m/minh-nguyen/>Minh Nguyen</a>
|
<a href=/people/g/gia-h-ngo/>Gia H Ngo</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3631><div class="card-body p-3 small">Finding that explicitly modeling structures leads to better generalization, we consider the task of predicting Cantonese pronunciations of logographs (Chinese characters) using logographs&#8217; recursive structures. This task is a suitable case study for two reasons. First, logographs&#8217; pronunciations depend on structures (i.e. the hierarchies of sub-units in logographs) Second, the quality of logographic structures is consistent since the structures are constructed automatically using a set of rules. Thus, this task is less affected by confounds such as varying quality between annotators. Empirical results show that modeling structures explicitly using treeLSTM outperforms LSTM baseline, reducing prediction error by 6.0% relative.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3632 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3632/>Benchmarking Neural Machine Translation for <span class=acl-fixed-case>S</span>outhern <span class=acl-fixed-case>A</span>frican Languages</a></strong><br><a href=/people/j/jade-abbott/>Jade Abbott</a>
|
<a href=/people/l/laura-martinus/>Laura Martinus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3632><div class="card-body p-3 small">Unlike major Western languages, most African languages are very low-resourced. Furthermore, the resources that do exist are often scattered and difficult to obtain and discover. As a result, the data and code for existing research has rarely been shared, meaning researchers struggle to reproduce reported results, and almost no publicly available benchmarks or leaderboards for African machine translation models exist. To start to address these problems, we trained neural machine translation models for a subset of Southern African languages on publicly-available datasets. We provide the code for training the models and evaluate the models on a newly released evaluation set, with the aim of starting a leaderboard for Southern African languages and spur future research in the field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3633 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3633 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3633/><span class=acl-fixed-case>OCR</span> Quality and <span class=acl-fixed-case>NLP</span> Preprocessing</a></strong><br><a href=/people/m/margot-mieskes/>Margot Mieskes</a>
|
<a href=/people/s/stefan-schmunk/>Stefan Schmunk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3633><div class="card-body p-3 small">We present initial experiments to evaluate the performance of tasks such as Part of Speech Tagging on data corrupted by Optical Character Recognition (OCR). Our results, based on English and German data, using artificial experiments as well as initial real OCRed data indicate that already a small drop in OCR quality considerably increases the error rates, which would have a significant impact on subsequent processing steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3634 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3634 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3634/>Developing a Fine-grained Corpus for a Less-resourced Language: the case of <span class=acl-fixed-case>K</span>urdish</a></strong><br><a href=/people/r/roshna-abdulrahman/>Roshna Abdulrahman</a>
|
<a href=/people/h/hossein-hassani/>Hossein Hassani</a>
|
<a href=/people/s/sina-ahmadi/>Sina Ahmadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3634><div class="card-body p-3 small">Kurdish is a less-resourced language consisting of different dialects written in various scripts. Approximately 30 million people in different countries speak the language. The lack of corpora is one of the main obstacles in Kurdish language processing. In this paper, we present KTC-the Kurdish Textbooks Corpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is normalized and categorized into 12 educational subjects containing 693,800 tokens (110,297 types). Our resource is publicly available for non-commercial use under the CC BY-NC-SA 4.0 license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3635 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3635/><span class=acl-fixed-case>A</span>mharic Question Answering for Biography, Definition, and Description Questions</a></strong><br><a href=/people/t/tilahun-abedissa-taffa/>Tilahun Abedissa Taffa</a>
|
<a href=/people/m/mulugeta-libsie/>Mulugeta Libsie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3635><div class="card-body p-3 small">A broad range of information needs can often be stated as a question. Question Answering (QA) systems attempt to provide users concise answer(s) to natural language questions. The existing Amharic QA systems handle fact-based questions that usually take named entities as an answer. To deal with more complex information needs we developed an Amharic non-factoid QA for biography, definition, and description questions. A hybrid approach has been used for the question classification. For document filtering and answer extraction we have used lexical patterns. On the other hand to answer biography questions we have used a summarizer and the generated summary is validated using a text classifier. Our QA system is evaluated and has shown a promising result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3636 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3636/>Polysemous Language in Child Directed Speech</a></strong><br><a href=/people/s/sammy-floyd/>Sammy Floyd</a>
|
<a href=/people/l/libby-barak/>Libby Barak</a>
|
<a href=/people/a/adele-goldberg/>Adele Goldberg</a>
|
<a href=/people/c/casey-lew-williams/>Casey Lew-Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3636><div class="card-body p-3 small">Polysemous Language in Child Directed Speech Learning the meaning of words is one of the fundamental building blocks of verbal communication. Models of child language acquisition have generally made the simplifying assumption that each word appears in child-directed speech with a single meaning. To understand naturalistic word learning during childhood, it is essential to know whether children hear input that is in fact constrained to single meaning per word, or whether the environment naturally contains multiple senses.In this study, we use a topic modeling approach to automatically induce word senses from child-directed speech. Our results confirm the plausibility of our automated analysis approach and reveal an increasing rate of using multiple senses in child-directed speech, starting with corpora from children as early as the first year of life.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3637 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3637/>Principled Frameworks for Evaluating Ethics in <span class=acl-fixed-case>NLP</span> Systems</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3637><div class="card-body p-3 small">We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3638 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3638 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3638/>Understanding the Shades of Sexism in Popular <span class=acl-fixed-case>TV</span> Series</a></strong><br><a href=/people/n/nayeon-lee/>Nayeon Lee</a>
|
<a href=/people/y/yejin-bang/>Yejin Bang</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3638><div class="card-body p-3 small">[Multiple-submission] In the midst of a generation widely exposed to and influenced by media entertainment, the NLP research community has shown relatively little attention on the sexist comments in popular TV series. To understand sexism in TV series, we propose a way of collecting distant supervision dataset using Character Persona information with the psychological theories on sexism. We assume that sexist characters from TV shows are more prone to making sexist comments when talking about women, and show that this hypothesis is valid through experiment. Finally, we conduct an interesting analysis on popular TV show characters and successfully identify different shades of sexism that is often overlooked.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3639 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3639/>Evaluating Ways of Adapting Word Similarity</a></strong><br><a href=/people/l/libby-barak/>Libby Barak</a>
|
<a href=/people/a/adele-goldberg/>Adele Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3639><div class="card-body p-3 small">People judge pairwise similarity by deciding which aspects of the words&#8217; meanings are relevant for the comparison of the given pair. However, computational representations of meaning rely on dimensions of the vector representation for similarity comparisons, without considering the specific pairing at hand. Prior work has adapted computational similarity judgments by using the softmax function in order to address this limitation by capturing asymmetry in human judgments. We extend this analysis by showing that a simple modification of cosine similarity offers a better correlation with human judgments over a comprehensive dataset. The modification performs best when the similarity between two words is calculated with reference to other words that are most similar and dissimilar to the pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3640 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3640 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3640/>Exploring the Use of Lexicons to aid Deep Learning towards the Detection of Abusive Language</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/j/jason-scott/>Jason Scott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3640><div class="card-body p-3 small">Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focused on detecting personal attacks in online conversations. State-of-the-art research on this task has largely used deep learning with word embeddings. We explored the use of sentiment lexicons as well as semantic lexicons towards improving the accuracy of the baseline Convolutional Neural Network (CNN) using regular word embeddings. This is a work in progress, limited by time constraints and appropriate infrastructure. Our preliminary results showed promise for utilizing lexicons, especially semantic lexicons, for the task of detecting abusive language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3641 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3641/>Entity-level Classification of Adverse Drug Reactions: a Comparison of Neural Network Models</a></strong><br><a href=/people/i/ilseyar-alimova/>Ilseyar Alimova</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3641><div class="card-body p-3 small">This paper presents our experimental work on exploring the potential of neural network models developed for aspect-based sentiment analysis for entity-level adverse drug reaction (ADR) classification. Our goal is to explore how to represent local context around ADR mentions and learn an entity representation, interacting with its context. We conducted extensive experiments on various sources of text-based information, including social media, electronic health records, and abstracts of scientific articles from PubMed. The results show that Interactive Attention Neural Network (IAN) outperformed other models on four corpora in terms of macro F-measure. This work is an abridged version of our recent paper accepted to Programming and Computer Software journal in 2019.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3642 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3642 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3642/>Context Effects on Human Judgments of Similarity</a></strong><br><a href=/people/l/libby-barak/>Libby Barak</a>
|
<a href=/people/n/noe-kong-johnson/>Noe Kong-Johnson</a>
|
<a href=/people/a/adele-goldberg/>Adele Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3642><div class="card-body p-3 small">The semantic similarity of words forms the basis of many natural language processing methods. These computational similarity measures are often based on a mathematical comparison of vector representations of word meanings, while human judgments of similarity differ in lacking geometrical properties, e.g., symmetric similarity and triangular similarity. In this study, we propose a novel task design to further explore human behavior by asking whether a pair of words is deemed more similar depending on an immediately preceding judgment. Results from a crowdsourcing experiment show that people consistently judge words as more similar when primed by a judgment that evokes a relevant relationship. Our analysis further shows that word2vec similarity correlated significantly better with the out-of-context judgments, thus confirming the methodological differences in human-computer judgments, and offering a new testbed for probing the differences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3643 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3643/><span class=acl-fixed-case>NLP</span> Automation to Read Radiological Reports to Detect the Stage of Cancer Among Lung Cancer Patients</a></strong><br><a href=/people/k/khushbu-gupta/>Khushbu Gupta</a>
|
<a href=/people/r/ratchainant-thammasudjarit/>Ratchainant Thammasudjarit</a>
|
<a href=/people/a/ammarin-thakkinstian/>Ammarin Thakkinstian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3643><div class="card-body p-3 small">A common challenge in the healthcare industry today is physicians have access to massive amounts of healthcare data but have little time and no appropriate tools. For instance, the risk prediction model generated by logistic regression could predict the probability of diseases occurrence and thus prioritizing patients&#8217; waiting list for further investigations. However, many medical reports available in current clinical practice system are not yet ready for analysis using either statistics or machine learning as they are in unstructured text format. The complexity of medical information makes the annotation or validation of data very challenging and thus acts as a bottleneck to apply machine learning techniques in medical data. This study is therefore conducted to create such annotations automatically where the computer can read radiological reports for oncologists and mark the staging of lung cancer. This staging information is obtained using the rule-based method implemented using the standards of Tumor Node Metastasis (TNM) staging along with deep learning technology called Long Short Term Memory (LSTM) to extract clinical information from the Computed Tomography (CT) text report. The empirical experiment shows promising results being the accuracy of up to 85%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3644 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3644/>Augmenting Named Entity Recognition with Commonsense Knowledge</a></strong><br><a href=/people/g/gaith-dekhili/>Gaith Dekhili</a>
|
<a href=/people/t/tan-ngoc-le/>Tan Ngoc Le</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3644><div class="card-body p-3 small">Commonsense can be vital in some applications like Natural Language Understanding (NLU), where it is often required to resolve ambiguity arising from implicit knowledge and underspecification. In spite of the remarkable success of neural network approaches on a variety of Natural Language Processing tasks, many of them struggle to react effectively in cases that require commonsense knowledge. In the present research, we take advantage of the availability of the open multilingual knowledge graph ConceptNet, by using it as an additional external resource in Named Entity Recognition (NER). Our proposed architecture involves BiLSTM layers combined with a CRF layer that was augmented with some features such as pre-trained word embedding layers and dropout layers. Moreover, apart from using word representations, we used also character-based representation to capture the morphological and the orthographic information. Our experiments and evaluations showed an improvement in the overall performance with +2.86 in the F1-measure. Commonsense reasonnig has been employed in other studies and NLP tasks but to the best of our knowledge, there is no study relating the integration of a commonsense knowledge base in NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3645 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3645 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3645/>Pardon the Interruption: Automatic Analysis of Gender and Competitive Turn-Taking in <span class=acl-fixed-case>U</span>nited <span class=acl-fixed-case>S</span>tates <span class=acl-fixed-case>S</span>upreme <span class=acl-fixed-case>C</span>ourt Hearings</a></strong><br><a href=/people/h/haley-lepp/>Haley Lepp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3645><div class="card-body p-3 small">The United States Supreme Court plays a key role in defining the legal basis for gender discrimination throughout the country, yet there are few checks on gender bias within the court itself. In conversational turn-taking, interruptions have been documented as a marker of bias between speakers of different genders. The goal of this study is to automatically differentiate between respectful and disrespectful conversational turns taken during official hearings, which could help in detecting bias and finding remediation techniques for discourse in the courtroom. In this paper, I present a corpus of turns annotated by legal professionals, and describe the design of a semi-supervised classifier that will use acoustic and lexical features to analyze turn-taking at scale. On completion of annotations, this classifier will be trained to extract the likelihood that turns are respectful or disrespectful for use in studies of speech trends.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3646 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3646/>Evaluating Coherence in Dialogue Systems using Entailment</a></strong><br><a href=/people/n/nouha-dziri/>Nouha Dziri</a>
|
<a href=/people/e/ehsan-kamalloo/>Ehsan Kamalloo</a>
|
<a href=/people/k/kory-mathewson/>Kory Mathewson</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaiane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3646><div class="card-body p-3 small">Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses. This paper has been accepted in NAACL 2019.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3647 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3647/>Exploiting machine algorithms in vocalic quantification of <span class=acl-fixed-case>A</span>frican <span class=acl-fixed-case>E</span>nglish corpora</a></strong><br><a href=/people/l/lasisi-adeiza-isiaka/>Lasisi Adeiza Isiaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3647><div class="card-body p-3 small">Towards procedural fidelity in the processing of African English speech corpora, this work demonstrates how the adaptation of machine-assisted segmentation of phonemes and automatic extraction of acoustic values can significantly speed up the processing of naturalistic data and make the vocalic analysis of the varieties less impressionistic. Research in African English phonology has, till date, been least data-driven &#8211; much less the use of comparative corpora for cross-varietal assessments. Using over 30 hours of naturalistic data (from 28 speakers in 5 Nigerian cities), the procedures for segmenting audio files into phonemic units via the Munich Automatic Segmentation System (MAUS), and the extraction of their spectral values in Praat are explained. Evidence from the speech corpora supports a more complex vocalic inventory than attested in previous auditory/manual-based accounts &#8211; thus reinforcing the resourcefulness of the algorithms for the current data and cognate varieties. Keywords: machine algorithms; naturalistic data; African English phonology; vowel segmentation</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3648 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3648/>Assessing the Ability of Neural Machine Translation Models to Perform Syntactic Rewriting</a></strong><br><a href=/people/j/jahkel-robin/>Jahkel Robin</a>
|
<a href=/people/a/alvin-grissom-ii/>Alvin Grissom II</a>
|
<a href=/people/m/matthew-roselli/>Matthew Roselli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3648><div class="card-body p-3 small">We describe work in progress for evaluating performance of sequence-to-sequence neural networks on the task of syntax-based reordering for rules applicable to simultaneous machine translation. We train models that attempt to rewrite English sentences using rules that are commonly used by human interpreters. We examine the performance of these models to determine which forms of rewriting are more difficult for them to learn and which architectures are the best at learning them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3649 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3649 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3649/>Authorship Recognition with Short-Text using Graph-based Techniques</a></strong><br><a href=/people/l/laura-cruz/>Laura Cruz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3649><div class="card-body p-3 small">In recent years, studies of authorship recognition has aroused great interest in graph-based analysis. Modeling the writing style of each author using a network of co-occurrence words. However, short texts can generate some changes in the topology of network that cause impact on techniques of feature extraction based on graph topology. In this work, we evaluate the robustness of global-strategy and local-strategy based on complex network measurements comparing with graph2vec a graph embedding technique based on skip-gram model. The experiment consists of evaluating how each modification in the length of text affects the accuracy of authorship recognition on both techniques using cross-validation and machine learning techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3650 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3650/>A Parallel Corpus <span class=acl-fixed-case>M</span>ixtec-<span class=acl-fixed-case>S</span>panish</a></strong><br><a href=/people/c/cynthia-montano/>Cynthia Montaño</a>
|
<a href=/people/g/gerardo-sierra-martinez/>Gerardo Sierra Martínez</a>
|
<a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a>
|
<a href=/people/h/helena-gomez/>Helena Gomez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3650><div class="card-body p-3 small">This work is about the compilation process of parallel documents Spanish-Mixtec. There are not many Spanish-Mixec parallel texts and most of the sources are non-digital books. Due to this, we need to face the errors when digitizing the sources and difficulties in sentence alignment, as well as the fact that does not exist a standard orthography. Our parallel corpus consists of sixty texts coming from books and digital repositories. These documents belong to different domains: history, traditional stories, didactic material, recipes, ethnographical descriptions of each town and instruction manuals for disease prevention. We have classified this material in five major categories: didactic (6 texts), educative (6 texts), interpretative (7 texts), narrative (39 texts), and poetic (2 texts). The final total of tokens is 49,814 Spanish words and 47,774 Mixtec words. The texts belong to the states of Oaxaca (48 texts), Guerrero (9 texts) and Puebla (3 texts). According to this data, we see that the corpus is unbalanced in what refers to the representation of the different territories. While 55% of speakers are in Oaxaca, 80% of texts come from this region. Guerrero has the 30% of speakers and the 15% of texts and Puebla, with the 15% of the speakers has a representation of the 5% in the corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3651 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3651 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3651/>Emoji Usage Across Platforms: A Case Study for the Charlottesville Event</a></strong><br><a href=/people/k/khyati-mahajan/>Khyati Mahajan</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3651><div class="card-body p-3 small">We study emoji usage patterns across two social media platforms, one of them considered a fringe community called Gab, and the other Twitter. We find that Gab tends to comparatively use more emotionally charged emoji, but also seems more apathetic towards the violence during the event, while Twitter takes a more empathetic approach to the event.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3652 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3652 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3652/>Reading <span class=acl-fixed-case>KITTY</span>: Pitch Range as an Indicator of Reading Skill</a></strong><br><a href=/people/a/alfredo-gomez/>Alfredo Gomez</a>
|
<a href=/people/a/alicia-ngo/>Alicia Ngo</a>
|
<a href=/people/a/alessandra-otondo/>Alessandra Otondo</a>
|
<a href=/people/j/julie-medero/>Julie Medero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3652><div class="card-body p-3 small">While affective outcomes are generally positive for the use of eBooks and computer-based reading tutors in teaching children to read, learning outcomes are often poorer (Korat and Shamir, 2004). We describe the first iteration of Reading Kitty, an iOS application that uses NLP and speech processing to focus children&#8217;s time on close reading and prosody in oral reading, while maintaining an emphasis on creativity and artifact creation. We also share preliminary results demonstrating that pitch range can be used to automatically predict readers&#8217; skill level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3653 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3653/>Adversarial Attack on Sentiment Classification</a></strong><br><a href=/people/y/yi-ting-alicia-tsai/>Yi-Ting (Alicia) Tsai</a>
|
<a href=/people/m/min-chu-yang/>Min-Chu Yang</a>
|
<a href=/people/h/han-yu-chen/>Han-Yu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3653><div class="card-body p-3 small">In this paper, we propose a white-box attack algorithm called &#8220;Global Search&#8221; method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called &#8220;Greedy Search&#8221;. The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed &#8220;Global Search&#8221; method generates more powerful adversarial examples with less distortion or less modification to the source text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3654 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3654/><span class=acl-fixed-case>CSI</span> <span class=acl-fixed-case>P</span>eru News: finding the culprit, victim and location in news articles</a></strong><br><a href=/people/g/gina-bustamante/>Gina Bustamante</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3654><div class="card-body p-3 small">We introduce a shift on the DS method over the domain of crime-related news from Peru, attempting to find the culprit, victim and location of a crime description from a RE perspective. Obtained results are highly promising and show that proposed modifications are effective in non-traditional domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3655 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3655 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3655/>Exploring Social Bias in Chatbots using Stereotype Knowledge</a></strong><br><a href=/people/n/nayeon-lee/>Nayeon Lee</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3655><div class="card-body p-3 small">Exploring social bias in chatbot is an important, yet relatively unexplored problem. In this paper, we propose an approach to understand social bias in chatbots by leveraging stereotype knowledge. It allows interesting comparison of bias between chatbots and humans, and provides intuitive analysis of existing chatbots by borrowing the finer-grain concepts of sexism and racism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3656 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3656/>Cross-Sentence Transformations in Text Simplification</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3656><div class="card-body p-3 small">Current approaches to Text Simplification focus on simplifying sentences individually. However, certain simplification transformations span beyond single sentences (e.g. joining and re-ordering sentences). In this paper, we motivate the need for modelling the simplification task at the document level, and assess the performance of sequence-to-sequence neural models in this setup. We analyse parallel original-simplified documents created by professional editors and show that there are frequent rewriting transformations that are not restricted to sentence boundaries. We also propose strategies to automatically evaluate the performance of a simplification model on these cross-sentence transformations. Our experiments show the inability of standard sequence-to-sequence neural models to learn these transformations, and suggest directions towards document-level simplification.</div></div></div><hr><div id=w19-37><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-37.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-37/>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3700/>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</a></strong><br><a href=/people/t/tomaz-erjavec/>Tomaž Erjavec</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3702/>Multiple Admissibility : Judging Grammaticality using Unlabeled Data in Language Learning</a></strong><br><a href=/people/a/anisia-katinskaia/>Anisia Katinskaia</a>
|
<a href=/people/s/sardana-ivanova/>Sardana Ivanova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3702><div class="card-body p-3 small">We present our work on the problem of Multiple Admissibility (MA) in <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a>. Multiple Admissibility occurs in many languages when more than one <a href=https://en.wikipedia.org/wiki/Grammar>grammatical form</a> of a word fits syntactically and semantically in a given context. In second language (L2) education-in particular, in <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a> / computer-aided language learning (ITS / CALL) systems, which generate exercises automatically-this implies that multiple alternative answers are possible. We treat the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a grammaticality judgement task. We train a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> with an objective to label sentences as grammatical or ungrammatical, using a simulated learner corpus : a dataset with correct text, and with artificial errors generated automatically. While MA occurs commonly in many languages, this paper focuses on learning <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. We present a detailed classification of the types of constructions in <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, in which MA is possible, and evaluate the model using a test set built from answers provided by the users of a running <a href=https://en.wikipedia.org/wiki/Machine_learning>language learning system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3705/>AGRR 2019 : Corpus for Gapping Resolution in Russian<span class=acl-fixed-case>AGRR</span> 2019: Corpus for Gapping Resolution in <span class=acl-fixed-case>R</span>ussian</a></strong><br><a href=/people/m/maria-ponomareva/>Maria Ponomareva</a>
|
<a href=/people/k/kira-droganova/>Kira Droganova</a>
|
<a href=/people/i/ivan-smurov/>Ivan Smurov</a>
|
<a href=/people/t/tatiana-shavrina/>Tatiana Shavrina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3705><div class="card-body p-3 small">This paper provides a comprehensive overview of the <a href=https://en.wikipedia.org/wiki/Gapping>gapping dataset</a> for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> that consists of 7.5k sentences with <a href=https://en.wikipedia.org/wiki/Gapping>gapping</a> (as well as 15k relevant negative sentences) and comprises data from various genres : <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Fiction>fiction</a>, <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019)-a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. In this paper, we pay special attention to the gapping resolution methods that were introduced within the shared task as well as an alternative test set that illustrates that our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is a diverse and representative subset of Russian language gapping sufficient for effective utilization of machine learning techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3707/>Data Set for Stance and Sentiment Analysis from User Comments on Croatian News<span class=acl-fixed-case>C</span>roatian News</a></strong><br><a href=/people/m/mihaela-bosnjak/>Mihaela Bošnjak</a>
|
<a href=/people/m/mladen-karan/>Mladen Karan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3707><div class="card-body p-3 small">Nowadays it is becoming more important than ever to find new ways of extracting useful information from the evergrowing amount of user-generated data available online. In this paper, we describe the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> that contains <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> and corresponding comments from Croatian news outlet 24 sata. Our annotation scheme is specifically tailored for the task of detecting stances and sentiment from user comments as well as assessing if commentator claims are verifiable. Through this <a href=https://en.wikipedia.org/wiki/Data>data</a>, we hope to get a better understanding of the publics viewpoint on various events. In addition, we also explore the potential of applying supervised machine learning models toautomate annotation of more data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3708/>A Dataset for Noun Compositionality Detection for a <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Language</a><span class=acl-fixed-case>S</span>lavic Language</a></strong><br><a href=/people/d/dmitry-puzyrev/>Dmitry Puzyrev</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3708><div class="card-body p-3 small">This paper presents the first gold-standard resource for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> annotated with compositionality information of noun compounds. The compound phrases are collected from the Universal Dependency treebanks according to part of speech patterns, such as ADJ+NOUN or NOUN+NOUN, using the gold-standard annotations. Each <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound phrase</a> is annotated by two experts and a moderator according to the following schema : the phrase can be either compositional, non-compositional, or ambiguous (i.e., depending on the context it can be interpreted both as compositional or non-compositional). We conduct an experimental evaluation of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for predicting compositionality of noun compounds in unsupervised and supervised setups. We show that <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> from previous work evaluated on the proposed Russian-language resource achieve the performance comparable with results on <a href=https://en.wikipedia.org/wiki/English_language>English corpora</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3709/>The Second Cross-Lingual Challenge on Recognition, <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>Normalization</a>, <a href=https://en.wikipedia.org/wiki/Language_classification>Classification</a>, and Linking of Named Entities across <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Languages</a><span class=acl-fixed-case>S</span>lavic Languages</a></strong><br><a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/l/laska-laskova/>Laska Laskova</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3709><div class="card-body p-3 small">We describe the Second Multilingual Named Entity Challenge in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a>. The task is recognizing mentions of named entities in <a href=https://en.wikipedia.org/wiki/Web_page>Web documents</a>, their normalization, and cross-lingual linking. The Challenge was organized as part of the 7th Balto-Slavic Natural Language Processing Workshop, co-located with the ACL-2019 conference. Eight teams participated in the <a href=https://en.wikipedia.org/wiki/Competition>competition</a>, which covered four languages and five entity types. Performance for the named entity recognition task reached 90 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>, much higher than reported in the first edition of the Challenge. Seven teams covered all four languages, and five teams participated in the cross-lingual entity linking task. Detailed evaluation information is available on the shared task web page.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3711/>TLR at BSNLP2019 : A Multilingual Named Entity Recognition System<span class=acl-fixed-case>TLR</span> at <span class=acl-fixed-case>BSNLP</span>2019: A Multilingual Named Entity Recognition System</a></strong><br><a href=/people/j/jose-g-moreno/>Jose G. Moreno</a>
|
<a href=/people/e/elvys-linhares-pontes/>Elvys Linhares Pontes</a>
|
<a href=/people/m/mickael-coustaty/>Mickael Coustaty</a>
|
<a href=/people/a/antoine-doucet/>Antoine Doucet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3711><div class="card-body p-3 small">This paper presents our participation at the shared task on multilingual named entity recognition at BSNLP2019. Our strategy is based on a standard neural architecture for <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. In particular, we use a <a href=https://en.wikipedia.org/wiki/Mixed_model>mixed model</a> which combines multilingualcontextual and language-specific embeddings. Our only submitted run is based on a voting schema using multiple models, one for each of the four languages of the task (Bulgarian, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, and Russian) and another for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Results for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> are encouraging for all languages, varying from 60 % to 83 % in terms of Strict and Relaxed metrics, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3714/>JRC TMA-CC : Slavic Named Entity Recognition and Linking. Participation in the BSNLP-2019 shared task<span class=acl-fixed-case>JRC</span> <span class=acl-fixed-case>TMA</span>-<span class=acl-fixed-case>CC</span>: <span class=acl-fixed-case>S</span>lavic Named Entity Recognition and Linking. Participation in the <span class=acl-fixed-case>BSNLP</span>-2019 shared task</a></strong><br><a href=/people/g/guillaume-jacquet/>Guillaume Jacquet</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/h/hristo-tanev/>Hristo Tanev</a>
|
<a href=/people/r/ralf-steinberger/>Ralf Steinberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3714><div class="card-body p-3 small">We report on the participation of the JRC Text Mining and Analysis Competence Centre (TMA-CC) in the BSNLP-2019 Shared Task, which focuses on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Lemmatisation>lemmatisation</a> and cross-lingual linking. We propose a <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a> combining a <a href=https://en.wikipedia.org/wiki/Rule-based_system>rule-based approach</a> and light ML techniques. We use multilingual lexical resources such as JRC-NAMES and <a href=https://en.wikipedia.org/wiki/Babelnet>BABELNET</a> together with a named entity guesser to recognise names. In a second step, we combine known names with wild cards to increase <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recognition recall</a> by also capturing <a href=https://en.wikipedia.org/wiki/Inflection>inflection variants</a>. In a third step, we increase precision by filtering these name candidates with automatically learnt inflection patterns derived from name occurrences in large news article collections. Our major requirement is to achieve high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. We achieved an average of 65 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> with 93 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> on the four languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3716" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3716/>Improving Sentiment Classification in Slovak Language<span class=acl-fixed-case>S</span>lovak Language</a></strong><br><a href=/people/s/samuel-pecar/>Samuel Pecar</a>
|
<a href=/people/m/marian-simko/>Marian Simko</a>
|
<a href=/people/m/maria-bielikova/>Maria Bielikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3716><div class="card-body p-3 small">Using different neural network architectures is widely spread for many different NLP tasks. Unfortunately, most of the research is performed and evaluated only in <a href=https://en.wikipedia.org/wiki/English_language>English language</a> and minor languages are often omitted. We believe using similar <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> for other languages can show interesting results. In this paper, we present our study on <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for improving sentiment classification in <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak language</a>. We performed several experiments for two different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, one containing <a href=https://en.wikipedia.org/wiki/Customer_review>customer reviews</a>, the other one general <a href=https://en.wikipedia.org/wiki/Twitter>Twitter posts</a>. We show comparison of performance of different neural network architectures and also different word representations. We show that another improvement can be achieved by using a <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>model ensemble</a>. We performed experiments utilizing different methods of <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>model ensemble</a>. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieved better results than previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Our experiments showed also other potential research areas.</div></div></div><hr><div id=w19-38><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-38.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-38/>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3800/>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/w/will-radford/>Will Radford</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3803/>Relating Word Embedding Gender Biases to Gender Gaps : A Cross-Cultural Analysis</a></strong><br><a href=/people/s/scott-friedman/>Scott Friedman</a>
|
<a href=/people/s/sonja-schmer-galunder/>Sonja Schmer-Galunder</a>
|
<a href=/people/a/anthony-chen/>Anthony Chen</a>
|
<a href=/people/j/jeffrey-rye/>Jeffrey Rye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3803><div class="card-body p-3 small">Modern <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for common NLP tasks often employ <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a> and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them ; however, these <a href=https://en.wikipedia.org/wiki/Bias>biases</a> may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through <a href=https://en.wikipedia.org/wiki/Big_data>big data</a>. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in <a href=https://en.wikipedia.org/wiki/Education>education</a>, <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, <a href=https://en.wikipedia.org/wiki/Economics>economics</a>, and <a href=https://en.wikipedia.org/wiki/Health>health</a>. We validate these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3804" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3804/>Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories</a></strong><br><a href=/people/k/kaytlin-chaloner/>Kaytlin Chaloner</a>
|
<a href=/people/a/alfredo-maldonado/>Alfredo Maldonado</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3804><div class="card-body p-3 small">Prior work has shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> capture <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>human stereotypes</a>, including <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a>. However, there is a lack of studies testing the presence of specific gender bias categories in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains : <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking</a>, <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical</a> and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> than others, and that the categories of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> present also vary for each set of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We detect some <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT&#8217;s hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3805/>Evaluating the Underlying <a href=https://en.wikipedia.org/wiki/Gender_bias>Gender Bias</a> in Contextualized Word Embeddings</a></strong><br><a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/n/noe-casas/>Noe Casas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3805><div class="card-body p-3 small">Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a>. Our analysis includes different measures previously applied in the literature to standard <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3806/>Conceptor Debiasing of Word Representations Evaluated on WEAT<span class=acl-fixed-case>WEAT</span></a></strong><br><a href=/people/s/saket-karve/>Saket Karve</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3806><div class="card-body p-3 small">Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for <a href=https://en.wikipedia.org/wiki/Post-processing>post-processing</a> both traditional and contextualized word embeddings. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can simultaneously remove racial and gender biases from <a href=https://en.wikipedia.org/wiki/Word_formation>word representations</a>. Unlike standard <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing methods</a>, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3807/>Filling Gender & Number Gaps in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Black-box Context Injection</a></strong><br><a href=/people/a/amit-moryossef/>Amit Moryossef</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3807><div class="card-body p-3 small">When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must guess this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>translation accuracy</a> in up to 2.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3811/>BERT Masked Language Modeling for Co-reference Resolution<span class=acl-fixed-case>BERT</span> Masked Language Modeling for Co-reference Resolution</a></strong><br><a href=/people/f/felipe-alfaro/>Felipe Alfaro</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3811><div class="card-body p-3 small">This paper explains the TALP-UPC participation for the Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. We have implemented two models for mask language modeling using pre-trained BERT adjusted to work for a classification problem. The proposed solutions are based on the word probabilities of the original BERT model, but using common English names to replace the original test names.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3814" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3814/>Look Again at the Syntax : Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution</a></strong><br><a href=/people/y/yinchuan-xu/>Yinchuan Xu</a>
|
<a href=/people/j/junlin-yang/>Junlin Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3814><div class="card-body p-3 small">Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9 % F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN&#8217;s embeddings outperform the original BERT embeddings on the coreference task. Our <a href=https://en.wikipedia.org/wiki/Work_(thermodynamics)>work</a> significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9 % to 80.3 %. We participated in the Gender Bias for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> 2019 shared task, and our codes are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3818 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3818" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3818/>Anonymized BERT : An Augmentation Approach to the Gendered Pronoun Resolution Challenge<span class=acl-fixed-case>BERT</span>: An Augmentation Approach to the Gendered Pronoun Resolution Challenge</a></strong><br><a href=/people/b/bo-liu/>Bo Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3818><div class="card-body p-3 small">We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data size</a>, this approach diversifies idiosyncratic information embedded in <a href=https://en.wikipedia.org/wiki/Name>names</a>. Using same set of common first names can also help the model recognize <a href=https://en.wikipedia.org/wiki/Name>names</a> better, shorten token length, and remove gender and regional biases associated with <a href=https://en.wikipedia.org/wiki/Name>names</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the <a href=https://en.wikipedia.org/wiki/System>system</a> scores 0.1799 which would be third place.</div></div></div><hr><div id=w19-39><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-39.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-39/>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3900/>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a></strong><br><a href=/people/j/jason-eisner/>Jason Eisner</a>
|
<a href=/people/m/matthias-galle/>Matthias Gallé</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a>
|
<a href=/people/a/ariadna-quattoni/>Ariadna Quattoni</a>
|
<a href=/people/g/guillaume-rabusseau/>Guillaume Rabusseau</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3903/>Relating RNN Layers with the Spectral WFA Ranks in Sequence Modelling<span class=acl-fixed-case>RNN</span> Layers with the Spectral <span class=acl-fixed-case>WFA</span> Ranks in Sequence Modelling</a></strong><br><a href=/people/f/farhana-ferdousi-liza/>Farhana Ferdousi Liza</a>
|
<a href=/people/m/marek-grzes/>Marek Grzes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3903><div class="card-body p-3 small">We analyse Recurrent Neural Networks (RNNs) to understand the significance of multiple LSTM layers. We argue that the Weighted Finite-state Automata (WFA) trained using a spectral learning algorithm are helpful to analyse RNNs. Our results suggest that multiple LSTM layers in RNNs help learning distributed hidden states, but have a smaller impact on the ability to learn long-term dependencies. The analysis is based on the empirical results, however relevant theory (whenever possible) was discussed to justify and support our conclusions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3904/>Multi-Element Long Distance Dependencies : Using SPk Languages to Explore the Characteristics of Long-Distance Dependencies<span class=acl-fixed-case>SP</span>k Languages to Explore the Characteristics of Long-Distance Dependencies</a></strong><br><a href=/people/a/abhijit-mahalunkar/>Abhijit Mahalunkar</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3904><div class="card-body p-3 small">In order to successfully model Long Distance Dependencies (LDDs) it is necessary to under-stand the full-range of the characteristics of the LDDs exhibited in a target dataset. In this paper, we use Strictly k-Piecewise languages to generate <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with various properties. We then compute the characteristics of the LDDs in these datasets using <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> and analyze the impact of factors such as (i) k, (ii) length of LDDs, (iii) vocabulary size, (iv) forbidden strings, and (v) dataset size. This analysis reveal that the number of interacting elements in a dependency is an important characteristic of LDDs. This leads us to the challenge of modelling multi-element long-distance dependencies. Our results suggest that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> may aide in modeling datasets with multi-element long-distance dependencies. However, we conclude that there is a need to develop more efficient attention mechanisms to address this issue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3905/>LSTM Networks Can Perform Dynamic Counting<span class=acl-fixed-case>LSTM</span> Networks Can Perform Dynamic Counting</a></strong><br><a href=/people/m/mirac-suzgun/>Mirac Suzgun</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/s/stuart-m-shieber/>Stuart Shieber</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3905><div class="card-body p-3 small">In this paper, we systematically assess the ability of standard <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent networks</a> to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for recognition.</div></div></div><hr><div id=w19-40><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-40.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-40/>Proceedings of the 13th Linguistic Annotation Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4000/>Proceedings of the 13th Linguistic Annotation Workshop</a></strong><br><a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/d/deniz-zeyrek/>Deniz Zeyrek</a>
|
<a href=/people/j/jet-hoek/>Jet Hoek</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4002/>WiRe57 : A Fine-Grained Benchmark for Open Information Extraction<span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>R</span>e57 : A Fine-Grained Benchmark for Open Information Extraction</a></strong><br><a href=/people/w/william-lechelle/>William Lechelle</a>
|
<a href=/people/f/fabrizio-gotti/>Fabrizio Gotti</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4002><div class="card-body p-3 small">We build a <a href=https://en.wikipedia.org/wiki/Reference_work>reference</a> for the task of <a href=https://en.wikipedia.org/wiki/Open_information_extraction>Open Information Extraction</a>, on five documents. We tentatively resolve a number of issues that arise, including <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, and we take steps toward addressing <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, a significant problem. We seek to better pinpoint the requirements for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We produce our annotation guidelines specifying what is correct to extract and what is not. In turn, we use this <a href=https://en.wikipedia.org/wiki/Reference_(computer_science)>reference</a> to score existing Open IE systems. We address the non-trivial problem of evaluating the extractions produced by <a href=https://en.wikipedia.org/wiki/System>systems</a> against the reference tuples, and share our evaluation script. Among seven compared extractors, we find the MinIE system to perform best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4003/>Crowdsourcing Discourse Relation Annotations by a Two-Step Connective Insertion Task</a></strong><br><a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/m/merel-scholman/>Merel Scholman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4003><div class="card-body p-3 small">The perspective of being able to crowd-source coherence relations bears the promise of acquiring <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for new texts quickly, which could then increase the size and variety of discourse-annotated corpora. It would also open the avenue to answering new research questions : Collecting annotations from a larger number of individuals per instance would allow to investigate the distribution of inferred relations, and to study individual differences in coherence relation interpretation. However, annotating <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence relations</a> with untrained workers is not trivial. We here propose a novel two-step annotation procedure, which extends an earlier method by Scholman and Demberg (2017a). In our approach, coherence relation labels are inferred from <a href=https://en.wikipedia.org/wiki/Logical_connective>connectives</a> that workers insert into the text. We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from PDTB and RSTDT on the same texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4004/>Annotating and analyzing the interactions between meaning relations</a></strong><br><a href=/people/d/darina-gold/>Darina Gold</a>
|
<a href=/people/v/venelin-kovatchev/>Venelin Kovatchev</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4004><div class="card-body p-3 small">Pairs of sentences, phrases, or other text pieces can hold semantic relations such as <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a>, textual entailment, <a href=https://en.wikipedia.org/wiki/Contradiction>contradiction</a>, specificity, and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. These <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> are usually studied in isolation and no dataset exists where they can be compared empirically. Here we present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> annotated with these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> and the analysis of these results. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains 520 sentence pairs, annotated with these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. We measure the annotation reliability of each individual relation and we examine their interactions and correlations. Among the unexpected results revealed by our analysis is that the traditionally considered direct relationship between <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> and bi-directional entailment does not hold in our <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4008/>Tagging modality in Oceanic languages of Melanesia</a></strong><br><a href=/people/a/annika-tjuka/>Annika Tjuka</a>
|
<a href=/people/l/lena-weissmann/>Lena Weißmann</a>
|
<a href=/people/k/kilu-von-prince/>Kilu von Prince</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4008><div class="card-body p-3 small">Primary data from small, low-resource languages of Oceania have only recently become available through <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a>. In our study, we explore corpus data of five Oceanic languages of Melanesia which are known to be mood-prominent (in the sense of Bhat, 1999). In order to find out more about <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_modality>modality</a>, and <a href=https://en.wikipedia.org/wiki/Grammatical_polarity>polarity</a>, we tagged these categories in a subset of our corpora. For the category of modality, we developed a novel tag set (MelaTAMP, 2017), which categorizes clauses into factual, possible, and counterfactual. Based on an analysis of the inter-annotator consistency, we argue that our tag set for the modal domain is efficient for our subject languages and might be useful for other languages and purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4009 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4009/>Harmonizing Different Lemmatization Strategies for Building a Knowledge Base of Linguistic Resources for Latin<span class=acl-fixed-case>L</span>atin</a></strong><br><a href=/people/f/francesco-mambrini/>Francesco Mambrini</a>
|
<a href=/people/m/marco-passarotti/>Marco Passarotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4009><div class="card-body p-3 small">The interoperability between lemmatized corpora of Latin and other resources that use the lemma as indexing key is hampered by the multiple lemmatization strategies that different projects adopt. In this paper we discuss how we tackle the challenges raised by harmonizing different lemmatization criteria in the context of a project that aims to connect linguistic resources for <a href=https://en.wikipedia.org/wiki/Latin>Latin</a> using the Linked Data paradigm. The paper introduces the architecture supporting an open-ended, lemma-based Knowledge Base, built to make textual and lexical resources for Latin interoperable. Particularly, the paper describes the inclusion into the <a href=https://en.wikipedia.org/wiki/Knowledge_Base>Knowledge Base</a> of its lexical basis, of a word formation lexicon and of a lemmatized and syntactically annotated corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4012/>An Online Annotation Assistant for Argument Schemes</a></strong><br><a href=/people/j/john-lawrence/>John Lawrence</a>
|
<a href=/people/j/jacky-visser/>Jacky Visser</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4012><div class="card-body p-3 small">Understanding the inferential principles underpinning an <a href=https://en.wikipedia.org/wiki/Argument>argument</a> is essential to the proper interpretation and evaluation of persuasive discourse. Argument schemes capture the conventional patterns of reasoning appealed to in <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>. The empirical study of these patterns relies on the availability of data about the actual use of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> in communicative practice. Annotated corpora of argument schemes, however, are scarce, small, and unrepresentative. Aiming to address this issue, we present one step in the development of improved datasets by integrating the Argument Scheme Key a novel annotation method based on one of the most popular typologies of argument schemes into the widely used OVA software for argument analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4016 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4016/>Explaining Simple Natural Language Inference</a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/a/annebeth-buis/>Annebeth Buis</a>
|
<a href=/people/l/livy-real/>Livy Real</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4016><div class="card-body p-3 small">The vast amount of research introducing new <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> and techniques for semi-automatically annotating corpora shows the important role that <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> play in today&#8217;s research, especially in the <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning community</a>. This rapid development raises concerns about the quality of the datasets created and consequently of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained, as recently discussed with respect to the Natural Language Inference (NLI) task. In this work we conduct an annotation experiment based on a small subset of the SICK corpus. The experiment reveals several problems in the annotation guidelines, and various challenges of the NLI task itself. Our quantitative evaluation of the experiment allows us to assign our empirical observations to specific linguistic phenomena and leads us to recommendations for future annotation tasks, for NLI and possibly for other tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4017/>On the role of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> in persuasive texts</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4017><div class="card-body p-3 small">This paper investigates the use of explicitly signalled discourse relations in persuasive texts. We present a corpus study where we control for speaker and topic and show that the distribution of different discourse connectives varies considerably across different discourse settings. While this variation can be explained by genre differences, we also observe variation regarding the distribution of discourse relations across different settings. This variation, however, can not be easily explained by <a href=https://en.wikipedia.org/wiki/Genre>genre differences</a>. We argue that the differences regarding the use of discourse relations reflects different strategies of persuasion and that these might be due to <a href=https://en.wikipedia.org/wiki/Audience_design>audience design</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4018/>One format to rule them all The emtsv pipeline for Hungarian<span class=acl-fixed-case>H</span>ungarian</a></strong><br><a href=/people/b/balazs-indig/>Balázs Indig</a>
|
<a href=/people/b/balint-sass/>Bálint Sass</a>
|
<a href=/people/e/eszter-simon/>Eszter Simon</a>
|
<a href=/people/i/ivan-mittelholcz/>Iván Mittelholcz</a>
|
<a href=/people/n/noemi-vadasz/>Noémi Vadász</a>
|
<a href=/people/m/marton-makrai/>Márton Makrai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4018><div class="card-body p-3 small">We present a more efficient version of the e-magyar NLP pipeline for <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a> called emtsv. It integrates Hungarian NLP tools in a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> whose individual modules can be developed or replaced independently and allows new ones to be added. The <a href=https://en.wikipedia.org/wiki/Design>design</a> also allows convenient investigation and manual correction of the data flow from one module to another. The improvements we publish include effective communication between the modules and support of the use of individual <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> both in the chain and standing alone. Our goals are accomplished using extended tsv (tab separated values) files, a simple, uniform, generic and self-documenting input / output format. Our vision is maintaining the <a href=https://en.wikipedia.org/wiki/System>system</a> for a long time and making it easier for external developers to fit their own modules into the <a href=https://en.wikipedia.org/wiki/System>system</a>, thus sharing existing competencies in the field of processing <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, a mid-resourced language. The source code is available under LGPL 3.0 license at https://github.com/dlt-rilmta/emtsv.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4019/>Turkish Treebanking : Unifying and Constructing Efforts<span class=acl-fixed-case>T</span>urkish Treebanking: Unifying and Constructing Efforts</a></strong><br><a href=/people/u/utku-turk/>Utku Türk</a>
|
<a href=/people/f/furkan-atmaca/>Furkan Atmaca</a>
|
<a href=/people/s/saziye-betul-ozates/>Şaziye Betül Özateş</a>
|
<a href=/people/a/abdullatif-koksal/>Abdullatif Köksal</a>
|
<a href=/people/b/balkiz-ozturk-basaran/>Balkiz Ozturk Basaran</a>
|
<a href=/people/t/tunga-gungor/>Tunga Gungor</a>
|
<a href=/people/a/arzucan-ozgur/>Arzucan Özgür</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4019><div class="card-body p-3 small">In this paper, we present the current version of two different treebanks, the re-annotation of the Turkish PUD Treebank and the first annotation of the Turkish National Corpus Universal Dependency (henceforth TNC-UD). The annotation of both treebanks, the Turkish PUD Treebank and TNC-UD, was carried out based on the decisions concerning linguistic adequacy of re-annotation of the Turkish IMST-UD Treebank (Trk et. al., forthcoming). Both of the <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> were annotated with the same <a href=https://en.wikipedia.org/wiki/Annotation>annotation process</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological and syntactic analyses</a>. The TNC-UD is planned to have 10,000 sentences. In this paper, we will present the first 500 sentences along with the annotation PUD Treebank. Moreover, this paper also offers the parsing results of a graph-based neural parser on the previous and re-annotated PUD, as well as the TNC-UD. In light of the comparisons, even though we observe a slight decrease in the attachment scores of the <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish PUD treebank</a>, we demonstrate that the annotation of the TNC-UD improves the parsing accuracy of <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. In addition to the <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>, we have also constructed a custom <a href=https://en.wikipedia.org/wiki/Annotation>annotation software</a> with advanced filtering and morphological editing options. Both the treebanks, including a full edit-history and the annotation guidelines, and the custom software are publicly available under an open license online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4020/>A Dataset for Semantic Role Labelling of Hindi-English Code-Mixed Tweets<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Mixed Tweets</a></strong><br><a href=/people/r/riya-pal/>Riya Pal</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4020><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 1460 Hindi-English code-mixed tweets consisting of 20,949 tokens labelled with Proposition Bank labels marking their semantic roles. We created verb frames for complex predicates present in the corpus and formulated mappings from Paninian dependency labels to Proposition Bank labels. With the help of these mappings and the dependency tree, we propose a baseline rule based system for <a href=https://en.wikipedia.org/wiki/Semantic_Role_Labelling>Semantic Role Labelling</a> of Hindi-English code-mixed data. We obtain an accuracy of 96.74 % for Argument Identification and are able to further classify 73.93 % of the labels correctly. While there is relevant ongoing research on Semantic Role Labelling and on building tools for code-mixed social media data, this is the first attempt at labelling semantic roles in code-mixed data, to the best of our knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4021/>A Multi-Platform Annotation Ecosystem for Domain Adaptation</a></strong><br><a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/n/nancy-ide/>Nancy Ide</a>
|
<a href=/people/j/jin-dong-kim/>Jin-Dong Kim</a>
|
<a href=/people/j/jan-christoph-klie/>Jan-Christoph Klie</a>
|
<a href=/people/k/keith-suderman/>Keith Suderman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4021><div class="card-body p-3 small">This paper describes an <a href=https://en.wikipedia.org/wiki/Ecosystem>ecosystem</a> consisting of three independent text annotation platforms. To demonstrate their ability to work in concert, we illustrate how to use them to address an interactive domain adaptation task in biomedical entity recognition. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> and the approach are in general domain-independent and can be readily applied to other areas of science.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4024/>Comparative judgments are more consistent than <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> for labelling word complexity</a></strong><br><a href=/people/s/sian-gooding/>Sian Gooding</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/a/advait-sarkar/>Advait Sarkar</a>
|
<a href=/people/a/alan-blackwell/>Alan Blackwell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4024><div class="card-body p-3 small">Lexical simplification systems replace complex words with simple ones based on a model of which words are complex in context. We explore how users can help train complex word identification models through <a href=https://en.wikipedia.org/wiki/Labelling>labelling</a> more efficiently and reliably. We show that using an interface where annotators make comparative rather than binary judgments leads to more reliable and consistent labels, and explore whether comparative judgments may provide a faster way for collecting labels.</div></div></div><hr><div id=w19-41><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-41.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-41/>Proceedings of the First Workshop on NLP for Conversational AI</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4100/>Proceedings of the First Workshop on NLP for Conversational AI</a></strong><br><a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/t/tania-bedrax-weiss/>Tania Bedrax-Weiss</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/t/thang-minh-luong/>Thang-Minh Luong</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4101/>A Repository of Conversational Datasets</a></strong><br><a href=/people/m/matthew-henderson/>Matthew Henderson</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/g/girish-kumar/>Girish Kumar</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4101><div class="card-body p-3 small">Progress in <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The <a href=https://en.wikipedia.org/wiki/Disciplinary_repository>repository</a> contains scripts that allow researchers to reproduce the standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4104/>Building a <a href=https://en.wikipedia.org/wiki/Production_model>Production Model</a> for Retrieval-Based Chatbots</a></strong><br><a href=/people/k/kyle-swanson/>Kyle Swanson</a>
|
<a href=/people/l/lili-yu/>Lili Yu</a>
|
<a href=/people/c/christopher-fox/>Christopher Fox</a>
|
<a href=/people/j/jeremy-wohlwend/>Jeremy Wohlwend</a>
|
<a href=/people/t/tao-lei/>Tao Lei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4104><div class="card-body p-3 small">Response suggestion is an important task for building <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer conversation systems</a>. Recent approaches to conversation modeling have introduced new model architectures with impressive results, but relatively little attention has been paid to whether these models would be practical in a production setting. In this paper, we describe the unique challenges of building a production retrieval-based conversation system, which selects outputs from a whitelist of candidate responses. To address these challenges, we propose a dual encoder architecture which performs rapid inference and scales well with the size of the whitelist. We also introduce and compare two methods for generating <a href=https://en.wikipedia.org/wiki/Whitelisting>whitelists</a>, and we carry out a comprehensive analysis of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and <a href=https://en.wikipedia.org/wiki/Whitelisting>whitelists</a>. Experimental results on a large, proprietary help desk chat dataset, including both offline metrics and a human evaluation, indicate production-quality performance and illustrate key lessons about conversation modeling in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4107/>DSTC7 Task 1 : Noetic End-to-End Response Selection<span class=acl-fixed-case>DSTC</span>7 Task 1: Noetic End-to-End Response Selection</a></strong><br><a href=/people/c/chulaka-gunasekara/>Chulaka Gunasekara</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4107><div class="card-body p-3 small">Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> provided two new resources that presented different challenges : one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem : (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4108/>End-to-End Neural Context Reconstruction in Chinese Dialogue<span class=acl-fixed-case>C</span>hinese Dialogue</a></strong><br><a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/r/rui-qiao/>Rui Qiao</a>
|
<a href=/people/h/haocheng-qin/>Haocheng Qin</a>
|
<a href=/people/a/amy-sun/>Amy Sun</a>
|
<a href=/people/l/luchen-tan/>Luchen Tan</a>
|
<a href=/people/k/kun-xiong/>Kun Xiong</a>
|
<a href=/people/m/ming-li/>Ming Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4108><div class="card-body p-3 small">We tackle the problem of context reconstruction in Chinese dialogue, where the task is to replace <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>, zero pronouns, and other referring expressions with their referent nouns so that sentences can be processed in isolation without context. Following a standard decomposition of the context reconstruction task into referring expression detection and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, we propose a novel end-to-end architecture for separately and jointly accomplishing this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Key features of this model include POS and position encoding using CNNs and a novel pronoun masking mechanism. One perennial problem in building such models is the paucity of training data, which we address by augmenting previously-proposed methods to generate a large amount of realistic training data. The combination of more data and better <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yields <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> higher than the state-of-the-art method in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and end-to-end context reconstruction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4115/>Relevant and Informative Response Generation using <a href=https://en.wikipedia.org/wiki/Pointwise_mutual_information>Pointwise Mutual Information</a></a></strong><br><a href=/people/j/junya-takayama/>Junya Takayama</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4115><div class="card-body p-3 small">A sequence-to-sequence model tends to generate generic responses with little information for input utterances. To solve this problem, we propose a neural model that generates relevant and informative responses. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has simple architecture to enable easy application to existing neural dialogue models. Specifically, using positive pointwise mutual information, it first identifies <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> that frequently co-occur in responses given an utterance. Then, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> encourages the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> to use the <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> for response generation. Experiment results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> successfully diversifies responses relative to previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4116/>Responsive and Self-Expressive Dialogue Generation</a></strong><br><a href=/people/k/kozo-chikai/>Kozo Chikai</a>
|
<a href=/people/j/junya-takayama/>Junya Takayama</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4116><div class="card-body p-3 small">A neural conversation model is a promising approach to develop dialogue systems with the ability of <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a>. It allows training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in an end-to-end manner without complex rule design nor <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. However, as a side effect, the neural model tends to generate safe but uninformative and insensitive responses like OK and I do n&#8217;t know. Such replies are called generic responses and regarded as a critical problem for user-engagement of dialogue systems. For a more engaging chit-chat experience, we propose a neural conversation model that generates responsive and self-expressive replies. Specifically, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates domain-aware and sentiment-rich responses. Experiments empirically confirmed that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> outperformed the sequence-to-sequence model ; 68.1 % of our responses were domain-aware with <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment polarities</a>, which was only 2.7 % for responses generated by the sequence-to-sequence model.</div></div></div><hr><div id=w19-42><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-42.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-42/>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4200/>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4201/>AX Semantics’ Submission to the SIGMORPHON 2019 Shared Task<span class=acl-fixed-case>AX</span> Semantics’ Submission to the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task</a></strong><br><a href=/people/a/andreas-madsack/>Andreas Madsack</a>
|
<a href=/people/r/robert-weissgraeber/>Robert Weißgraeber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4201><div class="card-body p-3 small">This paper describes the AX Semantics&#8217; submission to the SIGMORPHON 2019 shared task on morphological reinflection. We implemented two systems, both tackling the task for all languages in one codebase, without any underlying language specific features. The first one is an encoder-decoder model using AllenNLP ; the second system uses the same <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> modified by a custom trainer that trains only with the target language resources after a specific threshold. We especially focused on building an <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> using AllenNLP with out-of-the-box methods to facilitate easy operation and reuse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4207/>ITIST at the SIGMORPHON 2019 Shared Task : Sparse Two-headed Models for Inflection<span class=acl-fixed-case>IT</span>–<span class=acl-fixed-case>IST</span> at the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task: Sparse Two-headed Models for Inflection</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4207><div class="card-body p-3 small">This paper presents the Instituto de TelecomunicaesInstituto Superior Tcnico submission to Task 1 of the SIGMORPHON 2019 Shared Task. Our models combine sparse sequence-to-sequence models with a two-headed attention mechanism that learns separate attention distributions for the <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemma</a> and inflectional tags. Among submissions to Task 1, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> rank second and third. Despite the low data setting of the task (only 100 in-language training examples), they learn plausible inflection patterns and often concentrate all probability mass into a small set of hypotheses, making <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> exact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4208/>CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology<span class=acl-fixed-case>CMU</span>-01 at the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task on Crosslinguality and Context in Morphology</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4208><div class="card-body p-3 small">This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatization</a> in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, <a href=https://en.wikipedia.org/wiki/Case_(disambiguation)>Case</a>, etc.) independently. However, most <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> are under-resourced, thus making it challenging to train <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a> for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4210/>THOMAS : The Hegemonic OSU Morphological Analyzer using Seq2seq<span class=acl-fixed-case>THOMAS</span>: The Hegemonic <span class=acl-fixed-case>OSU</span> Morphological Analyzer using Seq2seq</a></strong><br><a href=/people/b/byung-doh-oh/>Byung-Doh Oh</a>
|
<a href=/people/p/pranav-maneriker/>Pranav Maneriker</a>
|
<a href=/people/n/nanjiang-jiang/>Nanjiang Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4210><div class="card-body p-3 small">This paper describes the OSU submission to the SIGMORPHON 2019 shared task, Crosslinguality and Context in <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphology</a>. Our system addresses the contextual morphological analysis subtask of Task 2, which is to produce the morphosyntactic description (MSD) of each fully inflected word within a given sentence. We frame this as a sequence generation task and employ a neural encoder-decoder (seq2seq) architecture to generate the sequence of MSD tags given the encoded representation of each token. Follow-up analyses reveal that our system most significantly improves performance on morphologically complex languages whose inflected word forms typically have longer MSD tag sequences. In addition, our system seems to capture the structured correlation between MSD tags, such as that between the verb tag and TAM-related tags.<i>contextual morphological analysis</i> subtask of Task 2, which is to produce the morphosyntactic description (MSD) of each fully inflected word within a given sentence. We frame this as a sequence generation task and employ a neural encoder-decoder (seq2seq) architecture to generate the sequence of MSD tags given the encoded representation of each token. Follow-up analyses reveal that our system most significantly improves performance on morphologically complex languages whose inflected word forms typically have longer MSD tag sequences. In addition, our system seems to capture the structured correlation between MSD tags, such as that between the &#8220;verb&#8221; tag and TAM-related tags.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4211/>Sigmorphon 2019 Task 2 system description paper : <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphological analysis</a> in context for many languages, with supervision from only a few</a></strong><br><a href=/people/b/brad-aiken/>Brad Aiken</a>
|
<a href=/people/j/jared-kelly/>Jared Kelly</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/s/suleyman-olcay-polat/>Suleyman Olcay Polat</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/r/rodney-nielsen/>Rodney Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4211><div class="card-body p-3 small">This paper presents the UNT HiLT+Ling system for the Sigmorphon 2019 shared Task 2 : <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphological Analysis</a> and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatization</a> in Context. Our core approach focuses on the morphological tagging task ; <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> are treated as secondary tasks. Given the highly multilingual nature of the task, we propose an approach which makes minimal use of the supplied training data, in order to be extensible to languages without labeled training data for the morphological inflection task. Specifically, we use a parallel Bible corpus to align contextual embeddings at the <a href=https://en.wikipedia.org/wiki/Chapters_and_verses_of_the_Bible>verse level</a>. The aligned verses are used to build cross-language translation matrices, which in turn are used to map between embedding spaces for the various languages. Finally, we use sets of inflected forms, primarily from a high-resource language, to induce vector representations for individual UniMorph tags. Morphological analysis is performed by matching <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a> to <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for individual tokens. While our <a href=https://en.wikipedia.org/wiki/System>system</a> results are dramatically below the average system submitted for the shared task evaluation campaign, our method is (we suspect) unique in its minimal reliance on labeled training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4212/>UDPipe at SIGMORPHON 2019 : Contextualized Embeddings, Regularization with Morphological Categories, Corpora Merging<span class=acl-fixed-case>UDP</span>ipe at <span class=acl-fixed-case>SIGMORPHON</span> 2019: Contextualized Embeddings, Regularization with Morphological Categories, Corpora Merging</a></strong><br><a href=/people/m/milan-straka/>Milan Straka</a>
|
<a href=/people/j/jana-strakova/>Jana Straková</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4212><div class="card-body p-3 small">We present our contribution to the SIGMORPHON 2019 Shared Task : Crosslinguality and Context in Morphology, Task 2 : contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pretrained contextualized embeddings (BERT) as additional inputs to the network ; secondly, we use individual morphological features as regularization ; and finally, we merge the selected corpora of the same language. In the lemmatization task, our <a href=https://en.wikipedia.org/wiki/System>system</a> exceeds all the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> by a wide margin with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>lemmatization accuracy</a> 95.78 (second best was 95.00, third 94.46). In the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> placed tightly second : our <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis accuracy</a> was 93.19, the winning <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s 93.23.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4214/>A Little Linguistics Goes a Long Way : Unsupervised Segmentation with Limited Language Specific Guidance</a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/m/mai-oudah/>Mai Oudah</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4214><div class="card-body p-3 small">We present de-lexical segmentation, a linguistically motivated alternative to greedy or other unsupervised methods, requiring only minimal language specific input. Our technique involves creating a small grammar of closed-class affixes which can be written in a few hours. The <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> over generates analyses for word forms attested in a raw corpus which are disambiguated based on features of the linguistic base proposed for each form. Extending the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> to cover orthographic, morpho-syntactic or lexical variation is simple, making it an ideal solution for challenging corpora with noisy, dialect-inconsistent, or otherwise non-standard content. In two evaluations, we consistently outperform competitive unsupervised baselines and approach the performance of state-of-the-art <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> trained on large amounts of data, providing evidence for the value of linguistic input during preprocessing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4215/>Equiprobable mappings in weighted constraint grammars</a></strong><br><a href=/people/a/arto-anttila/>Arto Anttila</a>
|
<a href=/people/s/scott-borgeson/>Scott Borgeson</a>
|
<a href=/people/g/giorgio-magri/>Giorgio Magri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4215><div class="card-body p-3 small">We show that <a href=https://en.wikipedia.org/wiki/MaxEnt>MaxEnt</a> is so rich that it can distinguish between any two different <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> : there always exists a nonnegative weight vector which assigns them different <a href=https://en.wikipedia.org/wiki/MaxEnt>MaxEnt probabilities</a>. Stochastic HG instead does admit equiprobable mappings and we give a complete formal characterization of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4216/>Unbounded Stress in Subregular Phonology</a></strong><br><a href=/people/y/yiding-hao/>Yiding Hao</a>
|
<a href=/people/s/samuel-andersson/>Samuel Andersson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4216><div class="card-body p-3 small">This paper situates culminative unbounded stress systems within the subregular hierarchy for <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. While Baek (2018) has argued that such systems can be uniformly understood as input tier-based strictly local constraints, we show here that default-to-opposite-side and default-to-same-side stress systems belong to distinct subregular classes when they are viewed as functions that assign primary stress to underlying forms. While the former system can be captured by input tier-based input strictly local functions, a subsequential function class that we define here, the latter system is not subsequential, though it is weakly deterministic according to McCollum et al.&#8217;s (2018) non-interaction criterion. Our results motivate the extension of recently proposed subregular language classes to subregular functions and argue in favor of McCollum et al&#8217;s definition of weak determinism over that of Heinz and Lai (2013).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4218/>Convolutional neural networks for low-resource morpheme segmentation : baseline or state-of-the-art?</a></strong><br><a href=/people/a/alexey-sorokin/>Alexey Sorokin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4218><div class="card-body p-3 small">We apply <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> to the task of shallow morpheme segmentation using low-resource datasets for 5 different languages. We show that both in fully supervised and semi-supervised settings our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> beats previous state-of-the-art approaches. We argue that <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> reflect local nature of morpheme segmentation better than other semi-supervised approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4222/>Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages</a></strong><br><a href=/people/r/ramy-eskander/>Ramy Eskander</a>
|
<a href=/people/j/judith-l-klavans/>Judith Klavans</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4222><div class="card-body p-3 small">Polysynthetic languages pose a challenge for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> due to the root-morpheme complexity and to the word class squish. In addition, many of these <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic languages</a> are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the <a href=https://en.wikipedia.org/wiki/Uto-Aztecan_languages>Uto-Aztecan family</a>. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4226 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4226/>The SIGMORPHON 2019 Shared Task : Morphological Analysis in Context and Cross-Lingual Transfer for <a href=https://en.wikipedia.org/wiki/Inflection>Inflection</a><span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4226><div class="card-body p-3 small">The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years&#8217; inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and morphological feature analysis in context. All submissions featured a neural component and built on either this year&#8217;s strong baselines or highly ranked systems from previous years&#8217; shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.</div></div></div><hr><div id=w19-43><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-43.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-43/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4300/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4301/>Deep Generalized Canonical Correlation Analysis</a></strong><br><a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/b/biman-gujral/>Biman Gujral</a>
|
<a href=/people/d/dee-ann-reisinger/>Dee Ann Reisinger</a>
|
<a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/r/raman-arora/>Raman Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4301><div class="card-body p-3 small">We present Deep Generalized Canonical Correlation Analysis (DGCCA) a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> are maximally informative of each other. While methods for nonlinear two view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks : <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic transcription</a> from acoustic & articulatory measurements, recommending hashtags and recommending friends on a dataset of Twitter users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4305/>Multilingual NMT with a Language-Independent Attention Bridge<span class=acl-fixed-case>NMT</span> with a Language-Independent Attention Bridge</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4305><div class="card-body p-3 small">In this paper, we propose an architecture for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> from each language for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and develops into a language-agnostic meaning representation that can efficiently be used for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We present a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4306/>Efficient <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> with Automatic Relevance Determination in Recurrent Neural Networks</a></strong><br><a href=/people/m/maxim-kodryan/>Maxim Kodryan</a>
|
<a href=/people/a/artem-grachev/>Artem Grachev</a>
|
<a href=/people/d/dmitry-ignatov/>Dmitry Ignatov</a>
|
<a href=/people/d/dmitry-vetrov/>Dmitry Vetrov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4306><div class="card-body p-3 small">Reduction of the number of parameters is one of the most important goals in <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a>. In this article we propose an adaptation of Doubly Stochastic Variational Inference for Automatic Relevance Determination (DSVI-ARD) for neural networks compression. We find this method to be especially useful in language modeling tasks, where large number of parameters in the input and output layers is often excessive. We also show that DSVI-ARD can be applied together with encoder-decoder weight tying allowing to achieve even better sparsity and performance. Our experiments demonstrate that more than 90 % of the weights in both encoder and decoder layers can be removed with a minimal quality loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4310/>Specializing Distributional Vectors of All Words for Lexical Entailment</a></strong><br><a href=/people/a/aishwarya-kamath/>Aishwarya Kamath</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4310><div class="card-body p-3 small">Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g. WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first post-processing method that specializes vectors of all vocabulary words including those unseen in the resources for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep feed-forward neural network</a> : its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4311/>Composing Noun Phrase Vector Representations</a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a>
|
<a href=/people/r/richard-crouch/>Richard Crouch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4311><div class="card-body p-3 small">Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks. While there seems to be a consensus about the usefulness of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and how to learn them, it is still unclear which <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> can capture the meaning of phrases or even whole sentences. Recent work has shown that simple operations outperform more complex <a href=https://en.wikipedia.org/wiki/Deep_learning>deep architectures</a>. In this work, we propose two novel <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> for computing noun phrase vector representations. First, we propose that the semantic and not the syntactic contribution of each component of a <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrase</a> should be considered, so that the resulting composed vectors express more of the phrase meaning. Second, the composition process of the two phrase vectors should apply suitable dimensions&#8217; selection in a way that specific semantic features captured by the phrase&#8217;s meaning become more salient. Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets. Our results show that these <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4312 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4312" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4312/>Towards Robust Named Entity Recognition for Historic German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/s/stefan-schweter/>Stefan Schweter</a>
|
<a href=/people/j/johannes-baiter/>Johannes Baiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4312><div class="card-body p-3 small">In this paper we study the influence of using language model pre-training for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Historic German. We achieve new state-of-the-art results using carefully chosen training data for <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. For a low-resource domain like <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer-learning</a> with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4313/>On Evaluating Embedding Models for Knowledge Base Completion</a></strong><br><a href=/people/y/yanjie-wang/>Yanjie Wang</a>
|
<a href=/people/d/daniel-ruffinelli/>Daniel Ruffinelli</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/s/samuel-broscheit/>Samuel Broscheit</a>
|
<a href=/people/c/christian-meilicke/>Christian Meilicke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4313><div class="card-body p-3 small">Knowledge graph embedding models have recently received significant attention in the literature. These models learn latent semantic representations for the entities and relations in a given <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> ; the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> can be used to infer missing knowledge. In this paper, we study the question of how well recent <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> perform for the task of knowledge base completion, i.e., the task of inferring new facts from an <a href=https://en.wikipedia.org/wiki/Complete_knowledge_base>incomplete knowledge base</a>. We argue that the entity ranking protocol, which is currently used to evaluate knowledge graph embedding models, is not suitable to answer this question since only a subset of the model predictions are evaluated. We propose an alternative entity-pair ranking protocol that considers all model predictions as a whole and is thus more suitable to the task. We conducted an experimental study on standard datasets and found that the performance of popular embeddings models was unsatisfactory under the new protocol, even on datasets that are generally considered to be too easy. Moreover, we found that a simple rule-based model often provided superior performance. Our findings suggest that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4314/>Constructive Type-Logical Supertagging With Self-Attention Networks</a></strong><br><a href=/people/k/konstantinos-kogkalidis/>Konstantinos Kogkalidis</a>
|
<a href=/people/m/michael-moortgat/>Michael Moortgat</a>
|
<a href=/people/t/tejaswini-deoskar/>Tejaswini Deoskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4314><div class="card-body p-3 small">We propose a novel application of self-attention networks towards <a href=https://en.wikipedia.org/wiki/Grammar_induction>grammar induction</a>. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of the grammar&#8217;s type system along with its <a href=https://en.wikipedia.org/wiki/Denotational_semantics>denotational semantics</a>. This lifts the <a href=https://en.wikipedia.org/wiki/Closed-world_assumption>closed world assumption</a> commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4317/>An Empirical Study on Pre-trained Embeddings and <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Bot Detection</a></strong><br><a href=/people/a/andres-garcia-silva/>Andres Garcia-Silva</a>
|
<a href=/people/c/cristian-berrio/>Cristian Berrio</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>José Manuel Gómez-Pérez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4317><div class="card-body p-3 small">Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are learned from large and well-formed text corpora from e.g. encyclopedic resources, <a href=https://en.wikipedia.org/wiki/Book>books</a> or <a href=https://en.wikipedia.org/wiki/News>news</a>. However, a significant amount of the text to be analyzed nowadays is Web data, often from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper we consider the research question : How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>? To answer this question, we focus on bot detection in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4318/>Probing Multilingual Sentence Representations With X-Probe<span class=acl-fixed-case>X</span>-Probe</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4318><div class="card-body p-3 small">This paper extends the task of probing sentence representations for linguistic insight in a multilingual domain. In doing so, we make two contributions : first, we provide datasets for multilingual probing, derived from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, in five languages, viz. English, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Second, we evaluate six sentence encoders for each language, each trained by mapping sentence representations to English sentence representations, using sentences in a parallel corpus. We discover that cross-lingually mapped representations are often better at retaining certain linguistic information than <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> derived from English encoders trained on natural language inference (NLI) as a downstream task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4320/>Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4320><div class="card-body p-3 small">In this paper, we propose Multilingual Meta-Embeddings (MME), an effective method to learn multilingual representations by leveraging monolingual pre-trained embeddings. MME learns to utilize information from these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> via a self-attention mechanism without explicit language identification. We evaluate the proposed embedding method on the code-switching English-Spanish Named Entity Recognition dataset in a multilingual and cross-lingual setting. The experimental results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4321/>Investigating Sub-Word Embedding Strategies for the Morphologically Rich and Free Phrase-Order Hungarian<span class=acl-fixed-case>H</span>ungarian</a></strong><br><a href=/people/b/balint-dobrossy/>Bálint Döbrössy</a>
|
<a href=/people/m/marton-makrai/>Márton Makrai</a>
|
<a href=/people/b/balazs-tarjan/>Balázs Tarján</a>
|
<a href=/people/g/gyorgy-szaszak/>György Szaszák</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4321><div class="card-body p-3 small">For morphologically rich languages, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> provide less consistent semantic representations due to higher variance in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>word forms</a>. Moreover, these <a href=https://en.wikipedia.org/wiki/Language>languages</a> often allow for less constrained word order, which further increases variance. For the highly agglutinative Hungarian, semantic accuracy of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> measured on word analogy tasks drops by 50-75 % compared to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We observed that <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learn morphosyntax quite well instead. Therefore, we explore and evaluate several sub-word unit based embedding strategies character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor) to boost semantic consistency in Hungarian word vectors. The effect of changing <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding dimension</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>context window size</a> have also been considered. Morphological analysis based lemmatization was found to be the best strategy to improve embeddings&#8217; semantic accuracy, whereas adding character n-grams was found consistently counterproductive in this regard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4322/>A Self-Training Approach for Short Text Clustering</a></strong><br><a href=/people/a/amir-hadifar/>Amir Hadifar</a>
|
<a href=/people/l/lucas-sterckx/>Lucas Sterckx</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4322><div class="card-body p-3 small">Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations of the short texts. Low-dimensional continuous representations or <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> can counter that sparseness problem : their high representational power is exploited in deep clustering algorithms. While deep clustering has been studied extensively in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, relatively little work has focused on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The method we propose, learns discriminative features from both an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> and a <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a>, then uses assignments from a <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering algorithm</a> as supervision to update weights of the encoder network. Experiments on three short text datasets empirically validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4323 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4323/>Improving <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>Word Embeddings</a> Using Kernel PCA<span class=acl-fixed-case>PCA</span></a></strong><br><a href=/people/v/vishwani-gupta/>Vishwani Gupta</a>
|
<a href=/people/s/sven-giesselbach/>Sven Giesselbach</a>
|
<a href=/people/s/stefan-ruping/>Stefan Rüping</a>
|
<a href=/people/c/christian-bauckhage/>Christian Bauckhage</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4323><div class="card-body p-3 small">Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections ; however, they fail to do so with small datasets. Extensions such as <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data. In this paper, we introduce a new approach to warm-start embedding models with morphological information, in order to reduce <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> and enhance their performance. We use word embeddings generated using both <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices. This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities. Evaluating our models on word similarity and analogy tasks in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time. Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies. Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4324" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4324/>Assessing Incrementality in Sequence-to-Sequence Models</a></strong><br><a href=/people/d/dennis-ulmer/>Dennis Ulmer</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/e/elia-bruni/>Elia Bruni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4324><div class="card-body p-3 small">Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. The most recent successes are predominantly due to the use of different variations of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language : continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to assess the behavior of RNNs with and without an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and identify key differences in the way the different model types process sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4325/>On Committee Representations of Adversarial Learning Models for Question-Answer Ranking</a></strong><br><a href=/people/s/sparsh-gupta/>Sparsh Gupta</a>
|
<a href=/people/v/vitor-carvalho/>Vitor Carvalho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4325><div class="card-body p-3 small">Adversarial training is a process in <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> that explicitly trains models on <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial inputs</a> (inputs designed to deceive or trick the learning process) in order to make it more robust or accurate. In this paper we investigate how representing adversarial training models as <a href=https://en.wikipedia.org/wiki/Committee>committees</a> can be used to effectively improve the performance of Question-Answer (QA) Ranking. We start by empirically probing the effects of adversarial training over multiple QA ranking algorithms, including the state-of-the-art Multihop Attention Network model. We evaluate these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> on several benchmark datasets and observe that, while adversarial training is beneficial to most baseline algorithms, there are cases where it may lead to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and performance degradation. We investigate the causes of such degradation, and then propose a new representation procedure for this adversarial learning problem, based on committee learning, that not only is capable of consistently improving all baseline algorithms, but also outperforms the previous state-of-the-art algorithm by as much as 6 % in NDCG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4327 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4327/>Best Practices for Learning Domain-Specific Cross-Lingual Embeddings</a></strong><br><a href=/people/l/lena-shakurova/>Lena Shakurova</a>
|
<a href=/people/b/beata-nyari/>Beata Nyari</a>
|
<a href=/people/c/chao-li/>Chao Li</a>
|
<a href=/people/m/mihai-rotaru/>Mihai Rotaru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4327><div class="card-body p-3 small">Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>linear projection</a> from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4329 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4329/>Learning Word Embeddings without Context Vectors</a></strong><br><a href=/people/a/alexey-zobnin/>Alexey Zobnin</a>
|
<a href=/people/e/evgenia-elistratova/>Evgenia Elistratova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4329><div class="card-body p-3 small">Most word embedding algorithms such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> or <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> construct two sort of vectors : for words and for contexts. Naive use of vectors of only one sort leads to poor results. We suggest using indefinite inner product in skip-gram negative sampling algorithm. This allows us to use only one sort of vectors without loss of quality. Our context-free cf algorithm performs on par with SGNS on word similarity datasets</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4331 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4331/>Modality-based Factorization for Multimodal Fusion</a></strong><br><a href=/people/e/elham-j-barezi/>Elham J. Barezi</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4331><div class="card-body p-3 small">We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (M+1)-way tensor to consider the high-order relationships between M modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> avoiding <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. We have applied this method to three different multimodal datasets in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, personality trait recognition, and <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>. We are able to recognize relationships and relative importance of different modalities in these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and achieves a 1 % to 4 % improvement on several evaluation measures compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for all three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.<i>M</i>+1)-way tensor to consider the high-order relationships between <i>M</i> modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks.</div></div></div><hr><div id=w19-44><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-44.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-44/>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4400/>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></strong><br><a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4401/>The many dimensions of <a href=https://en.wikipedia.org/wiki/Algorithmic_fairness>algorithmic fairness</a> in educational applications</a></strong><br><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/k/klaus-zechner/>Klaus Zechner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4401><div class="card-body p-3 small">The issues of <a href=https://en.wikipedia.org/wiki/Algorithmic_fairness>algorithmic fairness</a> and <a href=https://en.wikipedia.org/wiki/Bias>bias</a> have recently featured prominently in many publications highlighting the fact that training the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for maximum performance may often result in predictions that are biased against various groups. Educational applications based on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Speech_processing>speech processing technologies</a> often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> as other machine learning systems. Yet such <a href=https://en.wikipedia.org/wiki/System>systems</a> can have high impact on people&#8217;s lives especially when deployed as part of <a href=https://en.wikipedia.org/wiki/Test_(assessment)>high-stakes tests</a>. In this paper we discuss different definitions of <a href=https://en.wikipedia.org/wiki/Fair_division>fairness</a> and possible ways to apply them to <a href=https://en.wikipedia.org/wiki/Educational_technology>educational applications</a>. We then use simulated and real data to consider how test-takers&#8217; native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of <a href=https://en.wikipedia.org/wiki/Equity_(economics)>fairness</a> may require different solutions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4404/>Computationally Modeling the Impact of Task-Appropriate Language Complexity and Accuracy on Human Grading of German Essays<span class=acl-fixed-case>G</span>erman Essays</a></strong><br><a href=/people/z/zarah-weiss/>Zarah Weiss</a>
|
<a href=/people/a/anja-riemenschneider/>Anja Riemenschneider</a>
|
<a href=/people/p/pauline-schroter/>Pauline Schröter</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4404><div class="card-body p-3 small">Computational linguistic research on the <a href=https://en.wikipedia.org/wiki/Language_complexity>language complexity</a> of student writing typically involves human ratings as a gold standard. However, educational science shows that teachers find it difficult to identify and cleanly separate <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, different aspects of <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>, <a href=https://en.wikipedia.org/wiki/Content_(media)>contents</a>, and <a href=https://en.wikipedia.org/wiki/Structure>structure</a>. In this paper, we therefore explore the use of computational linguistic methods to investigate how task-appropriate complexity and accuracy relate to the grading of overall performance, content performance, and language performance as assigned by teachers. Based on texts written by students for the official school-leaving state examination (Abitur), we show that teachers successfully assign higher language performance grades to essays with higher task-appropriate language complexity and properly separate this from content scores. Yet, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> impacts teacher assessment for all grading rubrics, also the content score, overemphasizing the role of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our analysis is based on broad computational linguistic modeling of German language complexity and an innovative theory- and data-driven feature aggregation method inferring task-appropriate language complexity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4405/>Analysing Rhetorical Structure as a Key Feature of Summary Coherence</a></strong><br><a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/t/tamara-sladoljev-agejev/>Tamara Sladoljev-Agejev</a>
|
<a href=/people/s/svjetlana-kolic-vehovec/>Svjetlana Kolić Vehovec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4405><div class="card-body p-3 small">We present a model for automatic scoring of coherence based on comparing the rhetorical structure (RS) of college student summaries in L2 (English) against expert summaries. Coherence is conceptualised as a construct consisting of the rhetorical relation and its arguments. Comparison with expert-assigned scores shows that RS scores correlate with both <a href=https://en.wikipedia.org/wiki/Group_cohesiveness>cohesion</a> and <a href=https://en.wikipedia.org/wiki/Group_cohesiveness>coherence</a>. Furthermore, RS scores improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for cohesion score prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4406/>The BEA-2019 Shared Task on Grammatical Error Correction<span class=acl-fixed-case>BEA</span>-2019 Shared Task on Grammatical Error Correction</a></strong><br><a href=/people/c/christopher-bryant/>Christopher Bryant</a>
|
<a href=/people/m/mariano-felice/>Mariano Felice</a>
|
<a href=/people/o/oistein-e-andersen/>Øistein E. Andersen</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4406><div class="card-body p-3 small">This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write&Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of <a href=https://en.wikipedia.org/wiki/Track_(navigation)>tracks</a>, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F_0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4407" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4407/>A Benchmark Corpus of English Misspellings and a Minimally-supervised Model for Spelling Correction<span class=acl-fixed-case>E</span>nglish Misspellings and a Minimally-supervised Model for Spelling Correction</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/m/michael-fried/>Michael Fried</a>
|
<a href=/people/a/alla-rozovskaya/>Alla Rozovskaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4407><div class="card-body p-3 small">Spelling correction has attracted a lot of attention in the NLP community. However, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English language learners</a>. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our <a href=https://en.wikipedia.org/wiki/Data>data</a> : 88.12 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1 %). Furthermore, this approach allows easy portability to <a href=https://en.wikipedia.org/wiki/Domain_name>new domains</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on data from a medical domain and demonstrate that it rivals the performance of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained and tuned on in-domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4409/>Regression or classification? Automated Essay Scoring for Norwegian<span class=acl-fixed-case>N</span>orwegian</a></strong><br><a href=/people/s/stig-johan-berggren/>Stig Johan Berggren</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4409><div class="card-body p-3 small">In this paper we present first results for the task of <a href=https://en.wikipedia.org/wiki/Automated_essay_scoring>Automated Essay Scoring</a> for <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian learner language</a>. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> or <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for joint prediction of essay scoring and native language identification. We find that a GRU-based attention model trained in a single-task setting performs best at the AES task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4411/>How to account for mispellings : Quantifying the benefit of character representations in neural content scoring models</a></strong><br><a href=/people/b/brian-riordan/>Brian Riordan</a>
|
<a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/r/robert-pugh/>Robert Pugh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4411><div class="card-body p-3 small">Character-based representations in neural models have been claimed to be a tool to overcome spelling variation in in word token-based input. We examine this claim in neural models for content scoring. We formulate precise hypotheses about the possible effects of adding <a href=https://en.wikipedia.org/wiki/Character_(computing)>character representations</a> to word-based models and test these hypotheses on large-scale real world content scoring datasets. We find that, while character representations may provide small performance gains in general, their effectiveness in accounting for spelling variation may be limited. We show that spelling correction can provide larger gains than character representations, and that spelling correction improves the performance of models with character representations. With these insights, we report a new state of the art on the ASAP-SAS content scoring dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4412" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4412/>The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction</a></strong><br><a href=/people/d/dimitris-alikaniotis/>Dimitris Alikaniotis</a>
|
<a href=/people/v/vipul-raheja/>Vipul Raheja</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4412><div class="card-body p-3 small">Recent work on Grammatical Error Correction (GEC) has highlighted the importance of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4415/>Erroneous data generation for Grammatical Error Correction</a></strong><br><a href=/people/s/shuyao-xu/>Shuyao Xu</a>
|
<a href=/people/j/jiehao-zhang/>Jiehao Zhang</a>
|
<a href=/people/j/jin-chen/>Jin Chen</a>
|
<a href=/people/l/long-qin/>Long Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4415><div class="card-body p-3 small">It has been demonstrated that the utilization of a monolingual corpus in neural Grammatical Error Correction (GEC) systems can significantly improve the system performance. The previous state-of-the-art neural GEC system is an ensemble of four Transformer models pretrained on a large amount of Wikipedia Edits. The Singsound GEC system follows a similar approach but is equipped with a sophisticated erroneous data generating component. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved an <a href=https://en.wikipedia.org/wiki/Formal_grammar>F0:5</a> of 66.61 in the BEA 2019 Shared Task : Grammatical Error Correction. With our novel erroneous data generating component, the Singsound neural GEC system yielded an M2 of 63.2 on the CoNLL-2014 benchmark (8.4 % relative improvement over the previous state-of-the-art system).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4416/>The LAIX Systems in the BEA-2019 GEC Shared Task<span class=acl-fixed-case>LAIX</span> Systems in the <span class=acl-fixed-case>BEA</span>-2019 <span class=acl-fixed-case>GEC</span> Shared Task</a></strong><br><a href=/people/r/ruobing-li/>Ruobing Li</a>
|
<a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/y/yefei-zha/>Yefei Zha</a>
|
<a href=/people/y/yonghong-yu/>Yonghong Yu</a>
|
<a href=/people/s/shiman-guo/>Shiman Guo</a>
|
<a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/h/hui-lin/>Hui Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4416><div class="card-body p-3 small">In this paper, we describe two <a href=https://en.wikipedia.org/wiki/System>systems</a> we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble systems</a> to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4421/>The BLCU System in the BEA 2019 Shared Task<span class=acl-fixed-case>BLCU</span> System in the <span class=acl-fixed-case>BEA</span> 2019 Shared Task</a></strong><br><a href=/people/l/liner-yang/>Liner Yang</a>
|
<a href=/people/c/chencheng-wang/>Chencheng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4421><div class="card-body p-3 small">This paper describes the BLCU Group submissions to the Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC). The task is to detect and correct <a href=https://en.wikipedia.org/wiki/Error_(linguistics)>grammatical errors</a> that occurred in essays. We participate in 2 tracks including the Restricted Track and the Unrestricted Track. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on a Transformer model architecture. We integrate many effective <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> proposed in recent years. Such as, Byte Pair Encoding, model ensemble, checkpoints average and <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checker</a>. We also corrupt the public monolingual data to further improve the performance of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. On the test data of the BEA 2019 Shared Task, our <a href=https://en.wikipedia.org/wiki/System>system</a> yields F0.5 = 58.62 and 59.50, ranking twelfth and fourth respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4424/>Neural and FST-based approaches to grammatical error correction<span class=acl-fixed-case>FST</span>-based approaches to grammatical error correction</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4424><div class="card-body p-3 small">In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>system pipeline</a> that utilises both <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection and correction models</a>. The input text is first corrected by two complementary neural machine translation systems : one using convolutional networks and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, and another using a neural Transformer-based system. Training is performed on publicly available data, along with artificial examples generated through <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>. The n-best lists of these two <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> are then combined and scored using a <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>finite state transducer (FST)</a>. Finally, an unsupervised re-ranking system is applied to the n-best output of the <a href=https://en.wikipedia.org/wiki/Finite-state_machine>FST</a>. The re-ranker uses a number of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection features</a> to re-rank the FST n-best list and identify the final 1-best correction hypothesis. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 66.75 % F 0.5 on error correction (ranking 4th), and 82.52 % F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4425/>Improving Precision of Grammatical Error Correction with a Cheat Sheet</a></strong><br><a href=/people/m/mengyang-qiu/>Mengyang Qiu</a>
|
<a href=/people/x/xuejiao-chen/>Xuejiao Chen</a>
|
<a href=/people/m/maggie-liu/>Maggie Liu</a>
|
<a href=/people/k/krishna-parvathala/>Krishna Parvathala</a>
|
<a href=/people/a/apurva-patil/>Apurva Patil</a>
|
<a href=/people/j/jungyeul-park/>Jungyeul Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4425><div class="card-body p-3 small">In this paper, we explore two approaches of generating error-focused phrases and examine whether these <a href=https://en.wikipedia.org/wiki/Phrase>phrases</a> can lead to better performance in <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction</a> for the restricted track of BEA 2019 Shared Task on GEC. Our results show that phrases directly extracted from GEC corpora outperform phrases from statistical machine translation phrase table by a large margin. Appending error+context phrases to the original GEC corpora yields comparably high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. We also explore the generation of artificial syntactic error sentences using error+context phrases for the unrestricted track. The additional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> greatly facilitates syntactic error correction (e.g., verb form) and contributes to better overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4428 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4428" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4428/>Evaluation of automatic collocation extraction methods for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a></a></strong><br><a href=/people/v/vishal-bhalla/>Vishal Bhalla</a>
|
<a href=/people/k/klara-klimcikova/>Klara Klimcikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4428><div class="card-body p-3 small">A number of methods have been proposed to automatically extract collocations, i.e., conventionalized lexical combinations, from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>. However, the attempts to evaluate and compare <a href=https://en.wikipedia.org/wiki/Them_(band)>them</a> with a specific application in mind lag behind. This paper compares three end-to-end resources for collocation learning, all of which used the same corpus but different methods. Adopting a gold-standard evaluation method, the results show that the method of dependency parsing outperforms regex-over-pos in collocation identification. The lexical association measures (AMs) used for collocation ranking perform about the same overall but differently for individual collocation types. Further analysis has also revealed that there are considerable differences between other commonly used <a href=https://en.wikipedia.org/wiki/Amplitude_modulation>AMs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4429/>Anglicized Words and Misspelled Cognates in <a href=https://en.wikipedia.org/wiki/Native_Language_Identification>Native Language Identification</a></a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4429><div class="card-body p-3 small">In this paper, we present experiments that estimate the impact of specific lexical choices of people writing in a second language (L2). In particular, we look at misspelled words that indicate lexical uncertainty on the part of the author, and separate them into three categories : misspelled cognates, L2-ed (in our case, anglicized) words, and all other spelling errors. We test the assumption that such <a href=https://en.wikipedia.org/wiki/Error_(linguistics)>errors</a> contain clues about the native language of an essay&#8217;s author through the task of native language identification. The results of the experiments show that the information brought by each of these <a href=https://en.wikipedia.org/wiki/Category_(mathematics)>categories</a> is complementary. We also note that while the distribution of such <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> changes with the proficiency level of the writer, their contribution towards native language identification remains significant at all levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4430 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4430/>Linguistically-Driven Strategy for Concept Prerequisites Learning on Italian<span class=acl-fixed-case>I</span>talian</a></strong><br><a href=/people/a/alessio-miaschi/>Alessio Miaschi</a>
|
<a href=/people/c/chiara-alzetta/>Chiara Alzetta</a>
|
<a href=/people/f/franco-alberto-cardillo/>Franco Alberto Cardillo</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4430><div class="card-body p-3 small">We present a new concept prerequisite learning method for Learning Object (LO) ordering that exploits only linguistic features extracted from textual educational resources. The method was tested in a cross- and in- domain scenario both for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Additionally, we performed experiments based on a incremental training strategy to study the impact of the training set size on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performances. The paper also introduces ITA-PREREQ, to the best of our knowledge the first Italian dataset annotated with prerequisite relations between pairs of educational concepts, and describe the automatic strategy devised to build it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4431/>Grammatical-Error-Aware Incorrect Example Retrieval System for Learners of <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> as a Second Language<span class=acl-fixed-case>J</span>apanese as a Second Language</a></strong><br><a href=/people/m/mio-arai/>Mio Arai</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4431><div class="card-body p-3 small">Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. Even if a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners need to know whether their query includes errors or not. Considering the usability of retrieving incorrect examples, our proposed method uses a large-scale corpus and presents correct expressions along with incorrect expressions using a grammatical error detection system so that the learner do not need to be aware of how to search for the examples. Intrinsic and extrinsic evaluations indicate that our method improves accuracy of example sentence retrieval and quality of learner&#8217;s writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4432/>Toward Automated Content Feedback Generation for Non-native Spontaneous Speech</a></strong><br><a href=/people/s/su-youn-yoon/>Su-Youn Yoon</a>
|
<a href=/people/c/ching-ni-hsieh/>Ching-Ni Hsieh</a>
|
<a href=/people/k/klaus-zechner/>Klaus Zechner</a>
|
<a href=/people/m/matthew-mulholland/>Matthew Mulholland</a>
|
<a href=/people/y/yuan-wang/>Yuan Wang</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4432><div class="card-body p-3 small">In this study, we developed an <a href=https://en.wikipedia.org/wiki/Algorithm>automated algorithm</a> to provide feedback about the specific content of non-native English speakers&#8217; spoken responses. The responses were spontaneous speech, elicited using integrated tasks where the language learners listened to and/or read passages and integrated the core content in their spoken responses. Our models detected the absence of key points considered to be important in a spoken response to a particular test question, based on two different models : (a) a model using word-embedding based content features and (b) a state-of-the art short response scoring engine using traditional n-gram based features. Both <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> achieved a substantially improved performance over the majority baseline, and the combination of the two <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> achieved a significant further improvement. In particular, the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> were robust to automated speech recognition (ASR) errors, and performance based on the ASR word hypotheses was comparable to that based on manual transcriptions. The <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of the best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the questions included in the train set were 0.80 and 0.68, respectively. Finally, we discussed possible approaches to generating targeted feedback about the content of a language learner&#8217;s response, based on automatically detected missing key points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4435 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4435/>Curio SmartChat : A system for Natural Language Question Answering for Self-Paced K-12 Learning<span class=acl-fixed-case>S</span>mart<span class=acl-fixed-case>C</span>hat : A system for Natural Language Question Answering for Self-Paced K-12 Learning</a></strong><br><a href=/people/s/srikrishna-raamadhurai/>Srikrishna Raamadhurai</a>
|
<a href=/people/r/ryan-baker/>Ryan Baker</a>
|
<a href=/people/v/vikraman-poduval/>Vikraman Poduval</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4435><div class="card-body p-3 small">During learning, students often have questions which they would benefit from responses to in real time. In class, a student can ask a question to a teacher. During homework, or even in class if the student is shy, it can be more difficult to receive a rapid response. In this work, we introduce Curio SmartChat, an automated question answering system for middle school Science topics. Our <a href=https://en.wikipedia.org/wiki/System>system</a> has now been used by around 20,000 students who have so far asked over 100,000 questions. We present data on the challenge created by students&#8217; <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> and spelling mistakes, and discuss our <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s approach and degree of effectiveness at disambiguating questions that the <a href=https://en.wikipedia.org/wiki/System>system</a> is initially unsure about. We also discuss the prevalence of student small talk not related to science topics, the pluses and minuses of this behavior, and how a <a href=https://en.wikipedia.org/wiki/System>system</a> should respond to these conversational acts. We conclude with discussions and point to directions for potential future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4436/>Supporting content evaluation of student summaries by Idea Unit embedding</a></strong><br><a href=/people/m/marcello-gecchele/>Marcello Gecchele</a>
|
<a href=/people/h/hiroaki-yamada/>Hiroaki Yamada</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a>
|
<a href=/people/y/yasuyo-sawaki/>Yasuyo Sawaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4436><div class="card-body p-3 small">This paper discusses the computer-assisted content evaluation of summaries. We propose a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> to make a correspondence between the segments of the source text and its summary. As a unit of the segment, we adopt Idea Unit (IU) which is proposed in <a href=https://en.wikipedia.org/wiki/Applied_linguistics>Applied Linguistics</a>. Introducing IUs enables us to make a correspondence even for the sentences that contain multiple ideas. The IU correspondence is made based on the similarity between vector representations of IU. An evaluation experiment with two source texts and 20 summaries showed that the proposed method is more robust against rephrased expressions than the conventional ROUGE-based baselines. Also, the proposed method outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in recall. We im-plemented the proposed method in a GUI toolSegment Matcher that aids teachers to estab-lish a link between corresponding IUs acrossthe summary and source text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4437" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4437/>On Understanding the Relation between Expert Annotations of Text Readability and Target Reader Comprehension</a></strong><br><a href=/people/s/sowmya-vajjala/>Sowmya Vajjala</a>
|
<a href=/people/i/ivana-lucic/>Ivana Lucic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4437><div class="card-body p-3 small">Automatic readability assessment aims to ensure that readers read texts that they can comprehend. However, <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> are typically trained on texts created from the perspective of the text writer, not the target reader. There is little experimental research on the relationship between expert annotations of readability, reader&#8217;s language proficiency, and different levels of <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. To address this gap, we conducted a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> in which over a 100 participants read texts of different reading levels and answered questions created to test three forms of <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a>. Our results indicate that more than readability annotation or reader proficiency, it is the type of comprehension question asked that shows differences between reader responses-inferential questions were difficult for users of all levels of proficiency across reading levels. The data collected from this study will be released with this paper, which will, for the first time, provide a collection of 45 reader bench marked texts to evaluate readability assessment systems developed for <a href=https://en.wikipedia.org/wiki/Adult_learner>adult learners</a> of <a href=https://en.wikipedia.org/wiki/English_language>English</a>. It can also potentially be useful for the development of question generation approaches in <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems research</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4440 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4440/>Analyzing Linguistic Complexity and Accuracy in Academic Language Development of <a href=https://en.wikipedia.org/wiki/German_language>German</a> across Elementary and Secondary School<span class=acl-fixed-case>G</span>erman across Elementary and Secondary School</a></strong><br><a href=/people/z/zarah-weiss/>Zarah Weiss</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4440><div class="card-body p-3 small">We track the development of writing complexity and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in German students&#8217; early academic language development from first to eighth grade. Combining an empirically broad approach to linguistic complexity with the high-quality error annotation included in the Karlsruhe Children&#8217;s Text corpus (Lavalley et al. 2015) used, we construct models of German academic language development that successfully identify the student&#8217;s grade level. We show that <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> for the early years rely more on accuracy development, whereas development in <a href=https://en.wikipedia.org/wiki/Secondary_school>secondary school</a> is better characterized by increasingly complex language in all domains : <a href=https://en.wikipedia.org/wiki/Linguistic_system>linguistic system</a>, language use, and human sentence processing characteristics. We demonstrate the generalizability and robustness of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using such a broad complexity feature set across writing topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4442/>Learning Outcomes and Their Relatedness in a Medical Curriculum</a></strong><br><a href=/people/s/sneha-mondal/>Sneha Mondal</a>
|
<a href=/people/t/tejas-dhamecha/>Tejas Dhamecha</a>
|
<a href=/people/s/shantanu-godbole/>Shantanu Godbole</a>
|
<a href=/people/s/smriti-pathak/>Smriti Pathak</a>
|
<a href=/people/r/red-mendoza/>Red Mendoza</a>
|
<a href=/people/k/k-gayathri-wijayarathna/>K Gayathri Wijayarathna</a>
|
<a href=/people/n/nabil-zary/>Nabil Zary</a>
|
<a href=/people/s/swarnadeep-saha/>Swarnadeep Saha</a>
|
<a href=/people/m/malolan-chetlur/>Malolan Chetlur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4442><div class="card-body p-3 small">A typical medical curriculum is organized in a hierarchy of instructional objectives called Learning Outcomes (LOs) ; a few thousand LOs span five years of study. Gaining a thorough understanding of the <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a> requires learners to recognize and apply related LOs across years, and across different parts of the curriculum. However, given the large scope of the curriculum, manually labeling related LOs is tedious, and almost impossible to scale. In this paper, we build a <a href=https://en.wikipedia.org/wiki/System>system</a> that learns relationships between LOs, and we achieve up to human-level performance in the LO relationship extraction task. We then present an application where the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is employed to build a map of related LOs and Learning Resources (LRs) pertaining to a virtual patient case. We believe that our system can help medical students grasp the curriculum better, within classroom as well as in Intelligent Tutoring Systems (ITS) settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4446 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4446/>Equity Beyond Bias in Language Technologies for Education</a></strong><br><a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/m/michael-madaio/>Michael Madaio</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/d/david-gerritsen/>David Gerritsen</a>
|
<a href=/people/b/brittany-mclaughlin/>Brittany McLaughlin</a>
|
<a href=/people/e/ezekiel-dixon-roman/>Ezekiel Dixon-Román</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4446><div class="card-body p-3 small">There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We present case studies in a range of topics like <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a>, <a href=https://en.wikipedia.org/wiki/Computer-assisted_language_learning>computer-assisted language learning</a>, automated essay scoring, and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in classrooms, and provide an actionable agenda for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4447/>From Receptive to Productive : Learning to Use Confusing Words through Automatically Selected Example Sentences</a></strong><br><a href=/people/c/chieh-yang-huang/>Chieh-Yang Huang</a>
|
<a href=/people/y/yi-ting-huang/>Yi-Ting Huang</a>
|
<a href=/people/m/meihua-chen/>MeiHua Chen</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4447><div class="card-body p-3 small">Knowing how to use words appropriately has been a key to improving language proficiency. Previous studies typically discuss how students learn receptively to select the correct candidate from a set of confusing words in the fill-in-the-blank task where specific context is given. In this paper, we go one step further, assisting students to learn to use confusing words appropriately in a productive task : sentence translation. We leverage the GiveMe-Example system, which suggests example sentences for each confusing word, to achieve this goal. In this study, students learn to differentiate the confusing words by reading the example sentences, and then choose the appropriate word(s) to complete the sentence translation task. Results show students made substantial progress in terms of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a>. In addition, highly proficient students better managed to learn confusing words. In view of the influence of the first language on learners, we further propose an effective approach to improve the quality of the suggested sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4450 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4450/>Automated Essay Scoring with Discourse-Aware Neural Models</a></strong><br><a href=/people/f/farah-nadeem/>Farah Nadeem</a>
|
<a href=/people/h/huy-nguyen/>Huy Nguyen</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4450><div class="card-body p-3 small">Automated essay scoring systems typically rely on hand-crafted features to predict essay quality, but such systems are limited by the cost of <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. Neural networks offer an alternative to <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>, but they typically require more annotated data. This paper explores network structures, contextualized embeddings and pre-training strategies aimed at capturing discourse characteristics of essays. Experiments on three essay scoring tasks show benefits from all three <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> in different combinations, with simpler <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> being more effective when less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4452.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4452 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4452 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4452" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4452/>Rubric Reliability and Annotation of Content and Argument in Source-Based Argument Essays</a></strong><br><a href=/people/y/yanjun-gao/>Yanjun Gao</a>
|
<a href=/people/a/alex-driban/>Alex Driban</a>
|
<a href=/people/b/brennan-xavier-mcmanus/>Brennan Xavier McManus</a>
|
<a href=/people/e/elena-musi/>Elena Musi</a>
|
<a href=/people/p/patricia-davies/>Patricia Davies</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca J. Passonneau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4452><div class="card-body p-3 small">We present a unique dataset of student source-based argument essays to facilitate research on the relations between <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>, <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation skills</a>, and <a href=https://en.wikipedia.org/wiki/Educational_assessment>assessment</a>. Two classroom writing assignments were given to college students in a STEM major, accompanied by a carefully designed rubric. The paper presents a reliability study of the <a href=https://en.wikipedia.org/wiki/Rubric>rubric</a>, showing it to be highly reliable, and initial annotation on content and argumentation annotation of the essays.</div></div></div><hr><div id=w19-45><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-45.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-45/>Proceedings of the 6th Workshop on Argument Mining</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4500/>Proceedings of the 6th Workshop on Argument Mining</a></strong><br><a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4502/>A Cascade Model for Proposition Extraction in Argumentation</a></strong><br><a href=/people/y/yohan-jo/>Yohan Jo</a>
|
<a href=/people/j/jacky-visser/>Jacky Visser</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4502><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to tackle a fundamental but understudied problem in computational argumentation : proposition extraction. Propositions are the basic units of an argument and the primary building blocks of most <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining systems</a>. However, they are usually substituted by argumentative discourse units obtained via surface-level text segmentation, which may yield text segments that lack semantic information necessary for subsequent argument mining processes. In contrast, our cascade model aims to extract complete propositions by handling <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a>, <a href=https://en.wikipedia.org/wiki/Text_segmentation>text segmentation</a>, reported speech, questions, <a href=https://en.wikipedia.org/wiki/Imperative_mood>imperatives</a>, missing subjects, and revision. We formulate each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as a <a href=https://en.wikipedia.org/wiki/Computational_problem>computational problem</a> and test various <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using a corpus of the 2016 <a href=https://en.wikipedia.org/wiki/United_States_presidential_debates>U.S. presidential debates</a>. We show promising performance for some <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and discuss main challenges in proposition extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4503/>Dissecting Content and Context in Argumentative Relation Analysis</a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4503><div class="card-body p-3 small">When assessing relations between argumentative units (e.g., support or attack), computational systems often exploit disclosing indicators or markers that are not part of elementary argumentative units (EAUs) themselves, but are gained from their context (position in paragraph, preceding tokens, etc.). We show that this dependency is much stronger than previously assumed. In fact, we show that by completely masking the EAU text spans and only feeding information from their context, a competitive system may function even better. We argue that an argument analysis system that relies more on discourse context than the argument&#8217;s content is unsafe, since it can easily be tricked. To alleviate this issue, we separate argumentative units from their context such that the <a href=https://en.wikipedia.org/wiki/System>system</a> is forced to model and rely on an EAU&#8217;s content. We show that the resulting classification system is more robust, and argue that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are better suited for predicting argumentative relations across documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4506/>The Swedish PoliGraph : A Semantic Graph for Argument Mining of Swedish Parliamentary Data<span class=acl-fixed-case>S</span>wedish <span class=acl-fixed-case>P</span>oli<span class=acl-fixed-case>G</span>raph: A Semantic Graph for Argument Mining of <span class=acl-fixed-case>S</span>wedish Parliamentary Data</a></strong><br><a href=/people/s/stian-rodven-eide/>Stian Rødven Eide</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4506><div class="card-body p-3 small">As part of a larger project on <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> of Swedish parliamentary data, we have created a semantic graph that, together with named entity recognition and resolution (NER), should make it easier to establish connections between arguments in a given debate. The <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> is essentially a semantic database that keeps track of Members of Parliament (MPs), in particular their presence in the parliament and activity in debates, but also party affiliation and participation in commissions. The hope is that the Swedish PoliGraph will enable us to perform named entity resolution on debates in the <a href=https://en.wikipedia.org/wiki/Riksdag>Swedish parliament</a> with a high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, with the aim of determining to whom an argument is directed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4507/>Towards Effective Rebuttal : <a href=https://en.wikipedia.org/wiki/Listening_comprehension>Listening Comprehension</a> Using Corpus-Wide Claim Mining</a></strong><br><a href=/people/t/tamar-lavee/>Tamar Lavee</a>
|
<a href=/people/m/matan-orbach/>Matan Orbach</a>
|
<a href=/people/l/lili-kotlerman/>Lili Kotlerman</a>
|
<a href=/people/y/yoav-kantor/>Yoav Kantor</a>
|
<a href=/people/s/shai-gretz/>Shai Gretz</a>
|
<a href=/people/l/lena-dankin/>Lena Dankin</a>
|
<a href=/people/m/michal-jacovi/>Michal Jacovi</a>
|
<a href=/people/y/yonatan-bilu/>Yonatan Bilu</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4507><div class="card-body p-3 small">Engaging in a live debate requires, among other things, the ability to effectively rebut arguments claimed by your opponent. In particular, this requires identifying these arguments. Here, we suggest doing so by automatically mining claims from a corpus of news articles containing billions of sentences, and searching for them in a given speech. This raises the question of whether such claims indeed correspond to those made in spoken speeches. To this end, we collected a large dataset of 400 speeches in English discussing 200 controversial topics, mined claims for each topic, and asked annotators to identify the mined claims mentioned in each speech. Results show that in the vast majority of speeches debaters indeed make use of such claims. In addition, we present several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for the automatic detection of mined claims in <a href=https://en.wikipedia.org/wiki/Public_speaking>speeches</a>, forming the basis for future work. All collected data is freely available for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4509/>Is It Worth the Attention? A Comparative Evaluation of Attention Layers for Argument Unit Segmentation</a></strong><br><a href=/people/m/maximilian-spliethover/>Maximilian Spliethöver</a>
|
<a href=/people/j/jonas-klaff/>Jonas Klaff</a>
|
<a href=/people/h/hendrik-heuer/>Hendrik Heuer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4509><div class="card-body p-3 small">Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4512/>The Utility of Discourse Parsing Features for Predicting Argumentation Structure</a></strong><br><a href=/people/f/freya-hewett/>Freya Hewett</a>
|
<a href=/people/r/roshan-prakash-rane/>Roshan Prakash Rane</a>
|
<a href=/people/n/nina-harlacher/>Nina Harlacher</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4512><div class="card-body p-3 small">Research on argumentation mining from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> has frequently discussed relationships to discourse parsing, but few empirical results are available so far. One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the &#8216;argumentative microtexts&#8217; (Peldszus and Stede, 2016a). While results on perusing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the <a href=https://en.wikipedia.org/wiki/Microtext>microtexts</a>, we find that PDTB features can indeed improve its performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4516/>Categorizing Comparative Sentences</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/mirco-franzek/>Mirco Franzek</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4516><div class="card-body p-3 small">We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> has better NLP libraries than <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB Python</a>, better, <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB</a>). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27 % of the sentences contain an oriented comparison in the sense of better or worse). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85 % in our experimental evaluation. The model can be used to extract comparative sentences for pro / con argumentation in comparative / argument search engines or debating technologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4517 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4517/>Ranking Passages for Argument Convincingness</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/adam-ferguson/>Adam Ferguson</a>
|
<a href=/people/t/timothy-j-hazen/>Timothy J. Hazen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4517><div class="card-body p-3 small">In data ranking applications, pairwise annotation is often more consistent than cardinal annotation for learning ranking models. We examine this in a case study on ranking text passages for argument convincingness. Our task is to choose text passages that provide the highest-quality, most-convincing arguments for opposing sides of a topic. Using data from a deployed system within the <a href=https://en.wikipedia.org/wiki/Bing_(search_engine)>Bing search engine</a>, we construct a pairwise-labeled dataset for argument convincingness that is substantially more comprehensive in topical coverage compared to existing public resources. We detail the process of extracting topical passages for queries submitted to a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a>, creating annotated sets of passages aligned to different stances on a topic, and assessing argument convincingness of passages using pairwise annotation. Using a state-of-the-art convincingness model, we evaluate several methods for using pairwise-annotated data examples to train models for ranking passages. Our results show pairwise training outperforms training that regresses to a target score for each passage. Our results also show a simple &#8216;win-rate&#8217; score is a better regression target than the previously proposed page-rank target. Lastly, addressing the need to filter noisy crowd-sourced annotations when constructing a dataset, we show that filtering for transitivity within pairwise annotations is more effective than filtering based on annotation confidence measures for individual examples.</div></div></div><hr><div id=w19-46><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-46.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-46/>Proceedings of the Fourth Arabic Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4600/>Proceedings of the Fourth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich Belguith</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a>
|
<a href=/people/i/imed-zitouni/>Imed Zitouni</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4601" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4601/>Incremental Domain Adaptation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> in Low-Resource Settings</a></strong><br><a href=/people/m/marimuthu-kalimuthu/>Marimuthu Kalimuthu</a>
|
<a href=/people/m/michael-barz/>Michael Barz</a>
|
<a href=/people/d/daniel-sonntag/>Daniel Sonntag</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4601><div class="card-body p-3 small">We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>. In this paper, we propose a novel query strategy for selecting unlabeled samples from a new domain based on <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. We accelerate the <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning process</a> of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50 % of the annotation costs without loss in <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>, demonstrating the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4603/>POS Tagging for Improving Code-Switching Identification in Arabic<span class=acl-fixed-case>POS</span> Tagging for Improving Code-Switching Identification in <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/a/ali-elkahky/>Ali Elkahky</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4603><div class="card-body p-3 small">When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language. This paper explores the possibility of utilizing this pattern in improving code-switching identification between <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>Modern Standard Arabic (MSA)</a> and <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian Arabic (EA)</a>. We try to answer the question of how strong is the POS signal in word-level code-switching identification. We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9 % on the development set and 1.0 % on the test set. We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the <a href=https://en.wikipedia.org/wiki/Dialect>dialectal language</a> while the majority of content words come from the <a href=https://en.wikipedia.org/wiki/Standard_language>standard language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4604/>Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects<span class=acl-fixed-case>A</span>rabic Dialects</a></strong><br><a href=/people/h/hala-mulki/>Hala Mulki</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/m/mourad-gridach/>Mourad Gridach</a>
|
<a href=/people/i/ismail-babaoglu/>Ismail Babaoğlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4604><div class="card-body p-3 small">Arabic sentiment analysis models have employed compositional embedding features to represent the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialectal content</a>. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural frameworks</a>. With the <a href=https://en.wikipedia.org/wiki/Free_word_order>free word order</a> and the varying syntax nature across the different <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a>, a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis system</a> developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of several <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a>. The proposed <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4610/>Constrained Sequence-to-sequence Semitic Root Extraction for Enriching Word Embeddings<span class=acl-fixed-case>S</span>emitic Root Extraction for Enriching Word Embeddings</a></strong><br><a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/x/xingyu-fu/>Xingyu Fu</a>
|
<a href=/people/a/aseel-addawood/>Aseel Addawood</a>
|
<a href=/people/n/nahil-sobh/>Nahil Sobh</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4610><div class="card-body p-3 small">In this paper, we tackle the problem of <a href=https://en.wikipedia.org/wiki/Root_(linguistics)>root extraction</a> from words in the <a href=https://en.wikipedia.org/wiki/Semitic_languages>Semitic language family</a>. A challenge in applying natural language processing techniques to these languages is the data sparsity problem that arises from their rich internal morphology, where the substructure is inherently non-concatenative and morphemes are interdigitated in <a href=https://en.wikipedia.org/wiki/Word_formation>word formation</a>. While previous automated methods have relied on human-curated rules or multiclass classification, they have not fully leveraged the various combinations of regular, sequential concatenative morphology within the words and the internal interleaving within templatic stems of roots and patterns. To address this, we propose a constrained sequence-to-sequence root extraction method. Experimental results show our constrained model outperforms a variety of methods at <a href=https://en.wikipedia.org/wiki/Root_extraction>root extraction</a>. Furthermore, by enriching word embeddings with resulting decompositions, we show improved results on word analogy, word similarity, and language modeling tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4611/>En-Ar Bilingual Word Embeddings without <a href=https://en.wikipedia.org/wiki/Word_alignment>Word Alignment</a> : Factors Effects</a></strong><br><a href=/people/t/taghreed-alqaisi/>Taghreed Alqaisi</a>
|
<a href=/people/s/simon-okeefe/>Simon O’Keefe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4611><div class="card-body p-3 small">This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA). We investigate the effect of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> and embedding size on the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a>. Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the ATB and D0 schemes in all different training settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4614.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4614 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4614/>Assessing Arabic Weblog Credibility via Deep Co-learning<span class=acl-fixed-case>A</span>rabic Weblog Credibility via Deep Co-learning</a></strong><br><a href=/people/c/chadi-helwe/>Chadi Helwe</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a>
|
<a href=/people/a/ayman-al-zaatari/>Ayman Al Zaatari</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4614><div class="card-body p-3 small">Assessing the credibility of <a href=https://en.wikipedia.org/wiki/Online_content>online content</a> has garnered a lot of attention lately. We focus on one such type of <a href=https://en.wikipedia.org/wiki/Online_content>online content</a>, namely <a href=https://en.wikipedia.org/wiki/Blog>weblogs</a> or <a href=https://en.wikipedia.org/wiki/Blog>blogs</a> for short. Some recent work attempted the task of automatically assessing the credibility of <a href=https://en.wikipedia.org/wiki/Blog>blogs</a>, typically via <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. However, in the case of Arabic blogs, there are hardly any datasets available that can be used to train robust <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> for this difficult task. To overcome the lack of sufficient training data, we propose deep co-learning, a semi-supervised end-to-end deep learning approach to assess the credibility of Arabic blogs. In deep co-learning, multiple weak deep neural network classifiers are trained using a small labeled dataset, and each using a different view of the data. Each one of these <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> is then used to classify unlabeled data, and its prediction is used to train the other <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> in a semi-supervised fashion. We evaluate our deep co-learning approach on an Arabic blogs dataset, and we report significant improvements in performance compared to many baselines including fully-supervised deep learning models as well as ensemble models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4616/>Construction and Annotation of the Jordan Comprehensive Contemporary Arabic Corpus (JCCA)<span class=acl-fixed-case>J</span>ordan Comprehensive Contemporary <span class=acl-fixed-case>A</span>rabic Corpus (<span class=acl-fixed-case>JCCA</span>)</a></strong><br><a href=/people/m/majdi-sawalha/>Majdi Sawalha</a>
|
<a href=/people/f/faisal-alshargi/>Faisal Alshargi</a>
|
<a href=/people/a/abdallah-alshdaifat/>Abdallah AlShdaifat</a>
|
<a href=/people/s/sane-yagi/>Sane Yagi</a>
|
<a href=/people/m/mohammad-a-qudah/>Mohammad A. Qudah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4616><div class="card-body p-3 small">To compile a modern dictionary that catalogues the words in currency, and to study linguistic patterns in the contemporary language, it is necessary to have a corpus of authentic texts that reflect current usage of the language. Although there are numerous Arabic corpora, none claims to be representative of the language in terms of the combination of geographical region, genre, subject matter, mode, and medium. This paper describes a 100-million-word corpus that takes the <a href=https://en.wikipedia.org/wiki/British_National_Corpus>British National Corpus (BNC)</a> as a model. The aim of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is to be balanced, annotated, comprehensive, and representative of contemporary Arabic as written and spoken in Arab countries today. It will be different from most others in not being heavily-dominated by the news or in mixing the classical with the modern. In this paper is an outline of the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> adopted for the design, construction, and annotation of this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. DIWAN (Alshargi and Rambow, 2015) was used to annotate a one-million-word snapshot of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. DIWAN is a dialectal word annotation tool, but we upgraded it by adding a new tag-set that is based on traditional <a href=https://en.wikipedia.org/wiki/Arabic_grammar>Arabic grammar</a> and by adding the roots and morphological patterns of nouns and verbs. Moreover, the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> we constructed covers the major spoken varieties of Arabic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4621/>Mazajak : An Online Arabic Sentiment Analyser<span class=acl-fixed-case>M</span>azajak: An Online <span class=acl-fixed-case>A</span>rabic Sentiment Analyser</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4621><div class="card-body p-3 small">Sentiment analysis (SA) is one of the most useful <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. Literature is flooding with many papers and systems addressing this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, but most of the work is focused on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In this paper, we present Mazajak, an online system for Arabic SA. The system is based on a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a>, which achieves state-of-the-art results on many Arabic dialect datasets including SemEval 2017 and ASTD. The availability of such <a href=https://en.wikipedia.org/wiki/System>system</a> should assist various applications and research that rely on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> as a tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4622/>The MADAR Shared Task on Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>MADAR</span> Shared Task on <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/s/sabit-hassan/>Sabit Hassan</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4622><div class="card-body p-3 small">In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. This shared task was organized as part of The Fourth Arabic Natural Language Processing Workshop, collocated with ACL 2019. The shared task includes two subtasks : the MADAR Travel Domain Dialect Identification subtask (Subtask 1) and the MADAR Twitter User Dialect Identification subtask (Subtask 2). This shared task is the first to target a large set of <a href=https://en.wikipedia.org/wiki/Dialect>dialect labels</a> at the city and country levels. The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project. A total of 21 teams from 15 countries participated in the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4623 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4623/>ZCU-NLP at MADAR 2019 : Recognizing Arabic Dialects<span class=acl-fixed-case>ZCU</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>MADAR</span> 2019: Recognizing <span class=acl-fixed-case>A</span>rabic Dialects</a></strong><br><a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/s/stephen-taylor/>Stephen Taylor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4623><div class="card-body p-3 small">In this paper, we present our <a href=https://en.wikipedia.org/wiki/System>systems</a> for the MADAR Shared Task : Arabic Fine-Grained Dialect Identification. The shared task consists of two subtasks. The goal of Subtask 1 (S-1) is to detect an Arabic city dialect in a given text and the goal of Subtask2 (S-2) is to predict the country of origin of a Twitter user by using tweets posted by the user. In S-1, our proposed <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a>. We use <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are later used as an input for other <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>. We also experiment with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a>, but these experiments showed that simpler <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> are more successful. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 0.658 macro F1-score and our rank is 6th out of 19 teams in S-1 and 7th in S-2 with 0.475 macro F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4624/>Simple But Not Nave : Fine-Grained Arabic Dialect Identification Using Only N-Grams<span class=acl-fixed-case>A</span>rabic Dialect Identification Using Only N-Grams</a></strong><br><a href=/people/s/sohaila-eltanbouly/>Sohaila Eltanbouly</a>
|
<a href=/people/m/may-bashendy/>May Bashendy</a>
|
<a href=/people/t/tamer-elsayed/>Tamer Elsayed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4624><div class="card-body p-3 small">This paper presents the participation of Qatar University team in MADAR shared task, which addresses the problem of sentence-level fine-grained Arabic Dialect Identification over 25 different Arabic dialects in addition to the Modern Standard Arabic. Arabic Dialect Identification is not a trivial task since different dialects share some <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>, e.g., utilizing the same <a href=https://en.wikipedia.org/wiki/Character_structure>character set</a> and some <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabularies</a>. We opted to adopt a very simple approach in terms of extracted features and classification models ; we only utilize word and character n-grams as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, and Na ve Bayes models as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Surprisingly, the simple <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approach</a> achieved non-na ve performance. The official results, reported on a <a href=https://en.wikipedia.org/wiki/Test_set>held-out testing set</a>, show that the dialect of a given sentence can be identified at an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 64.58 % by our best submitted run.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4625 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4625/>LIUM-MIRACL Participation in the MADAR Arabic Dialect Identification Shared Task<span class=acl-fixed-case>LIUM</span>-<span class=acl-fixed-case>MIRACL</span> Participation in the <span class=acl-fixed-case>MADAR</span> <span class=acl-fixed-case>A</span>rabic Dialect Identification Shared Task</a></strong><br><a href=/people/s/sameh-kchaou/>Saméh Kchaou</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich-Belguith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4625><div class="card-body p-3 small">This paper describes the joint participation of the LIUM and MIRACL Laboratories at the Arabic dialect identification challenge of the MADAR Shared Task (Bouamor et al., 2019) conducted during the Fourth Arabic Natural Language Processing Workshop (WANLP 2019). We participated to the Travel Domain Dialect Identification subtask. We built several <a href=https://en.wikipedia.org/wiki/System>systems</a> and explored different techniques including conventional <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning algorithms</a>. Deep learning approaches did not perform well on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We experimented several <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification systems</a> and we were able to identify the <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> of an input sentence with an F1-score of 65.41 % on the official test set using only the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> supplied by the shared task organizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4627 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4627/>MICHAEL : Mining Character-level Patterns for Arabic Dialect Identification (MADAR Challenge)<span class=acl-fixed-case>MICHAEL</span>: Mining Character-level Patterns for <span class=acl-fixed-case>A</span>rabic Dialect Identification (<span class=acl-fixed-case>MADAR</span> Challenge)</a></strong><br><a href=/people/d/dhaou-ghoul/>Dhaou Ghoul</a>
|
<a href=/people/g/gael-lejeune/>Gaël Lejeune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4627><div class="card-body p-3 small">We present MICHAEL, a simple lightweight method for automatic Arabic Dialect Identification on the MADAR travel domain Dialect Identification (DID). MICHAEL uses simple <a href=https://en.wikipedia.org/wiki/Character_(computing)>character-level features</a> in order to perform a pre-processing free classification. More precisely, Character N-grams extracted from the original sentences are used to train a Multinomial Naive Bayes classifier. This system achieved an official score (accuracy) of 53.25 % with 1 = N=3 but showed a much better result with character 4-grams (62.17 % accuracy).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4628 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4628/>Arabic Dialect Identification for <a href=https://en.wikipedia.org/wiki/Travel>Travel</a> and Twitter Text<span class=acl-fixed-case>A</span>rabic Dialect Identification for Travel and <span class=acl-fixed-case>T</span>witter Text</a></strong><br><a href=/people/p/pruthwik-mishra/>Pruthwik Mishra</a>
|
<a href=/people/v/vandan-mujadia/>Vandan Mujadia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4628><div class="card-body p-3 small">This paper presents the results of the experiments done as a part of MADAR Shared Task in WANLP 2019 on Arabic Fine-Grained Dialect Identification. Dialect Identification is one of the prominent tasks in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural language processing</a> where the subsequent <a href=https://en.wikipedia.org/wiki/Modular_programming>language modules</a> can be improved based on it. We explored the use of different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> like char, word n-gram, language model probabilities, etc on different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Results show that these <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> help to improve dialect classification accuracy. Results also show that traditional machine learning classifier tends to perform better when compared to <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> on this task in a low resource setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4629 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4629/>Mawdoo3 AI at MADAR Shared Task : Arabic Tweet Dialect Identification<span class=acl-fixed-case>AI</span> at <span class=acl-fixed-case>MADAR</span> Shared Task: <span class=acl-fixed-case>A</span>rabic Tweet Dialect Identification</a></strong><br><a href=/people/b/bashar-talafha/>Bashar Talafha</a>
|
<a href=/people/w/wael-farhan/>Wael Farhan</a>
|
<a href=/people/a/ahmed-altakrouri/>Ahmed Altakrouri</a>
|
<a href=/people/h/hussein-al-natsheh/>Hussein Al-Natsheh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4629><div class="card-body p-3 small">Arabic dialect identification is an inherently complex problem, as Arabic dialect taxonomy is convoluted and aims to dissect a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous space</a> rather than a <a href=https://en.wikipedia.org/wiki/Discrete_space>discrete one</a>. In this work, we present machine and deep learning approaches to predict 21 fine-grained dialects form a set of given tweets per user. We adopted numerous feature extraction methods most of which showed improvement in the final model, such as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, <a href=https://en.wikipedia.org/wiki/Tf-idf>Tf-idf</a>, and other tweet features. Our results show that a simple LinearSVC can outperform any complex deep learning model given a set of curated features. With a relatively complex user voting mechanism, we were able to achieve a Macro-Averaged F1-score of 71.84 % on MADAR shared subtask-2. Our best submitted <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> ranked second out of all participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4633.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4633 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4633 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4633/>The SMarT Classifier for Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>SM</span>ar<span class=acl-fixed-case>T</span> Classifier for <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/k/karima-meftouh/>Karima Meftouh</a>
|
<a href=/people/k/karima-abidi/>Karima Abidi</a>
|
<a href=/people/s/salima-harrat/>Salima Harrat</a>
|
<a href=/people/k/kamel-smaili/>Kamel Smaili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4633><div class="card-body p-3 small">This paper describes the approach adopted by the SMarT research group to build a dialect identification system in the framework of the Madar shared task on Arabic fine-grained dialect identification. We experimented several approaches, but we finally decided to use a <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Multinomial Naive Bayes classifier</a> based on word and character ngrams in addition to the language model probabilities. We achieved a score of 67.73 % in terms of Macro accuracy and a macro-averaged F1-score of 67.31 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4634.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4634 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4634 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4634/>JHU System Description for the MADAR Arabic Dialect Identification Shared Task<span class=acl-fixed-case>JHU</span> System Description for the <span class=acl-fixed-case>MADAR</span> <span class=acl-fixed-case>A</span>rabic Dialect Identification Shared Task</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a>
|
<a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4634><div class="card-body p-3 small">Our submission to the MADAR shared task on Arabic dialect identification employed a language modeling technique called Prediction by Partial Matching, an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models. We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2. Interestingly, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundant information</a>, and future work may focus on encouraging decorrelated learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4636 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4636/>A Character Level Convolutional BiLSTM for Arabic Dialect Identification<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> for <span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/m/mohamed-elaraby/>Mohamed Elaraby</a>
|
<a href=/people/a/ahmed-zahran/>Ahmed Zahran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4636><div class="card-body p-3 small">In this paper, we describe CU-RAISA teamcontribution to the 2019Madar shared task2, which focused on Twitter User fine-grained dialect identification. Among par-ticipating teams, our system ranked the4th(with 61.54 %) F1-Macro measure. Our sys-tem is trained using a character level convo-lutional bidirectional long-short-term memorynetwork trained on 2k users&#8217; data. We showthat training on concatenated user tweets asinput is further superior to training on usertweets separately and assign user&#8217;s label on themode of user&#8217;s tweets&#8217; predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4638.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4638 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4638 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4638/>Team JUST at the MADAR Shared Task on Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>JUST</span> at the <span class=acl-fixed-case>MADAR</span> Shared Task on <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/b/bashar-talafha/>Bashar Talafha</a>
|
<a href=/people/a/ali-fadel/>Ali Fadel</a>
|
<a href=/people/m/mahmoud-al-ayyoub/>Mahmoud Al-Ayyoub</a>
|
<a href=/people/y/yaser-jararweh/>Yaser Jararweh</a>
|
<a href=/people/m/mohammad-al-smadi/>Mohammad AL-Smadi</a>
|
<a href=/people/p/patrick-juola/>Patrick Juola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4638><div class="card-body p-3 small">In this paper, we describe our team&#8217;s effort on the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. The task requires building a <a href=https://en.wikipedia.org/wiki/System>system</a> capable of differentiating between 25 different <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> in addition to <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>MSA</a>. Our <a href=https://en.wikipedia.org/wiki/Tactic_(method)>approach</a> is simple. After preprocessing the data, we use Data Augmentation (DA) to enlarge the training data six times. We then build a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and extract n-gram word-level and character-level TF-IDF features and feed them into an MNB classifier. Despite its simplicity, the resulting model performs really well producing the 4th highest <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> and region-level accuracy and the 5th highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, <a href=https://en.wikipedia.org/wiki/Precision_recall>recall</a>, city-level accuracy and country-level accuracy among the participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4639.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4639 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4639/>QC-GO Submission for MADAR Shared Task : Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>QC</span>-<span class=acl-fixed-case>GO</span> Submission for <span class=acl-fixed-case>MADAR</span> Shared Task: <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/m/mohamed-eldesouki/>Mohamed Eldesouki</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4639><div class="card-body p-3 small">This paper describes the QC-GO team submission to the MADAR Shared Task Subtask 1 (travel domain dialect identification) and Subtask 2 (Twitter user location identification). In our participation in both subtasks, we explored a number of approaches and system combinations to obtain the best performance for both tasks. These include <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural nets</a> and <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>. Since individual <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> suffer from various shortcomings, the combination of different approaches was able to fill some of these gaps. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves F1-Scores of 66.1 % and 67.0 % on the development sets for Subtasks 1 and 2 respectively.</div></div></div><hr><div id=w19-47><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-47.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-47/>Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4700/>Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</a></strong><br><a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a>
|
<a href=/people/l/lars-borin/>Lars Borin</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4704/>Evaluation of Semantic Change of Harm-Related Concepts in Psychology</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/s/sean-murphy/>Sean Murphy</a>
|
<a href=/people/n/nicholas-haslam/>Nicholas Haslam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4704><div class="card-body p-3 small">The paper focuses on diachronic evaluation of semantic changes of harm-related concepts in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a>. More specifically, we investigate a hypothesis that certain concepts such as <a href=https://en.wikipedia.org/wiki/Addiction>addiction</a>, <a href=https://en.wikipedia.org/wiki/Bullying>bullying</a>, <a href=https://en.wikipedia.org/wiki/Harassment>harassment</a>, <a href=https://en.wikipedia.org/wiki/Prejudice>prejudice</a>, and <a href=https://en.wikipedia.org/wiki/Psychological_trauma>trauma</a> became broader during the last four decades. We evaluate semantic changes using two <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> : an LSA-based model from Sagi et al. (2009) and a diachronic adaptation of <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> from Hamilton et al. (2016), that are trained on a large corpus of journal abstracts covering the period of 1980 2019. Several <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> showed evidence of broadening. Addiction moved from physiological dependency on a substance to include psychological dependency on <a href=https://en.wikipedia.org/wiki/Video_game_culture>gaming</a> and the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a>. Similarly, <a href=https://en.wikipedia.org/wiki/Harassment>harassment</a> and <a href=https://en.wikipedia.org/wiki/Psychological_trauma>trauma</a> shifted towards more psychological meanings. On the other hand, <a href=https://en.wikipedia.org/wiki/Bullying>bullying</a> has transformed into a more victim-related concept and expanded to new areas such as <a href=https://en.wikipedia.org/wiki/Workplace_bullying>workplaces</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4707/>GASC : Genre-Aware Semantic Change for Ancient Greek<span class=acl-fixed-case>GASC</span>: Genre-Aware Semantic Change for <span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>G</span>reek</a></strong><br><a href=/people/v/valerio-perrone/>Valerio Perrone</a>
|
<a href=/people/m/marco-palma/>Marco Palma</a>
|
<a href=/people/s/simon-hengchen/>Simon Hengchen</a>
|
<a href=/people/a/alessandro-vatri/>Alessandro Vatri</a>
|
<a href=/people/j/jim-q-smith/>Jim Q. Smith</a>
|
<a href=/people/b/barbara-mcgillivray/>Barbara McGillivray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4707><div class="card-body p-3 small">Word meaning changes over time, depending on linguistic and extra-linguistic factors. Associating a word&#8217;s correct meaning in its historical context is a central challenge in diachronic research, and is relevant to a range of NLP tasks, including <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> and <a href=https://en.wikipedia.org/wiki/Semantic_search>semantic search</a> in historical texts. Bayesian models for <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> have emerged as a powerful tool to address this challenge, providing explicit and interpretable representations of <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change phenomena</a>. However, while corpora typically come with rich metadata, existing models are limited by their inability to exploit <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> (such as text genre) beyond the document time-stamp. This is particularly critical in the case of ancient languages, where lack of data and long diachronic span make it harder to draw a clear distinction between <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> (the fact that a word has several senses) and <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> (the process of acquiring, losing, or changing senses), and current systems perform poorly on these languages. We develop GASC, a dynamic semantic change model that leverages categorical metadata about the texts&#8217; genre to boost inference and uncover the evolution of meanings in Ancient Greek corpora. In a new evaluation framework, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves improved <a href=https://en.wikipedia.org/wiki/Predictive_analytics>predictive performance</a> compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4709/>A Method to Automatically Identify Diachronic Variation in Collocations.</a></strong><br><a href=/people/m/marcos-garcia/>Marcos Garcia</a>
|
<a href=/people/m/marcos-garcia-salido/>Marcos García Salido</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4709><div class="card-body p-3 small">This paper introduces a novel method to track collocational variations in diachronic corpora that can identify several changes undergone by these phraseological combinations and to propose alternative solutions found in later periods. The <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> consists of extracting syntactically-related candidates of collocations and ranking them using statistical association measures. Then, starting from the first period of the corpus, the system tracks each combination over time, verifying different types of historical variation such as the loss of one or both lemmas, the disappearance of the collocation, or its diachronic frequency trend. Using a distributional semantics strategy, it also proposes other linguistic structures which convey similar meanings to those extinct collocations. A case study on historical corpora of <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> shows that the system speeds up and facilitates the finding of some diachronic changes and phraseological shifts that are harder to identify without using automated methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4710/>Written on Leaves or in Stones? : Computational Evidence for the Era of Authorship of Old Thai Prose<span class=acl-fixed-case>T</span>hai Prose</a></strong><br><a href=/people/a/attapol-rutherford/>Attapol Rutherford</a>
|
<a href=/people/s/santhawat-thanyawong/>Santhawat Thanyawong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4710><div class="card-body p-3 small">We aim to provide computational evidence for the era of authorship of two important old Thai texts : Traiphumikatha and Pumratchatham. The era of authorship of these two books is still an ongoing debate among Thai literature scholars. Analysis of old Thai texts present a challenge for standard natural language processing techniques, due to the lack of corpora necessary for building old Thai word and syllable segmentation. We propose an accurate and interpretable model to classify each segment as one of the three eras of authorship (Sukhothai, Ayuddhya, or Rattanakosin) without sophisticated linguistic preprocessing. Contrary to previous hypotheses, our <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> suggests that both books were written during the <a href=https://en.wikipedia.org/wiki/Sukhothai_Kingdom>Sukhothai era</a>. Moreover, the second half of the Pumratchtham is uncharacteristic of the <a href=https://en.wikipedia.org/wiki/Sukhothai_Kingdom>Sukhothai era</a>, which may have confounded literary scholars in the past. Further, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> reveals that the most indicative linguistic changes stem from unidirectional grammaticalized words and polyfunctional words, which show up as most dominant features in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4711/>Identifying Temporal Trends Based on Perplexity and Clustering : Are We Looking at Language Change?</a></strong><br><a href=/people/s/sidsel-boldsen/>Sidsel Boldsen</a>
|
<a href=/people/m/manex-agirrezabal/>Manex Agirrezabal</a>
|
<a href=/people/p/patrizia-paggio/>Patrizia Paggio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4711><div class="card-body p-3 small">In this work we propose a data-driven methodology for identifying temporal trends in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of medieval charters</a>. We have used perplexities derived from RNNs as a distance measure between documents and then, performed <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> on those distances. We argue that <a href=https://en.wikipedia.org/wiki/Perplexity>perplexities</a> calculated by such <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are representative of temporal trends. The <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> produced using the <a href=https://en.wikipedia.org/wiki/K-Means_algorithm>K-Means algorithm</a> give an insight of the differences in language in different time periods at least partly due to <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>. We suggest that the temporal distribution of the individual clusters might provide a more nuanced picture of temporal trends compared to discrete bins, thus providing better results when used in a classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4713/>Predicting Historical Phonetic Features using Deep Neural Networks : A Case Study of the Phonetic System of Proto-Indo-European<span class=acl-fixed-case>P</span>roto-<span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>E</span>uropean</a></strong><br><a href=/people/f/frederik-hartmann/>Frederik Hartmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4713><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a> lacks the possibility to empirically assess its assumptions regarding the phonetic systems of past languages and language stages since most current methods rely on comparative tools to gain insights into phonetic features of sounds in proto- or ancestor languages. The paper at hand presents a computational method based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to predict <a href=https://en.wikipedia.org/wiki/Phonetics>phonetic features</a> of historical sounds where the exact quality is unknown and to test the overall coherence of reconstructed historical phonetic features. The method utilizes the principles of coarticulation, local predictability and statistical phonological constraints to predict <a href=https://en.wikipedia.org/wiki/Phoneme>phonetic features</a> by the features of their immediate phonetic environment. The validity of this method will be assessed using New High German phonetic data and its specific application to <a href=https://en.wikipedia.org/wiki/Historical_linguistics>diachronic linguistics</a> will be demonstrated in a case study of the phonetic system Proto-Indo-European.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4714/>ParHistVis : Visualization of Parallel Multilingual Historical Data<span class=acl-fixed-case>P</span>ar<span class=acl-fixed-case>H</span>ist<span class=acl-fixed-case>V</span>is: Visualization of Parallel Multilingual Historical Data</a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/r/rebecca-kehlbeck/>Rebecca Kehlbeck</a>
|
<a href=/people/r/rita-sevastjanova/>Rita Sevastjanova</a>
|
<a href=/people/k/katharina-kaiser/>Katharina Kaiser</a>
|
<a href=/people/g/georg-a-kaiser/>Georg A. Kaiser</a>
|
<a href=/people/m/miriam-butt/>Miriam Butt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4714><div class="card-body p-3 small">The study of <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> through <a href=https://en.wikipedia.org/wiki/Parallel_corpora>parallel corpora</a> can be advantageous for the analysis of complex interactions between time, text domain and language. Often, those advantages can not be fully exploited due to the sparse but high-dimensional nature of such historical data. To tackle this challenge, we introduce ParHistVis : a novel, free, easy-to-use, interactive visualization tool for parallel, multilingual, diachronic and synchronic linguistic data. We illustrate the suitability of the components of the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> based on a use case of word order change in Romance wh-interrogatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4716/>DiaHClust : an Iterative Hierarchical Clustering Approach for Identifying Stages in Language Change<span class=acl-fixed-case>D</span>ia<span class=acl-fixed-case>HC</span>lust: an Iterative Hierarchical Clustering Approach for Identifying Stages in Language Change</a></strong><br><a href=/people/c/christin-schatzle/>Christin Schätzle</a>
|
<a href=/people/h/hannah-booth/>Hannah Booth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4716><div class="card-body p-3 small">Language change is often assessed against a set of pre-determined time periods in order to be able to trace its diachronic trajectory. This is problematic, since a pre-determined periodization might obscure significant developments and lead to false assumptions about the data. Moreover, these <a href=https://en.wikipedia.org/wiki/Time>time periods</a> can be based on factors which are either arbitrary or non-linguistic, e.g., dividing the corpus data into equidistant stages or taking into account language-external events. Addressing this problem, in this paper we present a data-driven approach to <a href=https://en.wikipedia.org/wiki/Periodization>periodization</a> : &#8216;DiaHClust&#8217;. DiaHClust is based on iterative hierarchical clustering and offers a multi-layered perspective on change from text-level to broader time periods. We demonstrate the usefulness of DiaHClust via a case study investigating <a href=https://en.wikipedia.org/wiki/Syntactic_change>syntactic change</a> in <a href=https://en.wikipedia.org/wiki/Icelandic_language>Icelandic</a>, modelling the syntactic system of the language in terms of vectors of syntactic change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4718 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4718/>Times Are Changing : Investigating the Pace of Language Change in Diachronic Word Embeddings</a></strong><br><a href=/people/s/stephanie-brandl/>Stephanie Brandl</a>
|
<a href=/people/d/david-lassner/>David Lassner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4718><div class="card-body p-3 small">We propose Word Embedding Networks, a novel method that is able to learn word embeddings of individual data slices while simultaneously aligning and ordering them without feeding temporal information a priori to the model. This gives us the opportunity to analyse the dynamics in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on a large scale in a purely data-driven manner. In experiments on two different newspaper corpora, the New York Times (English) and die Zeit (German), we were able to show that time actually determines the dynamics of semantic change. However, there is by no means a uniform evolution, but instead times of faster and times of slower change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4720 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4720/>Studying Laws of Semantic Divergence across Languages using Cognate Sets</a></strong><br><a href=/people/a/ana-uban/>Ana Uban</a>
|
<a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4720><div class="card-body p-3 small">Semantic divergence in related languages is a key concern of <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. Intra-lingual semantic shift has been previously studied in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>, but this can only provide a limited picture of the evolution of word meanings, which often develop in a multilingual environment. In this paper we investigate <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> across languages by measuring the <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a> of cognate words in multiple languages. By comparing current meanings of cognates in different languages, we hope to uncover information about their previous meanings, and about how they diverged in their respective languages from their common original etymon. We further study the properties of their semantic divergence, by analyzing how the features of words such as frequency and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> are related to the divergence in their meaning, and thus make the first steps towards formulating laws of cross-lingual semantic change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4721.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4721 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4721 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4721" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4721/>Detecting Syntactic Change Using a Neural Part-of-Speech Tagger</a></strong><br><a href=/people/w/william-merrill/>William Merrill</a>
|
<a href=/people/g/gigi-stark/>Gigi Stark</a>
|
<a href=/people/r/robert-frank/>Robert Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4721><div class="card-body p-3 small">We train a diachronic long short-term memory (LSTM) part-of-speech tagger on a large corpus of American English from the 19th, 20th, and 21st centuries. We analyze the <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a>&#8217;s ability to implicitly learn temporal structure between years, and the extent to which this knowledge can be transferred to date new sentences. The learned year embeddings show a strong linear correlation between their first principal component and time. We show that temporal information encoded in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be used to predict novel sentences&#8217; years of composition relatively well. Comparisons to a feedforward baseline suggest that the temporal change learned by the LSTM is syntactic rather than purely lexical. Thus, our results suggest that our <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a> is implicitly learning to model <a href=https://en.wikipedia.org/wiki/Syntactic_change>syntactic change</a> in <a href=https://en.wikipedia.org/wiki/American_English>American English</a> over the course of the 19th, 20th, and early 21st centuries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4723 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4723/>Spatio-Temporal Prediction of Dialectal Variant Usage</a></strong><br><a href=/people/p/peter-jeszenszky/>Péter Jeszenszky</a>
|
<a href=/people/p/panote-siriaraya/>Panote Siriaraya</a>
|
<a href=/people/p/philipp-stoeckle/>Philipp Stoeckle</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4723><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Frequency_distribution>distribution</a> of most <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>dialectal variants</a> have not only spatial but also temporal patterns. Based on the &#8216;apparent time hypothesis&#8217;, much of dialect change is happening through younger speakers accepting innovations. Thus, synchronic diversity can be interpreted diachronically. With the assumption of the &#8216;contact effect&#8217;, i.e. contact possibility (contact and isolation) between speaker communities being responsible for <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>, and the apparent time hypothesis, we aim to predict the usage of dialectal variants. In this paper we model the contact possibility based on two of the most important factors in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> to be affecting <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> : <a href=https://en.wikipedia.org/wiki/Ageing>age</a> and <a href=https://en.wikipedia.org/wiki/Distance>distance</a>. The first steps of the approach involve modeling contact possibility using a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic predictor</a>, taking the age of respondents into account. We test the global, and the local role of age for variation where the local level means spatial subsets around each survey site, chosen based on k nearest neighbors. The prediction approach is tested on Swiss German syntactic survey data, featuring multiple respondents from different age cohorts at survey sites. The results show the relative success of the logistic prediction approach and the limitations of the <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, therefore further proposals are made to develop the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>.<i>global</i>, and the <i>local</i> role of age for variation where the local level means spatial subsets around each survey site, chosen based on <i>k</i> nearest neighbors. The prediction approach is tested on Swiss German syntactic survey data, featuring multiple respondents from different age cohorts at survey sites. The results show the relative success of the logistic prediction approach and the limitations of the method, therefore further proposals are made to develop the methodology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4724 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4724" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4724/>One-to-X Analogical Reasoning on Word Embeddings : a Case for Diachronic Armed Conflict Prediction from News Texts<span class=acl-fixed-case>X</span> Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4724><div class="card-body p-3 small">We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type &#8216;location : armed-group&#8217; based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of <a href=https://en.wikipedia.org/wiki/Trigonometric_functions>cosine distance</a> to decrease the number of false positives ; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4726.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4726 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4726 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4726/>Semantic Change in the Language of UK Parliamentary Debates<span class=acl-fixed-case>UK</span> Parliamentary Debates</a></strong><br><a href=/people/g/gavin-abercrombie/>Gavin Abercrombie</a>
|
<a href=/people/r/riza-theresa-batista-navarro/>Riza Batista-Navarro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4726><div class="card-body p-3 small">We investigate changes in the meanings of words used in the <a href=https://en.wikipedia.org/wiki/Parliament_of_the_United_Kingdom>UK Parliament</a> across two different epochs. We use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to explore changes in the distribution of words of interest and uncover words that appear to have undergone <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic transformation</a> in the intervening period, and explore different ways of obtaining target words for this purpose. We find that <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic changes</a> are generally in line with those found in other corpora, and little evidence that <a href=https://en.wikipedia.org/wiki/Parliamentary_language>parliamentary language</a> is more static than <a href=https://en.wikipedia.org/wiki/General_English>general English</a>. It also seems that words with senses that have been recorded in the <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a> as having fallen into disuse do not undergo semantic changes in this domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4727.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4727 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4727 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4727" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4727/>Semantic Change and Emerging Tropes In a Large Corpus of New High German Poetry<span class=acl-fixed-case>N</span>ew <span class=acl-fixed-case>H</span>igh <span class=acl-fixed-case>G</span>erman Poetry</a></strong><br><a href=/people/t/thomas-haider/>Thomas Haider</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4727><div class="card-body p-3 small">Due to its semantic succinctness and novelty of expression, <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a> is a great test-bed for semantic change analysis. However, so far there is a scarcity of large diachronic corpora. Here, we provide a large corpus of <a href=https://en.wikipedia.org/wiki/German_poetry>German poetry</a> which consists of about 75k <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> with more than 11 million tokens, with <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> ranging from the 16th to early 20th century. We then track <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> in this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by investigating the rise of <a href=https://en.wikipedia.org/wiki/Trope_(literature)>tropes</a> (&#8216;love is magic&#8217;) over time and detecting change points of meaning, which we find to occur particularly within the <a href=https://en.wikipedia.org/wiki/German_Romanticism>German Romantic period</a>. Additionally, through <a href=https://en.wikipedia.org/wiki/Self-similarity>self-similarity</a>, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4728 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4728/>Conceptual Change and Distributional Semantic Models : an Exploratory Study on Pitfalls and Possibilities</a></strong><br><a href=/people/p/pia-sommerauer/>Pia Sommerauer</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4728><div class="card-body p-3 small">Studying <a href=https://en.wikipedia.org/wiki/Conceptual_change>conceptual change</a> using embedding models has become increasingly popular in the Digital Humanities community while critical observations about them have received less attention. This paper investigates what the impact of known pitfalls can be on the conclusions drawn in a digital humanities study through the use case of <a href=https://en.wikipedia.org/wiki/Racism>Racism</a>. In addition, we suggest an approach for modeling a <a href=https://en.wikipedia.org/wiki/Complexity>complex concept</a> in terms of words and relations representative of the <a href=https://en.wikipedia.org/wiki/Conceptual_system>conceptual system</a>. Our results show that different models created from the same data yield different results, but also indicate that using different model architectures, comparing different corpora and comparing to control words and relations can help to identify which results are solid and which may be due to artefact. We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4730.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4730 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4730 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4730/>Towards Automatic Variant Analysis of Ancient Devotional Texts</a></strong><br><a href=/people/a/amir-hazem/>Amir Hazem</a>
|
<a href=/people/b/beatrice-daille/>Béatrice Daille</a>
|
<a href=/people/d/dominique-stutzmann/>Dominique Stutzmann</a>
|
<a href=/people/j/jacob-currie/>Jacob Currie</a>
|
<a href=/people/c/christine-jacquin/>Christine Jacquin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4730><div class="card-body p-3 small">We address in this paper the issue of text reuse in liturgical manuscripts of the middle ages. More specifically, we study variant readings of the Obsecro Te prayer, part of the devotional Books of Hours often used by Christians as guidance for their daily prayers. We aim at automatically extracting and categorising pairs of words and expressions that exhibit <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>variant relations</a>. For this purpose, we adopt a linguistic classification that allows to better characterize the <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>variants</a> than edit operations. Then, we study the evolution of Obsecro Te texts from a temporal and geographical axis. Finally, we contrast several unsupervised state-of-the-art approaches for the automatic extraction of Obsecro Te variants. Based on the manual observation of 772 Obsecro Te copies which show more than 21,000 variants, we show that the proposed methodology is helpful for an automatic study of variants and may serve as basis to analyze and to depict useful information from devotional texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4731.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4731 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4731 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4731/>Understanding the Evolution of Circular Economy through Language Change</a></strong><br><a href=/people/s/sampriti-mahanty/>Sampriti Mahanty</a>
|
<a href=/people/f/frank-boons/>Frank Boons</a>
|
<a href=/people/j/julia-handl/>Julia Handl</a>
|
<a href=/people/r/riza-theresa-batista-navarro/>Riza Theresa Batista-Navarro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4731><div class="card-body p-3 small">In this study, we propose to focus on understanding the evolution of a specific scientific conceptthat of Circular Economy (CE)by analysing how the language used in academic discussions has changed semantically. It is worth noting that the meaning and central theme of this <a href=https://en.wikipedia.org/wiki/Concept>concept</a> has remained the same ; however, we hypothesise that it has undergone <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> by way of additional layers being added to the <a href=https://en.wikipedia.org/wiki/Concept>concept</a>. We have shown that semantic change in language is a reflection of shifts in scientific ideas, which in turn help explain the evolution of a <a href=https://en.wikipedia.org/wiki/Concept>concept</a>. Focusing on the CE concept, our analysis demonstrated that the change over time in the language used in academic discussions of CE is indicative of the way in which the <a href=https://en.wikipedia.org/wiki/Concept>concept</a> evolved and expanded.</div></div></div><hr><div id=w19-48><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-48.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-48/>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4800/>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></strong><br><a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4802" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4802/>Sentiment Analysis Is Not Solved ! Assessing and Probing Sentiment Classification</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4802><div class="card-body p-3 small">Neural methods for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> have led to quantitative improvements over previous approaches, but these advances are not always accompanied with a thorough analysis of the qualitative differences. Therefore, it is not clear what outstanding conceptual challenges for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> remain. In this work, we attempt to discover what challenges still prove a problem for sentiment classifiers for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and to provide a challenging <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We collect the subset of sentences that an (oracle) ensemble of state-of-the-art sentiment classifiers misclassify and then annotate them for 18 linguistic and paralinguistic phenomena, such as <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modality</a>, etc. Finally, we provide a case study that demonstrates the usefulness of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to probe the performance of a given sentiment classifier with respect to <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic phenomena</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4805/>Multi-Granular Text Encoding for Self-Explaining Categorization</a></strong><br><a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/l/lin-pan/>Lin Pan</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/y/yousef-el-kurdi/>Yousef El-Kurdi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4805><div class="card-body p-3 small">Self-explaining text categorization requires a <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> to make a prediction along with supporting evidence. A popular type of evidence is sub-sequences extracted from the input text which are sufficient for the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to make the prediction. In this work, we define multi-granular ngrams as basic units for explanation, and organize all <a href=https://en.wikipedia.org/wiki/Ngram>ngrams</a> into a hierarchical structure, so that shorter <a href=https://en.wikipedia.org/wiki/Ngram>ngrams</a> can be reused while computing longer <a href=https://en.wikipedia.org/wiki/Ngram>ngrams</a>. We leverage the tree-structured LSTM to learn a context-independent representation for each unit via parameter sharing. Experiments on <a href=https://en.wikipedia.org/wiki/Medical_classification>medical disease classification</a> show that our model is more accurate, efficient and compact than the BiLSTM and CNN baselines. More importantly, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can extract intuitive multi-granular evidence to support its predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4806/>The Meaning of Most for Visual Question Answering Models</a></strong><br><a href=/people/a/alexander-kuhnle/>Alexander Kuhnle</a>
|
<a href=/people/a/ann-copestake/>Ann Copestake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4806><div class="card-body p-3 small">The correct interpretation of quantifier statements in the context of a <a href=https://en.wikipedia.org/wiki/Visual_system>visual scene</a> requires non-trivial <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference mechanisms</a>. For the example of most, we discuss two <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> which rely on fundamentally different <a href=https://en.wikipedia.org/wiki/Cognition>cognitive concepts</a>. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber&#8217;s law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4809/>Detecting Political Bias in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>News Articles</a> Using Headline Attention</a></strong><br><a href=/people/r/rama-rohit-reddy-gangula/>Rama Rohit Reddy Gangula</a>
|
<a href=/people/s/suma-reddy-duggenpudi/>Suma Reddy Duggenpudi</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4809><div class="card-body p-3 small">Language is a powerful tool which can be used to state the facts as well as express our views and perceptions. Most of the times, we find a subtle bias towards or against someone or something. When it comes to <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, media houses and journalists are known to create bias by shrewd means such as misinterpreting reality and distorting viewpoints towards some parties. This misinterpretation on a large scale can lead to the production of <a href=https://en.wikipedia.org/wiki/News_bias>biased news</a> and <a href=https://en.wikipedia.org/wiki/Conspiracy_theory>conspiracy theories</a>. Automating bias detection in newspaper articles could be a good challenge for research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We proposed a headline attention network for this bias detection. Our model has two distinctive characteristics : (i) it has a structure that mirrors a person&#8217;s way of reading a news article (ii) it has attention mechanism applied on the article based on its headline, enabling it to attend to more critical content to predict bias. As the required datasets were not available, we created a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprising of 1329 <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> collected from various <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_India>Telugu newspapers</a> and marked them for bias towards a particular political party. The experiments conducted on it demonstrated that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms various baseline methods by a substantial margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4810/>Testing the Generalization Power of Neural Network Models across NLI Benchmarks<span class=acl-fixed-case>NLI</span> Benchmarks</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4810><div class="card-body p-3 small">Neural network models have been very successful in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, with the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> reaching 90 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in some benchmarks. However, the success of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> turns out to be largely benchmark specific. We show that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on a natural language inference dataset drawn from one benchmark fail to perform well in others, even if the notion of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> assumed in these <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> is the same or similar. We train six high performing neural network models on different datasets and show that each one of these has problems of generalizing when we replace the original test set with a test set taken from another corpus designed for the same task. In light of these results, we argue that most of the current neural network models are not able to generalize well in the task of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>. We find that using large pre-trained language models helps with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> when the datasets are similar enough. Our results also highlight that the current NLI datasets do not cover the different nuances of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> extensively enough.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4811" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4811/>Character Eyes : Seeing Language through Character-Level Taggers</a></strong><br><a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/m/marc-marone/>Marc Marone</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4811><div class="card-body p-3 small">Character-level models have been used extensively in recent years in NLP tasks as both supplements and replacements for closed-vocabulary token-level word representations. In one popular architecture, character-level LSTMs are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (POS) tags. In this work, we examine the behavior of POS taggers across languages from the perspective of individual hidden units within the character LSTM. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4812.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4812 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4812 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4812/>Faithful Multimodal Explanation for Visual Question Answering</a></strong><br><a href=/people/j/jialin-wu/>Jialin Wu</a>
|
<a href=/people/r/raymond-mooney/>Raymond Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4812><div class="card-body p-3 small">AI systems&#8217; ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a> such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods using both automated metrics and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4813" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4813/>Evaluating Recurrent Neural Network Explanations</a></strong><br><a href=/people/l/leila-arras/>Leila Arras</a>
|
<a href=/people/a/ahmed-osman/>Ahmed Osman</a>
|
<a href=/people/k/klaus-robert-muller/>Klaus-Robert Müller</a>
|
<a href=/people/w/wojciech-samek/>Wojciech Samek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4813><div class="card-body p-3 small">Recently, several methods have been proposed to explain the predictions of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNNs)</a>, in particular of <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a>. The goal of these methods is to understand the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a>&#8217;s decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction. In previous works, some of these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> were not yet compared to one another, or were evaluated only qualitatively. We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations. Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4816 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4816" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4816/>Modeling Paths for Explainable Knowledge Base Completion</a></strong><br><a href=/people/j/josua-stadelmaier/>Josua Stadelmaier</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4816><div class="card-body p-3 small">A common approach in knowledge base completion (KBC) is to learn representations for entities and relations in order to infer missing facts by generalizing existing ones. A shortcoming of standard models is that they do not explain their predictions to make them verifiable easily to human inspection. In this paper, we propose the Context Path Model (CPM) which generates explanations for new facts in KBC by providing sets of context paths as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a <a href=https://en.wikipedia.org/wiki/Wrapper_function>wrapper</a> that can be applied on top of various existing KBC models. We evaluate <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness.<i>context paths</i> as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a wrapper that can be applied on top of various existing KBC models. We evaluate it for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4817 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4817/>Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a><span class=acl-fixed-case>F</span>rench and <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/p/paola-merlo/>Paola Merlo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4817><div class="card-body p-3 small">The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising <a href=https://en.wikipedia.org/wiki/Language>human languages</a>. Results are at present inconclusive. In this paper, we extend previous work on <a href=https://en.wikipedia.org/wiki/Long-distance_dependencies>long-distance dependencies</a> in three ways. We manipulate <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to translate them in a space that is attuned to the linguistic properties under study. We extend the <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> to <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> and to <a href=https://en.wikipedia.org/wiki/Formal_language>new languages</a>. We confirm previous negative results : <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> do not unequivocally encode fine-grained linguistic properties of long-distance dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4819.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4819 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4819 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4819/>Hierarchical Representation in Neural Language Models : Suppression and Recovery of Expectations</a></strong><br><a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a>
|
<a href=/people/r/richard-futrell/>Richard Futrell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4819><div class="card-body p-3 small">Work using artificial languages as training input has shown that LSTMs are capable of inducing the stack-like data structures required to represent <a href=https://en.wikipedia.org/wiki/Context-free_language>context-free</a> and certain mildly context-sensitive languages formal language classes which correspond in theory to the hierarchical structures of natural language. Here we present a suite of experiments probing whether neural language models trained on linguistic data induce these stack-like data structures and deploy them while incrementally predicting words. We study two natural language phenomena : center embedding sentences and syntactic island constraints on the fillergap dependency. In order to properly predict words in these structures, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4821.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4821 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4821 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4821" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4821/>An LSTM Adaptation Study of (Un)grammaticality<span class=acl-fixed-case>LSTM</span> Adaptation Study of (Un)grammaticality</a></strong><br><a href=/people/s/shammur-absar-chowdhury/>Shammur Absar Chowdhury</a>
|
<a href=/people/r/roberto-zamparelli/>Roberto Zamparelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4821><div class="card-body p-3 small">We propose a novel approach to the study of how <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural network</a> perceive the distinction between grammatical and ungrammatical sentences, a crucial task in the growing field of synthetic linguistics. The method is based on performance measures of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> trained on corpora and fine-tuned with either <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>grammatical or ungrammatical sentences</a>, then applied to (different types of) <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>grammatical or ungrammatical sentences</a>. The results show that both in the difficult and highly symmetrical task of detecting subject islands and in the more open CoLA dataset, grammatical sentences give rise to better scores than ungrammatical ones, possibly because they can be better integrated within the body of linguistic structural knowledge that the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> has accumulated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4825.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4825 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4825 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4825" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4825/>Open Sesame : Getting inside BERT’s Linguistic Knowledge<span class=acl-fixed-case>BERT</span>’s Linguistic Knowledge</a></strong><br><a href=/people/y/yongjie-lin/>Yongjie Lin</a>
|
<a href=/people/y/yi-chern-tan/>Yi Chern Tan</a>
|
<a href=/people/r/robert-frank/>Robert Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4825><div class="card-body p-3 small">How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT&#8217;s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT&#8217;s representation of subject-verb agreement and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphor-antecedent dependencies</a> through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT&#8217;s representations do indeed model linguistically relevant aspects of <a href=https://en.wikipedia.org/wiki/Hierarchical_structure>hierarchical structure</a>, though they do not appear to show the sharp sensitivity to <a href=https://en.wikipedia.org/wiki/Hierarchical_structure>hierarchical structure</a> that is found in human processing of reflexive anaphora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4826.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4826 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4826 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4826/>GEval : Tool for Debugging NLP Datasets and Models<span class=acl-fixed-case>GE</span>val: Tool for Debugging <span class=acl-fixed-case>NLP</span> Datasets and Models</a></strong><br><a href=/people/f/filip-gralinski/>Filip Graliński</a>
|
<a href=/people/a/anna-wroblewska/>Anna Wróblewska</a>
|
<a href=/people/t/tomasz-stanislawek/>Tomasz Stanisławek</a>
|
<a href=/people/k/kamil-grabowski/>Kamil Grabowski</a>
|
<a href=/people/t/tomasz-gorecki/>Tomasz Górecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4826><div class="card-body p-3 small">This paper presents a simple but general and effective method to debug the output of machine learning (ML) supervised models, including <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> looks for <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that lower the evaluation metric in such a way that it can not be ascribed to chance (as measured by their p-values). Using this method implemented as MLEval tool you can find : (1) anomalies in test sets, (2) issues in preprocessing, (3) problems in the ML model itself. It can give you an insight into what can be improved in the datasets and/or the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The same <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can be used to compare ML models or different versions of the same <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We present the <a href=https://en.wikipedia.org/wiki/Tool>tool</a>, the theory behind it and use cases for text-based models of various types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4827.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4827 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4827 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4827/>From Balustrades to Pierre Vinken : Looking for Syntax in Transformer Self-Attentions</a></strong><br><a href=/people/d/david-marecek/>David Mareček</a>
|
<a href=/people/r/rudolf-rosa/>Rudolf Rosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4827><div class="card-body p-3 small">We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> to existing constituency treebanks, both manually and by computing <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and recall.</div></div></div><hr><div id=w19-49><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-49.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-49/>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4900/>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a></strong><br><a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p></div><hr><div id=w19-50><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-50.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-50/>Proceedings of the 18th BioNLP Workshop and Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5000/>Proceedings of the 18th BioNLP Workshop and Shared Task</a></strong><br><a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5002/>Learning from the Experience of Doctors : Automated Diagnosis of Appendicitis Based on Clinical Notes</a></strong><br><a href=/people/s/steven-kester-yuwono/>Steven Kester Yuwono</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a>
|
<a href=/people/k/kee-yuan-ngiam/>Kee Yuan Ngiam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5002><div class="card-body p-3 small">The objective of this work is to develop an automated diagnosis system that is able to predict the probability of appendicitis given a free-text emergency department (ED) note and additional structured information (e.g., lab test results). Our clinical corpus consists of about 180,000 ED notes based on ten years of patient visits to the Accident and Emergency (A&E) Department of the National University Hospital (NUH), Singapore. We propose a novel neural network approach that learns to diagnose <a href=https://en.wikipedia.org/wiki/Appendicitis>acute appendicitis</a> based on doctors&#8217; free-text ED notes without any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. On a test set of 2,000 ED notes with equal number of appendicitis (positive) and non-appendicitis (negative) diagnosis and in which all the negative ED notes only consist of abdominal-related diagnosis, our model is able to achieve a promising F_0.5-score of 0.895 while <a href=https://en.wikipedia.org/wiki/Emergency_department>ED doctors</a> achieve F_0.5-score of 0.900. Visualization shows that our model is able to learn important features, signs, and symptoms of patients from unstructured free-text ED notes, which will help doctors to make better diagnosis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5003/>A Paraphrase Generation System for EHR Question Answering<span class=acl-fixed-case>EHR</span> Question Answering</a></strong><br><a href=/people/s/sarvesh-soni/>Sarvesh Soni</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5003><div class="card-body p-3 small">This paper proposes a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and method for automatically generating paraphrases for clinical questions relating to patient-specific information in electronic health records (EHRs). Crowdsourcing is used to collect 10,578 unique questions across 946 semantically distinct paraphrase clusters. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is then used with a deep learning-based question paraphrasing method utilizing variational autoencoder and LSTM encoder / decoder. The ultimate use of such a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is to improve the performance of automatic question answering methods for <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5006/>Transfer Learning in Biomedical Natural Language Processing : An Evaluation of BERT and ELMo on Ten Benchmarking Datasets<span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>ELM</span>o on Ten Benchmarking Datasets</a></strong><br><a href=/people/y/yifan-peng/>Yifan Peng</a>
|
<a href=/people/s/shankai-yan/>Shankai Yan</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5006><div class="card-body p-3 small">Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> consists of five tasks with ten <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on <a href=https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor>BERT</a> and ELMo and find that the <a href=https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor>BERT model</a> pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp / BLUE_Benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5007/>Combining Structured and Free-text Electronic Medical Record Data for Real-time Clinical Decision Support</a></strong><br><a href=/people/e/emilia-apostolova/>Emilia Apostolova</a>
|
<a href=/people/t/tony-wang/>Tony Wang</a>
|
<a href=/people/t/tim-tschampel/>Tim Tschampel</a>
|
<a href=/people/i/ioannis-koutroulis/>Ioannis Koutroulis</a>
|
<a href=/people/t/tom-velez/>Tom Velez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5007><div class="card-body p-3 small">The goal of this work is to utilize Electronic Medical Record (EMR) data for real-time Clinical Decision Support (CDS). We present a deep learning approach to combining in real time available diagnosis codes (ICD codes) and free-text notes : Patient Context Vectors. Patient Context Vectors are created by averaging ICD code embeddings, and by predicting the same from free-text notes via a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a>. The Patient Context Vectors were then simply appended to available structured data (vital signs and lab results) to build prediction models for a specific condition. Experiments on predicting ARDS, a rare and complex condition, demonstrate the utility of Patient Context Vectors as a means of summarizing the patient history and overall condition, and improve significantly the prediction model results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5010/>Deep Contextualized Biomedical Abbreviation Expansion</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/j/jinling-liu/>Jinling Liu</a>
|
<a href=/people/x/xinghua-lu/>Xinghua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5010><div class="card-body p-3 small">Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications, such as <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> and question answering systems. In this paper, we present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model. DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a>. Then it utilizes BioELMo to extract the contextualized features of words, and feed those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to abbreviation-specific bidirectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions. Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It also surpasses human performance for expanding a sample abbreviation, and remains robust in imbalanced, low-resources and clinical settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5011/>RNN Embeddings for Identifying Difficult to Understand Medical Words<span class=acl-fixed-case>RNN</span> Embeddings for Identifying Difficult to Understand Medical Words</a></strong><br><a href=/people/h/hanna-pylieva/>Hanna Pylieva</a>
|
<a href=/people/a/artem-chernodub/>Artem Chernodub</a>
|
<a href=/people/n/natalia-grabar/>Natalia Grabar</a>
|
<a href=/people/t/thierry-hamon/>Thierry Hamon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5011><div class="card-body p-3 small">Patients and their families often require a better understanding of medical information provided by doctors. We currently address this issue by improving the identification of difficult to understand medical words. We introduce novel embeddings received from RNN-FrnnMUTE (French RNN Medical Understandability Text Embeddings) which allow to reach up to 87.0 F1 score in identification of difficult words. We also note that adding pre-trained FastText word embeddings to the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature set</a> substantially improves the performance of the model which classifies words according to their difficulty. We study the generalizability of different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> through three cross-validation scenarios which allow testing <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> in real-world conditions : understanding of medical words by new users, and classification of new unseen words by the automatic models. The RNN-FrnnMUTE embeddings and the categorization code are being made available for the research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5012/>A distantly supervised dataset for <a href=https://en.wikipedia.org/wiki/Data_extraction>automated data extraction</a> from diagnostic studies</a></strong><br><a href=/people/c/christopher-norman/>Christopher Norman</a>
|
<a href=/people/m/mariska-leeflang/>Mariska Leeflang</a>
|
<a href=/people/r/rene-spijker/>René Spijker</a>
|
<a href=/people/e/evangelos-kanoulas/>Evangelos Kanoulas</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5012><div class="card-body p-3 small">Systematic reviews are important in <a href=https://en.wikipedia.org/wiki/Evidence-based_medicine>evidence based medicine</a>, but are expensive to produce. Automating or semi-automating the data extraction of index test, target condition, and reference standard from articles has the potential to decrease the cost of conducting <a href=https://en.wikipedia.org/wiki/Systematic_review>systematic reviews</a> of diagnostic test accuracy, but relevant training data is not available. We create a distantly supervised dataset of approximately 90,000 sentences, and let two experts manually annotate a small subset of around 1,000 sentences for evaluation. We evaluate the performance of BioBERT and logistic regression for <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> the sentences, and compare the performance for distant and direct supervision. Our results suggest that distant supervision can work as well as, or better than direct supervision on this problem, and that distantly trained models can perform as well as, or better than human annotators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5015/>A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics</a></strong><br><a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/r/ross-sparks/>Ross Sparks</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a>
|
<a href=/people/c/c-raina-macintyre/>C Raina MacIntyre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5015><div class="card-body p-3 small">Distributed representations of text can be used as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> when training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>statistical classifier</a>. These <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems : influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and <a href=https://en.wikipedia.org/wiki/FLAIR>FLAIR</a> are better than Word2Vec, <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a> and the two adapted using the MESH ontology. There is an improvement of 2-4 % in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> when these context-based representations are used instead of word-based representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5021/>Annotating Temporal Information in Clinical Notes for Timeline Reconstruction : Towards the Definition of Calendar Expressions</a></strong><br><a href=/people/n/natalia-viani/>Natalia Viani</a>
|
<a href=/people/h/hegler-tissot/>Hegler Tissot</a>
|
<a href=/people/a/ariane-bernardino/>Ariane Bernardino</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5021><div class="card-body p-3 small">To automatically analyse complex trajectory information enclosed in clinical text (e.g. timing of symptoms, duration of treatment), it is important to understand the related temporal aspects, anchoring each event on an absolute point in time. In the clinical domain, few temporally annotated corpora are currently available. Moreover, underlying annotation schemas-which mainly rely on the TimeML standard-are not necessarily easily applicable for applications such as patient timeline reconstruction. In this work, we investigated how temporal information is documented in clinical text by annotating a corpus of medical reports with time expressions (TIMEXes), based on <a href=https://en.wikipedia.org/wiki/TimeML>TimeML</a>. The developed <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is available to the NLP community. Starting from our annotations, we analysed the suitability of the TimeML TIMEX schema for capturing timeline information, identifying challenges and possible solutions. As a result, we propose a novel annotation schema that could be useful for timeline reconstruction : CALendar EXpression (CALEX).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5023/>Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding<span class=acl-fixed-case>PIO</span> Element Detection in Medical Text Using Contextualized Embedding</a></strong><br><a href=/people/h/hichem-mezaoui/>Hichem Mezaoui</a>
|
<a href=/people/i/isuru-gunasekara/>Isuru Gunasekara</a>
|
<a href=/people/a/aleksandr-gontcharov/>Aleksandr Gontcharov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5023><div class="card-body p-3 small">In this paper, we presented an improved <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to extract PIO elements, from abstracts of medical papers, that reduces <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>. The proposed technique was used to build a dataset of PIO elements that we call <a href=https://en.wikipedia.org/wiki/Piconet>PICONET</a>. We further proposed a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a>, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context. Furthermore, to enhance the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the model, we have investigated an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>text features</a> were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> when using the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with one of the base learners and the case where we combine the base learners along with the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. We obtained the highest score in terms of <a href=https://en.wikipedia.org/wiki/Analysis_of_covariance>AUC</a> when we combine the base learners. The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to <a href=https://en.wikipedia.org/wiki/Medical_literature>full medical articles</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5025 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5025/>Can Character Embeddings Improve Cause-of-Death Classification for Verbal Autopsy Narratives?</a></strong><br><a href=/people/z/zhaodong-yan/>Zhaodong Yan</a>
|
<a href=/people/s/serena-jeblee/>Serena Jeblee</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5025><div class="card-body p-3 small">We present two models for combining word and character embeddings for cause-of-death classification of verbal autopsy reports using the text of the narratives. We find that for smaller datasets (500 to 1000 records), adding character information to the model improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, making character-based CNNs a promising method for automated verbal autopsy coding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5026/>Is artificial data useful for biomedical Natural Language Processing algorithms?</a></strong><br><a href=/people/z/zixu-wang/>Zixu Wang</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5026><div class="card-body p-3 small">A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility. This problem can be addressed by generating medical data artificially. Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited. We propose a generic <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to guide the generation of clinical text with key phrases. We use the artificial data as additional training data in two key biomedical NLP tasks : <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and temporal relation extraction. We show that artificially generated training data used in conjunction with real training data can lead to performance boosts for data-greedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5027 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5027" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5027/>ChiMed : A Chinese Medical Corpus for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a><span class=acl-fixed-case>C</span>hi<span class=acl-fixed-case>M</span>ed: A <span class=acl-fixed-case>C</span>hinese Medical Corpus for Question Answering</a></strong><br><a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/w/weicheng-ma/>Weicheng Ma</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/y/yan-song/>Yan Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5027><div class="card-body p-3 small">Question answering (QA) is a challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>, especially when it is applied to specific domains. While <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained in the <a href=https://en.wikipedia.org/wiki/Domain_(biology)>general domain</a> can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such <a href=https://en.wikipedia.org/wiki/Data>data</a> are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed ; second we annotate a small fraction of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to check the quality of the answers ; third, we extract two datasets from the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and use them for the relevancy prediction task and the adoption prediction task. Several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark models</a> are applied to the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, producing good results for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5038 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5038/>Extracting relations between outcomes and <a href=https://en.wikipedia.org/wiki/Statistical_significance>significance levels</a> in Randomized Controlled Trials (RCTs) publications<span class=acl-fixed-case>RCT</span>s) publications</a></strong><br><a href=/people/a/anna-koroleva/>Anna Koroleva</a>
|
<a href=/people/p/patrick-paroubek/>Patrick Paroubek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5038><div class="card-body p-3 small">Randomized controlled trials assess the effects of an experimental intervention by comparing it to a control intervention with regard to some variables-trial outcomes. Statistical hypothesis testing is used to test if the <a href=https://en.wikipedia.org/wiki/Design_of_experiments>experimental intervention</a> is superior to the <a href=https://en.wikipedia.org/wiki/Scientific_control>control</a>. Statistical significance is typically reported for the measured outcomes and is an important characteristic of the results. We propose a machine learning approach to automatically extract reported outcomes, <a href=https://en.wikipedia.org/wiki/Statistical_significance>significance levels</a> and the relation between them. We annotated a corpus of 663 sentences with 2,552 outcome-significance level relations (1,372 positive and 1,180 negative relations). We compared several <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, using a manually crafted feature set, and a number of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. The best performance (F-measure of 94 %) was shown by the BioBERT fine-tuned model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5039 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5039/>Overview of the MEDIQA 2019 Shared Task on Textual Inference, <a href=https://en.wikipedia.org/wiki/Question_answering>Question Entailment</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a><span class=acl-fixed-case>MEDIQA</span> 2019 Shared Task on Textual Inference, Question Entailment and Question Answering</a></strong><br><a href=/people/a/asma-ben-abacha/>Asma Ben Abacha</a>
|
<a href=/people/c/chaitanya-shivade/>Chaitanya Shivade</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5039><div class="card-body p-3 small">This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks : Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98 % in the NLI task, 74.9 % in the RQE task, and 78.3 % in the QA task. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and the participants&#8217; approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> in the medical domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5043 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5043/>Surf at MEDIQA 2019 : Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model<span class=acl-fixed-case>MEDIQA</span> 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model</a></strong><br><a href=/people/j/jiin-nam/>Jiin Nam</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5043><div class="card-body p-3 small">While deep learning techniques have shown promising results in many natural language processing (NLP) tasks, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has not been widely applied to the clinical domain. The lack of <a href=https://en.wikipedia.org/wiki/Data_set>large datasets</a> and the pervasive use of <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific language</a> (i.e. abbreviations and acronyms) in the clinical domain causes slower progress in NLP tasks than that of the general NLP tasks. To fill this gap, we employ word / subword-level based models that adopt large-scale data-driven methods such as pre-trained language models and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> in analyzing text for the clinical domain. Empirical results demonstrate the superiority of the proposed <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>methods</a> by achieving 90.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>medical domain natural language inference task</a>. Furthermore, we inspect the independent strengths of the proposed approaches in quantitative and qualitative manners. This analysis will help researchers to select necessary components in building <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5044 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5044/>WTMED at MEDIQA 2019 : A Hybrid Approach to Biomedical Natural Language Inference<span class=acl-fixed-case>WTMED</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: A Hybrid Approach to Biomedical Natural Language Inference</a></strong><br><a href=/people/z/zhaofeng-wu/>Zhaofeng Wu</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/s/sicong-huang/>Sicong Huang</a>
|
<a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5044><div class="card-body p-3 small">Natural language inference (NLI) is challenging, especially when <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is applied to technical domains such as <a href=https://en.wikipedia.org/wiki/Biomedical_sciences>biomedical settings</a>. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5045 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5045/>KU_ai at MEDIQA 2019 : Domain-specific Pre-training and Transfer Learning for Medical NLI<span class=acl-fixed-case>KU</span>_ai at <span class=acl-fixed-case>MEDIQA</span> 2019: Domain-specific Pre-training and Transfer Learning for Medical <span class=acl-fixed-case>NLI</span></a></strong><br><a href=/people/c/cemil-cengiz/>Cemil Cengiz</a>
|
<a href=/people/u/ulas-sert/>Ulaş Sert</a>
|
<a href=/people/d/deniz-yuret/>Deniz Yuret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5045><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> and results submitted for the Natural Language Inference (NLI) track of the MEDIQA 2019 Shared Task. As KU_ai team, we used BERT as our baseline model and pre-processed the MedNLI dataset to mitigate the negative impact of de-identification artifacts. Moreover, we investigated different pre-training and transfer learning approaches to improve the performance. We show that pre-training the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> on rich biomedical corpora has a significant effect in teaching the model domain-specific language. In addition, training the model on large NLI datasets such as MultiNLI and SNLI helps in learning task-specific reasoning. Finally, we ensembled our highest-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and achieved 84.7 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the unseen test dataset and ranked 10th out of 17 teams in the official results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5048 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5048/>Dr. Quad at MEDIQA 2019 : Towards Textual Inference and Question Entailment using contextualized representations<span class=acl-fixed-case>D</span>r.<span class=acl-fixed-case>Q</span>uad at <span class=acl-fixed-case>MEDIQA</span> 2019: Towards Textual Inference and Question Entailment using contextualized representations</a></strong><br><a href=/people/v/vinayshekhar-bannihatti-kumar/>Vinayshekhar Bannihatti Kumar</a>
|
<a href=/people/a/ashwin-srinivasan/>Ashwin Srinivasan</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/j/james-route/>James Route</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5048><div class="card-body p-3 small">This paper presents the submissions by TeamDr. Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on the prior work Liu et al. (2019) which uses a multi-task objective function for <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> through <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> is a powerful strategy for addressing challenges posed specialized domains such as <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5049 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5049/>Sieg at MEDIQA 2019 : Multi-task Neural Ensemble for Biomedical Inference and Entailment<span class=acl-fixed-case>MEDIQA</span> 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment</a></strong><br><a href=/people/s/sai-abishek-bhaskar/>Sai Abishek Bhaskar</a>
|
<a href=/people/r/rashi-rungta/>Rashi Rungta</a>
|
<a href=/people/j/james-route/>James Route</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5049><div class="card-body p-3 small">This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomedical domain. Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH. We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn better biomedical feature representations. Our final models for the NLI and RQE tasks achieve the 4th and 2nd rank on the shared-task leaderboard respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5052 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5052/>MSIT_SRIB at MEDIQA 2019 : Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain.<span class=acl-fixed-case>MSIT</span>_<span class=acl-fixed-case>SRIB</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain.</a></strong><br><a href=/people/s/sahil-chopra/>Sahil Chopra</a>
|
<a href=/people/a/ankita-gupta/>Ankita Gupta</a>
|
<a href=/people/a/anupama-kaushik/>Anupama Kaushik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5052><div class="card-body p-3 small">In this paper, we present Biomedical Multi-Task Deep Neural Network (Bio-MTDNN) on the NLI task of MediQA 2019 challenge. Bio-MTDNN utilizes transfer learning based paradigm where not only the source and target domains are different but also the source and target tasks are varied, although related. Further, Bio-MTDNN integrates knowledge from external sources such as clinical databases (UMLS) enhancing its performance on the clinical domain. Our proposed method outperformed the official baseline and other prior models (such as ESIM and Infersent on dev set) by a considerable margin as evident from our experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5056 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5056/>IITP at MEDIQA 2019 : Systems Report for Natural Language Inference, Question Entailment and Question Answering<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: Systems Report for Natural Language Inference, Question Entailment and Question Answering</a></strong><br><a href=/people/d/dibyanayan-bandyopadhyay/>Dibyanayan Bandyopadhyay</a>
|
<a href=/people/b/baban-gain/>Baban Gain</a>
|
<a href=/people/t/tanik-saikh/>Tanik Saikh</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5056><div class="card-body p-3 small">This paper presents the experiments accomplished as a part of our participation in the MEDIQA challenge, an (Abacha et al., 2019) shared task. We participated in all the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> defined in this particular <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are viz. i. Natural Language Inference (NLI) ii. Recognizing Question Entailment(RQE) and their application in medical Question Answering (QA). We submitted runs using multiple deep learning based systems (runs) for each of these three tasks. We submitted five <a href=https://en.wikipedia.org/wiki/System>system</a> results in each of the NLI and RQE tasks, and four system results for the QA task. The <a href=https://en.wikipedia.org/wiki/System>systems</a> yield encouraging results in all the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The highest performance obtained in NLI, RQE and QA tasks are 81.8 %, 53.2 %, and 71.7 %, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5057 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5057/>LasigeBioTM at MEDIQA 2019 : Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition<span class=acl-fixed-case>L</span>asige<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>TM</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition</a></strong><br><a href=/people/a/andre-lamurias/>Andre Lamurias</a>
|
<a href=/people/f/francisco-m-couto/>Francisco M Couto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5057><div class="card-body p-3 small">Biomedical Question Answering (QA) aims at providing automated answers to user questions, regarding a variety of biomedical topics. For example, these questions may ask for related to <a href=https://en.wikipedia.org/wiki/Disease>diseases</a>, <a href=https://en.wikipedia.org/wiki/Drug>drugs</a>, <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a>, or <a href=https://en.wikipedia.org/wiki/Medical_procedure>medical procedures</a>. Automated biomedical QA systems could improve the retrieval of information necessary to answer these questions. The MEDIQA challenge consisted of three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> concerning various aspects of biomedical QA. This challenge aimed at advancing approaches to Natural Language Inference (NLI) and Recognizing Question Entailment (RQE), which would then result in enhanced approaches to biomedical QA. Our approach explored a common Transformer-based architecture that could be applied to each <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. This approach shared the same pre-trained weights, but which were then fine-tuned for each task using the provided training data. Furthermore, we augmented the training data with external datasets and enriched the question and answer texts using MER, a named entity recognition tool. Our approach obtained high levels of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, in particular on the NLI task, which classified pairs of text according to their relation. For the QA task, we obtained higher Spearman&#8217;s rank correlation values using the entities recognized by MER.</div></div></div><hr><div id=w19-51><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-51.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-51/>Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5100/>Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra Escartín</a>
|
<a href=/people/f/francis-bond/>Francis Bond</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5101/>When the whole is greater than the sum of its parts : Multiword expressions and <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a></a></strong><br><a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5101><div class="card-body p-3 small">Multiword expressions (MWEs) feature prominently in the mental lexicon of native speakers (Jackendoff, 1997) in all languages and domains, from informal to technical contexts (Biber et al., 1999) with about four MWEs being produced per minute of discourse (Glucksberg, 1989). MWEs come in all shapes and forms, including idioms like rock the boat (as cause problems or disturb a situation) and compound nouns like monkey business (as dishonest behaviour). Their accurate detection and understanding may often require more than knowledge about individual words and how they can be combined (Fillmore, 1979), as they may display various degrees of idiosyncrasy, including lexical, syntactic, semantic and statistical (Sag et al., 2002 ; Baldwin and Kim, 2010), which provide new challenges and opportunities for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a> (Constant et al., 2017). For instance, while for some combinations the meaning can be inferred from their parts like olive oil (oil made of olives) this is not always the case, as in dark horse (meaning an unknown candidate who unexpectedly succeeds), and when processing a sentence some of the challenges are to identify which words form an expression (Ramisch, 2015), and whether the expression is idiomatic (Cordeiro et al., 2019). In this talk I will give an overview of advances on the identification and treatment of multiword expressions, in particular concentrating on techniques for identifying their degree of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5102/>Hear about Verbal Multiword Expressions in the Bulgarian and the Romanian Wordnets Straight from the Horse’s Mouth<span class=acl-fixed-case>B</span>ulgarian and the <span class=acl-fixed-case>R</span>omanian Wordnets Straight from the Horse’s Mouth</a></strong><br><a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/i/ivelina-stoyanova/>Ivelina Stoyanova</a>
|
<a href=/people/s/svetlozara-leseva/>Svetlozara Leseva</a>
|
<a href=/people/m/maria-mitrofan/>Maria Mitrofan</a>
|
<a href=/people/t/tsvetana-dimitrova/>Tsvetana Dimitrova</a>
|
<a href=/people/m/maria-todorova/>Maria Todorova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5102><div class="card-body p-3 small">In this paper we focus on verbal multiword expressions (VMWEs) in <a href=https://en.wikipedia.org/wiki/Bulgarian_language>Bulgarian</a> and Romanian as reflected in the wordnets of the two languages. The <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of VMWEs relies on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> defined within the PARSEME Cost Action. After outlining the properties of various types of VMWEs, a cross-language comparison is drawn, aimed to highlight the similarities and the differences between <a href=https://en.wikipedia.org/wiki/Bulgarian_language>Bulgarian</a> and Romanian with respect to the <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a> and distribution of VMWEs. The contribution of this work is in outlining essential features of the description and classification of VMWEs and the cross-language comparison at the lexical level, which is essential for the understanding of the need for uniform annotation guidelines and a viable procedure for validation of the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5103/>The Romanian Corpus Annotated with Verbal Multiword Expressions<span class=acl-fixed-case>R</span>omanian Corpus Annotated with Verbal Multiword Expressions</a></strong><br><a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/m/mihaela-cristescu/>Mihaela Cristescu</a>
|
<a href=/people/m/mihaela-plamada-onofrei/>Mihaela Onofrei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5103><div class="card-body p-3 small">This paper reports on the Romanian journalistic corpus annotated with verbal multiword expressions following the PARSEME guidelines. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is sentence split, tokenized, part-of-speech tagged, lemmatized, syntactically annotated and verbal multiword expressions are identified and classified. It offers insights into the frequency of such Romanian word combinations and allows for their characterization. We offer data about the types of verbal multiword expressions in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and some of their characteristics, such as internal structure, diversity in the corpus, average length, productivity of the verbs. This is a language resource that is important per se, as well as for the task of automatic multiword expressions identification, which can be further used in other systems. It was already used as training and test material in the shared tasks for the automatic identification of verbal multiword expressions organized by PARSEME.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5106/>Unsupervised Compositional Translation of Multiword Expressions</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a>
|
<a href=/people/m/marcos-garcia/>Marcos Garcia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5106><div class="card-body p-3 small">This article describes a dependency-based strategy that uses compositional distributional semantics and cross-lingual word embeddings to translate multiword expressions (MWEs). Our unsupervised approach performs <a href=https://en.wikipedia.org/wiki/Translation>translation</a> as a process of word contextualization by taking into account lexico-syntactic contexts and selectional preferences. This <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> is suited to translate phraseological combinations and phrases whose constituent words are lexically restricted by each other. Several experiments in adjective-noun and verb-object compounds show that mutual contextualization (co-compositionality) clearly outperforms other compositional methods. The paper also contributes with a new freely available dataset of English-Spanish MWEs used to validate the proposed compositional strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5110/>Without <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, multiword expression identification will never fly : A position statement</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/s/silvio-cordeiro/>Silvio Cordeiro</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5110><div class="card-body p-3 small">Because most multiword expressions (MWEs), especially verbal ones, are semantically non-compositional, their automatic identification in running text is a prerequisite for semantically-oriented downstream applications. However, recent developments, driven notably by the PARSEME shared task on automatic identification of verbal MWEs, show that this task is harder than related tasks, despite recent contributions both in multilingual corpus annotation and in computational models. In this paper, we analyse possible reasons for this state of affairs. They lie in the nature of the MWE phenomenon, as well as in its <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional properties</a>. We also offer a comparative analysis of the state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a>, which exhibit particularly strong sensitivity to unseen data. On this basis, we claim that, in order to make strong headway in MWE identification, the community should bend its mind into coupling identification of MWEs with their discovery, via syntactic MWE lexicons. Such lexicons need not necessarily achieve a linguistically complete modelling of MWEs&#8217; behavior, but they should provide minimal morphosyntactic information to cover some potential uses, so as to complement existing MWE-annotated corpora. We define requirements for such minimal NLP-oriented lexicon, and we propose a roadmap for the MWE community driven by these requirements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5112/>Semantic Modelling of Adjective-Noun Collocations Using FrameNet<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/y/yana-strakatova/>Yana Strakatova</a>
|
<a href=/people/e/erhard-hinrichs/>Erhard Hinrichs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5112><div class="card-body p-3 small">In this paper we argue that <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>Frame Semantics</a> (Fillmore, 1982) provides a good framework for semantic modelling of adjective-noun collocations. More specifically, the notion of a frame is rich enough to account for nouns from different semantic classes and to model semantic relations that hold between an adjective and a <a href=https://en.wikipedia.org/wiki/Noun>noun</a> in terms of Frame Elements. We have substantiated these findings by considering a sample of adjective-noun collocations from <a href=https://en.wikipedia.org/wiki/German_language>German</a> such as enger Freund &#8216;close friend&#8217; and starker Regen &#8216;heavy rain&#8217;. The data sample is taken from different semantic fields identified in the German wordnet GermaNet (Hamp and Feldweg, 1997 ; Henrich and Hinrichs, 2010). The study is based on the electronic dictionary DWDS (Klein and Geyken, 2010) and uses the collocation extraction tool Wortprofil (Geyken et al., 2009). The FrameNet modelling is based on the online resource available at http://framenet.icsi.berkeley.edu. Since FrameNets are available for a range of typologically different languages, it is feasible to extend the current case study to other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5114/>Confirming the Non-compositionality of Idioms for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/a/alyssa-hwang/>Alyssa Hwang</a>
|
<a href=/people/c/christopher-hidey/>Christopher Hidey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5114><div class="card-body p-3 small">An <a href=https://en.wikipedia.org/wiki/Idiom>idiom</a> is defined as a non-compositional multiword expression, one whose meaning can not be deduced from the definitions of the component words. This definition does not explicitly define the compositionality of an <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom&#8217;s sentiment</a> ; this paper aims to determine whether the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> of the component words of an <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom</a> is related to the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> of that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom</a>. We use the Dictionary of Affect in Language augmented by <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> to give each idiom in the Sentiment Lexicon of IDiomatic Expressions (SLIDE) a component-wise sentiment score and compare it to the phrase-level sentiment label crowdsourced by the creators of SLIDE. We find that there is no discernible relation between these two measures of idiom sentiment. This supports the hypothesis that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> are not compositional for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> along with <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and motivates further work in handling <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5115/>IDION : A database for Modern Greek multiword expressions<span class=acl-fixed-case>IDION</span>: A database for <span class=acl-fixed-case>M</span>odern <span class=acl-fixed-case>G</span>reek multiword expressions</a></strong><br><a href=/people/s/stella-markantonatou/>Stella Markantonatou</a>
|
<a href=/people/p/panagiotis-minos/>Panagiotis Minos</a>
|
<a href=/people/g/george-zakis/>George Zakis</a>
|
<a href=/people/v/vassiliki-moutzouri/>Vassiliki Moutzouri</a>
|
<a href=/people/m/maria-chantou/>Maria Chantou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5115><div class="card-body p-3 small">We report on the ongoing development of IDION, a web resource of richly documented multiword expressions (MWEs) of Modern Greek addressed to the human user and to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. IDION contains about 2000 verb MWEs (VMWEs) of which about 850 are fully documented as regards their syntactic flexibility, their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and the semantic relations with other VMWEs. Sets of synonymous MWEs are defined in a bottom-up manner revealing the conceptual organization of the MG VMWE domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5118/>Evaluating Automatic Term Extraction Methods on Individual Documents</a></strong><br><a href=/people/a/antonio-sajatovic/>Antonio Šajatović</a>
|
<a href=/people/m/maja-buljan/>Maja Buljan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/b/bojana-dalbelo-basic/>Bojana Dalbelo Bašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5118><div class="card-body p-3 small">Automatic Term Extraction (ATE) extracts terminology from <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific corpora</a>. ATE is used in many NLP tasks, including <a href=https://en.wikipedia.org/wiki/Computer-aided_translation>Computer Assisted Translation</a>, where it is typically applied to individual documents rather than the entire corpus. While corpus-level ATE has been extensively evaluated, it is not obvious how the results transfer to document-level ATE. To fill this gap, we evaluate 16 state-of-the-art ATE methods on full-length documents from three different domains, on both corpus and document levels. Unlike existing studies, our evaluation is more realistic as we take into account all gold terms. We show that no single method is best in corpus-level ATE, but C-Value and KeyConceptRelatendess surpass others in document-level ATE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5119 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5119/>Cross-lingual Transfer Learning and <a href=https://en.wikipedia.org/wiki/Multitask_learning>Multitask Learning</a> for Capturing Multiword Expressions</a></strong><br><a href=/people/s/shiva-taslimipoor/>Shiva Taslimipoor</a>
|
<a href=/people/o/omid-rohanian/>Omid Rohanian</a>
|
<a href=/people/l/le-an-ha/>Le An Ha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5119><div class="card-body p-3 small">Recent developments in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have prompted a surge of interest in the application of multitask and transfer learning to NLP problems. In this study, we explore for the first time, the application of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning (TRL)</a> and <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning (MTL)</a> to the identification of Multiword Expressions (MWEs). For MTL, we exploit the shared syntactic information between MWE and dependency parsing models to jointly train a single model on both tasks. We specifically predict two types of labels : MWE and dependency parse. Our neural MTL architecture utilises the supervision of dependency parsing in lower layers and predicts MWE tags in upper layers. In the TRL scenario, we overcome the scarcity of data by learning a model on a larger MWE dataset and transferring the knowledge to a resource-poor setting in another language. In both scenarios, the resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieved higher performance compared to standard neural approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5120/>Ilfhocail : A Lexicon of Irish MWEs<span class=acl-fixed-case>I</span>lfhocail: A Lexicon of <span class=acl-fixed-case>I</span>rish <span class=acl-fixed-case>MWE</span>s</a></strong><br><a href=/people/a/abigail-walsh/>Abigail Walsh</a>
|
<a href=/people/t/teresa-lynn/>Teresa Lynn</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5120><div class="card-body p-3 small">This paper describes the categorisation of Irish MWEs, and the construction of the first version of a lexicon of Irish MWEs for NLP purposes (Ilfhocail, meaning &#8216;Multiwords&#8217;), collected from a number of resources. For the purposes of <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality assurance</a>, 530 entries of this <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> were examined and manually annotated for <a href=https://en.wikipedia.org/wiki/Product_lifecycle>POS information</a> and <a href=https://en.wikipedia.org/wiki/Product_lifecycle>MWE category</a>.</div></div></div><hr><div id=w19-52><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-52.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-52/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5200/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5203/>Incorporating <a href=https://en.wikipedia.org/wiki/Source_code>Source Syntax</a> into Transformer-Based Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5203><div class="card-body p-3 small">Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018 ; Tang et al., 2018 ; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> into the Transformer architecture without modifying it. We introduce two methods : a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into twenty target languages, showing consistent improvements of 1.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5205/>Generalizing Back-Translation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/miguel-graca/>Miguel Graça</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/j/julian-schamper/>Julian Schamper</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5205><div class="card-body p-3 small">Back-translation data augmentation by translating target monolingual data is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling-based approaches</a> and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German-English news translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5208.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5208.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5208/>The Effect of Translationese in Machine Translation Test Sets</a></strong><br><a href=/people/m/mike-zhang/>Mike Zhang</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5208><div class="card-body p-3 small">The effect of translationese has been studied in the field of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>, mostly with respect to training data. We study in depth the effect of translationese on <a href=https://en.wikipedia.org/wiki/Test_data>test data</a>, using the test sets from the last three editions of WMT&#8217;s news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems ; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5209/>Customizing Neural Machine Translation for <a href=https://en.wikipedia.org/wiki/Subtitling>Subtitling</a></a></strong><br><a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/p/patrick-wilken/>Patrick Wilken</a>
|
<a href=/people/y/yota-georgakopoulou/>Yota Georgakopoulou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5209><div class="card-body p-3 small">In this work, we customized a neural machine translation system for translation of subtitles in the domain of <a href=https://en.wikipedia.org/wiki/Entertainment>entertainment</a>. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> learned from human segmentation decisions. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a <a href=https://en.wikipedia.org/wiki/Documentary_film>documentary</a> and a sitcom). It showed a notable productivity increase of up to 37 % as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5210/>Integration of Dubbing Constraints into <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/a/ashutosh-saboo/>Ashutosh Saboo</a>
|
<a href=/people/t/timo-baumann/>Timo Baumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5210><div class="card-body p-3 small">Translation systems aim to perform a meaning-preserving conversion of linguistic material (typically text but also speech) from a source to a target language (and, to a lesser degree, the corresponding socio-cultural contexts). Dubbing, i.e., the lip-synchronous translation and revoicing of speech adds to this constraints about the close matching of phonetic and resulting visemic synchrony characteristics of source and target material. There is an inherent conflict between a <a href=https://en.wikipedia.org/wiki/Translation>translation</a>&#8217;s meaning preservation and &#8216;dubbability&#8217; and the resulting trade-off can be controlled by weighing the synchrony constraints. We introduce our work, which to the best of our knowledge is the first of its kind, on integrating synchrony constraints into the machine translation paradigm. We present first results for the integration of synchrony constraints into encoder decoder-based neural machine translation and show that considerably more &#8216;dubbable&#8217; translations can be achieved with only a small impact on <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a>, and dubbability improves more steeply than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> degrades.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5211/>Widening the Representation Bottleneck in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Lexical Shortcuts</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5211><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> is a state-of-the-art neural translation model that uses <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. This enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical information</a> passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5212.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5212/>A High-Quality Multilingual Dataset for Structured Documentation Translation</a></strong><br><a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/r/raffaella-buschiazzo/>Raffaella Buschiazzo</a>
|
<a href=/people/j/james-bradbury/>James Bradbury</a>
|
<a href=/people/t/teresa-marshall/>Teresa Marshall</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5212><div class="card-body p-3 small">This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These <a href=https://en.wikipedia.org/wiki/Web_page>Web pages</a> have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from <a href=https://en.wikipedia.org/wiki/English_language>English</a>, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 16 translation settings. Our experiments show that learning to translate with the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>XML tags</a> improves translation accuracy, and the <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> accurately generates <a href=https://en.wikipedia.org/wiki/XML>XML structures</a>. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>.</div></div></div><hr><div id=w19-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-53/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5300/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5302.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5302/>Results of the WMT19 Metrics Shared Task : Segment-Level and Strong MT Systems Pose Big Challenges<span class=acl-fixed-case>WMT</span>19 Metrics Shared Task: Segment-Level and Strong <span class=acl-fixed-case>MT</span> Systems Pose Big Challenges</a></strong><br><a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5302><div class="card-body p-3 small">This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. 13 research groups submitted 24 metrics, 10 of which are reference-less metrics and constitute submissions to the joint task with WMT19 Quality Estimation Task, QE as a Metric. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> correlates with the WMT19 official manual ranking, and segment level, how well the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5303/>Findings of the First Shared Task on Machine Translation Robustness</a></strong><br><a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/p/paul-michel/>Paul Michel</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5303><div class="card-body p-3 small">We share the findings of the first shared task on improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>models&#8217; robustness</a> to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved large improvements over <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, with the best improvement having +22.33 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a>. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson&#8217;s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> using compare-mt, which revealed their salient differences in handling challenges in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Such analysis provides additional insights when there is occasional disagreement between <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, e.g. systems better at producing <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial expressions</a> received higher score from <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5304/>The University of Edinburgh’s Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/r/rachel-bawden/>Rachel Bawden</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/f/faheem-kirefu/>Faheem Kirefu</a>
|
<a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5304><div class="card-body p-3 small">The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions : <a href=https://en.wikipedia.org/wiki/English_language>EnglishGujarati</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>EnglishChinese</a>, <a href=https://en.wikipedia.org/wiki/German_language>GermanEnglish</a>, and <a href=https://en.wikipedia.org/wiki/Czech_language>EnglishCzech</a>. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For EnglishGujarati, we also explored <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised MT</a> with cross-lingual language model pre-training, and translation pivoting through <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. For <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to and from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, we investigated character-based tokenisation vs. sub-word segmentation of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese text</a>. For GermanEnglish, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. For EnglishCzech, we compared different preprocessing and tokenisation regimes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5305/>GTCOM Neural Machine Translation Systems for WMT19<span class=acl-fixed-case>GTCOM</span> Neural Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a>
|
<a href=/people/c/conghu-yuan/>Conghu Yuan</a>
|
<a href=/people/q/qingming-liu/>Qingming Liu</a>
|
<a href=/people/b/baoyong-fan/>Baoyong Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5305><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT19 shared news translation task. We participate in six directions : English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English. Further, we get the best BLEU scores in the directions of English to <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a> and Lithuanian to English (28.2 and 36.3 respectively) among all the participants. The submitted systems mainly focus on <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, knowledge distillation and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> to build a competitive model for this task. Also, we apply <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to filter <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>, back-translated data and parallel data. The techniques we apply for data filtering include filtering by <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a>, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5309/>DBMS-KU Interpolation for WMT19 News Translation Task<span class=acl-fixed-case>DBMS</span>-<span class=acl-fixed-case>KU</span> Interpolation for <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/s/sari-dewi-budiwati/>Sari Dewi Budiwati</a>
|
<a href=/people/a/al-hafiz-akbar-maulana-siagian/>Al Hafiz Akbar Maulana Siagian</a>
|
<a href=/people/t/tirana-noor-fatyanosa/>Tirana Noor Fatyanosa</a>
|
<a href=/people/m/masayoshi-aritsugi/>Masayoshi Aritsugi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5309><div class="card-body p-3 small">This paper presents the participation of DBMS-KU Interpolation system in WMT19 shared task, namely, Kazakh-English language pair. We examine the use of <a href=https://en.wikipedia.org/wiki/Interpolation>interpolation method</a> using a different language model order. Our Interpolation system combines a direct translation with <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> as a pivot language. We use 3-gram and 5-gram language model orders to perform the language translation in this work. To reduce noise in the pivot translation process, we prune the phrase table of source-pivot and pivot-target. Our experimental results show that our Interpolation system outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>Baseline</a> in terms of <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>BLEU-cased score</a> by +0.5 and +0.1 points in Kazakh-English and English-Kazakh, respectively. In particular, using the 5-gram language model order in our system could obtain better BLEU-cased score than utilizing the 3-gram one. Interestingly, we found that by employing the Interpolation system could reduce the perplexity score of English-Kazakh when using 3-gram language model order.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5310/>Lingua Custodia at WMT’19 : Attempts to Control Terminology<span class=acl-fixed-case>WMT</span>’19: Attempts to Control Terminology</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5310><div class="card-body p-3 small">This paper describes Lingua Custodia&#8217;s submission to the WMT&#8217;19 news shared task for German-to-French on the topic of the EU elections. We report experiments on the adaptation of the terminology of a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> to a specific topic, aimed at providing more accurate translations of specific entities like <a href=https://en.wikipedia.org/wiki/Political_party>political parties</a> and <a href=https://en.wikipedia.org/wiki/Personal_name>person names</a>, given that the shared task provided no in-domain training parallel data dealing with the restricted topic. Our primary submission to the shared task uses backtranslation generated with a type of decoding allowing the insertion of constraints in the output in order to guarantee the correct translation of specific terms that are not necessarily observed in the data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5311.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5311/>The TALP-UPC Machine Translation Systems for WMT19 News Translation Task : Pivoting Techniques for Low Resource MT<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 News Translation Task: Pivoting Techniques for Low Resource <span class=acl-fixed-case>MT</span></a></strong><br><a href=/people/n/noe-casas/>Noe Casas</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5311><div class="card-body p-3 small">In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for <a href=https://en.wikipedia.org/wiki/Kazakh_language>Kazakh-English</a>. Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively. Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5315/>UdS-DFKI Participation at WMT 2019 : Low-Resource (en-gu) and Coreference-Aware (en-de) Systems<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DFKI</span> Participation at <span class=acl-fixed-case>WMT</span> 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems</a></strong><br><a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5315><div class="card-body p-3 small">This paper describes the UdS-DFKI submission to the WMT2019 news translation task for GujaratiEnglish (low-resourced pair) and GermanEnglish (document-level evaluation). Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5316 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5316/>The IIIT-H Gujarati-English Machine Translation System for WMT19<span class=acl-fixed-case>IIIT</span>-<span class=acl-fixed-case>H</span> <span class=acl-fixed-case>G</span>ujarati-<span class=acl-fixed-case>E</span>nglish Machine Translation System for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/v/vikrant-goyal/>Vikrant Goyal</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5316><div class="card-body p-3 small">This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the GujaratiEnglish news translation shared task of WMT19. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is basedon encoder-decoder framework with <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5317/>Kingsoft’s Neural Machine Translation System for WMT19<span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/x/xinze-guo/>Xinze Guo</a>
|
<a href=/people/c/chang-liu/>Chang Liu</a>
|
<a href=/people/x/xiaolong-li/>Xiaolong Li</a>
|
<a href=/people/y/yiran-wang/>Yiran Wang</a>
|
<a href=/people/g/guoliang-li/>Guoliang Li</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a>
|
<a href=/people/z/zhitao-xu/>Zhitao Xu</a>
|
<a href=/people/l/liuyi-yang/>Liuyi Yang</a>
|
<a href=/people/l/li-ma/>Li Ma</a>
|
<a href=/people/c/changliang-li/>Changliang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5317><div class="card-body p-3 small">This paper describes the Kingsoft AI Lab&#8217;s submission to the WMT2019 news translation shared task. We participated in two language directions : <a href=https://en.wikipedia.org/wiki/English_language>English-Chinese</a> and <a href=https://en.wikipedia.org/wiki/Standard_Chinese>Chinese-English</a>. For both language directions, we trained several variants of Transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data. The best <a href=https://en.wikipedia.org/wiki/Translation>translation</a> result was obtained with <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensemble</a> and reranking techniques. According to automatic metrics (BLEU) our Chinese-English system reached the second highest score, and our English-Chinese system reached the second highest score for this subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5320.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5320/>The MLLP-UPV Supervised Machine Translation Systems for WMT19 News Translation Task<span class=acl-fixed-case>MLLP</span>-<span class=acl-fixed-case>UPV</span> Supervised Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/j/javier-iranzo-sanchez/>Javier Iranzo-Sánchez</a>
|
<a href=/people/g/goncal-garces-diaz-munio/>Gonçal Garcés Díaz-Munío</a>
|
<a href=/people/j/jorge-civera/>Jorge Civera</a>
|
<a href=/people/a/alfons-juan/>Alfons Juan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5320><div class="card-body p-3 small">This paper describes the participation of the MLLP research group of the Universitat Politcnica de Valncia in the WMT 2019 News Translation Shared Task. In this edition, we have submitted <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> for the German English and German French language pairs, participating in both directions of each pair. Our submitted systems, based on the Transformer architecture, make ample use of data filtering, synthetic data and <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> through <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5321.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5321/>Microsoft Translator at WMT 2019 : Towards Large-Scale Document-Level Neural Machine Translation<span class=acl-fixed-case>M</span>icrosoft Translator at <span class=acl-fixed-case>WMT</span> 2019: Towards Large-Scale Document-Level Neural Machine Translation</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5321><div class="card-body p-3 small">This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> seems to mainly help with translationese input. We explore <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning techniques</a>, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5322.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5322/>CUNI Submission for Low-Resource Languages in WMT News 2019<span class=acl-fixed-case>CUNI</span> Submission for Low-Resource Languages in <span class=acl-fixed-case>WMT</span> News 2019</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5322><div class="card-body p-3 small">This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages : Gujarati-English and Kazakh-English. We participated in both language pairs in both translation directions. Our system combines <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a different high-resource language pair followed by training on backtranslated monolingual data. Thanks to the simultaneous training in both directions, we can iterate the backtranslation process. We are using the Transformer model in a constrained submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5324/>A Comparison on Fine-grained Pre-trained Embeddings for the WMT19Chinese-English News Translation Task<span class=acl-fixed-case>WMT</span>19<span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish News Translation Task</a></strong><br><a href=/people/z/zhenhao-li/>Zhenhao Li</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5324><div class="card-body p-3 small">This paper describes our submission to the WMT 2019 Chinese-English (zh-en) news translation shared task. Our systems are based on RNN architectures with pre-trained embeddings which utilize character and sub-character information. We compare <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with these different granularity levels using different evaluating metics. We find that a finer granularity embeddings can help the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> according to character level evaluation and that the pre-trained embeddings can also be beneficial for <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performance marginally when the training data is limited.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5327 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5327/>Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring</a></strong><br><a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5327><div class="card-body p-3 small">This paper describes CAiRE&#8217;s submission to the unsupervised machine translation track of the WMT&#8217;19 news shared task from <a href=https://en.wikipedia.org/wiki/German_language>German</a> to <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> separately, and they are aligned using <a href=https://en.wikipedia.org/wiki/MUSE>MUSE</a> (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5330 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5330/>NICT’s Unsupervised Neural and Statistical Machine Translation Systems for the WMT19 News Translation Task<span class=acl-fixed-case>NICT</span>’s Unsupervised Neural and Statistical Machine Translation Systems for the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/h/haipeng-sun/>Haipeng Sun</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5330><div class="card-body p-3 small">This paper presents the NICT&#8217;s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction : <a href=https://en.wikipedia.org/wiki/German_language>German-Czech</a>. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (constraint&#8217;), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5333 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5333" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5333/>Facebook FAIR’s WMT19 News Translation Task Submission<span class=acl-fixed-case>F</span>acebook <span class=acl-fixed-case>FAIR</span>’s <span class=acl-fixed-case>WMT</span>19 News Translation Task Submission</a></strong><br><a href=/people/n/nathan-ng/>Nathan Ng</a>
|
<a href=/people/k/kyra-yee/>Kyra Yee</a>
|
<a href=/people/a/alexei-baevski/>Alexei Baevski</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5333><div class="card-body p-3 small">This paper describes Facebook FAIR&#8217;s submission to the WMT19 shared news translation task. We participate in four language directions, <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> and <a href=https://en.wikipedia.org/wiki/English_language>English-Russian</a> in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on domain-specific data, then decode using noisy channel model reranking. Our <a href=https://en.wikipedia.org/wiki/System>system</a> improves on our previous <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction EnglishRussian.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5335 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5335.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5335/>Tilde’s Machine Translation Systems for WMT 2019<span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/m/marcis-pinnis/>Marcis Pinnis</a>
|
<a href=/people/r/rihards-krislauks/>Rihards Krišlauks</a>
|
<a href=/people/m/matiss-rikters/>Matīss Rikters</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5335><div class="card-body p-3 small">The paper describes the development process of Tilde&#8217;s NMT systems for the WMT 2019 shared task on news translation. We trained systems for the English-Lithuanian and Lithuanian-English translation directions in constrained and unconstrained tracks. We build upon the best <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of the previous year&#8217;s competition and combine them with recent advancements in the field. We also present a new method to ensure source domain adherence in back-translated data. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved a shared first place in human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5336 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5336.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5336/>Apertium-fin-engRule-based Shallow Machine Translation for WMT 2019 Shared Task<span class=acl-fixed-case>WMT</span> 2019 Shared Task</a></strong><br><a href=/people/t/tommi-a-pirinen/>Tommi Pirinen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5336><div class="card-body p-3 small">In this paper we describe a rule-based, bi-directional machine translation system for the FinnishEnglish language pair. The baseline system was based on the existing data of FinnWordNet, omorfi and apertium-eng. We have built the <a href=https://en.wikipedia.org/wiki/Disambiguation>disambiguation</a>, lexical selection and translation rules by hand. The <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a> have been developed based on the shared task data. We describe in this article the use of the shared task data as a kind of a test-driven development workflow in RBMT development and show that it suits perfectly to a modern software engineering continuous integration workflow of RBMT and yields big increases to BLEU scores with minimal effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5337 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5337/>English-Czech Systems in WMT19 : Document-Level Transformer<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>zech Systems in <span class=acl-fixed-case>WMT</span>19: Document-Level Transformer</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/d/dominik-machacek/>Dominik Macháček</a>
|
<a href=/people/m/michal-auersperger/>Michal Auersperger</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5337><div class="card-body p-3 small">We describe our NMT systems submitted to the WMT19 shared task in EnglishCzech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2 T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2 T implementation, this document-level-trained system achieves a +0.6 BLEU improvement (p 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we can not draw any conclusions from this week evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5340 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5340/>CUED@WMT19 : EWC&LMs<span class=acl-fixed-case>CUED</span>@<span class=acl-fixed-case>WMT</span>19:<span class=acl-fixed-case>EWC</span>&<span class=acl-fixed-case>LM</span>s</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5340><div class="card-body p-3 small">Two techniques provide the fabric of the Cambridge University Engineering Department&#8217;s (CUED) entry to the WMT19 evaluation campaign : elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5342 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5342/>University of Tartu’s Multilingual Multi-domain WMT19 News Translation Shared Task Submission<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>T</span>artu’s Multilingual Multi-domain <span class=acl-fixed-case>WMT</span>19 News Translation Shared Task Submission</a></strong><br><a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/e/elizaveta-korotkova/>Elizaveta Korotkova</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5342><div class="card-body p-3 small">This paper describes the University of Tartu&#8217;s submission to the news translation shared task of WMT19, where the core idea was to train a single multilingual system to cover several language pairs of the shared task and submit its results. We only used the constrained data from the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>. We describe our approach and its results and discuss the technical issues we faced.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5344 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5344/>The LMU Munich Unsupervised Machine Translation System for WMT19<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation System for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5344><div class="card-body p-3 small">We describe LMU Munich&#8217;s machine translation system for GermanCzech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only from both languages. The final model is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised neural model</a> using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5345 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5345/>Combining Local and Document-Level Context : The LMU Munich Neural Machine Translation System at WMT19<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Neural Machine Translation System at <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5345><div class="card-body p-3 small">We describe LMU Munich&#8217;s machine translation system for EnglishGerman translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this <a href=https://en.wikipedia.org/wiki/Physical_model>model</a> by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5347 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5347/>The University of Helsinki Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/a/arvi-hurskainen/>Arvi Hurskainen</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5347><div class="card-body p-3 small">In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish-English</a>. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5352 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5352.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5352/>A Test Suite and Manual Evaluation of Document-Level NMT at WMT19<span class=acl-fixed-case>NMT</span> at <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/k/katerina-rysova/>Kateřina Rysová</a>
|
<a href=/people/m/magdalena-rysova/>Magdaléna Rysová</a>
|
<a href=/people/t/tomas-musil/>Tomáš Musil</a>
|
<a href=/people/l/lucie-polakova/>Lucie Poláková</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5352><div class="card-body p-3 small">As the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5355 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5355.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5355" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5355/>SAO WMT19 Test Suite : Machine Translation of Audit Reports<span class=acl-fixed-case>SAO</span> <span class=acl-fixed-case>WMT</span>19 Test Suite: Machine Translation of Audit Reports</a></strong><br><a href=/people/t/tereza-vojtechova/>Tereza Vojtěchová</a>
|
<a href=/people/m/michal-novak/>Michal Novák</a>
|
<a href=/people/m/milos-kloucek/>Miloš Klouček</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5355><div class="card-body p-3 small">This paper describes a machine translation test set of documents from the auditing domain and its use as one of the test suites in the WMT19 News Translation Task for translation directions involving <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5356 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5356/>WMDO : Fluency-based Word Mover’s Distance for Machine Translation Evaluation<span class=acl-fixed-case>WMDO</span>: Fluency-based Word Mover’s Distance for Machine Translation Evaluation</a></strong><br><a href=/people/j/julian-chow/>Julian Chow</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5356><div class="card-body p-3 small">We propose <a href=https://en.wikipedia.org/wiki/WMDO>WMDO</a>, a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> based on distance between distributions in the semantic vector space. Matching in the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> has been investigated for translation evaluation, but the constraints of a translation&#8217;s word order have not been fully explored. Building on the Word Mover&#8217;s Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5357 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5357/>Meteor++ 2.0 : Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation</a></strong><br><a href=/people/y/yinuo-guo/>Yinuo Guo</a>
|
<a href=/people/j/junfeng-hu/>Junfeng Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5357><div class="card-body p-3 small">This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical level</a> and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> outperforms all previous versions of Meteor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5358 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5358/>YiSi-a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources<span class=acl-fixed-case>Y</span>i<span class=acl-fixed-case>S</span>i - a Unified Semantic <span class=acl-fixed-case>MT</span> Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources</a></strong><br><a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5358><div class="card-body p-3 small">We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1&#8217;s scores with human judgment is made by using contextual embeddings in multilingual BERTBidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5359.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5359 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5359 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5359/>EED : Extended Edit Distance Measure for Machine Translation<span class=acl-fixed-case>EED</span>: Extended Edit Distance Measure for Machine Translation</a></strong><br><a href=/people/p/peter-stanchev/>Peter Stanchev</a>
|
<a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5359><div class="card-body p-3 small">Over the years a number of machine translation metrics have been developed in order to evaluate the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine-generated translations</a>. Metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and TER have been used for decades. However, with the rapid progress of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, the need for better <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> is growing. This paper proposes an extension of the <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a>, which achieves better human correlation, whilst remaining fast, flexible and easy to understand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5360 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5360/>Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/r/ryoma-yoshimura/>Ryoma Yoshimura</a>
|
<a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5360><div class="card-body p-3 small">In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose an improved version of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence BLEU</a> using filtered pseudo-references. We propose a method to filter pseudo-references by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudo-references filtered by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (SentBLEU) baselines with a single reference and with unfiltered pseudo-references.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5361 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5361/>Naver Labs Europe’s Systems for the WMT19 Machine Translation Robustness Task<span class=acl-fixed-case>E</span>urope’s Systems for the <span class=acl-fixed-case>WMT</span>19 Machine Translation Robustness Task</a></strong><br><a href=/people/a/alexandre-berard/>Alexandre Berard</a>
|
<a href=/people/i/ioan-calapodescu/>Ioan Calapodescu</a>
|
<a href=/people/c/claude-roux/>Claude Roux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5361><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>systems</a> that we submitted to the WMT19 Machine Translation robustness task. This task aims to improve MT&#8217;s robustness to noise found on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, like <a href=https://en.wikipedia.org/wiki/Informal_language>informal language</a>, spelling mistakes and other orthographic variations. The organizers provide parallel data extracted from a <a href=https://en.wikipedia.org/wiki/Social_media>social media website</a> in two language pairs : French-English and Japanese-English (one for each language direction). The goal is to obtain the best scores on unseen test sets from the same source, according to automatic metrics (BLEU) and human evaluation. We propose one single and one ensemble system for each translation direction. Our <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble models</a> ranked first in all language pairs, according to BLEU evaluation. We discuss the pre-processing choices that we made, and present our solutions for <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to noise and domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5363 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5363/>System Description : The Submission of FOKUS to the WMT 19 Robustness Task<span class=acl-fixed-case>FOKUS</span> to the <span class=acl-fixed-case>WMT</span> 19 Robustness Task</a></strong><br><a href=/people/c/cristian-grozea/>Cristian Grozea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5363><div class="card-body p-3 small">This paper describes the systems of Fraunhofer FOKUS for the WMT 2019 machine translation robustness task. We have made submissions to the EN-FR, FR-EN, and JA-EN language pairs. The first two were made with a baseline translator, trained on clean data for the WMT 2019 biomedical translation task. These baselines improved over the baselines from the MTNT paper by 2 to 4 BLEU points, but where not trained on the same data. The last one used the same model class and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a>, with induced typos in the training data to increase the <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5364 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5364/>CUNI System for the WMT19 Robustness Task<span class=acl-fixed-case>CUNI</span> System for the <span class=acl-fixed-case>WMT</span>19 Robustness Task</a></strong><br><a href=/people/j/jindrich-helcl/>Jindřich Helcl</a>
|
<a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5364><div class="card-body p-3 small">We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5365 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5365/>NTT’s Machine Translation Systems for WMT19 Robustness Task<span class=acl-fixed-case>NTT</span>’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 Robustness Task</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5365><div class="card-body p-3 small">This paper describes NTT&#8217;s submission to the WMT19 robustness task. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> mainly focuses on translating <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy text</a> (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as <a href=https://en.wikipedia.org/wiki/News>news</a>. Our submission combined techniques including utilization of a synthetic corpus, <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, and a <a href=https://en.wikipedia.org/wiki/Placeholder_name>placeholder mechanism</a>, which significantly improved over the previous <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> with special placeholder tokens during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, improves <a href=https://en.wikipedia.org/wiki/Translation>translation accuracy</a> even with noisy texts.</div></div></div><hr><div id=w19-54><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-54.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-54/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5400/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5404/>Findings of the WMT 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions<span class=acl-fixed-case>WMT</span> 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions</a></strong><br><a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5404><div class="card-body p-3 small">Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2 % and 10 % of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5405/>RTM Stacking Results for Machine Translation Performance Prediction<span class=acl-fixed-case>RTM</span> Stacking Results for Machine Translation Performance Prediction</a></strong><br><a href=/people/e/ergun-bicici/>Ergun Biçici</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5405><div class="card-body p-3 small">We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5407/>QE BERT : Bilingual BERT Using Multi-task Learning for Neural Quality Estimation<span class=acl-fixed-case>QE</span> <span class=acl-fixed-case>BERT</span>: Bilingual <span class=acl-fixed-case>BERT</span> Using Multi-task Learning for Neural Quality Estimation</a></strong><br><a href=/people/h/hyun-kim/>Hyun Kim</a>
|
<a href=/people/j/joon-ho-lim/>Joon-Ho Lim</a>
|
<a href=/people/h/hyun-ki-kim/>Hyun-Ki Kim</a>
|
<a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5407><div class="card-body p-3 small">For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5408/>MIPT System for World-Level Quality Estimation<span class=acl-fixed-case>MIPT</span> System for World-Level Quality Estimation</a></strong><br><a href=/people/m/mikhail-mosyagin/>Mikhail Mosyagin</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5408><div class="card-body p-3 small">We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation. We start with a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> similar to Shef-bRNN, which we modify by using conditional random fields for sequence labelling. Additionally, we use a different approach for labelling gaps and source words. We further develop this model by including features from different sources such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, baseline features for the task and transformer encoders. We evaluate the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the English-German dataset for the corresponding <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5409/>NJU Submissions for the WMT19 Quality Estimation Shared Task<span class=acl-fixed-case>NJU</span> Submissions for the <span class=acl-fixed-case>WMT</span>19 Quality Estimation Shared Task</a></strong><br><a href=/people/h/hou-qi/>Hou Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5409><div class="card-body p-3 small">In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair. We develop two approaches based on a two-stage neural QE model consisting of a <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a> and a quality estimator. More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions ; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks. To efficiently train these two-stage models, a joint learning training method is applied. Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5410/>Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings</a></strong><br><a href=/people/e/elizaveta-yankovskaya/>Elizaveta Yankovskaya</a>
|
<a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5410><div class="card-body p-3 small">We propose the use of pre-trained embeddings as features of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also <a href=https://en.wikipedia.org/wiki/Log_probability>log probability</a> of any <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) system</a>. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5411/>SOURCE : SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation<span class=acl-fixed-case>SOURCE</span>: <span class=acl-fixed-case>SOUR</span>ce-Conditional Elmo-style Model for Machine Translation Quality Estimation</a></strong><br><a href=/people/j/junpei-zhou/>Junpei Zhou</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/z/zecong-hu/>Zecong Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5411><div class="card-body p-3 small">Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> and adopt a bi-directional translation-like strategy. The <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategy</a> is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>, which makes the pre-training slightly harder, can bring improvements for <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a>. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5416/>Effort-Aware Neural Automatic Post-Editing</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5416><div class="card-body p-3 small">For this round of the WMT 2019 APE shared task, our submission focuses on addressing the over-correction problem in <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the <a href=https://en.wikipedia.org/wiki/System>system</a> about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5417.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5417/>UdS Submission for the WMT 19 Automatic Post-Editing Task<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span> Submission for the <span class=acl-fixed-case>WMT</span> 19 Automatic Post-Editing Task</a></strong><br><a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/q/qiuhui-liu/>Qiuhui Liu</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5417><div class="card-body p-3 small">In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>, implement this in our own transformer model and explore joint training of the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE task</a> with a de-noising encoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5418/>Terminology-Aware Segmentation and Domain Feature for the WMT19 Biomedical Translation Task<span class=acl-fixed-case>WMT</span>19 Biomedical Translation Task</a></strong><br><a href=/people/c/casimiro-pio-carrino/>Casimiro Pio Carrino</a>
|
<a href=/people/b/bardia-rafieian/>Bardia Rafieian</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5418><div class="card-body p-3 small">In this work, we give a description of the TALP-UPC systems submitted for the WMT19 Biomedical Translation Task. Our proposed strategy is NMT model-independent and relies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the <a href=https://en.wikipedia.org/wiki/Term_(logic)>terms information</a> at a token level. Finally, we trained the Transformer model with this terms-informed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5420/>Huawei’s NMT Systems for the WMT 2019 Biomedical Translation Task<span class=acl-fixed-case>NMT</span> Systems for the <span class=acl-fixed-case>WMT</span> 2019 Biomedical Translation Task</a></strong><br><a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/j/jianfeng-liu/>Jianfeng Liu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5420><div class="card-body p-3 small">This paper describes Huawei&#8217;s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering EnglishChinese, EnglishFrench and EnglishGerman language pairs. Our submitted systems achieve the best BLEU scores on EnglishFrench and EnglishGerman language pairs according to the official evaluation results. In the EnglishChinese translation task, our <a href=https://en.wikipedia.org/wiki/System>systems</a> are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5421/>UCAM Biomedical Translation at WMT19 : Transfer Learning Multi-domain Ensembles<span class=acl-fixed-case>UCAM</span> Biomedical Translation at <span class=acl-fixed-case>WMT</span>19: Transfer Learning Multi-domain Ensembles</a></strong><br><a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5421><div class="card-body p-3 small">The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of <a href=https://en.wikipedia.org/wiki/English_language>English-Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5425/>Machine Translation from an Intercomprehension Perspective</a></strong><br><a href=/people/y/yu-chen/>Yu Chen</a>
|
<a href=/people/t/tania-avgustinova/>Tania Avgustinova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5425><div class="card-body p-3 small">Within the first shared task on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between similar languages, we present our first attempts on Czech to Polish machine translation from an intercomprehension perspective. We propose methods based on the <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> of the two languages, taking advantage of their orthographic and phonological similarity, in the hope to improve over our baselines. The <a href=https://en.wikipedia.org/wiki/Translation>translation</a> results are evaluated using <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. On this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, none of our <a href=https://en.wikipedia.org/wiki/Proposal_(business)>proposals</a> could outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on the final test set. The current setups are rather preliminary, and there are several potential improvements we can try in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5426/>Utilizing Monolingual Data in NMT for Similar Languages : Submission to Similar Language Translation Task<span class=acl-fixed-case>NMT</span> for Similar Languages: Submission to Similar Language Translation Task</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5426><div class="card-body p-3 small">This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi-Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5427/>Neural Machine Translation : Hindi-Nepali<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>N</span>epali</a></strong><br><a href=/people/s/sahinur-rahman-laskar/>Sahinur Rahman Laskar</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5427><div class="card-body p-3 small">With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5429/>Panlingua-KMI MT System for Similar Language Translation Task at WMT 2019<span class=acl-fixed-case>KMI</span> <span class=acl-fixed-case>MT</span> System for Similar Language Translation Task at <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/akanksha-bansal/>Akanksha Bansal</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5429><div class="card-body p-3 small">The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-Hindi</a>, 1 PBSMT for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi-Nepali</a> and 2 NMT systems were developed for <a href=https://en.wikipedia.org/wiki/Nepali_language>NepaliHindi</a>. The results show that PBSMT could be an effective method for developing MT systems for <a href=https://en.wikipedia.org/wiki/Lingua_franca>closely-related languages</a>. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5430 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5430/>UDSDFKI Submission to the WMT2019 CzechPolish Similar Language Translation Shared Task<span class=acl-fixed-case>UDS</span>–<span class=acl-fixed-case>DFKI</span> Submission to the <span class=acl-fixed-case>WMT</span>2019 <span class=acl-fixed-case>C</span>zech–<span class=acl-fixed-case>P</span>olish Similar Language Translation Shared Task</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5430><div class="card-body p-3 small">In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and Nepali, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our <a href=https://en.wikipedia.org/wiki/System>system</a> in translating from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and comment on the impact of out-of-domain test data in the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a>. UDS-DFKI achieved competitive performance ranking second among ten teams in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to Polish translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5431/>Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation</a></strong><br><a href=/people/m/michael-przystupa/>Michael Przystupa</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5431><div class="card-body p-3 small">We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> on three low-resource, similar language pairs : <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech Polish</a>, and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi Nepali</a>. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish Portuguese and Czech Polish translation, whereas LSTMs with global attention worked best on Hindi Nepali translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5433 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5433.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5433/>Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data</a></strong><br><a href=/people/a/amittai-axelrod/>Amittai Axelrod</a>
|
<a href=/people/a/anish-kumar/>Anish Kumar</a>
|
<a href=/people/s/steve-sloto/>Steve Sloto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5433><div class="card-body p-3 small">We introduce a purely monolingual approach to <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering</a> for parallel data from a noisy corpus in a low-resource scenario. Our work is inspired by Junczysdowmunt:2018, but we relax the requirements to allow for cases where no parallel data is available. Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection Axelrod:2017, and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems. Our approach is featherweight, and runs end-to-end on a standard laptop in three hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5434 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5434/>NRC Parallel Corpus Filtering System for WMT 2019<span class=acl-fixed-case>NRC</span> Parallel Corpus Filtering System for <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5434><div class="card-body p-3 small">We describe the National Research Council Canada team&#8217;s submissions to the parallel corpus filtering task at the Fourth Conference on <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5436/>Quality and Coverage : The AFRL Submission to the WMT19 Parallel Corpus Filtering for Low-Resource Conditions Task<span class=acl-fixed-case>AFRL</span> Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering for Low-Resource Conditions Task</a></strong><br><a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5436><div class="card-body p-3 small">The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering a noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are the relatively low-resource language pairs of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> and the relative benefits of different options.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5437/>Webinterpret Submission to the WMT2019 Shared Task on Parallel Corpus Filtering<span class=acl-fixed-case>WMT</span>2019 Shared Task on Parallel Corpus Filtering</a></strong><br><a href=/people/j/jesus-gonzalez-rubio/>Jesús González-Rubio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5437><div class="card-body p-3 small">This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019). Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5439 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5439/>Filtering of Noisy Parallel Corpora Based on Hypothesis Generation</a></strong><br><a href=/people/z/zuzanna-parcheta/>Zuzanna Parcheta</a>
|
<a href=/people/g/german-sanchis-trilles/>Germán Sanchis-Trilles</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5439><div class="card-body p-3 small">The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems. In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model. We train translation models in both language pairs : NepaliEnglish and SinhalaEnglish using provided parallel corpora. We select the training subset for three language pairs (Nepali, <a href=https://en.wikipedia.org/wiki/Sinhala_language>Sinhala</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to English) jointly using bilingual cross-entropy selection to create the best possible translation model for both language pairs. Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair. We compute the smoothed BLEU score between the target sentence and generated hypothesis. In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score. These <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> are based on <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, source and target similarity and source language detection. We compare our results with the baseline published on the shared task website, which uses the Zipporah model, over which we achieve significant improvements in one of the conditions in the shared task. The designed <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering system</a> is domain independent and all experiments are conducted using <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5441/>The University of Helsinki Submission to the WMT19 Parallel Corpus Filtering Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering Task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5441><div class="card-body p-3 small">This paper describes the University of Helsinki Language Technology group&#8217;s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of <a href=https://en.wikipedia.org/wiki/Filter_(software)>filters</a> to remove the &#8216;bad&#8217; quality sentences. Then, we produced scores for each sentence by weighting these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with a <a href=https://en.wikipedia.org/wiki/Statistical_model>classification model</a>. This methodology allowed us to build a simple and reliable <a href=https://en.wikipedia.org/wiki/System>system</a> that is easily adaptable to other language pairs.</div></div></div><hr><div id=w19-55><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-55.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-55/>Proceedings of the First Workshop on Financial Technology and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5500/>Proceedings of the First Workshop on Financial Technology and Natural Language Processing</a></strong><br><a href=/people/c/chung-chi-chen/>Chung-Chi Chen</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p></div><hr><div id=w19-56><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-56.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-56/>Proceedings of the 3rd Workshop on Arabic Corpus Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5600/>Proceedings of the 3rd Workshop on Arabic Corpus Linguistics</a></strong><br><a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a>
|
<a href=/people/e/eric-atwell/>Eric Atwell</a>
|
<a href=/people/l/lama-alsudias/>Lama Alsudias</a></span></p></div><hr><div id=w19-57><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-57.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-57/>Proceedings of the 16th Meeting on the Mathematics of Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5700/>Proceedings of the 16th Meeting on the Mathematics of Language</a></strong><br><a href=/people/p/philippe-de-groote/>Philippe de Groote</a>
|
<a href=/people/f/frank-drewes/>Frank Drewes</a>
|
<a href=/people/g/gerald-penn/>Gerald Penn</a></span></p></div><hr><div id=w19-58><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-58.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-58/>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5800/>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p></div><hr><div id=w19-59><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-59.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-59/>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5900/>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</a></strong><br><a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a>
|
<a href=/people/i/ingrid-zuckerman/>Ingrid Zuckerman</a>
|
<a href=/people/g/gabriel-skantze/>Gabriel Skantze</a>
|
<a href=/people/m/mikio-nakano/>Mikio Nakano</a>
|
<a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5904/>Few-Shot Dialogue Generation Without Annotated Data : A Transfer Learning Approach</a></strong><br><a href=/people/i/igor-shalyminov/>Igor Shalyminov</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5904><div class="card-body p-3 small">Learning with minimal data is one of the key challenges in the development of practical, production-ready goal-oriented dialogue systems. In a real-world enterprise setting where dialogue systems are developed rapidly and are expected to work robustly for an ever-growing variety of domains, products, and scenarios, efficient <a href=https://en.wikipedia.org/wiki/Learning>learning</a> from a limited number of examples becomes indispensable. In this paper, we introduce a technique to achieve state-of-the-art dialogue generation performance in a few-shot setup, without using any <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a>. We do this by leveraging background knowledge from a larger, more highly represented dialogue source namely, the MetaLWOz dataset. We evaluate our model on the Stanford Multi-Domain Dialogue Dataset, consisting of human-human goal-oriented dialogues in <a href=https://en.wikipedia.org/wiki/Automotive_navigation_system>in-car navigation</a>, appointment scheduling, and weather information domains. We show that our few-shot approach achieves state-of-the art results on that dataset by consistently outperforming the previous best model in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and Entity F1 scores, while being more data-efficient than it by not requiring any data annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5905/>SIM : A Slot-Independent Neural Model for Dialogue State Tracking<span class=acl-fixed-case>SIM</span>: A Slot-Independent Neural Model for Dialogue State Tracking</a></strong><br><a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/m/michael-zeng/>Michael Zeng</a>
|
<a href=/people/x/xuedong-huang/>Xuedong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5905><div class="card-body p-3 small">Dialogue state tracking is an important component in task-oriented dialogue systems to identify users&#8217; goals and requests as a dialogue proceeds. However, as most previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are dependent on dialogue slots, the model complexity soars when the number of slots increases. In this paper, we put forward a slot-independent neural model (SIM) to track dialogue states while keeping the model complexity invariant to the number of dialogue slots. The <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> utilizes <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> between user utterance and <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>system actions</a>. SIM achieves state-of-the-art results on WoZ and DSTC2 tasks, with only 20 % of the model size of previous models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5907/>Time Masking : Leveraging Temporal Information in Spoken Dialogue Systems</a></strong><br><a href=/people/r/rylan-conway/>Rylan Conway</a>
|
<a href=/people/m/mathias-lambert/>Mathias Lambert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5907><div class="card-body p-3 small">In a spoken dialogue system, dialogue state tracker (DST) components track the state of the conversation by updating a distribution of values associated with each of the slots being tracked for the current user turn, using the interactions until then. Much of the previous work has relied on modeling the natural order of the conversation, using distance based offsets as an approximation of time. In this work, we hypothesize that leveraging the wall-clock temporal difference between turns is crucial for finer-grained control of dialogue scenarios. We develop a novel approach that applies a time mask, based on the wall-clock time difference, to the associated slot embeddings and empirically demonstrate that our proposed approach outperforms existing approaches that leverage distance offsets, on both an internal benchmark dataset as well as DSTC2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5908/>To Combine or Not To Combine? A Rainbow Deep Reinforcement Learning Agent for Dialog Policies</a></strong><br><a href=/people/d/dirk-vath/>Dirk Väth</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5908><div class="card-body p-3 small">In this paper, we explore state-of-the-art deep reinforcement learning methods for dialog policy training such as prioritized experience replay, double deep Q-Networks, dueling network architectures and distributional learning. Our main findings show that each individual method improves the rewards and the task success rate but combining these methods in a Rainbow agent, which performs best across tasks and environments, is a non-trivial task. We, therefore, provide insights about the influence of each method on the combination and how to combine them to form a Rainbow agent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5912" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5912/>Collaborative Multi-Agent Dialogue Model Training Via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/a/alexandros-papangelis/>Alexandros Papangelis</a>
|
<a href=/people/y/yi-chia-wang/>Yi-Chia Wang</a>
|
<a href=/people/p/piero-molino/>Piero Molino</a>
|
<a href=/people/g/gokhan-tur/>Gokhan Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5912><div class="card-body p-3 small">Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent&#8217;s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training pipeline</a> and model the <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a> as a stochastic collaborative game. Each agent (player) has a role (assistant, tourist, eater, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> (its own LU and LG, the other agent&#8217;s LU, <a href=https://en.wikipedia.org/wiki/Policy>Policy</a>, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5913/>Scoring Interactional Aspects of Human-Machine Dialog for Language Learning and Assessment using Text Features</a></strong><br><a href=/people/v/vikram-ramanarayanan/>Vikram Ramanarayanan</a>
|
<a href=/people/m/matthew-mulholland/>Matthew Mulholland</a>
|
<a href=/people/y/yao-qian/>Yao Qian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5913><div class="card-body p-3 small">While there has been much work in the language learning and assessment literature on human and automated scoring of essays and short constructed responses, there is little to no work examining text features for scoring of dialog data, particularly interactional aspects thereof, to assess conversational proficiency over and above constructed response skills. Our work bridges this gap by investigating both human and automated approaches towards scoring humanmachine text dialog in the context of a real-world language learning application. We collected conversational data of human learners interacting with a cloud-based standards-compliant dialog system, triple-scored these <a href=https://en.wikipedia.org/wiki/Data>data</a> along multiple dimensions of conversational proficiency, and then analyzed the performance trends. We further examined two different approaches to automated scoring of such data and show that these approaches are able to perform at or above par with human agreement for a majority of dimensions of the scoring rubric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5914/>Spoken Conversational Search for General Knowledge</a></strong><br><a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas Barahona</a>
|
<a href=/people/p/pascal-bellec/>Pascal Bellec</a>
|
<a href=/people/b/benoit-besset/>Benoit Besset</a>
|
<a href=/people/m/martinho-dossantos/>Martinho Dossantos</a>
|
<a href=/people/j/johannes-heinecke/>Johannes Heinecke</a>
|
<a href=/people/m/munshi-asadullah/>Munshi Asadullah</a>
|
<a href=/people/o/olivier-leblouch/>Olivier Leblouch</a>
|
<a href=/people/j/jeanyves-lancien/>Jeanyves. Lancien</a>
|
<a href=/people/g/geraldine-damnati/>Geraldine Damnati</a>
|
<a href=/people/e/emmanuel-mory/>Emmanuel Mory</a>
|
<a href=/people/f/frederic-herledan/>Frederic Herledan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5914><div class="card-body p-3 small">We present a spoken conversational question answering proof of concept that is able to answer questions about <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>. The dialogue agent does not only orchestrate various <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> but also solve <a href=https://en.wikipedia.org/wiki/Coreference>coreferences</a> and <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5915/>Graph2Bots, Unsupervised Assistance for Designing Chatbots<span class=acl-fixed-case>G</span>raph2<span class=acl-fixed-case>B</span>ots, Unsupervised Assistance for Designing Chatbots</a></strong><br><a href=/people/j/jean-leon-bouraoui/>Jean-Leon Bouraoui</a>
|
<a href=/people/s/sonia-le-meitour/>Sonia Le Meitour</a>
|
<a href=/people/r/romain-carbou/>Romain Carbou</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas Barahona</a>
|
<a href=/people/v/vincent-lemaire/>Vincent Lemaire</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5915><div class="card-body p-3 small">We present Graph2Bots, a tool for assisting conversational agent designers. It extracts a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph representation</a> from <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-human conversations</a> by using <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a>. The generated <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> contains the main stages of the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> and their inner transitions. The <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>graphical user interface (GUI)</a> then allows <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph editing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5916/>On a <a href=https://en.wikipedia.org/wiki/Chatbot>Chatbot</a> Conducting Dialogue-in-Dialogue</a></strong><br><a href=/people/b/boris-galitsky/>Boris Galitsky</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a>
|
<a href=/people/e/elizaveta-goncharova/>Elizaveta Goncharova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5916><div class="card-body p-3 small">We demo a <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a> that delivers content in the form of virtual dialogues automatically produced from plain texts extracted and selected from documents. This virtual dialogue content is provided in the form of answers derived from the found and selected documents split into fragments, and questions are automatically generated for these answers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5917/>DeepCopy : Grounded Response Generation with Hierarchical Pointer Networks<span class=acl-fixed-case>D</span>eep<span class=acl-fixed-case>C</span>opy: Grounded Response Generation with Hierarchical Pointer Networks</a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/g/guan-lin-chao/>Guan-Lin Chao</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5917><div class="card-body p-3 small">Recent advances in neural sequence-to-sequence models have led to promising results for several language generation-based tasks, including dialogue response generation, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. However, these models are known to have several problems, especially in the context of chit-chat based dialogue systems : they tend to generate short and dull responses that are often too generic. Furthermore, these models do not ground conversational responses on knowledge and facts, resulting in turns that are not accurate, informative and engaging for the users. In this paper, we propose and experiment with a series of response generation models that aim to serve in the general scenario where in addition to the dialogue context, relevant unstructured external knowledge in the form of text is also assumed to be available for <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to harness. Our proposed approach extends pointer-generator networks (See et al., 2017) by allowing the decoder to hierarchically attend and copy from external knowledge in addition to the dialogue context. We empirically show the effectiveness of the proposed model compared to several baselines including (Ghazvininejadet al., 2018 ; Zhang et al., 2018) through both automatic evaluation metrics and human evaluation on ConvAI2 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5918/>Towards End-to-End Learning for Efficient Dialogue Agent by Modeling Looking-ahead Ability</a></strong><br><a href=/people/z/zhuoxuan-jiang/>Zhuoxuan Jiang</a>
|
<a href=/people/x/xian-ling-mao/>Xian-Ling Mao</a>
|
<a href=/people/z/ziming-huang/>Ziming Huang</a>
|
<a href=/people/j/jie-ma/>Jie Ma</a>
|
<a href=/people/s/shaochun-li/>Shaochun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5918><div class="card-body p-3 small">Learning an efficient manager of dialogue agent from data with little manual intervention is important, especially for goal-oriented dialogues. However, existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> either take too many manual efforts (e.g. reinforcement learning methods) or can not guarantee the dialogue efficiency (e.g. sequence-to-sequence methods). In this paper, we address this problem by proposing a novel end-to-end learning model to train a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue agent</a> that can look ahead for several future turns and generate an optimal response to make the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> efficient. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is data-driven and does not require too much manual work for intervention during <a href=https://en.wikipedia.org/wiki/Systems_design>system design</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on two datasets of different scenarios and the experimental results demonstrate the efficiency of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5919.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5919 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5919 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5919/>Unsupervised Dialogue Spectrum Generation for Log Dialogue Ranking</a></strong><br><a href=/people/x/xinnuo-xu/>Xinnuo Xu</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/l/lars-liden/>Lars Liden</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5919><div class="card-body p-3 small">Although the data-driven approaches of some recent bot building platforms make it possible for a wide range of users to easily create dialogue systems, those <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> do n&#8217;t offer tools for quickly identifying which log dialogues contain problems. This is important since corrections to log dialogues provide a means to improve performance after deployment. A log dialogue ranker, which ranks problematic dialogues higher, is an essential tool due to the sheer volume of log dialogues that could be generated. However, training a <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> typically requires labelling a substantial amount of data, which is not feasible for most users. In this paper, we present a novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for dialogue ranking using GANs and release a corpus of labelled dialogues for evaluation and comparison with <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>. The evaluation result shows that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> compares favorably to <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> without any <a href=https://en.wikipedia.org/wiki/Data>labelled data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5920.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5920 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5920 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5920/>Tree-Structured Semantic Encoder with Knowledge Sharing for Domain Adaptation in Natural Language Generation</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5920><div class="card-body p-3 small">Domain adaptation in natural language generation (NLG) remains challenging because of the high complexity of input semantics across domains and limited data of a target domain. This is particularly the case for dialogue systems, where we want to be able to seamlessly include new domains into the conversation. Therefore, it is crucial for generation models to share knowledge across domains for the effective <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> from one domain to another. In this study, we exploit a tree-structured semantic encoder to capture the internal structure of complex semantic representations required for multi-domain dialogues in order to facilitate knowledge sharing across domains. In addition, a layer-wise attention mechanism between the <a href=https://en.wikipedia.org/wiki/Tree_traversal>tree encoder</a> and the <a href=https://en.wikipedia.org/wiki/Tree_traversal>decoder</a> is adopted to further improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s capability. The automatic evaluation results show that our model outperforms previous methods in terms of the BLEU score and the slot error rate, in particular when the <a href=https://en.wikipedia.org/wiki/Adaptation_data>adaptation data</a> is limited. In subjective evaluation, human judges tend to prefer the sentences generated by our model, rating them more highly on informativeness and <a href=https://en.wikipedia.org/wiki/Naturalness_(philosophy)>naturalness</a> than other <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5922.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5922 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5922 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5922" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5922/>Flexibly-Structured Model for Task-Oriented Dialogues</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/p/piero-molino/>Piero Molino</a>
|
<a href=/people/m/mahdi-namazifar/>Mahdi Namazifar</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/h/huaixiu-zheng/>Huaixiu Zheng</a>
|
<a href=/people/g/gokhan-tur/>Gokhan Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5922><div class="card-body p-3 small">This paper proposes a novel end-to-end architecture for task-oriented dialogue systems. It is based on a simple and practical yet very effective sequence-to-sequence approach, where <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and state tracking tasks are modeled jointly with a structured copy-augmented sequential decoder and a multi-label decoder for each slot. The policy engine and language generation tasks are modeled jointly following that. The copy-augmented sequential decoder deals with new or unknown values in the conversation, while the multi-label decoder combined with the <a href=https://en.wikipedia.org/wiki/Sequential_decoder>sequential decoder</a> ensures the explicit assignment of values to slots. On the generation part, slot binary classifiers are used to improve performance. This architecture is scalable to real-world scenarios and is shown through an empirical evaluation to achieve state-of-the-art performance on both the Cambridge Restaurant dataset and the Stanford in-car assistant dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5923.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5923 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5923 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5923/>FriendsQA : Open-Domain Question Answering on TV Show Transcripts<span class=acl-fixed-case>F</span>riends<span class=acl-fixed-case>QA</span>: Open-Domain Question Answering on <span class=acl-fixed-case>TV</span> Show Transcripts</a></strong><br><a href=/people/z/zhengzhe-yang/>Zhengzhe Yang</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5923><div class="card-body p-3 small">This paper presents FriendsQA, a challenging question answering dataset that contains 1,222 dialogues and 10,610 open-domain questions, to tackle machine comprehension on everyday conversations. Each dialogue, involving multiple speakers, is annotated with several types of questions regarding the dialogue contexts, and the answers are annotated with certain spans in the dialogue. A series of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing tasks</a> are conducted to ensure good annotation quality, resulting a high <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> of 81.82 %. A comprehensive annotation analytics is provided for a deeper understanding in this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Three state-of-the-art QA systems are experimented, R-Net, QANet, and BERT, and evaluated on this dataset. BERT in particular depicts promising results, an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 74.2 % for answer utterance selection and an F1-score of 64.2 % for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5928.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5928 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5928 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5928/>A Quantitative Analysis of Patients’ Narratives of Heart Failure</a></strong><br><a href=/people/s/sabita-acharya/>Sabita Acharya</a>
|
<a href=/people/b/barbara-di-eugenio/>Barbara Di Eugenio</a>
|
<a href=/people/a/andrew-boyd/>Andrew Boyd</a>
|
<a href=/people/r/richard-cameron/>Richard Cameron</a>
|
<a href=/people/k/karen-dunn-lopez/>Karen Dunn Lopez</a>
|
<a href=/people/p/pamela-martyn-nemeth/>Pamela Martyn-Nemeth</a>
|
<a href=/people/d/debaleena-chattopadhyay/>Debaleena Chattopadhyay</a>
|
<a href=/people/p/pantea-habibi/>Pantea Habibi</a>
|
<a href=/people/c/carolyn-dickens/>Carolyn Dickens</a>
|
<a href=/people/h/haleh-vatani/>Haleh Vatani</a>
|
<a href=/people/a/amer-ardati/>Amer Ardati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5928><div class="card-body p-3 small">Patients with <a href=https://en.wikipedia.org/wiki/Chronic_condition>chronic conditions</a> like <a href=https://en.wikipedia.org/wiki/Heart_failure>heart failure</a> are the most likely to be re-hospitalized. One step towards avoiding re-hospitalization is to devise strategies for motivating patients to take care of their own health. In this paper, we perform a quantitative analysis of patients&#8217; narratives of their experience with heart failure and explore the different topics that patients talk about. We compare two different groups of patients- those unable to take charge of their illness, and those who make efforts to improve their health. We will use the findings from our analysis to refine and personalize the summaries of hospitalizations that our <a href=https://en.wikipedia.org/wiki/System>system</a> automatically generates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5930.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5930 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5930 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5930/>Real Life Application of a <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering System</a> Using BERT Language Model<span class=acl-fixed-case>BERT</span> Language Model</a></strong><br><a href=/people/f/francesca-alloatti/>Francesca Alloatti</a>
|
<a href=/people/l/luigi-di-caro/>Luigi Di Caro</a>
|
<a href=/people/g/gianpiero-sportelli/>Gianpiero Sportelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5930><div class="card-body p-3 small">It is often hard to apply the newest advances in research to real life scenarios. They usually require the resolution of some specific <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> applied to a restricted domain, all the while providing small amounts of data to begin with. In this study we apply one of the newest innovations in <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> to a task of <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. We created a <a href=https://en.wikipedia.org/wiki/Question_answering>question answering system</a> in <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> that provides information about a specific subject, <a href=https://en.wikipedia.org/wiki/Electronic_invoicing>e-invoicing</a> and digital billing. Italy recently introduced a new legislation about <a href=https://en.wikipedia.org/wiki/Electronic_invoicing>e-invoicing</a> and people have some legit doubts, therefore a large share of professionals could benefit from this tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5932.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5932 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5932 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5932/>Dialog State Tracking : A Neural Reading Comprehension Approach</a></strong><br><a href=/people/s/shuyang-gao/>Shuyang Gao</a>
|
<a href=/people/a/abhishek-sethi/>Abhishek Sethi</a>
|
<a href=/people/s/sanchit-agarwal/>Sanchit Agarwal</a>
|
<a href=/people/t/tagyoung-chung/>Tagyoung Chung</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5932><div class="card-body p-3 small">Dialog state tracking is used to estimate the current belief state of a dialog given all the preceding conversation. Machine reading comprehension, on the other hand, focuses on building systems that read passages of text and answer questions that require some understanding of passages. We formulate dialog state tracking as a reading comprehension task to answer the question what is the state of the current dialog? after reading <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context</a>. In contrast to traditional state tracking methods where the dialog state is often predicted as a distribution over a closed set of all the possible slot values within an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>, our method uses a simple attention-based neural network to point to the slot values within the conversation. Experiments on MultiWOZ-2.0 cross-domain dialog dataset show that our simple <a href=https://en.wikipedia.org/wiki/System>system</a> can obtain similar accuracies compared to the previous more complex methods. By exploiting recent advances in contextual word embeddings, adding a model that explicitly tracks whether a slot value should be carried over to the next turn, and combining our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with a traditional joint state tracking method that relies on closed set vocabulary, we can obtain a joint-goal accuracy of 47.33 % on the standard test split, exceeding current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by 11.75 % * *.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5933.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5933 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5933 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5933/>Cross-Corpus Data Augmentation for Acoustic Addressee Detection</a></strong><br><a href=/people/o/oleg-akhtiamov/>Oleg Akhtiamov</a>
|
<a href=/people/i/ingo-siegert/>Ingo Siegert</a>
|
<a href=/people/a/alexey-karpov/>Alexey Karpov</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5933><div class="card-body p-3 small">Acoustic addressee detection (AD) is a modern paralinguistic and dialogue challenge that especially arises in <a href=https://en.wikipedia.org/wiki/Voice_assistant>voice assistants</a>. In the present study, we distinguish addressees in two settings (a conversation between several people and a spoken dialogue system, and a conversation between several adults and a child) and introduce the first competitive baseline (unweighted average recall equals 0.891) for the Voice Assistant Conversation Corpus that models the first setting. We jointly solve both classification problems, using three models : a linear support vector machine dealing with acoustic functionals and two <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> utilising raw waveforms alongside with acoustic low-level descriptors. We investigate how different corpora influence each other, applying the mixup approach to <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. We also study the influence of various acoustic context lengths on <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a>. Two-second speech fragments turn out to be sufficient for reliable <a href=https://en.wikipedia.org/wiki/Anno_Domini>AD</a>. Mixup is shown to be beneficial for merging acoustic data (extracted features but not raw waveforms) from different domains that allows us to reach a higher classification performance on human-machine AD and also for training a multipurpose neural network that is capable of solving both human-machine and adult-child AD problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5935.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5935 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5935 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5935/>A Large-Scale User Study of an Alexa Prize Chatbot : Effect of TTS Dynamism on Perceived Quality of Social Dialog<span class=acl-fixed-case>A</span>lexa <span class=acl-fixed-case>P</span>rize Chatbot: Effect of <span class=acl-fixed-case>TTS</span> Dynamism on Perceived Quality of Social Dialog</a></strong><br><a href=/people/m/michelle-cohn/>Michelle Cohn</a>
|
<a href=/people/c/chun-yen-chen/>Chun-Yen Chen</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5935><div class="card-body p-3 small">This study tests the effect of cognitive-emotional expression in an Alexa text-to-speech (TTS) voice on users&#8217; experience with a social dialog system. We systematically introduced emotionally expressive interjections (e.g., Wow !) and <a href=https://en.wikipedia.org/wiki/Filler_(media)>filler words</a> (e.g., um, mhmm) in an Amazon Alexa Prize socialbot, Gunrock. We tested whether these TTS manipulations improved users&#8217; ratings of their conversation across thousands of real user interactions (n=5,527). Results showed that <a href=https://en.wikipedia.org/wiki/Interjection>interjections</a> and <a href=https://en.wikipedia.org/wiki/Filler_(media)>fillers</a> each improved users&#8217; holistic ratings, an improvement that further increased if the <a href=https://en.wikipedia.org/wiki/System>system</a> used both manipulations. A separate perception experiment corroborated the findings from the user study, with improved social ratings for conversations including interjections ; however, no positive effect was observed for fillers, suggesting that the role of the rater in the conversationas active participant or external listeneris an important factor in assessing social dialogs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5936.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5936 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5936 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5936/>Influence of Time and Risk on Response Acceptability in a Simple Spoken Dialogue System</a></strong><br><a href=/people/a/andisheh-partovi/>Andisheh Partovi</a>
|
<a href=/people/i/ingrid-zukerman/>Ingrid Zukerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5936><div class="card-body p-3 small">We describe a <a href=https://en.wikipedia.org/wiki/Longitudinal_study>longitudinal user study</a> conducted in the context of a Spoken Dialogue System for a <a href=https://en.wikipedia.org/wiki/Home_robot>household robot</a>, where we examined the influence of <a href=https://en.wikipedia.org/wiki/Time_displacement>time displacement</a> and situational risk on users&#8217; preferred responses. To this effect, we employed a corpus of spoken requests that asked a robot to fetch or move objects in a room. In the first stage of our study, participants selected among four response types to these requests under two risk conditions : low and high. After some time, the same participants rated several responses to the previous requests these responses were instantiated from the four response types. Our results show that participants did not rate highly their own response types ; moreover, they rated their own response types similarly to different ones. This suggests that, at least in this context, people&#8217;s preferences at a particular point in time may not reflect their general attitudes, and that various reasonable response types may be equally acceptable. Our study also reveals that situational risk influences the acceptability of some response types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5937.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5937 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5937 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5937/>Characterizing the Response Space of Questions : a Corpus Study for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Polish<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>P</span>olish</a></strong><br><a href=/people/j/jonathan-ginzburg/>Jonathan Ginzburg</a>
|
<a href=/people/z/zulipiye-yusupujiang/>Zulipiye Yusupujiang</a>
|
<a href=/people/c/chuyuan-li/>Chuyuan Li</a>
|
<a href=/people/k/kexin-ren/>Kexin Ren</a>
|
<a href=/people/p/pawel-lupkowski/>Paweł Łupkowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5937><div class="card-body p-3 small">The main aim of this paper is to provide a characterization of the response space for questions using a <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> grounded in a dialogical formal semantics. As a starting point we take the <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> for responses in the form of questions provided in (Lupkowski and Ginzburg, 2016). This work develops a wide coverage taxonomy for question / question sequences observable in corpora including the BNC, <a href=https://en.wikipedia.org/wiki/CHILDES>CHILDES</a>, and BEE, as well as formal modelling of all the postulated classes. Our aim is to extend this work to cover all responses to questions. We present the extended typology of responses to questions based on a corpus studies of BNC, BEE and Maptask with include 506, 262, and 467 question / response pairs respectively. We compare the data for <a href=https://en.wikipedia.org/wiki/English_language>English</a> with data from <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> using the Spokes corpus (205 question / response pairs). We discuss annotation reliability and <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>disagreement analysis</a>. We sketch how each class can be formalized using a dialogical semantics appropriate for <a href=https://en.wikipedia.org/wiki/Dialogue_management>dialogue management</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5939.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5939 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5939 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5939/>Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/a/athanasios-lykartsis/>Athanasios Lykartsis</a>
|
<a href=/people/m/margarita-kotti/>Margarita Kotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5939><div class="card-body p-3 small">In this paper we aim to predict dialogue success and user satisfaction as well as <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> on a turn level. To achieve this, we investigate the use of <a href=https://en.wikipedia.org/wiki/Spectrogram>spectrogram representations</a>, extracted from <a href=https://en.wikipedia.org/wiki/Audio_file_format>audio files</a>, in combination with several types of <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. The experiments were performed on the Let&#8217;s Go V2 database, comprising 5065 audio files and having labels for subjective and objective dialogue turn success, as well as the emotional state of the user. Results show that by using only <a href=https://en.wikipedia.org/wiki/Sound>audio</a>, it is possible to predict turn success with very high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for all three labels (90 %). The best performing input representation were 1s long mel-spectrograms in combination with a CNN with a bottleneck architecture. The resulting <a href=https://en.wikipedia.org/wiki/System>system</a> has the potential to be used real-time. Our results significantly surpass the state of the art for dialogue success prediction based only on <a href=https://en.wikipedia.org/wiki/Sound>audio</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5941.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5941 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5941 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5941/>Coached Conversational Preference Elicitation : A Case Study in Understanding Movie Preferences</a></strong><br><a href=/people/f/filip-radlinski/>Filip Radlinski</a>
|
<a href=/people/k/krisztian-balog/>Krisztian Balog</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a>
|
<a href=/people/k/karthik-krishnamoorthi/>Karthik Krishnamoorthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5941><div class="card-body p-3 small">Conversational recommendation has recently attracted significant attention. As systems must understand users&#8217; preferences, training them has called for conversational corpora, typically derived from task-oriented conversations. We observe that such <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> often do not reflect how people naturally describe preferences. We present a new approach to obtaining user preferences in dialogue : Coached Conversational Preference Elicitation. It allows collection of natural yet structured conversational preferences. Studying the dialogues in one domain, we present a brief quantitative analysis of how people describe movie preferences at scale. Demonstrating the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>, we release the CCPE-M dataset to the community with over 500 movie preference dialogues expressing over 10,000 preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5944.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5944 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5944 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5944" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5944/>Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/s/shikib-mehri/>Shikib Mehri</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/a/amy-pavel/>Amy Pavel</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a>
|
<a href=/people/j/jeffrey-p-bigham/>Jeffrey Bigham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5944><div class="card-body p-3 small">The aim of this paper is to mitigate the shortcomings of automatic evaluation of open-domain dialog systems through multi-reference evaluation. Existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> have been shown to correlate poorly with <a href=https://en.wikipedia.org/wiki/Judgement>human judgement</a>, particularly in open-domain dialog. One alternative is to collect <a href=https://en.wikipedia.org/wiki/Annotation>human annotations</a> for <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, which can be expensive and time consuming. To demonstrate the effectiveness of multi-reference evaluation, we augment the test set of DailyDialog with multiple references. A series of experiments show that the use of multiple references results in improved correlation between several automatic metrics and human judgement for both the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> and the diversity of system output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5950.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5950 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5950 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5950/>Which aspects of <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relations</a> are hard to learn? Primitive decomposition for discourse relation classification</a></strong><br><a href=/people/c/charlotte-roze/>Charlotte Roze</a>
|
<a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/p/philippe-muller/>Philippe Muller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5950><div class="card-body p-3 small">Discourse relation classification has proven to be a hard task, with rather low performance on several corpora that notably differ on the relation set they use. We propose to decompose the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> into smaller, mostly binary tasks corresponding to various primitive concepts encoded into the discourse relation definitions. More precisely, we translate the discourse relations into a set of values for attributes based on distinctions used in the mappings between discourse frameworks proposed by Sanders et al. This arguably allows for a more robust representation of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>, and enables us to address usually ignored aspects of discourse relation prediction, namely multiple labels and underspecified annotations. We show experimentally which of the conceptual primitives are harder to learn from the Penn Discourse Treebank English corpus, and propose a correspondence to predict the original labels, with preliminary empirical comparisons with a direct model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5951.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5951 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5951 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5951/>Discourse Relation Prediction : Revisiting Word Pairs with Convolutional Networks</a></strong><br><a href=/people/s/siddharth-varia/>Siddharth Varia</a>
|
<a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5951><div class="card-body p-3 small">Word pairs across argument spans have been shown to be effective for predicting the <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relation</a> between them. We propose an approach to distill knowledge from word pairs for discourse relation classification with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> by incorporating joint learning of implicit and explicit relations. Our novel approach of representing the input as word pairs achieves state-of-the-art results on four-way classification of both implicit and explicit relations as well as one of the binary classification tasks. For explicit relation prediction, we achieve around 20 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> on the four-way task. At the same time, compared to a two-layered Bi-LSTM-CRF model, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to achieve these results with half the number of learnable parameters and approximately half the amount of training time.</div></div></div><hr><div id=w19-60><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-60.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-60/>Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6000/>Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers)</a></strong><br><a href=/people/a/antti-arppe/>Antti Arppe</a>
|
<a href=/people/j/jeff-good/>Jeff Good</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a>
|
<a href=/people/j/jordan-lachler/>Jordan Lachler</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/l/lane-schwartz/>Lane Schwartz</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a></span></p></div><hr><div id=w19-61><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-61.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-61/>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6100/>Proceedings of the 22nd Nordic Conference on Computational Linguistics</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6102/>Bootstrapping UD treebanks for Delexicalized Parsing<span class=acl-fixed-case>UD</span> treebanks for Delexicalized Parsing</a></strong><br><a href=/people/p/prasanth-kolachina/>Prasanth Kolachina</a>
|
<a href=/people/a/aarne-ranta/>Aarne Ranta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6102><div class="card-body p-3 small">Standard approaches to treebanking traditionally employ a <a href=https://en.wikipedia.org/wiki/Waterfall_model>waterfall model</a> (Sommerville, 2010), where annotation guidelines guide the annotation process and insights from the annotation process in turn lead to subsequent changes in the annotation guidelines. This process remains a very expensive step in creating linguistic resources for a target language, necessitates both linguistic expertise and manual effort to develop the annotations and is subject to inconsistencies in the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> due to human errors. In this paper, we propose an alternative approach to treebankingone that requires writing grammars. This approach is motivated specifically in the context of Universal Dependencies, an effort to develop uniform and cross-lingually consistent treebanks across multiple languages. We show here that a bootstrapping approach to treebanking via interlingual grammars is plausible and useful in a process where grammar engineering and treebanking are jointly pursued when creating resources for the target language. We demonstrate the usefulness of synthetic treebanks in the task of delexicalized parsing. Our experiments reveal that simple models for treebank generation are cheaper than human annotated treebanks, especially in the lower ends of the <a href=https://en.wikipedia.org/wiki/Learning_curve>learning curves</a> for delexicalized parsing, which is relevant in particular in the context of low-resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6105/>A larger-scale evaluation resource of terms and their shift direction for diachronic lexical semantics</a></strong><br><a href=/people/a/astrid-van-aggelen/>Astrid van Aggelen</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a>
|
<a href=/people/l/laura-hollink/>Laura Hollink</a>
|
<a href=/people/j/jacco-van-ossenbruggen/>Jacco van Ossenbruggen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6105><div class="card-body p-3 small">Determining how words have changed their meaning is an important topic in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. However, evaluations of methods to characterise such change have been limited to small, handcrafted resources. We introduce an English evaluation set which is larger, more varied, and more realistic than seen to date, with terms derived from a historical thesaurus. Moreover, the dataset is unique in that it represents change as a shift from the term of interest to a WordNet synset. Using the synset lemmas, we can use this set to evaluate (standard) methods that detect change between word pairs, as well as (adapted) methods that detect the change between a term and a sense overall. We show that performance on the new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> is much lower than earlier reported findings, setting a new standard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6107/>An evaluation of Czech word embeddings<span class=acl-fixed-case>C</span>zech word embeddings</a></strong><br><a href=/people/k/karolina-horenovska/>Karolína Hořeňovská</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6107><div class="card-body p-3 small">We present an evaluation of Czech low-dimensional distributed word representations, also known as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We describe five different approaches to training the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and three different corpora used in training. We evaluate the resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on five different datasets, report the results and provide their further analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6120/>Aspect-Based Sentiment Analysis using BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/m/mickel-hoang/>Mickel Hoang</a>
|
<a href=/people/o/oskar-alija-bihorac/>Oskar Alija Bihorac</a>
|
<a href=/people/j/jacobo-rouces/>Jacobo Rouces</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6120><div class="card-body p-3 small">Sentiment analysis has become very popular in both research and business due to the increasing amount of opinionated text from Internet users. Standard <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> deals with classifying the overall sentiment of a text, but this does n&#8217;t include other important information such as towards which entity, topic or aspect within the text the sentiment is directed. Aspect-based sentiment analysis (ABSA) is a more complex task that consists in identifying both sentiments and aspects. This paper shows the potential of using the contextual word representations from the pre-trained language model BERT, together with a fine-tuning method with additional generated text, in order to solve out-of-domain ABSA and outperform previous state-of-the-art results on SemEval-2015 Task 12 subtask 2 and SemEval-2016 Task 5. To the best of our knowledge, no other existing work has been done on out-of-domain ABSA for aspect classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6122 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-6122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-6122/>Joint Rumour Stance and Veracity Prediction</a></strong><br><a href=/people/a/anders-edelbo-lillie/>Anders Edelbo Lillie</a>
|
<a href=/people/e/emil-refsgaard-middelboe/>Emil Refsgaard Middelboe</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6122><div class="card-body p-3 small">The net is rife with <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a> that spread through <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Not all the claims in these can be verified. However, recent work has shown that the stances alone that commenters take toward claims can be sufficiently good indicators of claim veracity, using e.g. an <a href=https://en.wikipedia.org/wiki/Head-up_display>HMM</a> that takes conversational stance sequences as the only input. Existing results are monolingual (English) and mono-platform (Twitter). This paper introduces a stance-annotated Reddit dataset for the <a href=https://en.wikipedia.org/wiki/Danish_language>Danish language</a>, and describes various implementations of stance classification models. Of these, a Linear SVM provides predicts stance best, with 0.76 accuracy / 0.42 macro F1. Stance labels are then used to predict veracity across platforms and also across languages, training on conversations held in one language and using the model on conversations held in another. In our experiments, monolinugal scores reach stance-based veracity accuracy of 0.83 (F1 0.68) ; applying the model across languages predicts veracity of claims with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.82 (F1 0.67). This demonstrates the surprising and powerful viability of transferring stance-based veracity prediction across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6126 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6126/>Matching Keys and Encrypted Manuscripts</a></strong><br><a href=/people/e/eva-pettersson/>Eva Pettersson</a>
|
<a href=/people/b/beata-megyesi/>Beata Megyesi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6126><div class="card-body p-3 small">Historical cryptology is the study of historical encrypted messages aiming at their <a href=https://en.wikipedia.org/wiki/Cryptography>decryption</a> by analyzing the mathematical, linguistic and other coding patterns and their historical context. In <a href=https://en.wikipedia.org/wiki/Library>libraries</a> and archives we can find quite a lot of <a href=https://en.wikipedia.org/wiki/Cipher>ciphers</a>, as well as keys describing the method used to transform the plaintext message into a <a href=https://en.wikipedia.org/wiki/Ciphertext>ciphertext</a>. In this paper, we present work on automatically mapping <a href=https://en.wikipedia.org/wiki/Key_(cryptography)>keys</a> to <a href=https://en.wikipedia.org/wiki/Cipher>ciphers</a> to reconstruct the original <a href=https://en.wikipedia.org/wiki/Plaintext>plaintext message</a>, and use language models generated from historical texts to guess the underlying <a href=https://en.wikipedia.org/wiki/Plaintext>plaintext language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6127 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6127/>Perceptual and acoustic analysis of voice similarities between parents and young children</a></strong><br><a href=/people/e/evgeniia-rykova/>Evgeniia Rykova</a>
|
<a href=/people/s/stefan-werner/>Stefan Werner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6127><div class="card-body p-3 small">Human voice provides the means for <a href=https://en.wikipedia.org/wiki/Linguistics>verbal communication</a> and forms a part of personal identity. Due to genetic and environmental factors, a voice of a child should resemble the voice of her parent(s), but voice similarities between parents and young children are underresearched. Read-aloud speech of Finnish-speaking and Russian-speaking parent-child pairs was subject to perceptual and multi-step instrumental and statistical analysis. Finnish-speaking listeners could not discriminate family pairs auditorily in an XAB paradigm, but the Russian-speaking listeners&#8217; mean accuracy of answers reached 72.5 %. On average, in both language groups family-internal f0 similarities were stronger than family-external, with parents showing greater family-internal similarities than children. Auditory similarities did not reflect acoustic similarities in a straightforward way.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6128 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6128/>Enhancing <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> through Cross-Modal Interaction : Meaning Recovery from Acoustically Noisy Speech</a></strong><br><a href=/people/o/ozge-alacam/>Ozge Alacam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6128><div class="card-body p-3 small">Cross-modality between vision and language is a key component for effective and efficient <a href=https://en.wikipedia.org/wiki/Communication>communication</a>, and human language processing mechanism successfully integrates information from various modalities to extract the intended meaning. However, incomplete linguistic input, i.e. due to a noisy environment, is one of the challenges for a successful <a href=https://en.wikipedia.org/wiki/Communication>communication</a>. In that case, an <a href=https://en.wikipedia.org/wiki/Completeness_(logic)>incompleteness</a> in one channel can be compensated by information from another one. In this paper, by conducting visual-world paradigm, we investigated the dynamics between syntactically possible gap fillers and the visual arrangements in incomplete German sentences and their effect on overall sentence interpretation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6129 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-6129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-6129/>Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/a/antti-suni/>Antti Suni</a>
|
<a href=/people/h/hande-celikkanat/>Hande Celikkanat</a>
|
<a href=/people/s/sofoklis-kakouros/>Sofoklis Kakouros</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/martti-vainio/>Martti Vainio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6129><div class="card-body p-3 small">In this paper we introduce a new natural language processing dataset and <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for predicting prosodic prominence from written text. To our knowledge this will be the largest publicly available dataset with <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic labels</a>. We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature-based classifiers to <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network systems</a> for the prediction of discretized prosodic prominence. We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10 % of the training data. Finally we discuss the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> in light of the results and point to future research and plans for further improving both the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and methods of predicting prosodic prominence from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the code for the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> will be made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6130 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6130/>Toward Multilingual Identification of Online Registers</a></strong><br><a href=/people/v/veronika-laippala/>Veronika Laippala</a>
|
<a href=/people/r/roosa-kyllonen/>Roosa Kyllönen</a>
|
<a href=/people/j/jesse-egbert/>Jesse Egbert</a>
|
<a href=/people/d/douglas-biber/>Douglas Biber</a>
|
<a href=/people/s/sampo-pyysalo/>Sampo Pyysalo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6130><div class="card-body p-3 small">We consider cross- and multilingual text classification approaches to the identification of online registers (genres), i.e. text varieties with specific situational characteristics. Register is the most important predictor of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a>, and register information could improve the potential of online data for many applications. We introduce the first manually annotated non-English corpus of online registers featuring the full range of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> found online. The data set consists of 2,237 Finnish documents and follows the register taxonomy developed for the Corpus of Online Registers of English (CORE). Using CORE and the newly introduced corpus, we demonstrate the feasibility of cross-lingual register identification using a simple approach based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> and multilingual word embeddings. We further find that register identification results can be improved through multilingual training even when a substantial number of annotations is available in the target language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6131 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6131/>A Wide-Coverage Symbolic Natural Language Inference System</a></strong><br><a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6131><div class="card-body p-3 small">We present a system for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a> which uses a dynamic semantics converter from <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>abstract syntax trees</a> to <a href=https://en.wikipedia.org/wiki/Coq>Coq types</a>. It combines the fine-grainedness of a dynamic semantics system with the powerfulness of a state-of-the-art <a href=https://en.wikipedia.org/wiki/Proof_assistant>proof assistant</a>, like <a href=https://en.wikipedia.org/wiki/Coq>Coq</a>. We evaluate the <a href=https://en.wikipedia.org/wiki/System>system</a> on all sections of the FraCaS test suite, excluding section 6. This is the first system that does a complete run on the anaphora and ellipsis sections of the FraCaS. It has a better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>overall accuracy</a> than any previous <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6133 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6133/>Nefnir : A high accuracy lemmatizer for Icelandic<span class=acl-fixed-case>N</span>efnir: A high accuracy lemmatizer for <span class=acl-fixed-case>I</span>celandic</a></strong><br><a href=/people/s/svanhvit-lilja-ingolfsdottir/>Svanhvít Lilja Ingólfsdóttir</a>
|
<a href=/people/h/hrafn-loftsson/>Hrafn Loftsson</a>
|
<a href=/people/j/jon-fridrik-dadason/>Jón Friðrik Daðason</a>
|
<a href=/people/k/kristin-bjarnadottir/>Kristín Bjarnadóttir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6133><div class="card-body p-3 small">Lemmatization, finding the basic <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological form</a> of a word in a corpus, is an important step in many natural language processing tasks when working with morphologically rich languages. We describe and evaluate Nefnir, a new open source lemmatizer for <a href=https://en.wikipedia.org/wiki/Icelandic_language>Icelandic</a>. Nefnir uses suffix substitution rules, derived from a large morphological database, to lemmatize tagged text. Evaluation shows that for correctly tagged text, Nefnir obtains an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 99.55 %, and for text tagged with a PoS tagger, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> obtained is 96.88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6134 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6134/>Natural Language Processing in Policy Evaluation : Extracting Policy Conditions from IMF Loan Agreements<span class=acl-fixed-case>IMF</span> Loan Agreements</a></strong><br><a href=/people/j/joakim-akerstrom/>Joakim Åkerström</a>
|
<a href=/people/a/adel-daoud/>Adel Daoud</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6134><div class="card-body p-3 small">Social science researchers often use text as the raw data in investigations : for instance, when investigating the effects of IMF policies on the development of countries under IMF programs, researchers typically encode structured descriptions of the programs using a time-consuming manual effort. Making this process automatic may open up new opportunities in scaling up such investigations. As a first step towards automatizing this coding process, we describe an experiment where we apply a sentence classifier that automatically detects mentions of policy conditions in IMF loan agreements and divides them into different types. The results show that the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> is generally able to detect the policy conditions, although some types are hard to distinguish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6137 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6137/>Predicates as Boxes in Bayesian Semantics for Natural Language<span class=acl-fixed-case>B</span>ayesian Semantics for Natural Language</a></strong><br><a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/r/rasmus-blanck/>Rasmus Blanck</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/s/shalom-lappin/>Shalom Lappin</a>
|
<a href=/people/a/aleksandre-maskharashvili/>Aleksandre Maskharashvili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6137><div class="card-body p-3 small">In this paper, we present a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian approach</a> to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language semantics</a>. Our main focus is on the <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference task</a> in an environment where <a href=https://en.wikipedia.org/wiki/Judgement>judgments</a> require <a href=https://en.wikipedia.org/wiki/Probabilistic_reasoning>probabilistic reasoning</a>. We treat <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>, <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>, <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>, etc. as <a href=https://en.wikipedia.org/wiki/Unary_operation>unary predicates</a>, and we model them as boxes in a <a href=https://en.wikipedia.org/wiki/Bounded_set>bounded domain</a>. We apply <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian learning</a> to satisfy <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> expressed as premises. In this way we construct a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, by specifying boxes for the <a href=https://en.wikipedia.org/wiki/Predicate_(mathematical_logic)>predicates</a>. The probability of the hypothesis (the conclusion) is evaluated against the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that incorporates the premises as <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6146 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6146/>The OPUS Resource Repository : An Open Package for Creating Parallel Corpora and Machine Translation Services<span class=acl-fixed-case>OPUS</span> Resource Repository: An Open Package for Creating Parallel Corpora and Machine Translation Services</a></strong><br><a href=/people/m/mikko-aulamo/>Mikko Aulamo</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6146><div class="card-body p-3 small">This paper presents a flexible and powerful system for creating parallel corpora and for running neural machine translation services. Our package provides a scalable data repository backend that offers transparent data pre-processing pipelines and automatic alignment procedures that facilitate the compilation of extensive parallel data sets from a variety of sources. Moreover, we develop a <a href=https://en.wikipedia.org/wiki/Web_application>web-based interface</a> that constitutes an intuitive frontend for end-users of the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>. The whole <a href=https://en.wikipedia.org/wiki/System>system</a> can easily be distributed over virtual machines and implements a sophisticated permission system with secure connections and a flexible database for storing arbitrary metadata. Furthermore, we also provide an interface for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> that can run as a service on <a href=https://en.wikipedia.org/wiki/Virtual_machine>virtual machines</a>, which also incorporates a connection to the data repository software.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6147 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6147/>Garnishing a <a href=https://en.wikipedia.org/wiki/Phonetic_dictionary>phonetic dictionary</a> for ASR intake<span class=acl-fixed-case>ASR</span> intake</a></strong><br><a href=/people/i/iben-nyholm-debess/>Iben Nyholm Debess</a>
|
<a href=/people/s/sandra-saxov-lamhauge/>Sandra Saxov Lamhauge</a>
|
<a href=/people/p/peter-juel-henrichsen/>Peter Juel Henrichsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6147><div class="card-body p-3 small">We present a new method for preparing a lexical-phonetic database as a resource for acoustic model training. The <a href=https://en.wikipedia.org/wiki/Research>research</a> is an offshoot of the ongoing Project Ravnur (Speech Recognition for Faroese), but the method is language-independent. At NODALIDA 2019 we demonstrate the method (called SHARP) online, showing how a traditional lexical-phonetic dictionary (with a very rich phone inventory) is transformed into an ASR-friendly database (with reduced phonetics, preventing data sparseness). The <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping procedure</a> is informed by a <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus of speech transcripts</a>. We conclude with a discussion on the benefits of a well-thought-out BLARK design (Basic Language Resource Kit), making tools like SHARP possible.</div></div></div><hr><div id=w19-62><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-62.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-62/>Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6200/>Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing</a></strong><br><a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/l/leon-derczynski/>Leon Derczynski</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/b/bjorn-lindi/>Bjørn Lindi</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/j/jorg-tidemann/>Jörg Tidemann</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6202/>Improving Semantic Dependency Parsing with Syntactic Features</a></strong><br><a href=/people/r/robin-kurtz/>Robin Kurtz</a>
|
<a href=/people/d/daniel-roxbo/>Daniel Roxbo</a>
|
<a href=/people/m/marco-kuhlmann/>Marco Kuhlmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6202><div class="card-body p-3 small">We extend a state-of-the-art deep neural architecture for semantic dependency parsing with features defined over syntactic dependency trees. Our empirical results show that only gold-standard syntactic information leads to consistent improvements in semantic parsing accuracy, and that the magnitude of these improvements varies with the specific combination of the syntactic and the semantic representation used. In contrast, automatically predicted syntax does not seem to help <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. Our <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> suggests that there is a significant overlap between syntactic and semantic representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-6204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-6204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-6204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-6204/>Is Multilingual BERT Fluent in <a href=https://en.wikipedia.org/wiki/Language_generation>Language Generation</a>?<span class=acl-fixed-case>BERT</span> Fluent in Language Generation?</a></strong><br><a href=/people/s/samuel-ronnqvist/>Samuel Rönnqvist</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/t/tapio-salakoski/>Tapio Salakoski</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-6204><div class="card-body p-3 small">The multilingual BERT model is trained on 104 languages and meant to serve as a universal language model and tool for encoding sentences. We explore how well the model performs on several languages across several tasks : a diagnostic classification probing the embeddings for a particular syntactic property, a cloze task testing the language modelling ability to fill in gaps in a sentence, and a natural language generation task testing for the ability to produce coherent text fitting a given context. We find that the currently available multilingual BERT model is clearly inferior to the monolingual counterparts, and can not in many cases serve as a substitute for a well-trained monolingual model. We find that the English and German models perform well at generation, whereas the multilingual model is lacking, in particular, for <a href=https://en.wikipedia.org/wiki/North_Germanic_languages>Nordic languages</a>. The code of the experiments in the paper is available at : https://github.com/TurkuNLP/bert-eval</div></div></div><hr><div id=w19-63><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-63.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-63/>Proceedings of the 8th Workshop on NLP for Computer Assisted Language Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6300/>Proceedings of the 8th Workshop on NLP for Computer Assisted Language Learning</a></strong><br><a href=/people/d/david-alfter/>David Alfter</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a>
|
<a href=/people/l/lars-borin/>Lars Borin</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilan</a>
|
<a href=/people/h/herbert-lange/>Herbert Lange</a></span></p></div><hr><div id=w19-64><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-64.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-64/>Proceedings of the Second Financial Narrative Processing Workshop (FNP 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6400/>Proceedings of the Second Financial Narrative Processing Workshop (FNP 2019)</a></strong><br><a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a>
|
<a href=/people/s/steve-young/>Steven Young</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/s/sira-ferradans/>Sira Ferradans</a></span></p></div><hr><div id=w19-65><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-65.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-65/>Proceedings of the Workshop on NLP and Pseudonymisation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6500/>Proceedings of the Workshop on NLP and Pseudonymisation</a></strong><br><a href=/people/l/lars-ahrenberg/>Lars Ahrenberg</a>
|
<a href=/people/b/beata-megyesi/>Beata Megyesi</a></span></p></div><hr><div id=w19-66><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-66.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-66/>Proceedings of Machine Translation Summit XVII: Research Track</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6600/>Proceedings of Machine Translation Summit XVII: Research Track</a></strong><br><a href=/people/m/mikel-l-forcada/>Mikel Forcada</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p></div><hr><div id=w19-67><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-67.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-67/>Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6700/>Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks</a></strong><br><a href=/people/m/mikel-l-forcada/>Mikel Forcada</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/j/john-tinsley/>John Tinsley</a>
|
<a href=/people/d/dimitar-shterionov/>Dimitar Shterionov</a>
|
<a href=/people/c/celia-rico/>Celia Rico</a>
|
<a href=/people/f/federico-gaspari/>Federico Gaspari</a></span></p></div><hr><div id=w19-68><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-68.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-68/>Proceedings of the 2nd Workshop on Technologies for MT of Low Resource Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6800/>Proceedings of the 2nd Workshop on Technologies for MT of Low Resource Languages</a></strong><br><a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a>
|
<a href=/people/j/jonathan-washington/>Jonathan Washington</a>
|
<a href=/people/n/nathaniel-oco/>Nathaniel Oco</a>
|
<a href=/people/s/surafel-melaku-lakew/>Surafel Melaku Lakew</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a>
|
<a href=/people/x/xiaobing-zhao/>Xiaobing Zhao</a></span></p></div><hr><div id=w19-69><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-69.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-69/>Proceedings of the Celtic Language Technology Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-6900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-6900/>Proceedings of the Celtic Language Technology Workshop</a></strong><br><a href=/people/t/teresa-lynn/>Teresa Lynn</a>
|
<a href=/people/d/delyth-prys/>Delyth Prys</a>
|
<a href=/people/c/colin-batchelor/>Colin Batchelor</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a></span></p></div><hr><div id=w19-70><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-70.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-70/>Proceedings of the Second MEMENTO workshop on Modelling Parameters of Cognitive Effort in Translation Production</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7000/>Proceedings of the Second MEMENTO workshop on Modelling Parameters of Cognitive Effort in Translation Production</a></strong><br><a href=/people/m/michael-carl/>Michael Carl</a>
|
<a href=/people/s/silvia-hansen-schirra/>Silvia Hansen-Schirra</a></span></p></div><hr><div id=w19-71><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-71.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-71/>Proceedings of the Second Workshop on Multilingualism at the Intersection of Knowledge Bases and Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7100/>Proceedings of the Second Workshop on Multilingualism at the Intersection of Knowledge Bases and Machine Translation</a></strong><br><a href=/people/m/mihael-arcan/>Mihael Arcan</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/j/jinhua-du/>Jinhua Du</a>
|
<a href=/people/d/dimitar-shterionov/>Dimitar Shterionov</a>
|
<a href=/people/d/daniel-torregrosa/>Daniel Torregrosa</a></span></p></div><hr><div id=w19-72><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-72.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-72/>Proceedings of The 8th Workshop on Patent and Scientific Literature Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7200/>Proceedings of The 8th Workshop on Patent and Scientific Literature Translation</a></strong><br><a href=/people/t/takehito-utsuro/>Takehito Utsuro</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/t/takashi-tsunakawa/>Takashi Tsunakawa</a></span></p></div><hr><div id=w19-73><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-73.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-73/>Proceedings of the Qualities of Literary Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7300/>Proceedings of the Qualities of Literary Machine Translation</a></strong><br><a href=/people/j/james-hadley/>James Hadley</a>
|
<a href=/people/m/maja-popovic/>Maja Popović</a>
|
<a href=/people/h/haithem-afli/>Haithem Afli</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p></div><hr><div id=w19-74><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-74.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-74/>Proceedings of the 3rd International Conference on Natural Language and Speech Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7400/>Proceedings of the 3rd International Conference on Natural Language and Speech Processing</a></strong><br><a href=/people/m/mourad-abbas/>Mourad Abbas</a>
|
<a href=/people/a/abed-alhakim-freihat/>Abed Alhakim Freihat</a></span></p></div><hr><div id=w19-75><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-75.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-75/>Proceedings of the 6th International Sanskrit Computational Linguistics Symposium</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7500/>Proceedings of the 6th International Sanskrit Computational Linguistics Symposium</a></strong><br><a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p></div><hr><div id=w19-76><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W19-76/>Proceedings of Machine Translation Summit XVII: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7600/>Proceedings of Machine Translation Summit XVII: Tutorial Abstracts</a></strong><br><a href=/people/l/laura-rossi/>Laura Rossi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-7601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-7601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7601/>The unreasonable effectiveness of Neural Models in Language Decoding</a></strong><br><a href=/people/t/tony-o-dowd/>Tony O'Dowd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-7601><div class="card-body p-3 small">This tutorial will provide an in-depth look at the experiments, jointly carried out by KantanMT and eBay during 2018, to determine which Neural Model delivers the best translation performance for eBay Customer Service content. It will lay out the timeline, process and mechanisms used to customise Neural MT models and how these were used in conjunction with Human Based evaluations to determine which approach to Neural MT provided the best translation outcomes.\n\nThe tutorial will cover the following topics and methods:\n- Structural differences in Neural Networks and how they assist the language decoding process &#8211; RNN, CNN and TNN will be covered in detailed.\n- Customisation of Neural MT using the KantanMT Platform\n- Using MQM Framework for the evaluation and comparison of Translation Outputs and comparison to Human Translation\n- Collation and analysis of experimental findings in reaching our decision to standardise on Transformer type networks.\n\nParticipants of the tutorial will get a clear understanding of Neural Model types and the differences, it will also cover how to customise these models and then how to set up a controlled experiment to determine translation performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-7602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-7602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-7602.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-7602/>Challenge Test Sets for <span class=acl-fixed-case>MT</span> Evaluation</a></strong><br><a href=/people/m/maja-popovic/>Maja Popović</a>
|
<a href=/people/s/sheila-castilho/>Sheila Castilho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-7602><div class="card-body p-3 small">Most of the test sets used for the evaluation of MT systems reflect the frequency distribution of different phenomena found in naturally occurring data (&#8221;standard&#8221; or &#8221;natural&#8221; test sets). However, to better understand particular strengths and weaknesses of MT systems, especially those based on neural networks, it is necessary to apply more focused evaluation procedures. Therefore, another type of test sets (&#8221;challenge&#8221; test sets, also called &#8221;test suites&#8221;) is being increasingly employed in order to highlight points of difficulty which are relevant to model development, training, or using of the given system. This tutorial will be useful for anyone (researchers, developers, users, translators) interested in detailed evaluation and getting a better understanding of machine translation (MT) systems and models. The attendees will learn about the motivation and linguistic background of challenge test sets and a range of testing possibilities applied to the state-of-the-art MT systems, as well as a number of practical aspects and challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-7603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-7603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7603/>A Deep Learning Curve for Post-Editing 2</a></strong><br><a href=/people/l/lena-marg/>Lena Marg</a>
|
<a href=/people/a/alex-yanishevsky/>Alex> Yanishevsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-7603><div class="card-body p-3 small">In the last couple of years, machine translation technology has seen major changes with the breakthrough of neural machine translation (NMT), a growing number of providers and translation platforms. Machine Translation generally is experiencing a peak in demand from translation buyers, thanks to Machine Learning and AI being omnipresent in the media and at industry events. At the same time, new models for defining translation quality are becoming more widely adopted. These changes have profound implications for translators, LSPs and translation buyers: translators have to adjust their post-editing approaches, while LSPs and translation buyers are faced with decisions on selecting providers, best approaches for updating MT systems, financial investments, integrating tools, and getting the timing for implementation right for an optimum ROI.\n\nIn this tutorial on MT and post-editing we would like to continue sharing the latest trends in the field of MT technologies, and discuss their impact on post-editing practices as well as integrating MT on large, multi-language translation programs. We will look at tool compatibility, different use cases of MT and dynamic quality models, and share our experience of measuring performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-7604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-7604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7604/>Practical Statistics for Research in Machine Translation and Translation Studies</a></strong><br><a href=/people/a/antonio-toral/>Antonio Toral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-7604><div class="card-body p-3 small">The tutorial will introduce a set of very useful statistical tests for conducting analyses in the research areas of Machine Translation (MT) and Translation Studies (TS). For each statistical test, the presenter will: 1) introduce it in the context of a common research example that pertains to the area of MT and/or TS 2) explain the technique behind the test and its assumptions 3) cover common pitfalls when the test is applied in research studies, and 4) conduct a hands-on activity so that attendees can put the knowledge acquired in practice straight-away. All examples and exercises will be in R. The following statistical tests will be covered: t-tests (both parametric and non-parametric), bootstrap resampling, Pearson and Spearman correlation coefficients, linear mixed-effects models.</div></div></div><hr><div id=w19-77><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-77.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-77/>Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7700/>Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019)</a></strong><br><a href=/people/k/kim-gerdes/>Kim Gerdes</a>
|
<a href=/people/s/sylvain-kahane/>Sylvain Kahane</a></span></p></div><hr><div id=w19-78><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-78.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-78/>Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7800/>Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)</a></strong><br><a href=/people/m/marie-candito/>Marie Candito</a>
|
<a href=/people/k/kilian-evang/>Kilian Evang</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/d/djame-seddah/>Djamé Seddah</a></span></p></div><hr><div id=w19-79><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-79.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-79/>Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-7900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-7900/>Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019)</a></strong><br><a href=/people/x/xinying-chen/>Xinying Chen</a>
|
<a href=/people/r/ramon-ferrer-i-cancho/>Ramon Ferrer-i-Cancho</a></span></p></div><hr><div id=w19-80><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-80.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-80/>Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8000/>Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)</a></strong><br><a href=/people/a/alexandre-rademaker/>Alexandre Rademaker</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a></span></p></div><hr><div id=w19-81><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-81.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-81/>Proceedings of the 1st Workshop on Discourse Structure in Neural NLG</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8100/>Proceedings of the 1st Workshop on Discourse Structure in Neural NLG</a></strong><br><a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/c/chandra-khatri/>Chandra Khatri</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/d/donia-scott/>Donia Scott</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/m/michael-white/>Michael White</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8102/>Incorporating Textual Evidence in Visual Storytelling</a></strong><br><a href=/people/t/tianyi-li/>Tianyi Li</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8102><div class="card-body p-3 small">Previous work on <a href=https://en.wikipedia.org/wiki/Visual_storytelling>visual storytelling</a> mainly focused on exploring image sequence as evidence for <a href=https://en.wikipedia.org/wiki/Storytelling>storytelling</a> and neglected textual evidence for guiding <a href=https://en.wikipedia.org/wiki/Storytelling>story generation</a>. Motivated by human storytelling process which recalls stories for familiar images, we exploit textual evidence from similar images to help generate coherent and meaningful stories. To pick the <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> which may provide textual experience, we propose a two-step ranking method based on image object recognition techniques. To utilize textual information, we design an extended Seq2Seq model with two-channel encoder and attention. Experiments on the VIST dataset show that our method outperforms state-of-the-art baseline models without heavy engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8103/>Fine-Grained Control of <a href=https://en.wikipedia.org/wiki/Sentence_segmentation>Sentence Segmentation</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>Entity Positioning</a> in Neural NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/k/kritika-mehta/>Kritika Mehta</a>
|
<a href=/people/r/raheel-qader/>Raheel Qader</a>
|
<a href=/people/c/cyril-labbe/>Cyril Labbe</a>
|
<a href=/people/f/francois-portet/>François Portet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8103><div class="card-body p-3 small">The move from pipeline Natural Language Generation (NLG) approaches to neural end-to-end approaches led to a loss of control in sentence planning operations owing to the conflation of intermediary micro-planning stages into a single model. Such control is highly necessary when the text should be tailored to respect some constraints such as which entity to be mentioned first, the entity position, the complexity of sentences, etc. In this paper, we introduce fine-grained control of sentence planning in neural data-to-text generation models at two levels-realization of input entities in desired sentences and realization of the input entities in the desired position among individual sentences. We show that by augmenting the input with explicit position identifiers, the neural model can achieve a great control over the output structure while keeping the naturalness of the generated text intact. Since sentence level metrics are not entirely suitable to evaluate this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we used a metric specific to our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that accounts for the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s ability to achieve control. The results demonstrate that the position identifiers do constraint the neural model to respect the intended output structure which can be useful in a variety of domains that require the generated text to be in a certain structure.</div></div></div><hr><div id=w19-83><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-83.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-83/>Proceedings of the 1st International Workshop of AI Werewolf and Dialog System (AIWolfDial2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8300/>Proceedings of the 1st International Workshop of <span class=acl-fixed-case>AI</span> Werewolf and Dialog System (<span class=acl-fixed-case>AIW</span>olf<span class=acl-fixed-case>D</span>ial2019)</a></strong><br><a href=/people/y/yoshinobu-kano/>Yoshinobu Kano</a>
|
<a href=/people/c/claus-aranha/>Claus Aranha</a>
|
<a href=/people/m/michimasa-inaba/>Michimasa Inaba</a>
|
<a href=/people/f/fujio-toriumi/>Fujio Toriumi</a>
|
<a href=/people/h/hirotaka-osawa/>Hirotaka Osawa</a>
|
<a href=/people/d/daisuke-katagami/>Daisuke Katagami</a>
|
<a href=/people/t/takashi-otsuki/>Takashi Otsuki</a></span></p></div><hr><div id=w19-84><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-84.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-84/>Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8400/>Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019)</a></strong><br><a href=/people/j/jose-m-alonso/>Jose M. Alonso</a>
|
<a href=/people/a/alejandro-catala/>Alejandro Catala</a></span></p></div><hr><div id=w19-85><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-85.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-85/>Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8500/>Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology</a></strong><br><a href=/people/m/magda-sevcikova/>Magda Ševčíková</a>
|
<a href=/people/z/zdenek-zabokrtsky/>Zdeněk Žabokrtský</a>
|
<a href=/people/e/eleonora-litta/>Eleonora Litta</a>
|
<a href=/people/m/marco-passarotti/>Marco Passarotti</a></span></p></div><hr><div id=w19-86><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W19-86/>Proceedings of the 12th International Conference on Natural Language Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8600/>Proceedings of the 12th International Conference on Natural Language Generation</a></strong><br><a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8605/>Choosing between Long and Short Word Forms in Mandarin<span class=acl-fixed-case>M</span>andarin</a></strong><br><a href=/people/l/lin-li/>Lin Li</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/d/denis-paperno/>Denis Paperno</a>
|
<a href=/people/j/jingyu-fan/>Jingyu Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8605><div class="card-body p-3 small">Between 80 % and 90 % of all Chinese words have long and short form such as / (lao-hu / hu, tiger) (Duanmu:2013). Consequently, the choice between long and short forms is a key problem for <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a> across <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLG</a>. Following an earlier work on <a href=https://en.wikipedia.org/wiki/Abbreviation>abbreviations</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> (Mahowald et al, 2013), we bring a probabilistic perspective to these questions, using both a behavioral and a corpus-based approach. We hypothesized that there is a higher probability of choosing short form in supportive context than in neutral context in <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a>. Consistent with our prediction, our findings revealed that predictability of contexts makes effect on speakers&#8217; long and short form choice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8609/>Importance of Search and Evaluation Strategies in Neural Dialogue Modeling</a></strong><br><a href=/people/i/ilia-kulikov/>Ilia Kulikov</a>
|
<a href=/people/a/alexander-miller/>Alexander Miller</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8609><div class="card-body p-3 small">We investigate the impact of <a href=https://en.wikipedia.org/wiki/Search_algorithm>search strategies</a> in neural dialogue modeling. We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses. We evaluate these strategies in realistic full conversations with humans and propose a model-based Bayesian calibration to address annotator bias. These conversations are analyzed using two automatic metrics : log-probabilities assigned by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and utterance diversity. Our experiments reveal that better <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithms</a> lead to higher rated conversations. However, finding the optimal selection mechanism to choose from a more diverse set of candidates is still an open question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8610" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8610/>Towards Best Experiment Design for Evaluating Dialogue System Output</a></strong><br><a href=/people/s/sashank-santhanam/>Sashank Santhanam</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8610><div class="card-body p-3 small">To overcome the limitations of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automated metrics</a> (e.g. BLEU, METEOR) for evaluating dialogue systems, researchers typically use <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> to provide convergent evidence. While it has been demonstrated that <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> can suffer from the inconsistency of ratings, extant research has also found that the design of the evaluation task affects the consistency and quality of human judgments. We conduct a between-subjects study to understand the impact of four experiment conditions on human ratings of dialogue system output. In addition to discrete and continuous scale ratings, we also experiment with a novel application of Best-Worst scaling to dialogue evaluation. Through our systematic study with 40 crowdsourced workers in each task, we find that using continuous scales achieves more consistent ratings than Likert scale or ranking-based experiment design. Additionally, we find that factors such as time taken to complete the task and no prior experience of participating in similar studies of rating dialogue system output positively impact <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a> and agreement amongst raters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8611/>A Tree-to-Sequence Model for Neural NLG in Task-Oriented Dialog<span class=acl-fixed-case>NLG</span> in Task-Oriented Dialog</a></strong><br><a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/k/kartikeya-upasani/>Kartikeya Upasani</a>
|
<a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/m/michael-white/>Michael White</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8611><div class="card-body p-3 small">Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Sequence-to-sequence models on flat meaning representations (MR) have been dominant in this task, for example in the E2E NLG Challenge. Previous work has shown that a tree-structured MR can improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for better discourse-level structuring and sentence-level planning. In this work, we propose a tree-to-sequence model that uses a tree-LSTM encoder to leverage the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structures</a> in the input MR, and further enhance the decoding by a structure-enhanced attention mechanism. In addition, we explore combining these enhancements with constrained decoding to improve semantic correctness. Our experiments not only show significant improvements over standard seq2seq baselines, but also is more data-efficient and generalizes better to hard scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8615/>MinWikiSplit : A Sentence Splitting Corpus with Minimal Propositions<span class=acl-fixed-case>M</span>in<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>S</span>plit: A Sentence Splitting Corpus with Minimal Propositions</a></strong><br><a href=/people/c/christina-niklaus/>Christina Niklaus</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a>
|
<a href=/people/s/siegfried-handschuh/>Siegfried Handschuh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8615><div class="card-body p-3 small">We compiled a new sentence splitting corpus that is composed of 203 K pairs of aligned complex source and simplified target sentences. Contrary to previously proposed text simplification corpora, which contain only a small number of split examples, we present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> where each input sentence is broken down into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances with each of them presenting a minimal semantic unit that can not be further decomposed into meaningful propositions. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is useful for developing sentence splitting approaches that learn how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8621/>Tell Me More : A Dataset of Visual Scene Description Sequences</a></strong><br><a href=/people/n/nikolai-ilinykh/>Nikolai Ilinykh</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8621><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of what we call image description sequences, which are multi-sentence descriptions of the contents of an image. These descriptions were collected in a pseudo-interactive setting, where the describer was told to describe the given image to a listener who needs to identify the image within a set of images, and who successively asks for more information. As we show, this setup produced nicely structured data that, we think, will be useful for learning models capable of planning and realising such description discourses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8622.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8622/>A Closer Look at Recent Results of Verb Selection for Data-to-Text NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8622><div class="card-body p-3 small">Automatic natural language generation systems need to use the contextually-appropriate verbs when describing different kinds of facts or events, which has triggered research interest on verb selection for data-to-text generation. In this paper, we discuss a few limitations of the current task settings and the evaluation metrics. We also provide two simple, efficient, interpretable baseline approaches for statistical selection of trend verbs, which give a strong performance on both previously used <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation metrics</a> and our new <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8624/>BERT for Question Generation<span class=acl-fixed-case>BERT</span> for Question Generation</a></strong><br><a href=/people/y/ying-hong-chan/>Ying-Hong Chan</a>
|
<a href=/people/y/yao-chung-fan/>Yao-Chung Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8624><div class="card-body p-3 small">In this study, we investigate the employment of the pre-trained BERT language model to tackle question generation tasks. We introduce two neural architectures built on top of BERT for question generation tasks. The first one is a straightforward BERT employment, which reveals the defects of directly using BERT for text generation. And, the second one remedies the first one by restructuring the BERT employment into a sequential manner for taking information from previous decoded results. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained and evaluated on the question-answering dataset SQuAD. Experiment results show that our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields state-of-the-art performance which advances the BLEU4 score of existing best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> from 16.85 to 18.91.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8627 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8627/>Neural Conversation Model Controllable by Given Dialogue Act Based on Adversarial Learning and Label-aware Objective</a></strong><br><a href=/people/s/seiya-kawano/>Seiya Kawano</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8627><div class="card-body p-3 small">Building a controllable neural conversation model (NCM) is an important task. In this paper, we focus on controlling the responses of NCMs by using dialogue act labels of responses as conditions. We introduce an adversarial learning framework for the task of generating conditional responses with a new objective to a discriminator, which explicitly distinguishes sentences by using labels. This change strongly encourages the generation of label-conditioned sentences. We compared the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with some existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for generating conditional responses. The experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> has higher <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> for dialogue acts even though it has higher or comparable naturalness to existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8632 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8632/>Margin Call : an Accessible Web-based Text Viewer with Generated Paragraph Summaries in the Margin</a></strong><br><a href=/people/n/naba-rizvi/>Naba Rizvi</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/l/lidan-wang/>Lidan Wang</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8632><div class="card-body p-3 small">We present <a href=https://en.wikipedia.org/wiki/Margin_Call>Margin Call</a>, a web-based text viewer that automatically generates short summaries for each paragraph of the text and displays the summaries in the margin of the text next to the corresponding paragraph. On the back-end, the summarizer first identifies the most important sentence for each paragraph in the text file uploaded by the user. The selected sentence is then automatically compressed to produce the short summary. The resulting summary is a few words long. The displayed summaries can help the user understand and retrieve information faster from the text, while increasing the retention of information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8634.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8634 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8634 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8634/>Personalized Substitution Ranking for Lexical Simplification</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/c/chak-yan-yeung/>Chak Yan Yeung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8634><div class="card-body p-3 small">A lexical simplification (LS) system substitutes difficult words in a text with simpler ones to make it easier for the user to understand. In the typical LS pipeline, the Substitution Ranking step determines the best substitution out of a set of candidates. Most current <a href=https://en.wikipedia.org/wiki/System>systems</a> do not consider the user&#8217;s vocabulary proficiency, and always aim for the simplest candidate. This approach may overlook less-simple candidates that the user can understand, and that are semantically closer to the original word. We propose a personalized approach for Substitution Ranking to identify the candidate that is the closest synonym and is non-complex for the user. In experiments on learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a> at different proficiency levels, we show that this approach enhances the semantic faithfulness of the output, at the cost of a relatively small increase in the number of complex words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8635.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8635 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8635/>Revisiting the Binary Linearization Technique for Surface Realization</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8635><div class="card-body p-3 small">End-to-end neural approaches have achieved state-of-the-art performance in many natural language processing (NLP) tasks. Yet, they often lack transparency of the underlying <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making process</a>, hindering error analysis and certain model improvements. In this work, we revisit the binary linearization approach to surface realization, which exhibits more interpretable behavior, but was falling short in terms of prediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the <a href=https://en.wikipedia.org/wiki/System>system</a>. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the <a href=https://en.wikipedia.org/wiki/System>system</a> which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8637.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8637 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8637" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8637/>Let’s FACE it. Finnish Poetry Generation with Aesthetics and Framing<span class=acl-fixed-case>FACE</span> it. <span class=acl-fixed-case>F</span>innish Poetry Generation with Aesthetics and Framing</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8637><div class="card-body p-3 small">We present a creative poem generator for the morphologically rich Finnish language. Our method falls into the master-apprentice paradigm, where a computationally creative genetic algorithm teaches a BRNN model to generate <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>. We model several parts of poetic aesthetics in the <a href=https://en.wikipedia.org/wiki/Fitness_function>fitness function</a> of the <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a>, such as sonic features, semantic coherence, <a href=https://en.wikipedia.org/wiki/Imagery>imagery</a> and <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>. Furthermore, we justify the creativity of our method based on the FACE theory on <a href=https://en.wikipedia.org/wiki/Computational_creativity>computational creativity</a> and take additional care in evaluating our system by automatic metrics for concepts together with human evaluation for <a href=https://en.wikipedia.org/wiki/Aesthetics>aesthetics</a>, <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> and expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8639.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8639 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8639" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8639/>Revisiting Challenges in Data-to-Text Generation with Fact Grounding</a></strong><br><a href=/people/h/hongmin-wang/>Hongmin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8639><div class="card-body p-3 small">Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> where only about 60 % of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50 % more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved <a href=https://en.wikipedia.org/wiki/Fidelity>data fidelity</a> over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8640.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8640 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8640 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8640/>Controlling Contents in Data-to-Document Generation with Human-Designed Topic Labels</a></strong><br><a href=/people/k/kasumi-aoki/>Kasumi Aoki</a>
|
<a href=/people/a/akira-miyazawa/>Akira Miyazawa</a>
|
<a href=/people/t/tatsuya-ishigaki/>Tatsuya Ishigaki</a>
|
<a href=/people/t/tatsuya-aoki/>Tatsuya Aoki</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/k/keiichi-goshima/>Keiichi Goshima</a>
|
<a href=/people/i/ichiro-kobayashi/>Ichiro Kobayashi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8640><div class="card-body p-3 small">We propose a data-to-document generator that can easily control the contents of output texts based on a neural language model. Conventional data-to-text model is useful when a reader seeks a global summary of data because it has only to describe an important part that has been extracted beforehand. However, because depending on users, it differs what they are interested in, so it is necessary to develop a method to generate various summaries according to users&#8217; interests. We develop a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to generate various summaries and to control their contents by providing the explicit targets for a reference to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as controllable factors. In the experiments, we used five-minute or one-hour charts of 9 indicators (e.g., Nikkei225), as <a href=https://en.wikipedia.org/wiki/Time_series>time-series data</a>, and daily summaries of Nikkei Quick News as textual data. We conducted comparative experiments using two pieces of information : human-designed topic labels indicating the contents of a sentence and automatically extracted <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> as the referential information for generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8641 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8641/>A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation</a></strong><br><a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/y/yuya-taguchi/>Yuya Taguchi</a>
|
<a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/k/ko-kikuta/>Ko Kikuta</a>
|
<a href=/people/j/jiro-nishitoba/>Jiro Nishitoba</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8641><div class="card-body p-3 small">Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to <a href=https://en.wikipedia.org/wiki/News_media>news production</a>. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> of three different lengths composed by professional editors. We report new findings on these corpora ; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8645.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8645 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8645 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8645.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8645/>Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation</a></strong><br><a href=/people/a/amit-moryossef/>Amit Moryossef</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8645><div class="card-body p-3 small">We follow the step-by-step approach to neural data-to-text generation proposed by Moryossef et al (2019), in which the generation process is divided into a text planning stage followed by a plan realization stage. We suggest four extensions to that framework : (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner ; (2) we incorporate typing hints that improve the model&#8217;s ability to deal with unseen relations and entities ; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts ; (4) we incorporate a simple but effective referring expression generation module. These <a href=https://en.wikipedia.org/wiki/Plug-in_(computing)>extensions</a> result in a generation process that is faster, more fluent, and more accurate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8647.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8647 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8647/>Hotel Scribe : Generating High Variation Hotel Descriptions</a></strong><br><a href=/people/s/saad-mahamood/>Saad Mahamood</a>
|
<a href=/people/m/maciej-zembrzuski/>Maciej Zembrzuski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8647><div class="card-body p-3 small">This paper describes the implementation of the Hotel Scribe system. A commercial Natural Language Generation (NLG) system which generates descriptions of hotels from accommodation metadata with a high level of content and <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> in English. It has been deployed live by * Anonymised Company Name * for the purpose of improving coverage of accommodation descriptions and for <a href=https://en.wikipedia.org/wiki/Search_engine_optimization>Search Engine Optimisation (SEO)</a>. In this paper, we describe the motivation for building this <a href=https://en.wikipedia.org/wiki/System>system</a>, the challenges faced when dealing with limited metadata, and the implementation used to generate the highly variate accommodation descriptions. Additionally, we evaluate the uniqueness of the texts generated by our <a href=https://en.wikipedia.org/wiki/System>system</a> against comparable human written accommodation description texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8651.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8651 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8651 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8651" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8651/>SimpleNLG-DE : Adapting SimpleNLG 4 to German<span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span>-<span class=acl-fixed-case>DE</span>: Adapting <span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span> 4 to <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/d/daniel-braun/>Daniel Braun</a>
|
<a href=/people/k/kira-klimt/>Kira Klimt</a>
|
<a href=/people/d/daniela-schneider/>Daniela Schneider</a>
|
<a href=/people/f/florian-matthes/>Florian Matthes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8651><div class="card-body p-3 small">SimpleNLG is a popular open source surface realiser for the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. For <a href=https://en.wikipedia.org/wiki/German_language>German</a>, however, the availability of open source and non-domain specific realisers is sparse, partly due to the complexity of the <a href=https://en.wikipedia.org/wiki/German_language>German language</a>. In this paper, we present SimpleNLG-DE, an adaption of SimpleNLG to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We discuss which parts of the <a href=https://en.wikipedia.org/wiki/German_language>German language</a> have been implemented and how we evaluated our <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> using the TIGER Corpus and newly created <a href=https://en.wikipedia.org/wiki/Data_set>data-sets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8653.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8653 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8653.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8653/>Can Neural Image Captioning be Controlled via Forced Attention?</a></strong><br><a href=/people/p/philipp-sadler/>Philipp Sadler</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8653><div class="card-body p-3 small">Learned dynamic weighting of the conditioning signal (attention) has been shown to improve neural language generation in a variety of settings. The <a href=https://en.wikipedia.org/wiki/Weighting>weights</a> applied when generating a particular output sequence have also been viewed as providing a potentially explanatory insight in the internal workings of the <a href=https://en.wikipedia.org/wiki/Electric_generator>generator</a>. In this paper, we reverse the direction of this connection and ask whether through the control of the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> we can control its output. Specifically, we take a standard neural image captioning model that uses <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, and fix the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to predetermined areas in the image. We evaluate whether the resulting output is more likely to mention the class of the object in that area than the normally generated caption. We introduce three effective methods to control the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and find that these are producing expected results in up to 27.43 % of the cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8654.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8654 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8654.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8654/>Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement</a></strong><br><a href=/people/j/jan-milan-deriu/>Jan Milan Deriu</a>
|
<a href=/people/m/mark-cieliebak/>Mark Cieliebak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8654><div class="card-body p-3 small">We present AutoJudge, an automated evaluation method for conversational dialogue systems. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> works by first generating <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> based on <a href=https://en.wikipedia.org/wiki/Self-talk>self-talk</a>, i.e. dialogue systems talking to itself. Then, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> uses human ratings on these <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> to train an <a href=https://en.wikipedia.org/wiki/Automated_reasoning>automated judgement model</a>. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing <a href=https://en.wikipedia.org/wiki/System>systems</a>. This works well for <a href=https://en.wikipedia.org/wiki/Ranking>re-ranking</a> a set of candidate utterances. However, our experiments show that AutoJudge can not be applied as <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward</a> for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, although the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8656 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8656/>A Personalized Data-to-Text Support Tool for Cancer Patients</a></strong><br><a href=/people/s/saar-hommes/>Saar Hommes</a>
|
<a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/f/felix-clouth/>Felix Clouth</a>
|
<a href=/people/j/jeroen-vermunt/>Jeroen Vermunt</a>
|
<a href=/people/x/xander-verbeek/>Xander Verbeek</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8656><div class="card-body p-3 small">In this paper, we present a novel data-to-text system for cancer patients, providing information on quality of life implications after treatment, which can be embedded in the context of shared decision making. Currently, information on quality of life implications is often not discussed, partly because (until recently) data has been lacking. In our work, we rely on a newly developed prediction model, which assigns patients to scenarios. Furthermore, we use data-to-text techniques to explain these scenario-based predictions in personalized and understandable language. We highlight the possibilities of NLG for <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, discuss ethical implications and also present the outcomes of a first evaluation with clinicians.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8658 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8658/>Using NLG for <a href=https://en.wikipedia.org/wiki/Speech_synthesis>speech synthesis</a> of mathematical sentences<span class=acl-fixed-case>NLG</span> for speech synthesis of mathematical sentences</a></strong><br><a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a>
|
<a href=/people/m/michele-monticone/>Michele Monticone</a>
|
<a href=/people/c/cristian-bernareggi/>Cristian Bernareggi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8658><div class="card-body p-3 small">People with sight impairments can access to a <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>mathematical expression</a> by using its LaTeX source. However, this mechanisms have several drawbacks : (1) it assumes the knowledge of the <a href=https://en.wikipedia.org/wiki/LaTeX>LaTeX</a>, (2) it is slow, since <a href=https://en.wikipedia.org/wiki/LaTeX>LaTeX</a> is verbose and (3) it is error-prone since <a href=https://en.wikipedia.org/wiki/LATEX>LATEX</a> is a typographical language. In this paper we study the design of a <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation system</a> for producing a <a href=https://en.wikipedia.org/wiki/Sentence_(mathematical_logic)>mathematical sentence</a>, i.e. a <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>natural language sentence</a> expressing the semantics of a <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>mathematical expression</a>. Moreover, we describe the main results of a first human based evaluation experiment of the <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8661.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8661 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8661 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8661.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-8661/>Towards Generating Math Word Problems from Equations and Topics</a></strong><br><a href=/people/q/qingyu-zhou/>Qingyu Zhou</a>
|
<a href=/people/d/danqing-huang/>Danqing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8661><div class="card-body p-3 small">A math word problem is a narrative with a specific topic that provides clues to the correct equation with numerical quantities and variables therein. In this paper, we focus on the task of <a href=https://en.wikipedia.org/wiki/Word_problem_(mathematics)>generating math word problems</a>. Previous works are mainly template-based with pre-defined rules. We propose a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> to generate <a href=https://en.wikipedia.org/wiki/Word_problem_(mathematics)>math word problems</a> from the given equations and topics. First, we design a <a href=https://en.wikipedia.org/wiki/Nuclear_fusion>fusion mechanism</a> to incorporate the information of both equations and topics. Second, an entity-enforced loss is introduced to ensure the relevance between the generated math problem and the equation. Automatic evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the baseline models. In human evaluations, the math word problems generated by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> are rated as being more relevant (in terms of solvability of the given equations and relevance to topics) and natural (i.e., <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, fluency) than the baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8662.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8662 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8662 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8662/>DisSim : A Discourse-Aware Syntactic Text Simplification Framework for English and German<span class=acl-fixed-case>D</span>is<span class=acl-fixed-case>S</span>im: A Discourse-Aware Syntactic Text Simplification Framework for <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/c/christina-niklaus/>Christina Niklaus</a>
|
<a href=/people/m/matthias-cetto/>Matthias Cetto</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a>
|
<a href=/people/s/siegfried-handschuh/>Siegfried Handschuh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8662><div class="card-body p-3 small">We introduce DisSim, a discourse-aware sentence splitting framework for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a> whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence structure</a> of the input and, hence, its interpretability for downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8663.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8663 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8663 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8663/>Real World Voice Assistant System for Cooking</a></strong><br><a href=/people/t/takahiko-ito/>Takahiko Ito</a>
|
<a href=/people/s/shintaro-inuzuka/>Shintaro Inuzuka</a>
|
<a href=/people/y/yoshiaki-yamada/>Yoshiaki Yamada</a>
|
<a href=/people/j/jun-harashima/>Jun Harashima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8663><div class="card-body p-3 small">This study presents a voice assistant system to support <a href=https://en.wikipedia.org/wiki/Cooking>cooking</a> by utilizing <a href=https://en.wikipedia.org/wiki/Smart_speaker>smart speakers</a> in Japan. This system not only speaks the procedures written in recipes point by point but also answers the common questions from users for the specified recipes. The <a href=https://en.wikipedia.org/wiki/System>system</a> applies machine comprehension techniques to millions of recipes for answering the common questions in <a href=https://en.wikipedia.org/wiki/Cooking>cooking</a> such as (How should I cook carrots?). Furthermore, numerous <a href=https://en.wikipedia.org/wiki/Machine_learning>machine-learning techniques</a> are applied to generate better responses to users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8665.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8665 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8665 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8665/>Generating Abstractive Summaries with Finetuned Language Models</a></strong><br><a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/z/zachary-ziegler/>Zachary Ziegler</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8665><div class="card-body p-3 small">Neural abstractive document summarization is commonly approached by <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that exhibit a mostly extractive behavior. This behavior is facilitated by a copy-attention which allows <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to copy words from a source document. While models in the mostly extractive news summarization domain benefit from this <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a>, they commonly fail to paraphrase or compress information from the source document. Recent advances in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer-learning</a> from large pretrained language models give rise to alternative approaches that do not rely on copy-attention and instead learn to generate concise and abstractive summaries. In this paper, as part of the TL;DR challenge, we compare the abstractiveness of summaries from different summarization approaches and show that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer-learning</a> can be efficiently utilized without any changes to the model architecture. We demonstrate that the approach leads to a higher level of <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>abstraction</a> for a similar performance on the TL;DR challenge tasks, enabling true natural language compression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8669.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8669 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8669 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8669/>Semi-Supervised Neural Text Generation by Joint Learning of Natural Language Generation and Natural Language Understanding Models</a></strong><br><a href=/people/r/raheel-qader/>Raheel Qader</a>
|
<a href=/people/f/francois-portet/>François Portet</a>
|
<a href=/people/c/cyril-labbe/>Cyril Labbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8669><div class="card-body p-3 small">In Natural Language Generation (NLG), End-to-End (E2E) systems trained through <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have recently gained a strong interest. Such <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a> need a large amount of carefully <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a> to reach satisfactory performance. However, acquiring such <a href=https://en.wikipedia.org/wiki/Data_set_(IBM_mainframe)>datasets</a> for every new NLG application is a tedious and time-consuming task. In this paper, we propose a semi-supervised deep learning scheme that can learn from non-annotated data and annotated data when available. It uses a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>NLG</a> and a Natural Language Understanding (NLU) sequence-to-sequence models which are learned jointly to compensate for the lack of annotation. Experiments on two benchmark datasets show that, with limited amount of annotated data, the method can achieve very competitive results while not using any pre-processing or re-scoring tricks. These findings open the way to the exploitation of non-annotated datasets which is the current bottleneck for the E2E NLG system development to new applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8670.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8670 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8670 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8670/>Neural Generation for <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> : Data and Baselines<span class=acl-fixed-case>C</span>zech: Data and Baselines</a></strong><br><a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/f/filip-jurcicek/>Filip Jurčíček</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8670><div class="card-body p-3 small">We present the first dataset targeted at end-to-end NLG in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> in the restaurant domain, along with several strong baseline models using the sequence-to-sequence approach. While non-English NLG is under-explored in general, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, as a morphologically rich language, makes the task even harder : Since <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> requires inflecting named entities, delexicalization or copy mechanisms do not work out-of-the-box and lexicalizing the generated outputs is non-trivial. In our experiments, we present two different approaches to this this problem : (1) using a neural language model to select the correct inflected form while lexicalizing, (2) a two-step generation setup : our sequence-to-sequence model generates an interleaved sequence of lemmas and morphological tags, which are then inflected by a morphological generator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8672.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8672 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8672 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-8672.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-8672" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-8672/>A Good Sample is Hard to Find : Noise Injection Sampling and Self-Training for Neural Language Generation Models</a></strong><br><a href=/people/c/chris-kedzie/>Chris Kedzie</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8672><div class="card-body p-3 small">Deep neural networks (DNN) are quickly becoming the de facto standard modeling method for many natural language generation (NLG) tasks. In order for such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to truly be useful, they must be capable of correctly generating utterances for novel meaning representations (MRs) at test time. In practice, even sophisticated DNNs with various forms of semantic control frequently fail to generate utterances faithful to the input MR. In this paper, we propose an architecture agnostic self-training method to sample novel MR / text utterance pairs to augment the original training data. Remarkably, after training on the augmented data, even simple encoder-decoder models with greedy decoding are capable of generating semantically correct utterances that are as good as state-of-the-art outputs in both automatic and human evaluations of quality.</div></div></div><hr><div id=w19-87><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-87.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-87/>Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8700/>Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8701/>Comparison between Automatic and Human Subtitling : A Case Study with Game of Thrones</a></strong><br><a href=/people/s/sabrina-baldo-de-brebisson/>Sabrina Baldo de Brébisson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8701><div class="card-body p-3 small">In this submission, I would like to share my experiences with the software DeepL and the comparison analysis I have made with human subtitling offered by the DVD version of the corpus I have chosen as the topic of my study the eight Seasons of <a href=https://en.wikipedia.org/wiki/Game_of_Thrones>Game of Thrones</a>. The idea is to study if the version proposed by an automatic translation program could be used as a first draft for the professional subtitler. It is expected that the latter would work on the form of the subtitles, that is to say mainly on their length, in a second step.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8702/>Parallel Corpus of Croatian-Italian Administrative Texts<span class=acl-fixed-case>C</span>roatian-<span class=acl-fixed-case>I</span>talian Administrative Texts</a></strong><br><a href=/people/m/marija-brkic-bakaric/>Marija Brkic Bakaric</a>
|
<a href=/people/i/ivana-lalli-pacelat/>Ivana Lalli Pacelat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8702><div class="card-body p-3 small">Parallel corpora constitute a unique re-source for providing assistance to human translators. The selection and preparation of the parallel corpora also conditions the quality of the resulting MT engine. Since <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a> is a national language and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> is officially recognized as a minority lan-guage in seven cities and twelve munici-palities of Istria County, a large amount of parallel texts is produced on a daily basis. However, there have been no attempts in using these texts for compiling a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. A domain-specific sentence-aligned parallel Croatian-Italian corpus of administrative texts would be of high value in creating different language tools and resources. The aim of this paper is, therefore, to explore the value of parallel documents which are publicly available mostly in pdf format and to investigate the use of automatically-built dictionaries in corpus compilation. The effects that a document format and, consequently sentence splitting, and the dictionary input have on the sentence alignment process are manually evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8703/>What Influences the Features of Post-editese? A Preliminary Study</a></strong><br><a href=/people/s/sheila-castilho/>Sheila Castilho</a>
|
<a href=/people/n/natalia-resende/>Natália Resende</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8703><div class="card-body p-3 small">While a number of studies have shown evidence of translationese phenomena, that is, statistical differences between original texts and translated texts (Gellerstam, 1986), results of studies searching for translationese features in postedited texts (what has been called posteditese (Daems et al., 2017)) have presented mixed results. This paper reports a preliminary study aimed at identifying the presence of post-editese features in machine-translated post-edited texts and at understanding how they differ from translationese features. We test the influence of factors such as post-editing (PE) levels (full vs. light), <a href=https://en.wikipedia.org/wiki/Language_proficiency>translation proficiency</a> (professionals vs. students) and text domain (news vs. literary). Results show evidence of post-editese features, especially in light PE texts and in certain domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8705/>Human Evaluation of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : The Case of Deep Learning</a></strong><br><a href=/people/m/marie-escribe/>Marie Escribe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8705><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural networks</a> now have a great impact on <a href=https://en.wikipedia.org/wiki/Translation_technology>translation technology</a>. A considerable achievement was reached in this field with the publication of L&#8217;Apprentissage Profond. This book, originally written in English (Deep Learning), was entirely machine-translated into <a href=https://en.wikipedia.org/wiki/French_language>French</a> and post-edited by several experts. In this context, it appears essential to have a clear vision of the performance of MT tools. Providing an evaluation of <a href=https://en.wikipedia.org/wiki/Neurotransmitter_transporter>NMT</a> is precisely the aim of the present research paper. To accomplish this objective, a framework for error categorisation was built and a comparative analysis of the raw translation output and the post-edited version was performed with the purpose of identifying recurring patterns of errors. The findings showed that even though some <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> were spotted, the output was generally correct from a linguistic point of view. The most recurring errors are linked to the specialised terminology employed in this book. Further errors include parts of text that were not translated as well as edits based on <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic preferences</a>. The major part of the output was not acceptable as such and required several edits per segment, but some sentences were of publishable quality and were therefore left untouched in the final version.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8710/>The Chinese / English Political Interpreting Corpus (CEPIC): A New Electronic Resource for Translators and Interpreters<span class=acl-fixed-case>C</span>hinese/<span class=acl-fixed-case>E</span>nglish Political Interpreting Corpus (<span class=acl-fixed-case>CEPIC</span>): A New Electronic Resource for Translators and Interpreters</a></strong><br><a href=/people/j/jun-pan/>Jun Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8710><div class="card-body p-3 small">The Chinese / English Political Interpreting Corpus (CEPIC) is a new electronic and open access resource developed for translators and interpreters, especially those working with political text types. Over 6 million word tokens in size, the online corpus consists of transcripts of Chinese (Cantonese & Putonghua) / English political speeches and their translated and interpreted texts. It includes rich meta-data and is POS-tagged and annotated with prosodic and paralinguistic features that are of concern to spoken language and interpreting. The online platform of the CEPIC features main functions including <a href=https://en.wikipedia.org/wiki/Keyword_search>Keyword Search</a>, Word Collocation and Expanded Keyword in Context, which are illustrated in the paper. The <a href=https://en.wikipedia.org/wiki/CEPIC>CEPIC</a> can shed light on online translation and interpreting corpora development in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8714/>Towards a Proactive MWE Terminological Platform for Cross-Lingual Mediation in the Age of <a href=https://en.wikipedia.org/wiki/Big_data>Big Data</a><span class=acl-fixed-case>MWE</span> Terminological Platform for Cross-Lingual Mediation in the Age of Big Data</a></strong><br><a href=/people/b/benjamin-k-tsou/>Benjamin K. Tsou</a>
|
<a href=/people/k/kapo-chow/>Kapo Chow</a>
|
<a href=/people/j/junru-nie/>Junru Nie</a>
|
<a href=/people/y/yuan-yuan/>Yuan Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8714><div class="card-body p-3 small">The emergence of <a href=https://en.wikipedia.org/wiki/China>China</a> as a global economic power in the 21st Century has brought about surging needs for cross-lingual and cross-cultural mediation, typically performed by translators. Advances in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a> and <a href=https://en.wikipedia.org/wiki/Language_engineering>Language Engineering</a> have been bolstered by <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine learning</a> and suitable Big Data cultivation. They have helped to meet some of the translator&#8217;s needs, though the technical specialists have not kept pace with the practical and expanding requirements in language mediation. One major technical and linguistic hurdle involves words outside the vocabulary of the translator or the lexical database he / she consults, especially Multi-Word Expressions (Compound Words) in technical subjects. A further problem is in the multiplicity of renditions of a term in the target language. This paper discusses a proactive approach following the successful extraction and application of sizable bilingual Multi-Word Expressions (Compound Words) for language mediation in technical subjects, which do not fall within the expertise of typical translators, who have inadequate appreciation of the range of new technical tools available to help him / her. Our approach draws on the personal reflections of translators and teachers of translation and is based on the prior R&D efforts relating to 300,000 comparable Chinese-English patents. The subsequent <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> we have developed aims to be proactive in meeting four identified practical challenges in technical translation (e.g. patents). It has broader economic implication in the Age of <a href=https://en.wikipedia.org/wiki/Big_data>Big Data</a> (Tsou et al, 2015) and <a href=https://en.wikipedia.org/wiki/Trade_war>Trade War</a>, as the workload, if not, the challenges, increasingly can not be met by currently available front-line translators. We shall demonstrate how new tools can be harnessed to spearhead the application of <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a> not only in language mediation but also in the teaching and learning of translation. It shows how a better appreciation of their needs may enhance the contributions of the technical specialists, and thus enhance the resultant synergetic benefits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8717 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8717/>The Four Stages of Machine Translation Acceptance in a Freelancer’s Life</a></strong><br><a href=/people/m/maria-sgourou/>Maria Sgourou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8717><div class="card-body p-3 small">Technology is a big challenge and raises many questions and issues when it comes to its application in the <a href=https://en.wikipedia.org/wiki/Translation>translation process</a>, but <a href=https://en.wikipedia.org/wiki/Translation>translation</a>&#8217;s biggest problem is not technology ; it is rather how <a href=https://en.wikipedia.org/wiki/Technology>technology</a> is perceived by translators. MT developers and researchers should take into account this perception and move towards a more democratized approach to include the base of the translation industry and perhaps its more valuable asset, the translators.</div></div></div><hr><div id=w19-89><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-89.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-89/>Proceedings of the Workshop MultiLing 2019: Summarization Across Languages, Genres and Sources</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8900/>Proceedings of the Workshop MultiLing 2019: Summarization Across Languages, Genres and Sources</a></strong><br><a href=/people/g/george-giannakopoulos/>George Giannakopoulos</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8901/>RANLP 2019 Multilingual Headline Generation Task Overview<span class=acl-fixed-case>RANLP</span> 2019 Multilingual Headline Generation Task Overview</a></strong><br><a href=/people/m/marina-litvak/>Marina Litvak</a>
|
<a href=/people/j/john-conroy/>John M. Conroy</a>
|
<a href=/people/p/peter-a-rankel/>Peter A. Rankel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8901><div class="card-body p-3 small">The objective of the 2019 RANLP Multilingual Headline Generation (HG) Task is to explore some of the challenges highlighted by current state of the art approaches on creating informative headlines to news articles : non-descriptive headlines, out-of-domain training data, generating headlines from long documents which are not well represented by the head heuristic, and dealing with multilingual domain. This tasks makes available a large set of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> for headline generation and provides an <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation methods</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> are drawn from <a href=https://en.wikipedia.org/wiki/Wikinews>Wikinews</a> as well as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Participants were required to generate <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> for at least 3 languages, which were evaluated via <a href=https://en.wikipedia.org/wiki/Automatic_programming>automatic methods</a>. A key aspect of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is <a href=https://en.wikipedia.org/wiki/Multilinguality>multilinguality</a>. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> measures the performance of multilingual headline generation systems using the Wikipedia and Wikinews articles in multiple languages. The objective is to assess the performance of automatic headline generation techniques on <a href=https://en.wikipedia.org/wiki/Text_file>text documents</a> covering a diverse range of languages and topics outside the <a href=https://en.wikipedia.org/wiki/News_media>news domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8902/>MultiLing 2019 : Financial Narrative Summarisation<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ing 2019: Financial Narrative Summarisation</a></strong><br><a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8902><div class="card-body p-3 small">The Financial Narrative Summarisation task at MultiLing 2019 aims to demonstrate the value and challenges of applying automatic text summarisation to financial text written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, usually referred to as financial narrative disclosures. The task dataset has been extracted from <a href=https://en.wikipedia.org/wiki/Office_of_Public_Sector_Information>UK annual reports</a> published in PDF file format. The participants were asked to provide structured summaries, based on real-world, publicly available financial annual reports of UK firms by extracting information from different key sections. Participants were asked to generate summaries that reflects the analysis and assessment of the financial trend of the business over the past year, as provided by <a href=https://en.wikipedia.org/wiki/Annual_report>annual reports</a>. The evaluation of the summaries was performed using AutoSummENG and Rouge automatic metrics. This paper focuses mainly on the data creation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8903/>The Summary Evaluation Task in the MultiLing-RANLP 2019 Workshop<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ing - <span class=acl-fixed-case>RANLP</span> 2019 Workshop</a></strong><br><a href=/people/g/george-giannakopoulos/>George Giannakopoulos</a>
|
<a href=/people/n/nikiforos-pittaras/>Nikiforos Pittaras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8903><div class="card-body p-3 small">This report covers the summarization evaluation task, proposed to the summarization community via the MultiLing 2019 Workshop of the RANLP 2019 conference. The task aims to encourage the development of automatic summarization evaluation methods closely aligned with manual, human-authored summary grades and judgements. A multilingual setting is adopted, building upon a corpus of Wikinews articles across 6 languages (English, <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, <a href=https://en.wikipedia.org/wiki/Greek_language>Greek</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and Czech). The <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> utilizes human (golden) and machine-generated (peer) summaries, which have been assigned human evaluation scores from previous MultiLing tasks. Using these resources, the original <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is augmented with synthetic data, combining summary texts under three different strategies (reorder, merge and replace), each engineered to introduce noise in the summary in a controlled and quantifiable way. We estimate that the utilization of such data can extract and highlight useful attributes of summary quality estimation, aiding the creation of data-driven automatic methods with an increased correlation to human summary evaluations across domains and languages. This paper provides a brief description of the summary evaluation task, the data generation protocol and the resources made available by the MultiLing community, towards improving automatic summarization evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8904/>Multi-lingual Wikipedia Summarization and Title Generation On Low Resource Corpus<span class=acl-fixed-case>W</span>ikipedia Summarization and Title Generation On Low Resource Corpus</a></strong><br><a href=/people/w/wei-liu/>Wei Liu</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/z/zuying-huang/>Zuying Huang</a>
|
<a href=/people/y/yinan-liu/>Yinan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8904><div class="card-body p-3 small">MultiLing 2019 Headline Generation Task on Wikipedia Corpus raised a critical and practical problem : multilingual task on low resource corpus. In this paper we proposed QDAS extractive summarization model enhanced by sentence2vec and try to apply <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> based on large multilingual pre-trained language model for Wikipedia Headline Generation task. We treat <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> as sequence labeling task and develop two schemes to handle with <a href=https://en.wikipedia.org/wiki/Information_technology>it</a>. Experimental results have shown that large pre-trained model can effectively utilize <a href=https://en.wikipedia.org/wiki/Learning>learned knowledge</a> to extract certain phrase using low resource supervised data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8907/>Social Web Observatory : An entity-driven, holistic information summarization platform across sources</a></strong><br><a href=/people/l/leonidas-tsekouras/>Leonidas Tsekouras</a>
|
<a href=/people/g/georgios-petasis/>Georgios Petasis</a>
|
<a href=/people/a/aris-kosmopoulos/>Aris Kosmopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8907><div class="card-body p-3 small">The Social Web Observatory is an entity-driven, sentiment-aware, event summarization web platform, combining various methods and tools to overview trends across <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and <a href=https://en.wikipedia.org/wiki/News_media>news sources</a> in Greek. SWO crawls, clusters and summarizes information following an entity-centric view of text streams, allowing to monitor the public sentiment towards a specific person, organization or other entity. In this paper, we overview the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>, outline the analysis pipeline and describe a user study aimed to quantify the usefulness of the <a href=https://en.wikipedia.org/wiki/System>system</a> and especially the meaningfulness and coherence of discovered events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-8908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-8908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-8908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-8908/>EASY-M : Evaluation System for Multilingual Summarizers<span class=acl-fixed-case>EASY</span>-<span class=acl-fixed-case>M</span>: Evaluation System for Multilingual Summarizers</a></strong><br></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-8908><div class="card-body p-3 small">Automatic text summarization aims at producing a shorter version of a document (or a document set). Evaluation of summarization quality is a challenging task. Because human evaluations are expensive and evaluators often disagree between themselves, many researchers prefer to evaluate their <a href=https://en.wikipedia.org/wiki/System>systems</a> automatically, with help of <a href=https://en.wikipedia.org/wiki/Programming_tool>software tools</a>. Such a tool usually requires a point of reference in the form of one or more human-written summaries for each text in the corpus. Then, a system-generated summary is compared to one or more human-written summaries, according to selected <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. However, a single <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> can not reflect all quality-related aspects of a summary. In this paper we present the EvAluation SYstem for Multilingual Summarization (EASY-M), which enables the evaluation of system-generated summaries in 17 different languages with several quality measures, based on comparison with their human-generated counterparts. The <a href=https://en.wikipedia.org/wiki/System>system</a> also provides comparative results with two built-in baselines. The source code and both online and offline versions of EASY-M is freely available for the NLP community.</div></div></div><hr><div id=w19-90><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-90.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-90/>Proceedings of the Workshop on Language Technology for Digital Historical Archives</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-9000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-9000/>Proceedings of the Workshop on Language Technology for Digital Historical Archives</a></strong><br><a href=/people/u/university-of-hamburg-cristina-vertan/>University of Hamburg Cristina Vertan</a>
|
<a href=/people/b/bulgarian-cdemy-of-sciences-petya-osenova/>Bulgarian cdemy of Sciences Petya Osenova</a>
|
<a href=/people/s/st-kliment-ohridski-university-of-sofia/>St. Kliment Ohridski University of Sofia</a>
|
<a href=/people/s/st-kliment-ohridski-university-of-sofia-dimitar-iliev/>St. Kliment Ohridski University of Sofia Dimitar Iliev</a></span></p></div><hr><div id=d19-50><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-50.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-50/>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5000/>Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</a></strong><br><a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto Barrón-Cedeño</a>
|
<a href=/people/c/chris-brew/>Chris Brew</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5002/>Detecting context abusiveness using hierarchical deep learning</a></strong><br><a href=/people/j/ju-hyoung-lee/>Ju-Hyoung Lee</a>
|
<a href=/people/j/jun-u-park/>Jun-U Park</a>
|
<a href=/people/j/jeong-won-cha/>Jeong-Won Cha</a>
|
<a href=/people/y/yo-sub-han/>Yo-Sub Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5002><div class="card-body p-3 small">Abusive text is a serious problem in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and causes many issues among users as the number of users and the content volume increase. There are several attempts for detecting or preventing abusive text effectively. One simple yet effective approach is to use an abusive lexicon and determine the existence of an abusive word in text. This approach works well even when an abusive word is obfuscated. On the other hand, it is still a challenging problem to determine <a href=https://en.wikipedia.org/wiki/Abusive_power_and_control>abusiveness</a> in a text having no explicit abusive words. Especially, it is hard to identify sarcasm or offensiveness in context without any <a href=https://en.wikipedia.org/wiki/Abuse>abusive words</a>. We tackle this problem using an ensemble deep learning model. Our model consists of two parts of extracting local features and global features, which are crucial for identifying implicit abusiveness in <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context level</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark data</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms all the previous models for detecting <a href=https://en.wikipedia.org/wiki/Abusive_power_and_control>abusiveness</a> in a <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text data</a> without abusive words. Furthermore, we combine our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and an abusive lexicon method. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has at least 4 % better performance compared with the previous approaches for identifying text abusiveness in case of with / without abusive words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5004/>Identifying Nuances in <a href=https://en.wikipedia.org/wiki/Fake_news>Fake News</a> vs. <a href=https://en.wikipedia.org/wiki/Satire>Satire</a> : Using Semantic and Linguistic Cues</a></strong><br><a href=/people/o/or-levi/>Or Levi</a>
|
<a href=/people/p/pedram-hosseini/>Pedram Hosseini</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/d/david-broniatowski/>David Broniatowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5004><div class="card-body p-3 small">The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. Further to the efforts of reducing exposure to <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, purveyors of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus <a href=https://en.wikipedia.org/wiki/Satire>satire</a>. Previous work have studied whether <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a> can be distinguished based on <a href=https://en.wikipedia.org/wiki/Language>language differences</a>. Contrary to <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>, <a href=https://en.wikipedia.org/wiki/Satire>satire stories</a> are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a>. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the <a href=https://en.wikipedia.org/wiki/Data>data</a> with current news events, to help identify a political or social message.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5007/>Generating Sentential Arguments from Diverse Perspectives on Controversial Topic</a></strong><br><a href=/people/c/chaehun-park/>ChaeHun Park</a>
|
<a href=/people/w/wonsuk-yang/>Wonsuk Yang</a>
|
<a href=/people/j/jong-c-park/>Jong Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5007><div class="card-body p-3 small">Considering diverse aspects of an argumentative issue is an essential step for mitigating a biased opinion and making reasonable decisions. A related generation model can produce flexible results that cover a wide range of topics, compared to the retrieval-based method that may show unstable performance for unseen data. In this paper, we study the problem of generating sentential arguments from multiple perspectives, and propose a neural method to address this problem. Our model, ArgDiver (Argument generation model from diverse perspectives), in a way a conversational system, successfully generates high-quality sentential arguments. At the same time, the automatically generated arguments by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> show a higher <a href=https://en.wikipedia.org/wiki/Diversity_index>diversity</a> than those generated by any other baseline models. We believe that our work provides evidence for the potential of a good generation model in providing diverse perspectives on a controversial topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5009/>Unraveling the Search Space of Abusive Language in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> with Dynamic Lexicon Acquisition<span class=acl-fixed-case>W</span>ikipedia with Dynamic Lexicon Acquisition</a></strong><br><a href=/people/w/wei-fan-chen/>Wei-Fan Chen</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al Khatib</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5009><div class="card-body p-3 small">Many discussions on online platforms suffer from users offending others by using abusive terminology, threatening each other, or being sarcastic. Since an automatic detection of abusive language can support human moderators of <a href=https://en.wikipedia.org/wiki/Internet_forum>online discussion platforms</a>, detecting abusiveness has recently received increased attention. However, the existing approaches simply train one <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for the whole variety of <a href=https://en.wikipedia.org/wiki/Abusive_power_and_control>abusiveness</a>. In contrast, our approach is to distinguish explicitly abusive cases from the more shadowed ones. By dynamically extending a lexicon of abusive terms (e.g., including new obfuscations of abusive terms), our approach can support a moderator with explicit unraveled explanations for why something was flagged as abusive : due to known explicitly abusive terms, due to newly detected (obfuscated) terms, or due to shadowed cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5013/>Fine-Tuned Neural Models for Propaganda Detection at the Sentence and Fragment levels</a></strong><br><a href=/people/t/tariq-alhindi/>Tariq Alhindi</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5013><div class="card-body p-3 small">This paper presents the CUNLP submission for the NLP4IF 2019 shared-task on Fine-Grained Propaganda Detection. Our system finished 5th out of 26 teams on the sentence-level classification task and 5th out of 11 teams on the fragment-level classification task based on our scores on the blind test set. We present our models, a discussion of our ablation studies and experiments, and an analysis of our performance on all eighteen propaganda techniques present in the corpus of the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5016/>JUSTDeep at NLP4IF 2019 Task 1 : Propaganda Detection using Ensemble Deep Learning Models<span class=acl-fixed-case>JUSTD</span>eep at <span class=acl-fixed-case>NLP</span>4<span class=acl-fixed-case>IF</span> 2019 Task 1: Propaganda Detection using Ensemble Deep Learning Models</a></strong><br><a href=/people/h/hani-al-omari/>Hani Al-Omari</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/o/ola-altiti/>Ola AlTiti</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5016><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Internet>internet</a> and the high use of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have enabled the modern-day journalism to publish, share and spread news that is difficult to distinguish if it is true or fake. Defining fake news is not well established yet, however, it can be categorized under several labels : false, biased, or framed to mislead the readers that are characterized as <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. Digital content production technologies with <a href=https://en.wikipedia.org/wiki/Fallacy>logical fallacies</a> and emotional language can be used as <a href=https://en.wikipedia.org/wiki/Propaganda_techniques>propaganda techniques</a> to gain more readers or mislead the audience. Recently, several researchers have proposed deep learning (DL) models to address this issue. This research paper provides an ensemble deep learning model using BiLSTM, <a href=https://en.wikipedia.org/wiki/XGBoost>XGBoost</a>, and <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> to detect <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. The proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> has been applied on the dataset provided by the challenge NLP4IF 2019, Task 1 Sentence Level Classification (SLC) and it shows a significant performance over the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5017 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5017/>Detection of Propaganda Using Logistic Regression</a></strong><br><a href=/people/j/jinfen-li/>Jinfen Li</a>
|
<a href=/people/z/zhihao-ye/>Zhihao Ye</a>
|
<a href=/people/l/lu-xiao/>Lu Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5017><div class="card-body p-3 small">Various <a href=https://en.wikipedia.org/wiki/Propaganda_techniques>propaganda techniques</a> are used to manipulate peoples perspectives in order to foster a predetermined agenda such as by the use of <a href=https://en.wikipedia.org/wiki/Fallacy>logical fallacies</a> or appealing to the emotions of the audience. In this paper, we develop a Logistic Regression-based tool that automatically classifies whether a sentence is propagandistic or not. We utilize features like TF-IDF, BERT vector, sentence length, readability grade level, emotion feature, LIWC feature and emphatic content feature to help us differentiate these two categories. The linguistic and semantic features combination results in 66.16 % of F1 score, which outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> hugely.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5019 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5019/>Understanding BERT performance in propaganda analysis<span class=acl-fixed-case>BERT</span> performance in propaganda analysis</a></strong><br><a href=/people/y/yiqing-hua/>Yiqing Hua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5019><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> used in the shared task for fine-grained propaganda analysis at <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>. Despite the challenging nature of the task, our pretrained BERT model (team YMJA) fine tuned on the training dataset provided by the shared task scored 0.62 F1 on the test set and ranked third among 25 teams who participated in the contest. We present a set of illustrative experiments to better understand the performance of our BERT model on this shared task. Further, we explore beyond the given <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false-positive cases</a> that likely to be produced by our <a href=https://en.wikipedia.org/wiki/System>system</a>. We show that despite the high performance on the given testset, our system may have the tendency of classifying opinion pieces as <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> and can not distinguish quotations of propaganda speech from actual usage of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda techniques</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5020 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5020/>Pretrained Ensemble Learning for Fine-Grained Propaganda Detection</a></strong><br><a href=/people/a/ali-fadel/>Ali Fadel</a>
|
<a href=/people/i/ibraheem-tuffaha/>Ibraheem Tuffaha</a>
|
<a href=/people/m/mahmoud-al-ayyoub/>Mahmoud Al-Ayyoub</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5020><div class="card-body p-3 small">In this paper, we describe our team&#8217;s effort on the fine-grained propaganda detection on sentence level classification (SLC) task of NLP4IF 2019 workshop co-located with the EMNLP-IJCNLP 2019 conference. Our top performing <a href=https://en.wikipedia.org/wiki/System>system</a> results come from applying <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble average</a> on three pretrained models to make their predictions. The first two models use the uncased and cased versions of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) while the third model uses Universal Sentence Encoder (USE) (Cer et al. Out of 26 participating teams, our system is ranked in the first place with 68.8312 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the <a href=https://en.wikipedia.org/wiki/Software_development_process>development dataset</a> and in the sixth place with 61.3870 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the <a href=https://en.wikipedia.org/wiki/Software_testing>testing dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5022/>Sentence-Level Propaganda Detection in News Articles with Transfer Learning and BERT-BiLSTM-Capsule Model<span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-Capsule Model</a></strong><br><a href=/people/g/george-alexandru-vlad/>George-Alexandru Vlad</a>
|
<a href=/people/m/mircea-adrian-tanase/>Mircea-Adrian Tanase</a>
|
<a href=/people/c/cristian-onose/>Cristian Onose</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5022><div class="card-body p-3 small">In recent years, the need for <a href=https://en.wikipedia.org/wiki/Communication>communication</a> increased in <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Propaganda is a mechanism which was used throughout history to influence public opinion and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is gaining a new dimension with the rising interest of online social media. This paper presents our submission to NLP4IF-2019 Shared Task SLC : Sentence-level Propaganda Detection in news articles. The challenge of this task is to build a robust binary classifier able to provide corresponding <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda labels</a>, <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> or non-propaganda. Our model relies on a unified neural network, which consists of several deep leaning modules, namely BERT, BiLSTM and Capsule, to solve the sentencelevel propaganda classification problem. In addition, we take a pre-training approach on a somewhat similar task (i.e., emotion classification) improving results against the cold-start model. Among the 26 participant teams in the NLP4IF-2019 Task SLC, our solution ranked 12th with an F1-score 0.5868 on the official test data. Our proposed <a href=https://en.wikipedia.org/wiki/Solution>solution</a> indicates promising results since our <a href=https://en.wikipedia.org/wiki/System>system</a> significantly exceeds the baseline approach of the organizers by 0.1521 and is slightly lower than the winning <a href=https://en.wikipedia.org/wiki/System>system</a> by 0.0454.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5023 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5023/>Synthetic Propaganda Embeddings To Train A Linear Projection</a></strong><br><a href=/people/a/adam-ek/>Adam Ek</a>
|
<a href=/people/m/mehdi-ghanimifard/>Mehdi Ghanimifard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5023><div class="card-body p-3 small">This paper presents a method of detecting fine-grained categories of propaganda in text. Given a sentence, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> aims to identify a span of words and predict the type of <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> used. To detect <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>, we explore a method for extracting features of propaganda from contextualized embeddings without fine-tuning the large parameters of the base model. We show that by generating synthetic embeddings we can train a <a href=https://en.wikipedia.org/wiki/Linear_function>linear function</a> with ReLU activation to extract useful labeled embeddings from an embedding space generated by a general-purpose language model. We also introduce an <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference technique</a> to detect continuous spans in sequences of propaganda tokens in sentences. A result of the ensemble model is submitted to the first shared task in fine-grained propaganda detection at NLP4IF as Team Stalin. In this paper, we provide additional analysis regarding our method of detecting spans of propaganda with synthetically generated representations.</div></div></div><hr><div id=d19-51><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-51.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-51/>Proceedings of the Second Workshop on Economics and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5100/>Proceedings of the Second Workshop on Economics and Natural Language Processing</a></strong><br><a href=/people/u/udo-hahn/>Udo Hahn</a>
|
<a href=/people/v/veronique-hoste/>Véronique Hoste</a>
|
<a href=/people/z/zhu-zhang/>Zhu Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5102 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5102/>Financial Event Extraction Using Wikipedia-Based Weak Supervision<span class=acl-fixed-case>W</span>ikipedia-Based Weak Supervision</a></strong><br><a href=/people/l/liat-ein-dor/>Liat Ein-Dor</a>
|
<a href=/people/a/ariel-gera/>Ariel Gera</a>
|
<a href=/people/o/orith-toledo-ronen/>Orith Toledo-Ronen</a>
|
<a href=/people/a/alon-halfon/>Alon Halfon</a>
|
<a href=/people/b/benjamin-sznajder/>Benjamin Sznajder</a>
|
<a href=/people/l/lena-dankin/>Lena Dankin</a>
|
<a href=/people/y/yonatan-bilu/>Yonatan Bilu</a>
|
<a href=/people/y/yoav-katz/>Yoav Katz</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5102><div class="card-body p-3 small">Extraction of financial and economic events from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> has previously been done mostly using rule-based methods, with more recent works employing <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a>. This work is in line with this latter approach, leveraging relevant Wikipedia sections to extract weak labels for sentences describing economic events. Whereas previous weakly supervised approaches required a knowledge-base of such events, or corresponding financial figures, our approach requires no such additional data, and can be employed to extract economic events related to companies which are not even mentioned in the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5104 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5104/>Forecasting Firm Material Events from 8-K Reports</a></strong><br><a href=/people/s/shuang-sophie-zhai/>Shuang (Sophie) Zhai</a>
|
<a href=/people/z/zhu-drew-zhang/>Zhu (Drew) Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5104><div class="card-body p-3 small">In this paper, we show deep learning models can be used to forecast firm material event sequences based on the contents in the company&#8217;s 8-K Current Reports. Specifically, we exploit state-of-the-art neural architectures, including sequence-to-sequence (Seq2Seq) architecture and attention mechanisms, in the model. Our 8K-powered deep learning model demonstrates promising performance in forecasting firm future event sequences. The model is poised to benefit various stakeholders, including management and investors, by facilitating <a href=https://en.wikipedia.org/wiki/Risk_management>risk management</a> and <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5105 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5105.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5105/>Incorporating Fine-grained Events in Stock Movement Prediction</a></strong><br><a href=/people/d/deli-chen/>Deli Chen</a>
|
<a href=/people/y/yanyan-zou/>Yanyan Zou</a>
|
<a href=/people/k/keiko-harimoto/>Keiko Harimoto</a>
|
<a href=/people/r/ruihan-bao/>Ruihan Bao</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5105><div class="card-body p-3 small">Considering event structure information has proven helpful in text-based stock movement prediction. However, existing works mainly adopt the coarse-grained events, which loses the specific semantic information of diverse event types. In this work, we propose to incorporate the fine-grained events in stock movement prediction. Firstly, we propose a professional finance event dictionary built by domain experts and use it to extract fine-grained events automatically from finance news. Then we design a neural model to combine finance news with fine-grained event structure and stock trade data to predict the stock movement. Besides, in order to improve the generalizability of the proposed method, we design an advanced model that uses the extracted fine-grained events as the distant supervised label to train a multi-task framework of <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event extraction</a> and stock prediction. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms all the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and has good generalizability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5106.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5106/>Group, Extract and Aggregate : Summarizing a Large Amount of Finance News for Forex Movement Prediction</a></strong><br><a href=/people/d/deli-chen/>Deli Chen</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/k/keiko-harimoto/>Keiko Harimoto</a>
|
<a href=/people/r/ruihan-bao/>Ruihan Bao</a>
|
<a href=/people/q/qi-su/>Qi Su</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5106><div class="card-body p-3 small">Incorporating related text information has proven successful in <a href=https://en.wikipedia.org/wiki/Stock_market_prediction>stock market prediction</a>. However, it is a huge challenge to utilize texts in the enormous forex (foreign currency exchange) market because the associated texts are too redundant. In this work, we propose a BERT-based Hierarchical Aggregation Model to summarize a large amount of finance news to predict <a href=https://en.wikipedia.org/wiki/Foreign_exchange_market>forex movement</a>. We firstly group news from different aspects : time, topic and category. Then we extract the most crucial news in each group by the SOTA extractive summarization method. Finally, we conduct interaction between the news and the trade data with attention to predict the <a href=https://en.wikipedia.org/wiki/Foreign_exchange_market>forex movement</a>. The experimental results show that the category based method performs best among three grouping methods and outperforms all the baselines. Besides, we study the influence of essential news attributes (category and region) by <a href=https://en.wikipedia.org/wiki/Statistical_inference>statistical analysis</a> and summarize the influence patterns for different currency pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5107 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5107/>Complaint Analysis and Classification for Economic and Food Safety</a></strong><br><a href=/people/j/joao-filgueiras/>João Filgueiras</a>
|
<a href=/people/l/luis-barbosa/>Luís Barbosa</a>
|
<a href=/people/g/gil-rocha/>Gil Rocha</a>
|
<a href=/people/h/henrique-lopes-cardoso/>Henrique Lopes Cardoso</a>
|
<a href=/people/l/luis-paulo-reis/>Luís Paulo Reis</a>
|
<a href=/people/j/joao-pedro-machado/>João Pedro Machado</a>
|
<a href=/people/a/ana-maria-oliveira/>Ana Maria Oliveira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5107><div class="card-body p-3 small">Governmental institutions are employing artificial intelligence techniques to deal with their specific problems and exploit their huge amounts of both structured and unstructured information. In particular, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a> are being used to process citizen feedback. In this paper, we report on the use of such techniques for analyzing and classifying complaints, in the context of the Portuguese Economic and Food Safety Authority. Grounded in its operational process, we address three different classification problems : target economic activity, implied infraction severity level, and institutional competence. We show promising results obtained using feature-based approaches and traditional <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, with accuracy scores above 70 %, and analyze the shortcomings of our current results and avenues for further improvement, taking into account the intended use of our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> in helping human officers to cope with thousands of yearly complaints.</div></div></div><hr><div id=d19-52><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-52.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-52/>Proceedings of the 6th Workshop on Asian Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5200/>Proceedings of the 6th Workshop on Asian Translation</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/n/nobushige-doi/>Nobushige Doi</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hidaya-mino/>Hidaya Mino</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5201 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5201/>Overview of the 6th Workshop on Asian Translation<span class=acl-fixed-case>A</span>sian Translation</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/n/nobushige-doi/>Nobushige Doi</a>
|
<a href=/people/s/shohei-higashiyama/>Shohei Higashiyama</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/h/hideya-mino/>Hideya Mino</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5201><div class="card-body p-3 small">This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including JaEn, JaZh scientific paper translation subtasks, JaEn, JaKo, JaEn patent translation subtasks, HiEn, MyEn, KmEn, TaEn mixed domain subtasks and RuJa news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5202 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5202/>Compact and Robust Models for Japanese-English Character-level Machine Translation<span class=acl-fixed-case>J</span>apanese-<span class=acl-fixed-case>E</span>nglish Character-level Machine Translation</a></strong><br><a href=/people/j/jinan-dai/>Jinan Dai</a>
|
<a href=/people/k/kazunori-yamaguchi/>Kazunori Yamaguchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5202><div class="card-body p-3 small">Character-level translation has been proved to be able to achieve preferable translation quality without explicit segmentation, but training a character-level model needs a lot of hardware resources. In this paper, we introduced two character-level translation models which are mid-gated model and multi-attention model for Japanese-English translation. We showed that the mid-gated model achieved the better performance with respect to BLEU scores. We also showed that a relatively narrow beam of width 4 or 5 was sufficient for the mid-gated model. As for unknown words, we showed that the mid-gated model could somehow translate the one containing Katakana by coining out a close word. We also showed that the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> managed to produce tolerable results for heavily noised sentences, even though the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> was trained with the dataset without noise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5207 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5207/>NICT’s participation to WAT 2019 : <a href=https://en.wikipedia.org/wiki/Multilingualism>Multilingualism</a> and Multi-step Fine-Tuning for Low Resource NMT<span class=acl-fixed-case>NICT</span>’s participation to <span class=acl-fixed-case>WAT</span> 2019: Multilingualism and Multi-step Fine-Tuning for Low Resource <span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5207><div class="card-body p-3 small">In this paper we describe our submissions to WAT 2019 for the following tasks : EnglishTamil translation and RussianJapanese translation. Our team, NICT-5, focused on multilingual domain adaptation and back-translation for RussianJapanese translation and on simple fine-tuning for EnglishTamil translation. We noted that multi-stage fine tuning is essential in leveraging the power of <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a> for an extremely low-resource language like RussianJapanese. Furthermore, we can improve the performance of such a low-resource language pair by exploiting a small but in-domain monolingual corpus via <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>. We managed to obtain second rank in both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> for all translation directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5216 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5216/>LTRC-MT Simple & Effective Hindi-English Neural Machine Translation Systems at WAT 2019<span class=acl-fixed-case>LTRC</span>-<span class=acl-fixed-case>MT</span> Simple & Effective <span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation Systems at <span class=acl-fixed-case>WAT</span> 2019</a></strong><br><a href=/people/v/vikrant-goyal/>Vikrant Goyal</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5216><div class="card-body p-3 small">This paper describes the Neural Machine Translation systems of IIIT-Hyderabad (LTRC-MT) for WAT 2019 Hindi-English shared task. We experimented with both Recurrent Neural Networks & Transformer architectures. We also show the results of our experiments of training NMT models using additional data via backtranslation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5218 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5218/>Supervised neural machine translation based on <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and improved training & inference process</a></strong><br><a href=/people/y/yixuan-tong/>Yixuan Tong</a>
|
<a href=/people/l/liang-liang/>Liang Liang</a>
|
<a href=/people/b/boyan-liu/>Boyan Liu</a>
|
<a href=/people/s/shanshan-jiang/>Shanshan Jiang</a>
|
<a href=/people/b/bin-dong/>Bin Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5218><div class="card-body p-3 small">This is the second time for SRCB to participate in WAT. This paper describes the <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation systems</a> for the shared translation tasks of WAT 2019. We participated in ASPEC tasks and submitted results on English-Japanese, Japanese-English, Chinese-Japanese, and Japanese-Chinese four language pairs. We employed the Transformer model as the baseline and experimented relative position representation, data augmentation, deep layer model, ensemble. Experiments show that all these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can yield substantial improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5222 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5222/>NLPRL at WAT2019 : Transformer-based Tamil English Indic Task Neural Machine Translation System<span class=acl-fixed-case>NLPRL</span> at <span class=acl-fixed-case>WAT</span>2019: Transformer-based <span class=acl-fixed-case>T</span>amil – <span class=acl-fixed-case>E</span>nglish Indic Task Neural Machine Translation System</a></strong><br><a href=/people/a/amit-kumar/>Amit Kumar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5222><div class="card-body p-3 small">This paper describes the Machine Translation system for Tamil-English Indic Task organized at WAT 2019. We use Transformer- based architecture for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5223 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5223/>Idiap NMT System for WAT 2019 Multimodal Translation Task<span class=acl-fixed-case>NMT</span> System for <span class=acl-fixed-case>WAT</span> 2019 Multimodal Translation Task</a></strong><br><a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/p/petr-motlicek/>Petr Motlicek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5223><div class="card-body p-3 small">This paper describes the Idiap submission to WAT 2019 for the English-Hindi Multi-Modal Translation Task. We have used the state-of-the-art Transformer model and utilized the IITB English-Hindi parallel corpus as an additional data source. Among the different tracks of the multi-modal task, we have participated in the Text-Only track for the evaluation and challenge test sets. Our submission tops in its track among the competitors in terms of both automatic and manual evaluation. Based on automatic scores, our text-only submission also outperforms systems that consider <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> in the multi-modal translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5226 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5226/>UCSYNLP-Lab Machine Translation Systems for WAT 2019<span class=acl-fixed-case>UCSYNLP</span>-Lab Machine Translation Systems for <span class=acl-fixed-case>WAT</span> 2019</a></strong><br><a href=/people/y/yimon-shwesin/>Yimon ShweSin</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/k/khinmar-soe/>KhinMar Soe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5226><div class="card-body p-3 small">This paper describes the UCSYNLP-Lab submission to WAT 2019 for Myanmar-English translation tasks in both direction. We have used the neural machine translation systems with attention model and utilized the UCSY-corpus and ALT corpus. In NMT with attention model, we use the word segmentation level as well as syllable segmentation level. Especially, we made the UCSY-corpus to be cleaned in WAT 2019. Therefore, the UCSY corpus for WAT 2019 is not identical to those used in WAT 2018. Experiments show that the <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation systems</a> can produce the substantial improvements.</div></div></div><hr><div id=d19-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-53/>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5300/>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5302/>Relation Prediction for Unseen-Entities Using Entity-Word Graphs</a></strong><br><a href=/people/y/yuki-tagawa/>Yuki Tagawa</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a>
|
<a href=/people/t/takayuki-yamamoto/>Takayuki Yamamoto</a>
|
<a href=/people/k/keiichi-nemoto/>Keiichi Nemoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5302><div class="card-body p-3 small">Knowledge graphs (KGs) are generally used for various <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. However, as <a href=https://en.wikipedia.org/wiki/Knowledge_graph>KGs</a> still miss some information, it is necessary to develop Knowledge Graph Completion (KGC) methods. Most KGC researches do not focus on the Out-of-KGs entities (Unseen-entities), we need a method that can predict the relation for the entity pairs containing Unseen-entities to automatically add new entities to the KGs. In this study, we focus on relation prediction and propose a method to learn entity representations via a graph structure that uses Seen-entities, Unseen-entities and words as nodes created from the descriptions of all entities. In the experiments, our method shows a significant improvement in the relation prediction for the entity pairs containing Unseen-entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5304.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5304/>Neural Speech Translation using <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>Lattice Transformations</a> and Graph Networks</a></strong><br><a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5304><div class="card-body p-3 small">Speech translation systems usually follow a pipeline approach, using word lattices as an <a href=https://en.wikipedia.org/wiki/Intermediate_representation>intermediate representation</a>. However, previous work assume access to the original <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a> used to train the ASR system, which can limit applicability in real scenarios. In this work we propose an approach for <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a> through lattice transformations and neural models based on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph networks</a>. Experimental results show that our approach reaches competitive performance without relying on <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>transcriptions</a>, while also being orders of magnitude faster than previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5305 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5305/>Using Graphs for Word Embedding with Enhanced Semantic Relations</a></strong><br><a href=/people/m/matan-zuckerman/>Matan Zuckerman</a>
|
<a href=/people/m/mark-last/>Mark Last</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5305><div class="card-body p-3 small">Word embedding algorithms have become a common tool in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. While some, like Word2Vec, are based on sequential text input, others are utilizing a <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph representation of text</a>. In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, named WordGraph2Vec, or in short WG2V, which combines the two approaches to gain the benefits of both. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> uses a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed word graph</a> to provide additional information for sequential text input algorithms. Our experiments on benchmark datasets show that text classification algorithms are nearly as accurate with WG2V as with other word embedding models while preserving more stable accuracy rankings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5306 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5306/>Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks</a></strong><br><a href=/people/m/mokanarangan-thayaparan/>Mokanarangan Thayaparan</a>
|
<a href=/people/m/marco-valentino/>Marco Valentino</a>
|
<a href=/people/v/viktor-schlegel/>Viktor Schlegel</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5306><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have resulted in models that surpass human performance when the answer is contained in a single, continuous passage of text. However, complex Question Answering (QA) typically requires multi-hop reasoning-i.e. the integration of supporting facts from different sources, to infer the correct answer. This paper proposes Document Graph Network (DGN), a message passing architecture for the identification of supporting facts over a graph-structured representation of text. The evaluation on HotpotQA shows that DGN obtains competitive results when compared to a reading comprehension baseline operating on raw text, confirming the relevance of structured representations for supporting multi-hop reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5307 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5307/>Essentia : Mining Domain-specific Paraphrases with Word-Alignment Graphs<span class=acl-fixed-case>E</span>ssentia: Mining Domain-specific Paraphrases with Word-Alignment Graphs</a></strong><br><a href=/people/d/danni-ma/>Danni Ma</a>
|
<a href=/people/c/chen-chen/>Chen Chen</a>
|
<a href=/people/b/behzad-golshan/>Behzad Golshan</a>
|
<a href=/people/w/wang-chiew-tan/>Wang-Chiew Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5307><div class="card-body p-3 small">Paraphrases are important linguistic resources for a wide variety of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Many techniques for automatic paraphrase mining from general corpora have been proposed. While these techniques are successful at discovering <a href=https://en.wikipedia.org/wiki/Paraphrase>generic paraphrases</a>, they often fail to identify <a href=https://en.wikipedia.org/wiki/Paraphrase>domain-specific paraphrases</a> (e.g., <a href=https://en.wikipedia.org/wiki/Employment>staff</a>, concierge in the hospitality domain). This is because current techniques are often based on <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>, while domain-specific corpora are too small to fit <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>. In this paper, we present an unsupervised graph-based technique to mine paraphrases from a small set of sentences that roughly share the same topic or intent. Our system, Essentia, relies on word-alignment techniques to create a word-alignment graph that merges and organizes tokens from input sentences. The resulting <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is then used to generate candidate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. We demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> obtains high quality paraphrases, as evaluated by crowd workers. We further show that the majority of the identified <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> are domain-specific and thus complement existing paraphrase databases.<fixed-case>staff, concierge</fixed-case> in the hospitality domain). This is because current techniques are often based on statistical methods, while domain-specific corpora are too small to fit statistical methods. In this paper, we present an unsupervised graph-based technique to mine paraphrases from a small set of sentences that roughly share the same topic or intent. Our system, Essentia, relies on word-alignment techniques to create a word-alignment graph that merges and organizes tokens from input sentences. The resulting graph is then used to generate candidate paraphrases. We demonstrate that our system obtains high quality paraphrases, as evaluated by crowd workers. We further show that the majority of the identified paraphrases are domain-specific and thus complement existing paraphrase databases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5308 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5308.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5308/>Layerwise Relevance Visualization in Convolutional Text Graph Classifiers</a></strong><br><a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/m/marc-hubner/>Marc Hübner</a>
|
<a href=/people/d/david-harbecke/>David Harbecke</a>
|
<a href=/people/c/christoph-alt/>Christoph Alt</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5308><div class="card-body p-3 small">Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a>, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on <a href=https://en.wikipedia.org/wiki/Intermediate_state>intermediate states</a>. In this work, we present a novel method that traces and visualizes <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that contribute to a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification decision</a> in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5310 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5310.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5310/>ASU at TextGraphs 2019 Shared Task : Explanation ReGeneration using Language Models and Iterative Re-Ranking<span class=acl-fixed-case>ASU</span> at <span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs 2019 Shared Task: Explanation <span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>G</span>eneration using Language Models and Iterative Re-Ranking</a></strong><br><a href=/people/p/pratyay-banerjee/>Pratyay Banerjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5310><div class="card-body p-3 small">In this work we describe the system from Natural Language Processing group at Arizona State University for the TextGraphs 2019 Shared Task. The task focuses on Explanation Regeneration, an intermediate step towards general multi-hop inference on large graphs. Our approach consists of modeling the explanation regeneration task as a learning to rank problem, for which we use state-of-the-art language models and explore dataset preparation techniques. We utilize an iterative reranking based approach to further improve the <a href=https://en.wikipedia.org/wiki/Ranking>rankings</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> secured 2nd rank in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with a mean average precision (MAP) of 41.3 % on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5313 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5313/>Chains-of-Reasoning at TextGraphs 2019 Shared Task : Reasoning over Chains of Facts for Explainable Multi-hop Inference<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs 2019 Shared Task: Reasoning over Chains of Facts for Explainable Multi-hop Inference</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/ameya-godbole/>Ameya Godbole</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/s/shehzaad-dhuliawala/>Shehzaad Dhuliawala</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5313><div class="card-body p-3 small">This paper describes our submission to the shared task on Multi-hop Inference Explanation Regeneration in TextGraphs workshop at EMNLP 2019 (Jansen and Ustalov, 2019). Our <a href=https://en.wikipedia.org/wiki/System>system</a> identifies chains of facts relevant to explain an answer to an elementary science examination question. To counter the problem of &#8216;spurious chains&#8217; leading to &#8216;semantic drifts&#8217;, we train a <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> that uses contextualized representation of facts to score its relevance for explaining an answer to a question. Our <a href=https://en.wikipedia.org/wiki/System>system</a> was ranked first w.r.t the mean average precision (MAP) metric outperforming the second best <a href=https://en.wikipedia.org/wiki/System>system</a> by 14.95 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5318 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5318.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5318/>Graph-Based Semi-Supervised Learning for Natural Language Understanding</a></strong><br><a href=/people/z/zimeng-qiu/>Zimeng Qiu</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/x/xiaochun-ma/>Xiaochun Ma</a>
|
<a href=/people/w/william-campbell/>William Campbell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5318><div class="card-body p-3 small">Semi-supervised learning is an efficient method to augment training data automatically from unlabeled data. Development of many natural language understanding (NLU) applications has a challenge where unlabeled data is relatively abundant while labeled data is rather limited. In this work, we propose transductive graph-based semi-supervised learning models as well as their inductive variants for NLU. We evaluate the approach&#8217;s applicability using publicly available NLU data and models. In order to find similar utterances and construct a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, we use a paraphrase detection model. Results show that applying the inductive graph-based semi-supervised learning can improve the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error rate</a> of the NLU model by 5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5319 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5319/>Graph Enhanced Cross-Domain Text-to-SQL Generation<span class=acl-fixed-case>SQL</span> Generation</a></strong><br><a href=/people/s/siyu-huo/>Siyu Huo</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/j/jie-chen/>Jie Chen</a>
|
<a href=/people/m/maria-chang/>Maria Chang</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5319><div class="card-body p-3 small">Semantic parsing is a fundamental problem in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, as it involves the mapping of natural language to structured forms such as <a href=https://en.wikipedia.org/wiki/Executable>executable queries</a> or <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>logic-like knowledge representations</a>. Existing deep learning approaches for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> have shown promise on a variety of benchmark data sets, particularly on text-to-SQL parsing. However, most text-to-SQL parsers do not generalize to unseen data sets in different domains. In this paper, we propose a new cross-domain learning scheme to perform text-to-SQL translation and demonstrate its use on Spider, a large-scale cross-domain text-to-SQL data set. We improve upon a state-of-the-art Spider model, SyntaxSQLNet, by constructing a graph of column names for all databases and using graph neural networks to compute their embeddings. The resulting embeddings offer better cross-domain representations and <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a>, as evidenced by substantial improvement on the Spider data set compared to SyntaxSQLNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5320 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5320/>Reasoning Over Paths via Knowledge Base Completion</a></strong><br><a href=/people/s/saatviga-sudhahar/>Saatviga Sudhahar</a>
|
<a href=/people/a/andrea-pierleoni/>Andrea Pierleoni</a>
|
<a href=/people/i/ian-roberts/>Ian Roberts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5320><div class="card-body p-3 small">Reasoning over paths in large scale knowledge graphs is an important problem for many applications. In this paper we discuss a simple approach to automatically build and rank paths between a source and target entity pair with learned embeddings using a knowledge base completion model (KBC). We assembled a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> by mining the available <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical scientific literature</a> and extracted a set of high frequency paths to use for <a href=https://en.wikipedia.org/wiki/Data_validation>validation</a>. We demonstrate that our method is able to effectively rank a list of known paths between a pair of entities and also come up with plausible paths that are not present in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. For a given entity pair we are able to reconstruct the highest ranking path 60 % of the time within the top 10 ranked paths and achieve 49 % mean average precision. Our approach is compositional since any KBC model that can produce vector representations of entities can be used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5321 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5321/>Node Embeddings for Graph Merging : Case of Knowledge Graph Construction</a></strong><br><a href=/people/i/ida-szubert/>Ida Szubert</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5321><div class="card-body p-3 small">Combining two <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> requires merging the nodes which are counterparts of each other. In this process errors occur, resulting in incorrect merging or incorrect failure to merge. We find a high prevalence of such errors when using AskNET, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for building Knowledge Graphs from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>. AskNET node matching method uses <a href=https://en.wikipedia.org/wiki/String_similarity>string similarity</a>, which we propose to replace with vector embedding similarity. We explore graph-based and word-based embedding models and show an overall <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> of from 56 % to 23.6 %, with a reduction of over a half in both types of incorrect node matching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5322 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5322/>DBee : A Database for Creating and Managing Knowledge Graphs and Embeddings<span class=acl-fixed-case>DB</span>ee: A Database for Creating and Managing Knowledge Graphs and Embeddings</a></strong><br><a href=/people/v/viktor-schlegel/>Viktor Schlegel</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5322><div class="card-body p-3 small">This paper describes DBee, a <a href=https://en.wikipedia.org/wiki/Database>database</a> to support the construction of data-intensive AI applications. DBee provides a unique <a href=https://en.wikipedia.org/wiki/Data_model>data model</a> which operates jointly over large-scale knowledge graphs (KGs) and embedding vector spaces (VSs). This model supports queries which exploit the semantic properties of both types of representations (KGs and VSs). Additionally, DBee aims to facilitate the construction of KGs and VSs, by providing a library of generators, which can be used to create, integrate and transform data into KGs and VSs.</div></div></div><hr><div id=d19-54><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-54.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-54/>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5400/>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5401 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5401/>Answering Naturally : Factoid to Full length Answer Generation</a></strong><br><a href=/people/v/vaishali-pal/>Vaishali Pal</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/i/irshad-bhat/>Irshad Bhat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5401><div class="card-body p-3 small">In recent years, the task of Question Answering over passages, also pitched as a <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, has evolved into a very active research area. A reading comprehension system extracts a span of text, comprising of <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>, <a href=https://en.wikipedia.org/wiki/Calendar_date>dates</a>, <a href=https://en.wikipedia.org/wiki/Phrase>small phrases</a>, etc., which serve as the answer to a given question. However, these spans of text would result in an unnatural reading experience in a conversational system. Usually, <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> solve this issue by using template-based language generation. These <a href=https://en.wikipedia.org/wiki/System>systems</a>, though adequate for a domain specific task, are too restrictive and predefined for a domain independent system. In order to present the user with a more conversational experience, we propose a pointer generator based full-length answer generator which can be used with most QA systems. Our system generates a full length answer given a question and the extracted factoid / span answer without relying on the passage from where the answer was extracted. We also present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 315000 question, factoid answer and full length answer triples. We have evaluated our system using ROUGE-1,2,L and BLEU and achieved 74.05 BLEU score and 86.25 Rogue-L score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5403 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5403/>Abstractive Timeline Summarization</a></strong><br><a href=/people/j/julius-steen/>Julius Steen</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5403><div class="card-body p-3 small">Timeline summarization (TLS) automatically identifies key dates of major events and provides short descriptions of what happened on these dates. Previous approaches to <a href=https://en.wikipedia.org/wiki/Transport_Layer_Security>TLS</a> have focused on extractive methods. In contrast, we suggest an abstractive timeline summarization system. Our system is entirely unsupervised, which makes it especially suited to TLS where there are very few gold summaries available for training of supervised systems. In addition, we present the first abstractive oracle experiments for <a href=https://en.wikipedia.org/wiki/Transport_Layer_Security>TLS</a>. Our system outperforms extractive competitors in terms of ROUGE when the number of input documents is high and the output requires strong <a href=https://en.wikipedia.org/wiki/Data_compression>compression</a>. In these cases, our oracle experiments confirm that our approach also has a higher <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>upper bound</a> for ROUGE scores than extractive methods. A study with human judges shows that our abstractive system also produces output that is easy to read and understand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5404/>Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization</a></strong><br><a href=/people/d/diego-antognini/>Diego Antognini</a>
|
<a href=/people/b/boi-faltings/>Boi Faltings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5404><div class="card-body p-3 small">Linking facts across documents is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, as the language used to express the same information in a sentence can vary significantly, which complicates the task of <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a>. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings : universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>summary</a>, consisting of 665 bytes and 100 words. Unlike other state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, neither hand-crafted features nor additional annotated data are necessary, and the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5408 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5408/>Towards Annotating and Creating Summary Highlights at Sub-sentence Level</a></strong><br><a href=/people/k/kristjan-arumae/>Kristjan Arumae</a>
|
<a href=/people/p/parminder-bhatia/>Parminder Bhatia</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5408><div class="card-body p-3 small">Highlighting is a powerful tool to pick out important content and emphasize. Creating summary highlights at the sub-sentence level is particularly desirable, because sub-sentences are more concise than whole sentences. They are also better suited than individual words and phrases that can potentially lead to disfluent, fragmented summaries. In this paper we seek to generate summary highlights by annotating summary-worthy sub-sentences and teaching <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> to do the same. We frame the task as jointly selecting important sentences and identifying a single most informative textual unit from each sentence. This <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a> dramatically reduces the <a href=https://en.wikipedia.org/wiki/Complexity>task complexity</a> involved in sentence compression. Our study provides new benchmarks and baselines for generating highlights at the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sub-sentence level</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5409/>SAMSum Corpus : A Human-annotated Dialogue Dataset for Abstractive Summarization<span class=acl-fixed-case>SAMS</span>um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization</a></strong><br><a href=/people/b/bogdan-gliwa/>Bogdan Gliwa</a>
|
<a href=/people/i/iwona-mochol/>Iwona Mochol</a>
|
<a href=/people/m/maciej-biesek/>Maciej Biesek</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5409><div class="card-body p-3 small">This paper introduces the SAMSum Corpus, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with abstractive dialogue summaries. We investigate the challenges it poses for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>automated summarization</a> by testing several <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and comparing their results with those obtained on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of news articles</a>. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news in contrast with human evaluators&#8217; judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and non-standard <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality measures</a>. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5410 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5410/>A Closer Look at Data Bias in Neural Extractive Summarization Models</a></strong><br><a href=/people/m/ming-zhong/>Ming Zhong</a>
|
<a href=/people/d/danqing-wang/>Danqing Wang</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5410><div class="card-body p-3 small">In this paper, we take stock of the current state of summarization datasets and explore how different factors of datasets influence the generalization behaviour of neural extractive summarization models. Specifically, we first propose several properties of datasets, which matter for the generalization of summarization models. Then we build the connection between <a href=https://en.wikipedia.org/wiki/Prior_probability>priors</a> residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5413 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5413/>Analyzing Sentence Fusion in Abstractive Summarization</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/j/john-muchovej/>John Muchovej</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/w/walter-chang/>Walter Chang</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5413><div class="card-body p-3 small">While recent work in abstractive summarization has resulted in higher scores in automatic metrics, there is little understanding on how these systems combine information taken from multiple document sentences. In this paper, we analyze the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion. We ask assessors to judge the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>, and method of fusion for summary sentences. Our analysis reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5414 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5414/>Summarizing Relationships for Interactive Concept Map Browsers</a></strong><br><a href=/people/a/abram-handler/>Abram Handler</a>
|
<a href=/people/p/premkumar-ganeshkumar/>Premkumar Ganeshkumar</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/m/mohamed-altantawy/>Mohamed AlTantawy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5414><div class="card-body p-3 small">Concept maps are visual summaries, structured as <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graphs</a> : important concepts from a dataset are displayed as vertexes, and edges between vertexes show natural language descriptions of the relationships between the concepts on the map. Thus far, preliminary attempts at automatically creating concept maps have focused on building static summaries. However, in interactive settings, users will need to dynamically investigate particular relationships between pairs of concepts. For instance, a historian using a concept map browser might decide to investigate the relationship between two politicians in a <a href=https://en.wikipedia.org/wiki/Archive>news archive</a>. We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> which responds to such queries by returning one or more short, importance-ranked, natural language descriptions of the relationship between two requested concepts, for display in a <a href=https://en.wikipedia.org/wiki/User_interface>visual interface</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained on a new public dataset, collected for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div></div><hr><div id=d19-55><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-55.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-55/>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5500/>Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5502 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5502.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5502/>Formality Style Transfer for Noisy, User-generated Conversations : Extracting Labeled, Parallel Data from Unlabeled Corpora</a></strong><br><a href=/people/i/isak-czeresnia-etinger/>Isak Czeresnia Etinger</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5502><div class="card-body p-3 small">Typical <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> used for style transfer in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> contain aligned pairs of two opposite extremes of a style. As each existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is sourced from a specific domain and context, most use cases will have a sizable mismatch from the vocabulary and sentence structures of any <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> available. This reduces the performance of the style transfer, and is particularly significant for noisy, user-generated text. To solve this problem, we show a technique to derive a dataset of aligned pairs (style-agnostic vs stylistic sentences) from an unlabeled corpus by using an auxiliary dataset, allowing for in-domain training. We test the technique with the Yahoo Formality Dataset and 6 novel datasets we produced, which consist of scripts from 5 popular TV-shows (Friends, Futurama, Seinfeld, Southpark, Stargate SG-1) and the Slate Star Codex online forum. We gather 1080 human evaluations, which show that our method produces a sizable change in formality while maintaining fluency and context ; and that it considerably outperforms OpenNMT&#8217;s Seq2Seq model directly trained on the Yahoo Formality Dataset. Additionally, we publish the full pipeline code and our novel <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5503 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5503/>Multilingual Whispers : Generating Paraphrases with Translation</a></strong><br><a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/o/oussama-elachqar/>Oussama Elachqar</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5503><div class="card-body p-3 small">Naturally occurring paraphrase data, such as multiple <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news stories</a> about the same event, is a useful but rare resource. This paper compares translation-based paraphrase gathering using human, automatic, or hybrid techniques to monolingual paraphrasing by experts and non-experts. We gather <a href=https://en.wikipedia.org/wiki/Translation>translations</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>, and empirical human quality assessments of these approaches. Neural machine translation techniques, especially when pivoting through related languages, provide a relatively robust source of <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with diversity comparable to expert human paraphrases. Surprisingly, human translators do not reliably outperform <a href=https://en.wikipedia.org/wiki/Nervous_system>neural systems</a>. The resulting data release will not only be a useful test set, but will also allow additional explorations in translation and paraphrase quality assessments and relationships.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5504 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5504.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5504/>Personalizing Grammatical Error Correction : Adaptation to Proficiency Level and L1<span class=acl-fixed-case>L</span>1</a></strong><br><a href=/people/m/maria-nadejde/>Maria Nadejde</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5504><div class="card-body p-3 small">Grammar error correction (GEC) systems have become ubiquitous in a variety of <a href=https://en.wikipedia.org/wiki/Application_software>software applications</a>, and have started to approach human-level performance for some datasets. However, very little is known about how to efficiently personalize these <a href=https://en.wikipedia.org/wiki/System>systems</a> to the user&#8217;s characteristics, such as their proficiency level and first language, or to emerging domains of text. We present the first results on adapting a general purpose neural GEC system to both the proficiency level and the first language of a writer, using only a few thousand annotated sentences. Our study is the broadest of its kind, covering five proficiency levels and twelve different languages, and comparing three different adaptation scenarios : adapting to the proficiency level only, to the first language only, or to both aspects simultaneously. We show that tailoring to both scenarios achieves the largest performance improvement (3.6 F0.5) relative to a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5506/>Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation</a></strong><br><a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5506><div class="card-body p-3 small">Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> to these variations, without diminishing performance on clean text. We focus on <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance on natural typos, and show that robustness to such <a href=https://en.wikipedia.org/wiki/Noise>noise</a> can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5508 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5508/>Tkol, Httt, and r / radiohead : High Affinity Terms in Reddit Communities<span class=acl-fixed-case>R</span>eddit Communities</a></strong><br><a href=/people/a/abhinav-bhandari/>Abhinav Bhandari</a>
|
<a href=/people/c/caitrin-armstrong/>Caitrin Armstrong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5508><div class="card-body p-3 small">Language is an important marker of a <a href=https://en.wikipedia.org/wiki/Cultural_group>cultural group</a>, large or small. One aspect of language variation between communities is the employment of highly specialized terms with unique significance to the group. We study these high affinity terms across a wide variety of communities by leveraging the rich diversity of <a href=https://en.wikipedia.org/wiki/Reddit>Reddit.com</a>. We provide a systematic exploration of high affinity terms, the often rapid semantic shifts they undergo, and their relationship to subreddit characteristics across 2600 diverse subreddits. Our results show that high affinity terms are effective signals of loyal communities, they undergo more semantic shift than low affinity terms, and that they are partial barrier to entry for new users. We conclude that <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> is a robust and valuable data source for testing further theories about high affinity terms across communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5511 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5511/>Predicting Algorithm Classes for Programming Word Problems</a></strong><br><a href=/people/v/vinayak-athavale/>Vinayak Athavale</a>
|
<a href=/people/a/aayush-naik/>Aayush Naik</a>
|
<a href=/people/r/rajas-vanjape/>Rajas Vanjape</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5511><div class="card-body p-3 small">We introduce the task of algorithm class prediction for programming word problems. A programming word problem is a problem written in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, which can be solved using an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> or a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a>. We define <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>classes</a> of various programming word problems which correspond to the class of algorithms required to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We present four new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for this task, two multiclass datasets with 550 and 1159 problems each and two multilabel datasets having 3737 and 3960 problems each. We pose the problem as a text classification problem and train <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and non-neural network based models on this task. Our best performing <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> gets an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 62.7 percent for the multiclass case on the five class classification dataset, Codeforces Multiclass-5 (CFMC5). We also do some human-level analysis and compare human performance with that of our text classification models. Our best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> has an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> only 9 percent lower than that of a human on this task. To the best of our knowledge, these are the first reported results on such a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We make our code and datasets publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5512 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5512.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5512/>Automatic identification of writers’ intentions : Comparing different methods for predicting relationship goals in online dating profile texts</a></strong><br><a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/t/tess-van-der-zanden/>Tess van der Zanden</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/m/maria-mos/>Maria Mos</a>
|
<a href=/people/a/alexander-schouten/>Alexander Schouten</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5512><div class="card-body p-3 small">Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and predictive ability of LIWC on a relationship goal classification task. In this paper, we compared the outcomes of (1) LIWC, (2) <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, and (3) a human baseline. A newly collected corpus of online dating profile texts (a genre not explored before in the ACL anthology) was used, accompanied by the profile writers&#8217; self-selected relationship goal (long-term versus date). These three approaches were tested by comparing their performance on identifying both the intended relationship goal and content-related text labels. Results show that LIWC and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> correlate with <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluations</a> in terms of content-related labels. LIWC&#8217;s content-related labels corresponded more strongly to humans than those of the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. Moreover, all <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> were similarly accurate in predicting the <a href=https://en.wikipedia.org/wiki/Goal>relationship goal</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5513/>Contextualized Word Representations from Distant Supervision with and for NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5513><div class="card-body p-3 small">We describe a special type of deep contextualized word representation that is learned from distant supervision annotations and dedicated to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our extensive experiments on 7 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> show systematic gains across all domains over strong baselines, and demonstrate that our representation is complementary to previously proposed <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. We report new state-of-the-art results on CONLL and ONTONOTES datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5515 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5515.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5515/>An In-depth Analysis of the Effect of Lexical Normalization on the Dependency Parsing of <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5515><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a> have often been designed with standard texts in mind. However, when these <a href=https://en.wikipedia.org/wiki/Tool>tools</a> are used on the substantially different texts from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, their performance drops dramatically. One solution is to translate social media data to standard language before processing, this is also called normalization. It is well-known that this improves performance for many natural language processing tasks on social media data. However, little is known about which types of <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization replacements</a> have the most effect. Furthermore, it is unknown what the weaknesses of existing lexical normalization systems are in an extrinsic setting. In this paper, we analyze the effect of manual as well as automatic lexical normalization for dependency parsing. After our analysis, we conclude that for most categories, automatic normalization scores close to manually annotated normalization and that small annotation differences are important to take into consideration when exploiting <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> in a pipeline setup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5518 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5518/>Normalising Non-standardised Orthography in Algerian Code-switched User-generated Data<span class=acl-fixed-case>A</span>lgerian Code-switched User-generated Data</a></strong><br><a href=/people/w/wafia-adouane/>Wafia Adouane</a>
|
<a href=/people/j/jean-philippe-bernardy/>Jean-Philippe Bernardy</a>
|
<a href=/people/s/simon-dobnik/>Simon Dobnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5518><div class="card-body p-3 small">We work with <a href=https://en.wikipedia.org/wiki/Algerian_language>Algerian</a>, an under-resourced non-standardised Arabic variety, for which we compile a new parallel corpus consisting of user-generated textual data matched with normalised and corrected human annotations following data-driven and our linguistically motivated standard. We use an end-to-end deep neural model designed to deal with context-dependent spelling correction and <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalisation</a>. Results indicate that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with two CNN sub-network encoders and an LSTM decoder performs the best, and that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>word context</a> matters. Additionally, pre-processing data token-by-token with an edit-distance based aligner significantly improves the performance. We get promising results for the spelling correction and normalisation, as a pre-processing step for downstream tasks, on detecting binary Semantic Textual Similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5519 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5519" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5519/>Dialect Text Normalization to Normative Standard Finnish<span class=acl-fixed-case>F</span>innish</a></strong><br><a href=/people/n/niko-partanen/>Niko Partanen</a>
|
<a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5519><div class="card-body p-3 small">We compare different LSTMs and transformer models in terms of their effectiveness in normalizing <a href=https://en.wikipedia.org/wiki/Finnish_dialects>dialectal Finnish</a> into the normative standard Finnish. As <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> is the common way of communication for people online in Finnish, such a normalization is a necessary step to improve the accuracy of the existing Finnish NLP tools that are tailored for normative Finnish text. We work on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of <a href=https://en.wikipedia.org/wiki/Dialect_continuum>dialectal data</a> of 23 distinct <a href=https://en.wikipedia.org/wiki/Finnish_dialects>Finnish dialects</a>. The best functioning BRNN approach lowers the initial <a href=https://en.wikipedia.org/wiki/Word_error_rate>word error rate</a> of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> from 52.89 to 5.73.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5521 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5521" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5521/>Exploring Multilingual Syntactic Sentence Representations</a></strong><br><a href=/people/c/chen-liu/>Chen Liu</a>
|
<a href=/people/a/anderson-de-andrade/>Anderson De Andrade</a>
|
<a href=/people/m/muhammad-osama/>Muhammad Osama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5521><div class="card-body p-3 small">We study <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for learning <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> with <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. We focus on methods of learning syntactic sentence-embeddings by using a multilingual parallel-corpus augmented by Universal Parts-of-Speech tags. We evaluate the quality of the learned embeddings by examining sentence-level nearest neighbours and functional dissimilarity in the <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a>. We also evaluate the ability of the method to learn syntactic sentence-embeddings for low-resource languages and demonstrate strong evidence for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. Our results show that syntactic sentence-embeddings can be learned while using less training data, fewer model parameters, and resulting in better evaluation metrics than state-of-the-art language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5523 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5523/>Latent semantic network induction in the context of linked example senses</a></strong><br><a href=/people/h/hunter-heidenreich/>Hunter Heidenreich</a>
|
<a href=/people/j/jake-williams/>Jake Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5523><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Princeton_WordNet>Princeton WordNet</a> is a powerful tool for studying <a href=https://en.wikipedia.org/wiki/Language>language</a> and developing natural language processing algorithms. With significant work developing it further, one line considers its extension through aligning its expert-annotated structure with other lexical resources. In contrast, this work explores a completely data-driven approach to network construction, forming a <a href=https://en.wikipedia.org/wiki/Wordnet>wordnet</a> using the entirety of the open-source, noisy, user-annotated dictionary, <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>. Comparing baselines to <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, we find compelling evidence that our <a href=https://en.wikipedia.org/wiki/Social_network>network induction process</a> constructs a <a href=https://en.wikipedia.org/wiki/Social_network>network</a> with useful semantic structure. With thousands of semantically-linked examples that demonstrate sense usage from basic lemmas to multiword expressions (MWEs), we believe this work motivates future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5525 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5525/>Modelling Uncertainty in Collaborative Document Quality Assessment</a></strong><br><a href=/people/a/aili-shen/>Aili Shen</a>
|
<a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/b/bahar-salehi/>Bahar Salehi</a>
|
<a href=/people/j/jianzhong-qi/>Jianzhong Qi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5525><div class="card-body p-3 small">In the context of document quality assessment, previous work has mainly focused on predicting the quality of a document relative to a putative gold standard, without paying attention to the subjectivity of this task. To imitate people&#8217;s disagreement over inherently subjective tasks such as rating the quality of a Wikipedia article, a document quality assessment system should provide not only a prediction of the article quality but also the uncertainty over its predictions. This motivates us to measure the uncertainty in document quality predictions, in addition to making the label prediction. Experimental results show that both Gaussian processes (GPs) and random forests (RFs) can yield competitive results in predicting the quality of Wikipedia articles, while providing an estimate of uncertainty when there is inconsistency in the quality labels from the Wikipedia contributors. We additionally evaluate our methods in the context of a semi-automated document quality class assignment decision-making process, where there is asymmetric risk associated with overestimates and underestimates of document quality. Our experiments suggest that GPs provide more reliable estimates in this context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5526 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5526/>Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge Extraction from Patient-Generated Texts</a></strong><br><a href=/people/a/anja-belz/>Anja Belz</a>
|
<a href=/people/r/richard-hoile/>Richard Hoile</a>
|
<a href=/people/e/elizabeth-ford/>Elizabeth Ford</a>
|
<a href=/people/a/azam-mullick/>Azam Mullick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5526><div class="card-body p-3 small">Approaches to knowledge extraction (KE) in the health domain often start by annotating text to indicate the knowledge to be extracted, and then use the annotated text to train systems to perform the KE. This may work for annotat- ing named entities or other contiguous noun phrases (drugs, some drug effects), but be- comes increasingly difficult when items tend to be expressed across multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions of drug effects in user-generated text). Other issues include that it is not al- ways clear how <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> map to actionable insights, or how they scale up to, or can form part of, more complex KE tasks. This paper reports our efforts in developing an approach to extracting knowledge about drug nonadher- ence from health forums which led us to con- clude that development can not proceed in sep- arate steps but that all aspectsfrom concep- tualisation to annotation scheme development, annotation, KE system training and knowl- edge graph instantiationare interdependent and need to be co-developed. Our aim in this paper is two-fold : we describe a generally ap- plicable framework for developing a KE ap- proach, and present a specific KE approach, developed with the framework, for the task of gathering information about antidepressant drug nonadherence. We report the conceptual- isation, the annotation scheme, the annotated corpus, and an analysis of annotated texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5527 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5527/>What A Sunny Day : Toward Emoji-Sensitive Irony Detection</a></strong><br><a href=/people/s/shirley-anugrah-hayati/>Shirley Anugrah Hayati</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/n/naoki-otani/>Naoki Otani</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5527><div class="card-body p-3 small">Irony detection is an important task with applications in identification of online abuse and <a href=https://en.wikipedia.org/wiki/Harassment>harassment</a>. With the ubiquitous use of <a href=https://en.wikipedia.org/wiki/Nonverbal_communication>non-verbal cues</a> such as <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, in this work we aim to study the role of these structures in irony detection. Since the existing irony detection datasets have 10 % ironic tweets with <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a>, <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on them are insensitive to <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a>. We propose an <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>automated pipeline</a> for creating a more balanced dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5529 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5529/>Dense Node Representation for <a href=https://en.wikipedia.org/wiki/Geolocation>Geolocation</a></a></strong><br><a href=/people/t/tommaso-fornaciari/>Tommaso Fornaciari</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5529><div class="card-body p-3 small">Prior research has shown that <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> can be substantially improved by including user network information. While effective, it suffers from the <a href=https://en.wikipedia.org/wiki/Curse_of_dimensionality>curse of dimensionality</a>, since networks are usually represented as sparse adjacency matrices of connections, which grow exponentially with the number of users. In order to incorporate this <a href=https://en.wikipedia.org/wiki/Information>information</a>, we therefore need to limit the network size, in turn limiting performance and risking sample bias. In this paper, we address these limitations by instead using dense network representations. We explore two methods to learn continuous node representations from either 1) the network structure with node2vec (Grover and Leskovec, 2016), or 2) textual user mentions via doc2vec (Le and Mikolov, 2014). We combine both methods with input from <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a> in an attention-based convolutional neural network and evaluate the contribution of each component on <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> performance. Our method enables us to incorporate arbitrarily large networks in a fixed-length vector, without limiting the network size. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve competitive results with similar state-of-the-art methods, but with much fewer model parameters, while being applicable to <a href=https://en.wikipedia.org/wiki/Complex_network>networks</a> of virtually any size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5533 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5533/>Distant Supervised Relation Extraction with Separate Head-Tail CNN<span class=acl-fixed-case>CNN</span></a></strong><br><a href=/people/r/rui-xing/>Rui Xing</a>
|
<a href=/people/j/jie-luo/>Jie Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5533><div class="card-body p-3 small">Distant supervised relation extraction is an efficient and effective strategy to find relations between entities in texts. However, it inevitably suffers from mislabeling problem and the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy data</a> will hinder the performance. In this paper, we propose the Separate Head-Tail Convolution Neural Network (SHTCNN), a novel neural relation extraction framework to alleviate this issue. In this method, we apply separate convolution and pooling to the head and tail entity respectively for extracting better semantic features of sentences, and coarse-to-fine strategy to filter out instances which do not have actual relations in order to alleviate noisy data issues. Experiments on a widely used dataset show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant and consistent improvements in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> compared to statistical and vanilla CNN-based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5536 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5536/>Benefits of Data Augmentation for NMT-based Text Normalization of <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Content</a><span class=acl-fixed-case>NMT</span>-based Text Normalization of User-Generated Content</a></strong><br><a href=/people/c/claudia-matos-veliz/>Claudia Matos Veliz</a>
|
<a href=/people/o/orphee-de-clercq/>Orphee De Clercq</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5536><div class="card-body p-3 small">One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficulty to automatically process and analyze UGC. Text normalization is the task of transforming lexical variants to their canonical forms and is often used as a pre-processing step for conventional NLP tasks in order to overcome the performance drop that NLP systems experience when applied to UGC. In this work, we follow a Neural Machine Translation approach to <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>. To train such an encoder-decoder model, large parallel training corpora of sentence pairs are required. However, obtaining large data sets with UGC and their normalized version is not trivial, especially for languages other than <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In this paper, we explore how to overcome this data bottleneck for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, a low-resource language. We start off with a small publicly available parallel Dutch data set comprising three UGC genres and compare two different approaches. The <a href=https://en.wikipedia.org/wiki/First_law_of_thermodynamics>first</a> is to manually normalize and add training data, a money and time-consuming task. The second approach is a set of data augmentation techniques which increase data size by converting existing resources into synthesized non-standard forms. Our results reveal that, while the different approaches yield similar results regarding the normalization issues in the test set, they also introduce a large amount of over-normalizations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5540 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5540/>No, you’re not alone : A better way to find people with similar experiences on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/z/zhilin-wang/>Zhilin Wang</a>
|
<a href=/people/e/elena-rastorgueva/>Elena Rastorgueva</a>
|
<a href=/people/w/weizhe-lin/>Weizhe Lin</a>
|
<a href=/people/x/xiaodong-wu/>Xiaodong Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5540><div class="card-body p-3 small">We present a probabilistic clustering algorithm that can help Reddit users to find posts that discuss experiences similar to their own. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is built upon the BERT Next Sentence Prediction model and reduces the <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> for clustering all posts in a corpus from O(n2) to O(n) with respect to the number of posts. We demonstrate that such probabilistic clustering can yield a performance better than baseline clustering methods based on <a href=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>Latent Dirichlet Allocation</a> (Blei et al., 2003) and Word2Vec (Mikolov et al., 2013). Furthermore, there is a high degree of coherence between our probabilistic clustering and the exhaustive comparison O(n2) algorithm in which the similarity between every pair of posts is found. This makes the use of the BERT Next Sentence Prediction model more practical for unsupervised clustering tasks due to the high runtime overhead of each BERT computation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5541 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5541/>Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge</a></strong><br><a href=/people/w/wenhao-ying/>Wenhao Ying</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5541><div class="card-body p-3 small">Deep learning based general language models have achieved state-of-the-art results in many popular <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and QA tasks. Text in domains like <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has its own salient characteristics. Domain knowledge should be helpful in domain relevant tasks. In this work, we devise a simple method to obtain <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and further propose a method to integrate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> with <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> based on deep language models to improve performance of <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. Experiments on Twitter data show that even though a deep language model fine-tuned by a target domain data has attained comparable results to that of previous state-of-the-art models, this fine-tuned model can still benefit from our extracted <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to obtain more improvement. This highlights the importance of making use of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> in <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5542 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5542/>Adapting Deep Learning Methods for Mental Health Prediction on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/i/ivan-sekulic/>Ivan Sekulic</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5542><div class="card-body p-3 small">Mental health poses a significant challenge for an individual&#8217;s well-being. Text analysis of rich resources, like <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, can contribute to deeper understanding of illnesses and provide means for their early detection. We tackle a challenge of detecting social media users&#8217; mental status through deep learning-based models, moving away from traditional approaches to the task. In a binary classification task on predicting if a user suffers from one of nine different disorders, a hierarchical attention network outperforms previously set benchmarks for four of the disorders. Furthermore, we explore the limitations of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> and analyze phrases relevant for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> by inspecting the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s word-level attention weights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5544 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5544.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5544/>An Ensemble of <a href=https://en.wikipedia.org/wiki/Humour>Humour</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a>, and Hate Speechfor Sentiment Classification in Online Reviews</a></strong><br><a href=/people/r/rohan-badlani/>Rohan Badlani</a>
|
<a href=/people/n/nishit-asnani/>Nishit Asnani</a>
|
<a href=/people/m/manan-rai/>Manan Rai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5544><div class="card-body p-3 small">Due to the nature of online user reviews, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on such <a href=https://en.wikipedia.org/wiki/Data>data</a> requires a deep semantic understanding of the text. Many <a href=https://en.wikipedia.org/wiki/Review>online reviews</a> are sarcastic, humorous, or hateful. Signals from such language nuances may reinforce or completely alter the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> of a review as predicted by a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> that attempts to detect <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> alone. Thus, having a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that is explicitly aware of these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> should help <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> perform better on reviews that are characterized by them. We propose a composite two-step model that extracts features pertaining to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Humour>humour</a>, <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, as well as <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, in the first step, feeding them in conjunction to inform <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment classification</a> in the second step. We show that this multi-step approach leads to a better empirical performance for sentiment classification than a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that predicts sentiment alone. A qualitative analysis reveals that the conjunctive approach can better capture the nuances of sentiment as expressed in online reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5547 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5547/>A Social Opinion Gold Standard for the Malta Government Budget 2018<span class=acl-fixed-case>M</span>alta Government Budget 2018</a></strong><br><a href=/people/k/keith-cortis/>Keith Cortis</a>
|
<a href=/people/b/brian-davis/>Brian Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5547><div class="card-body p-3 small">We present a gold standard of annotated social opinion for the Malta Government Budget 2018. It consists of over 500 online posts in English and/or the Maltese less-resourced language, gathered from social media platforms, specifically, social networking services and newswires, which have been annotated with information about opinions expressed by the general public and other entities, in terms of sentiment polarity, emotion, sarcasm / irony, and negation. This <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is a resource for <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> based on social data, within the context of <a href=https://en.wikipedia.org/wiki/Politics>politics</a>. It is the first opinion annotated social dataset from Malta, which has very limited language resources available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5549 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5549/>Y’all should read this ! Identifying Plurality in Second-Person Personal Pronouns in English Texts<span class=acl-fixed-case>Y</span>’all should read this! Identifying Plurality in Second-Person Personal Pronouns in <span class=acl-fixed-case>E</span>nglish Texts</a></strong><br><a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/r/ronen-tamari/>Ronen Tamari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5549><div class="card-body p-3 small">Distinguishing between singular and plural you in English is a challenging task which has potential for downstream applications, such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> or <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. While formal written English does not distinguish between these cases, other languages (such as Spanish), as well as other dialects of English (via phrases such as y&#8217; all), do make this distinction. We make use of this to obtain distantly-supervised labels for the task on a large-scale in two domains. Following, we train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to distinguish between the single / plural &#8216;you&#8217;, finding that although in-domain training achieves reasonable <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> (77 %), there is still a lot of room for improvement, especially in the domain-transfer scenario, which proves extremely challenging. Our code and data are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5552.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5552 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5552 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5552/>Contextualized context2vec</a></strong><br><a href=/people/k/kazuki-ashihara/>Kazuki Ashihara</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/s/satoru-uchida/>Satoru Uchida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5552><div class="card-body p-3 small">Lexical substitution ranks substitution candidates from the viewpoint of paraphrasability for a target word in a given sentence. There are two major approaches for <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> : (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. Herein we propose a method that combines these two approaches to contextualize word embeddings for <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>. Experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms the current state-of-the-art method. We also create CEFR-LP, a new evaluation dataset for the lexical substitution task. It has a wider coverage of substitution candidates than previous <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and assigns <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English proficiency levels</a> to all target words and substitution candidates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5555.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5555 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5555 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5555/>Unsupervised Neologism Normalization Using Embedding Space Mapping</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/k/kapil-thadani/>Kapil Thadani</a>
|
<a href=/people/a/aasish-pappu/>Aasish Pappu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5555><div class="card-body p-3 small">This paper presents an approach for detecting and normalizing neologisms in <a href=https://en.wikipedia.org/wiki/Social_media>social media content</a>. Neologisms refer to recent expressions that are specific to certain entities or events and are being increasingly used by the public, but have not yet been accepted in mainstream language. Automated methods for handling <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> are important for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and normalization, especially for informal genres with <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated content</a>. We present an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for detecting <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> and then normalizing them to canonical words without relying on parallel training data. Our approach builds on the text normalization literature and introduces <a href=https://en.wikipedia.org/wiki/Adaptation>adaptations</a> to fit the specificities of this task, including <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic and etymological considerations</a>. We evaluate the proposed techniques on a dataset of Reddit comments, with detected neologisms and corresponding <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalizations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5557 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5557/>Towards Actual (Not Operational) Textual Style Transfer Auto-Evaluation</a></strong><br><a href=/people/r/richard-yuanzhe-pang/>Richard Yuanzhe Pang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5557><div class="card-body p-3 small">Regarding the problem of automatically generating paraphrases with modified styles or attributes, the difficulty lies in the lack of parallel corpora. Numerous advances have been proposed for the <a href=https://en.wikipedia.org/wiki/Electricity_generation>generation</a>. However, significant problems remain with the auto-evaluation of style transfer tasks. Based on the summary of Pang and Gimpel (2018) and Mir et al. (2019), style transfer evaluations rely on three metrics : post-transfer style classification accuracy, content or semantic similarity, and naturalness or fluency. We elucidate the dangerous current state of style transfer auto-evaluation research. Moreover, we propose ways to aggregate the three <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> into one <a href=https://en.wikipedia.org/wiki/Evaluation>evaluator</a>. This abstract aims to bring researchers to think about the future of style transfer and style transfer evaluation research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5558 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5558/>CodeSwitch-Reddit : Exploration of Written Multilingual Discourse in Online Discussion Forums<span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>S</span>witch-<span class=acl-fixed-case>R</span>eddit: Exploration of Written Multilingual Discourse in Online Discussion Forums</a></strong><br><a href=/people/e/ella-rabinovich/>Ella Rabinovich</a>
|
<a href=/people/m/masih-sultani/>Masih Sultani</a>
|
<a href=/people/s/suzanne-stevenson/>Suzanne Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5558><div class="card-body p-3 small">In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into <a href=https://en.wikipedia.org/wiki/Code-switching>written code-switching</a> in <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion forums</a>. The released <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can further facilitate a range of research and practical activities.</div></div></div><hr><div id=d19-56><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-56.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-56/>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5600/>Proceedings of the 3rd Workshop on Neural Generation and Translation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/h/hiroaki-hayashi/>Hiroaki Hayashi</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/m/minh-thang-luong/>Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5603 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5603/>Recycling a Pre-trained BERT Encoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a><span class=acl-fixed-case>BERT</span> Encoder for Neural Machine Translation</a></strong><br><a href=/people/k/kenji-imamura/>Kenji Imamura</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5603><div class="card-body p-3 small">In this paper, a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is applied to Transformer-based neural machine translation (NMT). In contrast to monolingual tasks, the number of unlearned model parameters in an NMT decoder is as huge as the number of learned parameters in the BERT model. To train all the models appropriately, we employ two-stage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> without pre-training. Additionally, we confirmed that <a href=https://en.wikipedia.org/wiki/Neurotransmitter>NMT</a> with the BERT encoder is more effective in low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5604 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5604/>Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models</a></strong><br><a href=/people/w/woon-sang-cho/>Woon Sang Cho</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5604><div class="card-body p-3 small">Ambiguous user queries in <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> result in the retrieval of documents that often span multiple topics. One potential solution is for the <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> to generate multiple refined queries, each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework, called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document, question) pairs. At test time, given multiple documents, the Distribute step of our MSQG model predicts target word distributions for each document using the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. The Aggregate step aggregates these <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a> to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5607 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5607/>Transformer-based Model for Single Documents Neural Summarization</a></strong><br><a href=/people/e/elozino-egonmwan/>Elozino Egonmwan</a>
|
<a href=/people/y/yllias-chali/>Yllias Chali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5607><div class="card-body p-3 small">We propose a system that improves performance on single document summarization task using the CNN / DailyMail and Newsroom datasets. It follows the popular encoder-decoder paradigm, but with an extra focus on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. The intuition is that the probability of correctly decoding an information significantly lies in the pattern and correctness of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Hence we introduce, encode encode decode. A framework that encodes the source text first with a transformer, then a sequence-to-sequence (seq2seq) model. We find that the transformer and seq2seq model complement themselves adequately, making for a richer encoded vector representation. We also find that paying more attention to the vocabulary of target words during <a href=https://en.wikipedia.org/wiki/Abstraction>abstraction</a> improves performance. We experiment our hypothesis and framework on the task of extractive and abstractive single document summarization and evaluate using the standard CNN / DailyMail dataset and the recently released Newsroom dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5608 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5608/>Making Asynchronous Stochastic Gradient Descent Work for Transformers</a></strong><br><a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5608><div class="card-body p-3 small">Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so synchronous SGD has become the norm for Transformer training. This is unfortunate because asynchronous SGD is faster at raw training speed since it avoids waiting for <a href=https://en.wikipedia.org/wiki/Synchronization_(computer_science)>synchronization</a>. Moreover, the Transformer model is the basis for state-of-the-art models for several tasks, including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, so training speed matters. To understand why asynchronous SGD under-performs, we blur the lines between asynchronous and synchronous methods. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, the Transformer attains the same BLEU score 1.36 times as fast.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5612 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5612" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5612/>On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation<span class=acl-fixed-case>K</span>ullback-<span class=acl-fixed-case>L</span>eibler Divergence Term in Variational Autoencoders for Text Generation</a></strong><br><a href=/people/v/victor-prokhorov/>Victor Prokhorov</a>
|
<a href=/people/e/ehsan-shareghi/>Ehsan Shareghi</a>
|
<a href=/people/y/yingzhen-li/>Yingzhen Li</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5612><div class="card-body p-3 small">Variational Autoencoders (VAEs) are known to suffer from learning uninformative latent representation of the input due to issues such as approximated posterior collapse, or entanglement of the latent space. We impose an explicit <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> on the Kullback-Leibler (KL) divergence term inside the VAE objective function. While the explicit <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> naturally avoids posterior collapse, we use it to further understand the significance of the KL term in controlling the information transmitted through the VAE channel. Within this framework, we explore different properties of the estimated posterior distribution, and highlight the trade-off between the amount of information encoded in a latent code during training, and the generative capacity of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5615 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5615/>Enhanced Transformer Model for Data-to-Text Generation</a></strong><br><a href=/people/l/li-gong/>Li Gong</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5615><div class="card-body p-3 small">Neural models have recently shown significant progress on data-to-text generation tasks in which descriptive texts are generated conditioned on <a href=https://en.wikipedia.org/wiki/Record_(computer_science)>database records</a>. In this work, we present a new Transformer-based data-to-text generation model which learns content selection and summary generation in an end-to-end fashion. We introduce two extensions to the baseline transformer model : First, we modify the latent representation of the input, which helps to significantly improve the content correctness of the output summary ; Second, we include an additional learning objective that accounts for content selection modelling. In addition, we propose two data augmentation methods that succeed to further improve performance of the resulting generation models. Evaluation experiments show that our final model outperforms current state-of-the-art systems as measured by different metrics : BLEU, content selection precision and content ordering. We made publicly available the transformer extension presented in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5616 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5616.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5616/>Generalization in Generation : A closer look at Exposure Bias</a></strong><br><a href=/people/f/florian-schmidt/>Florian Schmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5616><div class="card-body p-3 small">Exposure bias refers to the train-test discrepancy that seemingly arises when an <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive generative model</a> uses only ground-truth contexts at training time but generated ones at test time. We separate the contribution of the learning framework and the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to clarify the debate on consequences and review proposed counter-measures. In this light, we argue that <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> is the underlying property to address and propose unconditional generation as its fundamental benchmark. Finally, we combine <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable modeling</a> with a recent formulation of exploration in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to obtain a rigorous handling of true and generated contexts. Results on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and variational sentence auto-encoding confirm the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s generalization capability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5621 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5621/>A Margin-based Loss with Synthetic Negative Samples for Continuous-output Machine Translation</a></strong><br><a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/s/sachin-kumar/>Sachin Kumar</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5621><div class="card-body p-3 small">Neural models that eliminate the softmax bottleneck by generating word embeddings (rather than <a href=https://en.wikipedia.org/wiki/Multinomial_distribution>multinomial distributions</a> over a vocabulary) attain faster training with fewer learnable parameters. These models are currently trained by maximizing densities of pretrained target embeddings under von Mises-Fisher distributions parameterized by corresponding model-predicted embeddings. This work explores the utility of margin-based loss functions in optimizing such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We present syn-margin loss, a novel margin-based loss that uses a synthetic negative sample constructed from only the predicted and target embeddings at every step. The <a href=https://en.wikipedia.org/wiki/Profit_(accounting)>loss</a> is efficient to compute, and we use a <a href=https://en.wikipedia.org/wiki/Geometric_analysis>geometric analysis</a> to argue that it is more consistent and interpretable than other margin-based losses. Empirically, we find that syn-margin provides small but significant improvements over both vMF and standard margin-based losses in continuous-output neural machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5622 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5622/>Mixed Multi-Head Self-Attention for Neural Machine Translation</a></strong><br><a href=/people/h/hongyi-cui/>Hongyi Cui</a>
|
<a href=/people/s/shohei-iida/>Shohei Iida</a>
|
<a href=/people/p/po-hsuan-hung/>Po-Hsuan Hung</a>
|
<a href=/people/t/takehito-utsuro/>Takehito Utsuro</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5622><div class="card-body p-3 small">Recently, the Transformer becomes a state-of-the-art architecture in the filed of neural machine translation (NMT). A key point of its high-performance is the multi-head self-attention which is supposed to allow the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to independently attend to information from different representation subspaces. However, there is no explicit mechanism to ensure that different attention heads indeed capture different features, and in practice, <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>redundancy</a> has occurred in multiple heads. In this paper, we argue that using the same <a href=https://en.wikipedia.org/wiki/Attentional_control>global attention</a> in multiple heads limits multi-head self-attention&#8217;s capacity for learning distinct features. In order to improve the expressiveness of multi-head self-attention, we propose a novel Mixed Multi-Head Self-Attention (MMA) which models not only global and local attention but also forward and backward attention in different attention heads. This enables the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn distinct representations explicitly among multiple heads. In our experiments on both WAT17 English-Japanese as well as IWSLT14 German-English translation task, we show that, without increasing the number of parameters, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> yield consistent and significant improvements (0.9 BLEU scores on average) over the strong Transformer baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5624 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5624" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5624/>Interrogating the Explanatory Power of <a href=https://en.wikipedia.org/wiki/Attention>Attention</a> in Neural Machine Translation</a></strong><br><a href=/people/p/pooya-moradi/>Pooya Moradi</a>
|
<a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5624><div class="card-body p-3 small">Attention models have become a crucial component in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>. They are often implicitly or explicitly used to justify the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of information in <a href=https://en.wikipedia.org/wiki/Mathematical_model>NMT</a>. To evaluate the explanatory power of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for NMT, we examine the possibility of yielding the same prediction but with counterfactual attention models that modify crucial aspects of the trained <a href=https://en.wikipedia.org/wiki/Attention>attention model</a>. Using these counterfactual attention mechanisms we assess the extent to which they still preserve the generation of function and content words in the translation process. Compared to a state of the art attention model, our counterfactual attention models produce 68 % of <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> and 21 % of <a href=https://en.wikipedia.org/wiki/Content_word>content words</a> in our German-English dataset. Our experiments demonstrate that attention models by themselves can not reliably explain the decisions made by a NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5625 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5625" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5625/>Auto-Sizing the Transformer Network : Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation</a></strong><br><a href=/people/k/kenton-murray/>Kenton Murray</a>
|
<a href=/people/j/jeffery-kinnison/>Jeffery Kinnison</a>
|
<a href=/people/t/toan-q-nguyen/>Toan Q. Nguyen</a>
|
<a href=/people/w/walter-scheirer/>Walter Scheirer</a>
|
<a href=/people/d/david-chiang/>David Chiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5625><div class="card-body p-3 small">Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Yet these <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are very sensitive to <a href=https://en.wikipedia.org/wiki/Network_architecture>architecture</a> and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires many training runs. In this paper, we incorporate <a href=https://en.wikipedia.org/wiki/Architecture_search>architecture search</a> into a single training run through auto-sizing, which uses <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> to delete <a href=https://en.wikipedia.org/wiki/Neuron>neurons</a> in a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a> from the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5628 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5628/>Monash University’s Submissions to the WNGT 2019 Document Translation Task<span class=acl-fixed-case>WNGT</span> 2019 Document Translation Task</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5628><div class="card-body p-3 small">We describe the work of Monash University for the shared task of Rotowire document translation organised by the 3rd Workshop on Neural Generation and Translation (WNGT 2019). We submitted systems for both directions of the English-German language pair. Our main focus is on employing an established document-level neural machine translation model for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We achieve a <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> of 39.83 (41.46 BLEU per WNGT evaluation) for En-De and 45.06 (47.39 BLEU per WNGT evaluation) for De-En translation directions on the Rotowire test set. All experiments conducted in the process are also described.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5630 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5630" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5630/>University of Edinburgh’s submission to the Document-level Generation and Translation Shared Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s submission to the Document-level Generation and Translation Shared Task</a></strong><br><a href=/people/r/ratish-puduppully/>Ratish Puduppully</a>
|
<a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5630><div class="card-body p-3 small">The University of Edinburgh participated in all six tracks : NLG, MT, and MT+NLG with both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a> as targeted languages. For the NLG track, we submitted a multilingual system based on the Content Selection and Planning model of Puduppully et al (2019). For the MT track, we submitted Transformer-based Neural Machine Translation models, where out-of-domain parallel data was augmented with in-domain data extracted from monolingual corpora. Our MT+NLG systems disregard the structured input data and instead rely exclusively on the source summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5631.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5631 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5631/>Naver Labs Europe’s Systems for the Document-Level Generation and Translation Task at WNGT 2019<span class=acl-fixed-case>E</span>urope’s Systems for the Document-Level Generation and Translation Task at <span class=acl-fixed-case>WNGT</span> 2019</a></strong><br><a href=/people/f/fahimeh-saleh/>Fahimeh Saleh</a>
|
<a href=/people/a/alexandre-berard/>Alexandre Berard</a>
|
<a href=/people/i/ioan-calapodescu/>Ioan Calapodescu</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5631><div class="card-body p-3 small">Recently, neural models led to significant improvements in both <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation tasks (NLG)</a>. However, generation of long descriptive summaries conditioned on <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> remains an open challenge. Likewise, <a href=https://en.wikipedia.org/wiki/Metadata>MT</a> that goes beyond <a href=https://en.wikipedia.org/wiki/Metadata>sentence-level context</a> is still an open issue (e.g., document-level MT or <a href=https://en.wikipedia.org/wiki/Metadata>MT</a> with metadata). To address these challenges, we propose to leverage data from both tasks and do <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> between MT, NLG, and MT with source-side metadata (MT+NLG). First, we train document-based MT systems with large amounts of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. Then, we adapt these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to pure NLG and MT+NLG tasks by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> with smaller amounts of domain-specific data. This end-to-end NLG approach, without data selection and planning, outperforms the previous state of the art on the Rotowire NLG task. We participated to the Document Generation and Translation task at WNGT 2019, and ranked first in all tracks.</div></div></div><hr><div id=d19-57><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-57.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-57/>Proceedings of The 5th Workshop on BioNLP Open Shared Tasks</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5700/>Proceedings of The 5th Workshop on BioNLP Open Shared Tasks</a></strong><br><a href=/people/k/kim-jin-dong/>Kim Jin-Dong</a>
|
<a href=/people/n/nedellec-claire/>Nédellec Claire</a>
|
<a href=/people/b/bossy-robert/>Bossy Robert</a>
|
<a href=/people/d/deleger-louise/>Deléger Louise</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5701 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5701/>PharmaCoNER : Pharmacological Substances, Compounds and proteins Named Entity Recognition track<span class=acl-fixed-case>P</span>harma<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NER</span>: Pharmacological Substances, Compounds and proteins Named Entity Recognition track</a></strong><br><a href=/people/a/aitor-gonzalez-agirre/>Aitor Gonzalez-Agirre</a>
|
<a href=/people/m/montserrat-marimon/>Montserrat Marimon</a>
|
<a href=/people/a/ander-intxaurrondo/>Ander Intxaurrondo</a>
|
<a href=/people/o/obdulia-rabal/>Obdulia Rabal</a>
|
<a href=/people/m/marta-villegas/>Marta Villegas</a>
|
<a href=/people/m/martin-krallinger/>Martin Krallinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5701><div class="card-body p-3 small">One of the biomedical entity types of relevance for <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a> or biosciences are <a href=https://en.wikipedia.org/wiki/Chemical_compound>chemical compounds</a> and <a href=https://en.wikipedia.org/wiki/Drug>drugs</a>. The correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. Although a significant effort was made to detect mentions of drugs / chemicals in English texts, so far only very limited attempts were made to recognize them in medical documents in other languages. Taking into account the growing amount of <a href=https://en.wikipedia.org/wiki/Medical_literature>medical publications</a> and <a href=https://en.wikipedia.org/wiki/Medical_record>clinical records</a> written in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, we have organized the first shared task on detecting drug and chemical entities in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish medical documents</a>. Additionally, we included a clinical concept-indexing sub-track asking teams to return SNOMED-CT identifiers related to drugs / chemicals for a collection of documents. For this task, named PharmaCoNER, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. A total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). Top scoring teams used sophisticated <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning approaches</a> yielding very competitive results with F-measures above 0.91. These results indicate that there is a real interest in promoting biomedical text mining efforts beyond <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We foresee that the PharmaCoNER annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of Spanish medical data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5704 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5704/>IxaMed at PharmacoNER Challenge 2019<span class=acl-fixed-case>I</span>xa<span class=acl-fixed-case>M</span>ed at <span class=acl-fixed-case>P</span>harmaco<span class=acl-fixed-case>NER</span> Challenge 2019</a></strong><br><a href=/people/x/xabier-lahuerta/>Xabier Lahuerta</a>
|
<a href=/people/i/iakes-goenaga/>Iakes Goenaga</a>
|
<a href=/people/k/koldo-gojenola/>Koldo Gojenola</a>
|
<a href=/people/a/aitziber-atutxa-salazar/>Aitziber Atutxa Salazar</a>
|
<a href=/people/m/maite-oronoz/>Maite Oronoz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5704><div class="card-body p-3 small">The aim of this paper is to present our approach (IxaMed) in the PharmacoNER 2019 task. The task consists of identifying chemical, drug, and gene / protein mentions from clinical case studies written in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The evaluation of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is divided in two scenarios : one corresponding to the detection of named entities and one corresponding to the indexation of named entities that have been previously identified. In order to identify <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> we have made use of a Bi-LSTM with a CRF on top in combination with different types of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We have achieved our best result (86.81 F-Score) combining pretrained word embeddings of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and <a href=https://en.wikipedia.org/wiki/Electronic_health_record>Electronic Health Records</a> (50 M words) with contextual string embeddings of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and <a href=https://en.wikipedia.org/wiki/Electronic_health_record>Electronic Health Records</a>. On the other hand, for the indexation of the named entities we have used the <a href=https://en.wikipedia.org/wiki/Levenshtein_distance>Levenshtein distance</a> obtaining a 85.34 <a href=https://en.wikipedia.org/wiki/F-score>F-Score</a> as our best result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5706 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5706/>A Deep Learning-Based System for PharmaCoNER<span class=acl-fixed-case>P</span>harma<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/y/ying-xiong/>Ying Xiong</a>
|
<a href=/people/y/yedan-shen/>Yedan Shen</a>
|
<a href=/people/y/yuanhang-huang/>Yuanhang Huang</a>
|
<a href=/people/s/shuai-chen/>Shuai Chen</a>
|
<a href=/people/b/buzhou-tang/>Buzhou Tang</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a>
|
<a href=/people/q/qingcai-chen/>Qingcai Chen</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/y/yi-zhou/>Yi Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5706><div class="card-body p-3 small">The Biological Text Mining Unit at BSC and CNIO organized the first shared task on chemical & drug mention recognition from Spanish medical texts called PharmaCoNER (Pharmacological Substances, Compounds and proteins and Named Entity Recognition track) in 2019, which includes two tracks : one for NER offset and entity classification (track 1) and the other one for concept indexing (track 2). We developed a pipeline system based on deep learning methods for this shared task, specifically, a subsystem based on BERT (Bidirectional Encoder Representations from Transformers) for NER offset and entity classification and a subsystem based on Bpool (Bi-LSTM with max / mean pooling) for concept indexing. Evaluation conducted on the shared task data showed that our system achieves a micro-average F1-score of 0.9105 on track 1 and a micro-average F1-score of 0.8391 on track 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5708 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5708/>A Neural Pipeline Approach for the PharmaCoNER Shared Task using Contextual Exhaustive Models<span class=acl-fixed-case>P</span>harma<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NER</span> Shared Task using Contextual Exhaustive Models</a></strong><br><a href=/people/m/mohammad-golam-sohrab/>Mohammad Golam Sohrab</a>
|
<a href=/people/m/minh-thang-pham/>Minh Thang Pham</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5708><div class="card-body p-3 small">We present a neural pipeline approach that performs named entity recognition (NER) and concept indexing (CI), which links them to concept unique identifiers (CUIs) in a knowledge base, for the PharmaCoNER shared task on pharmaceutical drugs and chemical entities. We proposed a neural NER model that captures the surrounding semantic information of a given sequence by capturing the forward- and backward-context of bidirectional LSTM (Bi-LSTM) output of a target span using contextual span representation-based exhaustive approach. The <a href=https://en.wikipedia.org/wiki/NER_model>NER model</a> enumerates all possible spans as potential entity mentions and classify them into entity types or no entity with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. For representing span, we compare several different neural network architectures and their <a href=https://en.wikipedia.org/wiki/Network_topology>ensembling</a> for the <a href=https://en.wikipedia.org/wiki/NER_model>NER model</a>. We then perform dictionary matching for CI and, if there is no matching, we further compute similarity scores between a mention and CUIs using entity embeddings to assign the CUI with the highest score to the mention. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> on the two sub-tasks in the shared task. Among the five submitted runs, the best run for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a> achieved the F-score of 86.76 % on Sub-task 1 (NER) and the F-score of 79.97 % (strict) on Sub-task 2 (CI).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5709 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5709" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5709/>Biomedical Named Entity Recognition with Multilingual BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/k/kai-hakala/>Kai Hakala</a>
|
<a href=/people/s/sampo-pyysalo/>Sampo Pyysalo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5709><div class="card-body p-3 small">We present the approach of the Turku NLP group to the PharmaCoNER task on Spanish biomedical named entity recognition. We apply a CRF-based baseline approach and multilingual BERT to the task, achieving an F-score of 88 % on the development data and 87 % on the test set with BERT. Our approach reflects a straightforward application of a state-of-the-art multilingual model that is not specifically tailored to either the language nor the application domain. The source code is available at : https://github.com/chaanim/pharmaconer</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5710 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5710" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5710/>An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>NLP</span> <span class=acl-fixed-case>OST</span> 2019 <span class=acl-fixed-case>AGAC</span> Track Tasks</a></strong><br><a href=/people/y/yuxing-wang/>Yuxing Wang</a>
|
<a href=/people/k/kaiyin-zhou/>Kaiyin Zhou</a>
|
<a href=/people/m/mina-gachloo/>Mina Gachloo</a>
|
<a href=/people/j/jingbo-xia/>Jingbo Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5710><div class="card-body p-3 small">The active gene annotation corpus (AGAC) was developed to support <a href=https://en.wikipedia.org/wiki/Knowledge_discovery>knowledge discovery</a> for <a href=https://en.wikipedia.org/wiki/Drug_repurposing>drug repurposing</a>. Based on the corpus, the AGAC track of the BioNLP Open Shared Tasks 2019 was organized, to facilitate cross-disciplinary collaboration across BioNLP and Pharmacoinformatics communities, for <a href=https://en.wikipedia.org/wiki/Drug_repurposing>drug repurposing</a>. The AGAC track consists of three subtasks : 1) <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, 2) thematic relation extraction, and 3) loss of function (LOF) / gain of function (GOF) topic classification. The AGAC track was participated by five teams, of which the performance are compared and analyzed. The the results revealed a substantial room for improvement in the design of the task, which we analyzed in terms of imbalanced data, selective annotation and latent topic annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5716 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5716/>A Multi-Task Learning Framework for Extracting Bacteria Biotope Information</a></strong><br><a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/c/chao-liu/>Chao Liu</a>
|
<a href=/people/y/ying-chi/>Ying Chi</a>
|
<a href=/people/x/xuansong-xie/>Xuansong Xie</a>
|
<a href=/people/x/xiansheng-hua/>Xiansheng Hua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5716><div class="card-body p-3 small">This paper presents a novel transfer multi-task learning method for Bacteria Biotope rel+ner task at BioNLP-OST 2019. To alleviate the data deficiency problem in domain-specific information extraction, we use BERT(Bidirectional Encoder Representations from Transformers) and pre-train it using mask language models and next sentence prediction on both general corpus and medical corpus like <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>. In fine-tuning stage, we fine-tune the relation extraction layer and mention recognition layer designed by us on the top of BERT to extract mentions and relations simultaneously. The evaluation results show that our method achieves the best performance on all metrics (including slot error rate, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and recall) in the Bacteria Biotope rel+ner subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5718 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5718/>Using Snomed to recognize and index chemical and drug mentions.</a></strong><br><a href=/people/p/pilar-lopez-ubeda/>Pilar López Úbeda</a>
|
<a href=/people/m/manuel-carlos-diaz-galiano/>Manuel Carlos Díaz Galiano</a>
|
<a href=/people/l/l-alfonso-urena-lopez/>L. Alfonso Urena Lopez</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Maite Martin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5718><div class="card-body p-3 small">In this paper we describe a new named entity extraction system. Our work proposes a system for the identification and annotation of drug names in Spanish biomedical texts based on machine learning and deep learning models. Subsequently, a standardized code using <a href=https://en.wikipedia.org/wiki/Snomed>Snomed</a> is assigned to these <a href=https://en.wikipedia.org/wiki/Medication>drugs</a>, for this purpose, Natural Language Processing tools and techniques have been used, and a dictionary of different sources of information has been built. The results are promising, we obtain 78 % in F1 score on the first sub-track and in the second task we map with <a href=https://en.wikipedia.org/wiki/Snomed>Snomed</a> correctly 72 % of the found entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5720 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5720" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5720/>Linguistically Informed Relation Extraction and Neural Architectures for Nested Named Entity Recognition in BioNLP-OST 2019<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>OST</span> 2019</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/u/usama-yaseen/>Usama Yaseen</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5720><div class="card-body p-3 small">Named Entity Recognition (NER) and Relation Extraction (RE) are essential tools in distilling knowledge from biomedical literature. This paper presents our findings from participating in BioNLP Shared Tasks 2019. We addressed <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> including <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>nested entities extraction</a>, Entity Normalization and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Relation Extraction</a>. Our proposed approach of <a href=https://en.wikipedia.org/wiki/Named_entity>Named Entities</a> can be generalized to different languages and we have shown it&#8217;s effectiveness for English and Spanish text. We investigated linguistic features, hybrid loss including ranking and Conditional Random Fields (CRF), multi-task objective and token level ensembling strategy to improve NER. We employed dictionary based fuzzy and semantic search to perform Entity Normalization. Finally, our RE system employed Support Vector Machine (SVM) with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. Our NER submission (team : MIC-CIS) ranked first in BB-2019 norm+NER task with standard error rate (SER) of 0.7159 and showed competitive performance on PharmaCo NER task with F1-score of 0.8662. Our RE system ranked first in the SeeDev-binary Relation Extraction Task with F1-score of 0.3738.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5721.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5721 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5721 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5721/>An ensemble CNN method for biomedical entity normalization<span class=acl-fixed-case>CNN</span> method for biomedical entity normalization</a></strong><br><a href=/people/p/pan-deng/>Pan Deng</a>
|
<a href=/people/h/haipeng-chen/>Haipeng Chen</a>
|
<a href=/people/m/mengyao-huang/>Mengyao Huang</a>
|
<a href=/people/x/xiaowen-ruan/>Xiaowen Ruan</a>
|
<a href=/people/l/liang-xu/>Liang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5721><div class="card-body p-3 small">Different representations of the same concept could often be seen in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific reports</a> and publications. Entity normalization (or entity linking) is the task to match the different <a href=https://en.wikipedia.org/wiki/Representation_(systemics)>representations</a> to their standard concepts. In this paper, we present a two-step ensemble CNN method that normalizes microbiology-related entities in free text to concepts in standard <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a>. The method is capable of linking entities when only a small microbiology-related biomedical corpus is available for training, and achieved reasonable performance in the online test of the BioNLP-OST19 shared task Bacteria Biotope.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5722.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5722 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5722 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5722/>BOUN-ISIK Participation : An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotopes<span class=acl-fixed-case>BOUN</span>-<span class=acl-fixed-case>ISIK</span> Participation: An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotopes</a></strong><br><a href=/people/i/ilknur-karadeniz/>İlknur Karadeniz</a>
|
<a href=/people/o/omer-faruk-tuna/>Ömer Faruk Tuna</a>
|
<a href=/people/a/arzucan-ozgur/>Arzucan Özgür</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5722><div class="card-body p-3 small">This paper presents our participation to the Bacteria Biotope Task of the BioNLP Shared Task 2019. Our participation includes two systems for the two subtasks of the Bacteria Biotope Task : the normalization of entities (BB-norm) and the identification of the relations between the entities given a biomedical text (BB-rel). For the normalization of entities, we utilized <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and syntactic re-ranking. For the relation extraction task, pre-defined rules are used. Although both approaches are unsupervised, in the sense that they do not need any labeled data, they achieved promising results. Especially, for the BB-norm task, the results have shown that the proposed method performs as good as <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based methods</a>, which require labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5724 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5724/>Integration of <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Traditional <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> for <a href=https://en.wikipedia.org/wiki/Knowledge_extraction>Knowledge Extraction</a> from Biomedical Literature</a></strong><br><a href=/people/j/jihang-mao/>Jihang Mao</a>
|
<a href=/people/w/wanli-liu/>Wanli Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5724><div class="card-body p-3 small">In this paper, we present our participation in the Bacteria Biotope (BB) task at BioNLP-OST 2019. Our system utilizes fine-tuned language representation models and machine learning approaches based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> and lexical features for entities recognition, normalization and relation extraction. It achieves the state-of-the-art performance and is among the top two systems in five of all six subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5725.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5725 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5725 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5725/>CRAFT Shared Tasks 2019 Overview Integrated Structure, <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a>, and <a href=https://en.wikipedia.org/wiki/Coreference>Coreference</a><span class=acl-fixed-case>CRAFT</span> Shared Tasks 2019 Overview — Integrated Structure, Semantics, and Coreference</a></strong><br><a href=/people/w/william-a-baumgartner-jr/>William Baumgartner</a>
|
<a href=/people/m/michael-bada/>Michael Bada</a>
|
<a href=/people/s/sampo-pyysalo/>Sampo Pyysalo</a>
|
<a href=/people/m/manuel-r-ciosici/>Manuel R. Ciosici</a>
|
<a href=/people/n/negacy-hailu/>Negacy Hailu</a>
|
<a href=/people/h/harrison-pielke-lombardo/>Harrison Pielke-Lombardo</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/l/lawrence-hunter/>Lawrence Hunter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5725><div class="card-body p-3 small">As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks dependency parse construction, coreference resolution, and ontology concept identification over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5726.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5726 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5726 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5726/>UZH@CRAFT-ST : a Sequence-labeling Approach to Concept Recognition<span class=acl-fixed-case>UZH</span>@<span class=acl-fixed-case>CRAFT</span>-<span class=acl-fixed-case>ST</span>: a Sequence-labeling Approach to Concept Recognition</a></strong><br><a href=/people/l/lenz-furrer/>Lenz Furrer</a>
|
<a href=/people/j/joseph-cornelius/>Joseph Cornelius</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5726><div class="card-body p-3 small">As our submission to the CRAFT shared task 2019, we present two neural approaches to concept recognition. We propose two different systems for joint named entity recognition (NER) and normalization (NEN), both of which model the task as a sequence labeling problem. Our first system is a BiLSTM network with two separate outputs for NER and NEN trained from scratch, whereas the second system is an instance of BioBERT fine-tuned on the concept-recognition task. We exploit two strategies for extending concept coverage, ontology pretraining and <a href=https://en.wikipedia.org/wiki/Backoff>backoff</a> with a dictionary lookup. Our results show that the backoff strategy effectively tackles the problem of unseen concepts, addressing a major limitation of the chosen <a href=https://en.wikipedia.org/wiki/Design>design</a>. In the cross-system comparison, BioBERT proves to be a strong basis for creating a concept-recognition system, although some entity types are predicted more accurately by the BiLSTM-based system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5728 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5728/>Neural Dependency Parsing of Biomedical Text : TurkuNLP entry in the CRAFT Structural Annotation Task<span class=acl-fixed-case>T</span>urku<span class=acl-fixed-case>NLP</span> entry in the <span class=acl-fixed-case>CRAFT</span> Structural Annotation Task</a></strong><br><a href=/people/t/thang-minh-ngo/>Thang Minh Ngo</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/s/sampo-pyysalo/>Sampo Pyysalo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5728><div class="card-body p-3 small">We present the approach taken by the TurkuNLP group in the CRAFT Structural Annotation task, a shared task on dependency parsing. Our approach builds primarily on the Turku neural parser, a native dependency parser that ranked among the best in the recent CoNLL tasks on parsing Universal Dependencies. To adapt the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to the biomedical domain, we considered and evaluated a number of approaches, including the generation of custom word embeddings, combination with other in-domain resources, and the incorporation of information from <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We achieved a labeled attachment score of 89.7 %, the best result among task participants.</div></div></div><hr><div id=d19-58><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-58.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-58/>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5800/>Proceedings of the 2nd Workshop on Machine Reading for Question Answering</a></strong><br><a href=/people/a/adam-fisch/>Adam Fisch</a>
|
<a href=/people/a/alon-talmor/>Alon Talmor</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5802 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5802/>Inspecting Unification of Encoding and Matching with Transformer : A Case Study of Machine Reading Comprehension</a></strong><br><a href=/people/h/hangbo-bao/>Hangbo Bao</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/s/songhao-piao/>Songhao Piao</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5802><div class="card-body p-3 small">Most machine reading comprehension (MRC) models separately handle encoding and matching with different <a href=https://en.wikipedia.org/wiki/Network_architecture>network architectures</a>. In contrast, pretrained language models with Transformer layers, such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2018), have achieved competitive performance on MRC. A research question that naturally arises is : apart from the benefits of pre-training, how many performance gain comes from the unified network architecture. In this work, we evaluate and analyze unifying encoding and matching components with Transformer for the MRC task. Experimental results on SQuAD show that the <a href=https://en.wikipedia.org/wiki/Unified_Model>unified model</a> outperforms previous networks that separately treat <a href=https://en.wikipedia.org/wiki/Code>encoding</a> and <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a>. We also introduce a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to inspect whether a Transformer layer tends to perform <a href=https://en.wikipedia.org/wiki/Code>encoding</a> or matching. The analysis results show that the <a href=https://en.wikipedia.org/wiki/Unified_model>unified model</a> learns different <a href=https://en.wikipedia.org/wiki/Mathematical_model>modeling strategies</a> compared with previous manually-designed models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5803 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5803/>CALOR-QUEST : generating a training corpus for Machine Reading Comprehension models from shallow semantic annotations<span class=acl-fixed-case>CALOR</span>-<span class=acl-fixed-case>QUEST</span> : generating a training corpus for Machine Reading Comprehension models from shallow semantic annotations</a></strong><br><a href=/people/f/frederic-bechet/>Frederic Bechet</a>
|
<a href=/people/c/cindy-aloui/>Cindy Aloui</a>
|
<a href=/people/d/delphine-charlet/>Delphine Charlet</a>
|
<a href=/people/g/geraldine-damnati/>Geraldine Damnati</a>
|
<a href=/people/j/johannes-heinecke/>Johannes Heinecke</a>
|
<a href=/people/a/alexis-nasr/>Alexis Nasr</a>
|
<a href=/people/f/frederic-herledan/>Frederic Herledan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5803><div class="card-body p-3 small">Machine reading comprehension is a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> related to <a href=https://en.wikipedia.org/wiki/Question_answering>Question-Answering</a> where questions are not generic in scope but are related to a particular document. Recently very large corpora (SQuAD, MS MARCO) containing triplets (document, question, answer) were made available to the scientific community to develop supervised methods based on deep neural networks with promising results. These methods need very large training corpus to be efficient, however such kind of <a href=https://en.wikipedia.org/wiki/Data>data</a> only exists for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> at the moment. The aim of this study is the development of such resources for other languages by proposing to generate in a semi-automatic way questions from the semantic Frame analysis of large corpora. The collect of natural questions is reduced to a <a href=https://en.wikipedia.org/wiki/Validity_(statistics)>validation / test set</a>. We applied this method on the CALOR-Frame French corpus to develop the CALOR-QUEST resource presented in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5805 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5805.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5805/>Answer-Supervised Question Reformulation for Enhancing Conversational Machine Comprehension</a></strong><br><a href=/people/q/qian-li/>Qian Li</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/d/daling-wang/>Daling Wang</a>
|
<a href=/people/z/zekang-li/>Zekang Li</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/y/yifei-zhang/>Yifei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5805><div class="card-body p-3 small">In conversational machine comprehension, it has become one of the research hotspots integrating conversational history information through question reformulation for obtaining better answers. However, the existing question reformulation models are trained only using supervised question labels annotated by annotators without considering any feedback information from answers. In this paper, we propose a novel Answer-Supervised Question Reformulation (ASQR) model for enhancing conversational machine comprehension with reinforcement learning technology. ASQR utilizes a pointer-copy-based question reformulation model as an agent, takes an action to predict the next word, and observes a reward for the whole sentence state after generating the end-of-sequence token. The experimental results on QuAC dataset prove that our ASQR model is more effective in conversational machine comprehension. Moreover, pretraining is essential in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning models</a>, so we provide a high-quality annotated dataset for question reformulation by sampling a part of QuAC dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5807 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5807/>Improving the Robustness of Deep Reading Comprehension Models by Leveraging Syntax Prior</a></strong><br><a href=/people/b/bowen-wu/>Bowen Wu</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/z/zongsheng-wang/>Zongsheng Wang</a>
|
<a href=/people/q/qihang-feng/>Qihang Feng</a>
|
<a href=/people/j/jingsong-yu/>Jingsong Yu</a>
|
<a href=/people/b/baoxun-wang/>Baoxun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5807><div class="card-body p-3 small">Despite the remarkable progress on Machine Reading Comprehension (MRC) with the help of open-source datasets, recent studies indicate that most of the current MRC systems unfortunately suffer from weak <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> against adversarial samples. To address this issue, we attempt to take <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence syntax</a> as the leverage in the answer predicting process which previously only takes account of phrase-level semantics. Furthermore, to better utilize the sentence syntax and improve the robustness, we propose a Syntactic Leveraging Network, which is designed to deal with adversarial samples by exploiting the syntactic elements of a question. The experiment results indicate that our method is promising for improving the generalization and robustness of MRC models against the influence of adversarial samples, with performance well-maintained.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5808 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5808.Attachment.tgz data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5808/>Reasoning Over Paragraph Effects in Situations</a></strong><br><a href=/people/k/kevin-lin/>Kevin Lin</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5808><div class="card-body p-3 small">A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation. In order to facilitate progress on this kind of reading, we present <a href=https://en.wikipedia.org/wiki/ROPES>ROPES</a>, a challenging <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> targeting Reasoning Over Paragraph Effects in Situations. We target expository language describing causes and effects (e.g., animal pollinators increase efficiency of fertilization in flowers), as they have clear implications for new situations. A <a href=https://en.wikipedia.org/wiki/System>system</a> is presented a background passage containing at least one of these relations, a novel situation that uses this <a href=https://en.wikipedia.org/wiki/Context_(language_use)>background</a>, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation. We collect background passages from science textbooks and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,322 question dataset. We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs only slightly better than randomly guessing an answer of the correct type, at 61.6 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>, well below the human performance of 89.0 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5809 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5809/>Towards Answer-unaware Conversational Question Generation</a></strong><br><a href=/people/m/mao-nakanishi/>Mao Nakanishi</a>
|
<a href=/people/t/tetsunori-kobayashi/>Tetsunori Kobayashi</a>
|
<a href=/people/y/yoshihiko-hayashi/>Yoshihiko Hayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5809><div class="card-body p-3 small">Conversational question generation is a novel area of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a> which has a range of potential applications. This paper is first to presents a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for conversational question generation that is unaware of the corresponding answers. To properly generate a question coherent to the grounding text and the current conversation history, the proposed framework first locates the focus of a question in the text passage, and then identifies the question pattern that leads the sequential generation of the words in a question. The experiments using the CoQA dataset demonstrate that the quality of generated questions greatly improves if the question foci and the question patterns are correctly identified. In addition, it was shown that the question foci, even estimated with a reasonable <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, could contribute to the quality improvement. These results established that our research direction may be promising, but at the same time revealed that the identification of question patterns is a challenging issue, and it has to be largely refined to achieve a better quality in the end-to-end automatic question generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5810 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5810/>Cross-Task Knowledge Transfer for Query-Based Text Summarization</a></strong><br><a href=/people/e/elozino-egonmwan/>Elozino Egonmwan</a>
|
<a href=/people/v/vittorio-castelli/>Vittorio Castelli</a>
|
<a href=/people/m/md-arafat-sultan/>Md Arafat Sultan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5810><div class="card-body p-3 small">We demonstrate the viability of <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> between two related tasks : machine reading comprehension (MRC) and query-based text summarization. Using an <a href=https://en.wikipedia.org/wiki/Model-driven_architecture>MRC model</a> trained on the SQuAD1.1 dataset as a core system component, we first build an extractive query-based summarizer. For better precision, this <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarizer</a> also compresses the output of the MRC model using a novel sentence compression technique. We further leverage pre-trained machine translation systems to abstract our extracted summaries. Our models achieve state-of-the-art results on the publicly available CNN / Daily Mail and Debatepedia datasets, and can serve as simple yet powerful baselines for future systems. We also hope that these results will encourage research on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from large MRC corpora to query-based summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5814 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5814/>Machine Comprehension Improves Domain-Specific Japanese Predicate-Argument Structure Analysis<span class=acl-fixed-case>J</span>apanese Predicate-Argument Structure Analysis</a></strong><br><a href=/people/n/norio-takahashi/>Norio Takahashi</a>
|
<a href=/people/t/tomohide-shibata/>Tomohide Shibata</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5814><div class="card-body p-3 small">To improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of predicate-argument structure (PAS) analysis, large-scale training data and knowledge for PAS analysis are indispensable. We focus on a specific domain, specifically Japanese blogs on driving, and construct two wide-coverage datasets as a form of QA using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> : a PAS-QA dataset and a reading comprehension QA (RC-QA) dataset. We train a machine comprehension (MC) model based on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to perform PAS analysis. Our experiments show that a stepwise training method is the most effective, which pre-trains an MC model based on the RC-QA dataset to acquire <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and then fine-tunes based on the PAS-QA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5818 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5818.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5818/>Bend but Do n’t Break? Multi-Challenge Stress Test for QA Models<span class=acl-fixed-case>QA</span> Models</a></strong><br><a href=/people/h/hemant-pugaliya/>Hemant Pugaliya</a>
|
<a href=/people/j/james-route/>James Route</a>
|
<a href=/people/k/kaixin-ma/>Kaixin Ma</a>
|
<a href=/people/y/yixuan-geng/>Yixuan Geng</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5818><div class="card-body p-3 small">The field of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering (QA)</a> has seen rapid growth in new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and <a href=https://en.wikipedia.org/wiki/Modeling_and_simulation>modeling approaches</a> in recent years. Large scale datasets and focus on challenging linguistic phenomena have driven development in neural models, some of which have achieved parity with human performance in limited cases. However, an examination of state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model output</a> reveals that a gap remains in reasoning ability compared to a human, and performance tends to degrade when <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are exposed to less-constrained tasks. We are interested in more clearly defining the strengths and limitations of leading models across diverse QA challenges, intending to help future researchers with identifying pathways to generalizable performance. We conduct extensive qualitative and quantitative analyses on the results of four <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> across four datasets and relate common errors to model capabilities. We also illustrate limitations in the datasets we examine and discuss a way forward for achieving generalizable models and datasets that broadly test QA capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5823.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5823 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5823 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5823/>Extractive NarrativeQA with Heuristic Pre-Training<span class=acl-fixed-case>N</span>arrative<span class=acl-fixed-case>QA</span> with Heuristic Pre-Training</a></strong><br><a href=/people/l/lea-frermann/>Lea Frermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5823><div class="card-body p-3 small">Although advances in neural architectures for NLP problems as well as <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised pre-training</a> have led to substantial improvements on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and natural language inference, understanding of and reasoning over long texts still poses a substantial challenge. Here, we consider the task of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> from full narratives (e.g., books or movie scripts), or their summaries, tackling the NarrativeQA challenge (NQA ; Kocisky et al. We introduce a heuristic extractive version of the <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, which allows us to approach the more feasible problem of answer extraction (rather than generation). We train systems for passage retrieval as well as answer span prediction using this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>. We use pre-trained BERT embeddings for injecting prior knowledge into our <a href=https://en.wikipedia.org/wiki/System>system</a>. We show that our setup leads to state of the art performance on summary-level QA. On QA from full narratives, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the METEOR metric. We analyze the relative contributions of pre-trained embeddings and the extractive training paradigm, and provide a detailed error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5824.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5824 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5824 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5824/>CLER : Cross-task Learning with Expert Representation to Generalize Reading and Understanding<span class=acl-fixed-case>CLER</span>: Cross-task Learning with Expert Representation to Generalize Reading and Understanding</a></strong><br><a href=/people/t/takumi-takahashi/>Takumi Takahashi</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5824><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension task</a> of the MRQA shared task. We propose CLER, which stands for Cross-task Learning with Expert Representation for the generalization of reading and understanding. To generalize its capabilities, the proposed model is composed of three key ideas : <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, mixture of experts, and <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble</a>. In-domain datasets are used to train and validate our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>, and other out-of-domain datasets are used to validate the generalization of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s performances. In a submission run result, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an average F1 score of 66.1 % in the out-of-domain setting, which is a 4.3 percentage point improvement over the official BERT baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5825.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5825 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5825 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5825/>Question Answering Using Hierarchical Attention on Top of BERT Features<span class=acl-fixed-case>BERT</span> Features</a></strong><br><a href=/people/r/reham-osama/>Reham Osama</a>
|
<a href=/people/n/nagwa-m-el-makky/>Nagwa El-Makky</a>
|
<a href=/people/m/marwan-torki/>Marwan Torki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5825><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Model_(person)>model</a> submitted works as follows. When supplied a question and a passage it makes use of the BERT embedding along with the hierarchical attention model which consists of 2 parts, the co-attention and the self-attention, to locate a continuous span of the passage that is the answer to the question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5827.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5827 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5827 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5827/>Generalizing Question Answering System with Pre-trained Language Model Fine-tuning</a></strong><br><a href=/people/d/dan-su/>Dan Su</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/h/hyeondey-kim/>Hyeondey Kim</a>
|
<a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5827><div class="card-body p-3 small">With a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC)tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these mod-els and techniques can generalize to out-of-domain and unseen RC tasks. To enhance the generalization ability, we propose a multi-task learning framework that learns the shared representation across different tasks. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is built on top of a large pre-trained language model, such as XLNet, and then fine-tuned on multiple RC datasets. Experimental results show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, with an average Exact Match score of 56.59 and an <a href=https://en.wikipedia.org/wiki/F-number>average F1 score</a> of 68.98, which significantly improves the BERT-Large baseline by8.39 and 7.22, respectively</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5828.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5828 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5828 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5828/>D-NET : A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension<span class=acl-fixed-case>D</span>-<span class=acl-fixed-case>NET</span>: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension</a></strong><br><a href=/people/h/hongyu-li/>Hongyu Li</a>
|
<a href=/people/x/xiyuan-zhang/>Xiyuan Zhang</a>
|
<a href=/people/y/yibing-liu/>Yibing Liu</a>
|
<a href=/people/y/yiming-zhang/>Yiming Zhang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/x/xiangyang-zhou/>Xiangyang Zhou</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5828><div class="card-body p-3 small">In this paper, we introduce a simple system Baidu submitted for MRQA (Machine Reading for Question Answering) 2019 Shared Task that focused on generalization of machine reading comprehension (MRC) models. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is built on a framework of pretraining and fine-tuning, namely D-NET. The techniques of pre-trained language models and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> are explored to improve the generalization of MRC models and we conduct experiments to examine the effectiveness of these strategies. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5829.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5829 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5829 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5829/>An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering</a></strong><br><a href=/people/s/shayne-longpre/>Shayne Longpre</a>
|
<a href=/people/y/yi-lu/>Yi Lu</a>
|
<a href=/people/z/zhucheng-tu/>Zhucheng Tu</a>
|
<a href=/people/c/chris-dubois/>Chris DuBois</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5829><div class="card-body p-3 small">To produce a domain-agnostic question answering model for the Machine Reading Question Answering (MRQA) 2019 Shared Task, we investigate the relative benefits of large pre-trained language models, various data sampling strategies, as well as query and context paraphrases generated by back-translation. We find a simple negative sampling technique to be particularly effective, even though it is typically used for <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that include unanswerable questions, such as SQuAD 2.0. When applied in conjunction with per-domain sampling, our XLNet (Yang et al., 2019)-based submission achieved the second best Exact Match and F1 in the MRQA leaderboard competition.</div></div></div><hr><div id=d19-59><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-59.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-59/>Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5900/>Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP</a></strong><br><a href=/people/s/silviu-paun/>Silviu Paun</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5901 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5901/>Dependency Tree Annotation with Mechanical Turk<span class=acl-fixed-case>M</span>echanical <span class=acl-fixed-case>T</span>urk</a></strong><br><a href=/people/s/stephen-tratz/>Stephen Tratz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5901><div class="card-body p-3 small">Crowdsourcing is frequently employed to quickly and inexpensively obtain valuable linguistic annotations but is rarely used for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, likely due to the perceived difficulty of the task and the limited training of the available workers. This paper presents what is, to the best of our knowledge, the first published use of Mechanical Turk (or similar platform) to crowdsource parse trees. We pay Turkers to construct unlabeled dependency trees for 500 English sentences using an interactive graphical dependency tree editor, collecting 10 annotations per sentence. Despite not requiring any training, several of the more prolific workers meet or exceed 90 % attachment agreement with the Penn Treebank (PTB) portion of our data, and, furthermore, for 72 % of these PTB sentences, at least one Turker produces a perfect parse. Thus, we find that, supported with a simple <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>graphical interface</a>, people with presumably no prior experience can achieve surprisingly high degrees of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. To facilitate research into aggregation techniques for complex crowdsourced annotations, we publicly release our annotated corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5905 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5905.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5905/>Crowd-sourcing annotation of complex NLU tasks : A case study of argumentative content annotation<span class=acl-fixed-case>NLU</span> tasks: A case study of argumentative content annotation</a></strong><br><a href=/people/t/tamar-lavee/>Tamar Lavee</a>
|
<a href=/people/l/lili-kotlerman/>Lili Kotlerman</a>
|
<a href=/people/m/matan-orbach/>Matan Orbach</a>
|
<a href=/people/y/yonatan-bilu/>Yonatan Bilu</a>
|
<a href=/people/m/michal-jacovi/>Michal Jacovi</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5905><div class="card-body p-3 small">Recent advancements in <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a> and listening comprehension involve the annotation of long texts. Such <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are typically time consuming, making crowd-annotations an attractive solution, yet their complexity often makes such a <a href=https://en.wikipedia.org/wiki/Solution>solution</a> unfeasible. In particular, a major concern is that crowd annotators may be tempted to skim through long texts, and answer questions without reading thoroughly. We present a case study of adapting this type of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to the crowd. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to identify claims in a several minute long debate speech. We show that sentence-by-sentence annotation does not scale and that labeling only a subset of sentences is insufficient. Instead, we propose a scheme for effectively performing the full, complex task with crowd annotators, allowing the collection of large scale annotated datasets. We believe that the encountered challenges and pitfalls, as well as lessons learned, are relevant in general when collecting data for large scale natural language understanding (NLU) tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5906 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5906/>Computer Assisted Annotation of Tension Development in <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED Talks</a> through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>Crowdsourcing</a><span class=acl-fixed-case>TED</span> Talks through Crowdsourcing</a></strong><br><a href=/people/s/seungwon-yoon/>Seungwon Yoon</a>
|
<a href=/people/w/wonsuk-yang/>Wonsuk Yang</a>
|
<a href=/people/j/jong-c-park/>Jong Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5906><div class="card-body p-3 small">We propose a method of machine-assisted annotation for the identification of tension development, annotating whether the <a href=https://en.wikipedia.org/wiki/Tension_(physics)>tension</a> is increasing, decreasing, or staying unchanged. We use a neural network based prediction model, whose predicted results are given to the annotators as initial values for the options that they are asked to choose. By presenting such initial values to the annotators, the annotation task becomes an evaluation task where the annotators inspect whether or not the predicted results are correct. To demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we performed the <a href=https://en.wikipedia.org/wiki/Annotation>annotation task</a> in both in-house and crowdsourced environments. For the crowdsourced environment, we compared the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> results with and without our method of machine-assisted annotation. We find that the results with our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> showed a higher agreement to the gold standard than those without, though our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> had little effect at reducing the time for annotation. Our codes for the experiment are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5907 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5907.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5907/>CoSSAT : Code-Switched Speech Annotation Tool<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>SSAT</span>: Code-Switched Speech Annotation Tool</a></strong><br><a href=/people/s/sanket-shah/>Sanket Shah</a>
|
<a href=/people/p/pratik-joshi/>Pratik Joshi</a>
|
<a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5907><div class="card-body p-3 small">Code-switching refers to the alternation of two or more languages in a conversation or utterance and is common in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual communities</a> across the world. Building code-switched speech and natural language processing systems are challenging due to the lack of annotated speech and text data. We present a speech annotation interface CoSSAT, which helps annotators transcribe code-switched speech faster, more easily and more accurately than a traditional <a href=https://en.wikipedia.org/wiki/Interface_(computing)>interface</a>, by displaying candidate words from monolingual speech recognizers. We conduct a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> on the transcription of Hindi-English code-switched speech with 10 annotators and describe quantitative and qualitative results.</div></div></div><hr><div id=d19-60><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-60.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-60/>Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6000/>Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</a></strong><br><a href=/people/s/simon-ostermann/>Simon Ostermann</a>
|
<a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6001/>Cracking the Contextual Commonsense Code : Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations</a></strong><br><a href=/people/j/jeff-da/>Jeff Da</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6001><div class="card-body p-3 small">Pretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT&#8217;s commonsense representation abilities. First, we probe BERT&#8217;s ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT&#8217;s pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6003/>Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering</a></strong><br><a href=/people/k/kaixin-ma/>Kaixin Ma</a>
|
<a href=/people/j/jonathan-francis/>Jonathan Francis</a>
|
<a href=/people/q/quanyang-lu/>Quanyang Lu</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/a/alessandro-oltramari/>Alessandro Oltramari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6003><div class="card-body p-3 small">Non-extractive commonsense QA remains a challenging AI task, as it requires systems to reason about, synthesize, and gather disparate pieces of information, in order to generate responses to queries. Recent approaches on such tasks show increased performance, only when <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are either pre-trained with additional information or when domain-specific heuristics are used, without any special consideration regarding the knowledge resource type. In this paper, we perform a survey of recent commonsense QA methods and we provide a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for <a href=https://en.wikipedia.org/wiki/Knowledge_integration>knowledge integration</a> and that the degree of domain overlap, between <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> and <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, plays a crucial role in determining model success.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6005/>Commonsense about Human Senses : Labeled Data Collection Processes</a></strong><br><a href=/people/n/ndapandula-nakashole/>Ndapa Nakashole</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6005><div class="card-body p-3 small">We consider the problem of extracting from text commonsense knowledge pertaining to <a href=https://en.wikipedia.org/wiki/Sense>human senses</a> such as sound and smell. First, we consider the problem of recognizing mentions of human senses in text. Our contribution is a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for acquiring <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. Experiments show the effectiveness of our proposed data labeling approach when used with standard <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> on the task of sense recognition in text. Second, we propose to extract novel, common sense relationships pertaining to sense perception concepts. Our contribution is a process for generating <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> by leveraging large corpora and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing questionnaires</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6009/>IIT-KGP at COIN 2019 : Using pre-trained Language Models for modeling Machine Comprehension<span class=acl-fixed-case>IIT</span>-<span class=acl-fixed-case>KGP</span> at <span class=acl-fixed-case>COIN</span> 2019: Using pre-trained Language Models for modeling Machine Comprehension</a></strong><br><a href=/people/p/prakhar-sharma/>Prakhar Sharma</a>
|
<a href=/people/s/sumegh-roychowdhury/>Sumegh Roychowdhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6009><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> for COIN 2019 Shared Task 1 : Commonsense Inference in Everyday Narrations. We show the power of leveraging state-of-the-art pre-trained language models such as BERT(Bidirectional Encoder Representations from Transformers) and XLNet over other Commonsense Knowledge Base Resources such as ConceptNet and NELL for modeling machine comprehension. We used an ensemble of BERT-Large and XLNet-Large. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> give substantial improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and other <a href=https://en.wikipedia.org/wiki/System>systems</a> incorporating <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. We bagged 2nd position on the final test set leaderboard with an accuracy of 90.5 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6011/>Pingan Smart Health and SJTU at COIN-Shared Task : utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks<span class=acl-fixed-case>SJTU</span> at <span class=acl-fixed-case>COIN</span> - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks</a></strong><br><a href=/people/x/xiepeng-li/>Xiepeng Li</a>
|
<a href=/people/z/zhexi-zhang/>Zhexi Zhang</a>
|
<a href=/people/w/wei-zhu/>Wei Zhu</a>
|
<a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/y/yuan-ni/>Yuan Ni</a>
|
<a href=/people/p/peng-gao/>Peng Gao</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a>
|
<a href=/people/g/guotong-xie/>Guotong Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6011><div class="card-body p-3 small">To solve the shared tasks of COIN : COmmonsense INference in Natural Language Processing) Workshop in, we need explore the impact of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The <a href=https://en.wikipedia.org/wiki/First_law_of_thermodynamics>first</a> is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks ; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply <a href=https://en.wikipedia.org/wiki/Concatenation>concatenation</a> or multi-head attention. We find out that : (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly ; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6012/>BLCU-NLP at COIN-Shared Task1 : Stagewise Fine-tuning BERT for Commonsense Inference in Everyday Narrations<span class=acl-fixed-case>BLCU</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>COIN</span>-Shared Task1: Stagewise Fine-tuning <span class=acl-fixed-case>BERT</span> for Commonsense Inference in Everyday Narrations</a></strong><br><a href=/people/c/chunhua-liu/>Chunhua Liu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6012><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> for COIN Shared Task 1 : Commonsense Inference in Everyday Narrations. To inject more external knowledge to better reason over the narrative passage, question and answer, the <a href=https://en.wikipedia.org/wiki/System>system</a> adopts a stagewise fine-tuning method based on pre-trained BERT model. More specifically, the first stage is to fine-tune on addi- tional machine reading comprehension dataset to learn more <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. The second stage is to fine-tune on target-task (MCScript2.0) with MCScript (2018) dataset assisted. Experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves significant improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> with 84.2 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the official test dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6014/>Diversity-aware Event Prediction based on a Conditional Variational Autoencoder with Reconstruction</a></strong><br><a href=/people/h/hirokazu-kiyomaru/>Hirokazu Kiyomaru</a>
|
<a href=/people/k/kazumasa-omura/>Kazumasa Omura</a>
|
<a href=/people/y/yugo-murawaki/>Yugo Murawaki</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6014><div class="card-body p-3 small">Typical event sequences are an important class of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. Formalizing the task as the generation of a next event conditioned on a current event, previous work in event prediction employs sequence-to-sequence (seq2seq) models. However, what can happen after a given event is usually diverse, a fact that can hardly be captured by deterministic models. In this paper, we propose to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its ability to represent diverse next events as a probabilistic distribution. We further extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversity-aware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> and that the reconstruction mechanism improves the <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>recall</a> of CVAE-based models without sacrificing <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6015 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6015/>Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text</a></strong><br><a href=/people/i/ian-porada/>Ian Porada</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6015><div class="card-body p-3 small">Modeling semantic plausibility requires <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> about the world and has been used as a testbed for exploring various <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representations</a>. Previous work has focused specifically on modeling physical plausibility and shown that <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional methods</a> fail when tested in a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised setting</a>. At the same time, distributional models, namely large pretrained language models, have led to improved results for many natural language understanding tasks. In this work, we show that these pretrained language models are in fact effective at modeling physical plausibility in the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised setting</a>. We therefore present the more difficult problem of learning to model physical plausibility directly from text. We create a training set by extracting <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>attested events</a> from a large corpus, and we provide a baseline for training on these <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>attested events</a> in a self-supervised manner and testing on a physical plausibility task. We believe results could be further improved by injecting explicit <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> into a <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional model</a>.</div></div></div><hr><div id=d19-61><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-61.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-61/>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6100/>Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)</a></strong><br><a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/g/gholamreza-haffari/>Reza Haffari</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6102 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6102/>A Comparative Analysis of Unsupervised Language Adaptation Methods</a></strong><br><a href=/people/g/gil-rocha/>Gil Rocha</a>
|
<a href=/people/h/henrique-lopes-cardoso/>Henrique Lopes Cardoso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6102><div class="card-body p-3 small">To overcome the lack of annotated resources in less-resourced languages, recent approaches have been proposed to perform unsupervised language adaptation. In this paper, we explore three recent proposals : Adversarial Training, Sentence Encoder Alignment and Shared-Private Architecture. We highlight the differences of these approaches in terms of unlabeled data requirements and capability to overcome additional domain shift in the data. A comparative analysis in two different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> is conducted, namely on Sentiment Classification and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a>. We show that adversarial training methods are more suitable when the source and target language datasets contain other variations in content besides the <a href=https://en.wikipedia.org/wiki/Language_shift>language shift</a>. Otherwise, sentence encoder alignment methods are very effective and can yield scores on the target language that are close to the source language scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6103 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6103/>A logical-based corpus for cross-lingual evaluation</a></strong><br><a href=/people/f/felipe-salvatore/>Felipe Salvatore</a>
|
<a href=/people/m/marcelo-finger/>Marcelo Finger</a>
|
<a href=/people/r/roberto-hirata-jr/>Roberto Hirata Jr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6103><div class="card-body p-3 small">At present, different deep learning models are presenting high accuracy on popular inference datasets such as SNLI, MNLI, and SciTail. However, there are different indicators that those <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> can be exploited by using some simple linguistic patterns. This fact poses difficulties to our understanding of the actual capacity of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> to solve the complex task of textual inference. We propose a new set of syntactic tasks focused on contradiction detection that require specific capacities over linguistic logical forms such as : Boolean coordination, <a href=https://en.wikipedia.org/wiki/Quantifier_(logic)>quantifiers</a>, <a href=https://en.wikipedia.org/wiki/Definite_description>definite description</a>, and counting operators. We evaluate two kinds of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> that implicitly exploit <a href=https://en.wikipedia.org/wiki/Language_structure>language structure</a> : <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent models</a> and the Transformer network BERT. We show that although BERT is clearly more efficient to generalize over most logical forms, there is space for improvement when dealing with counting operators. Since the syntactic tasks can be implemented in different languages, we show a successful case of cross-lingual transfer learning between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6105 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6105/>Bag-of-Words Transfer : Non-Contextual Techniques for Multi-Task Learning</a></strong><br><a href=/people/s/seth-ebner/>Seth Ebner</a>
|
<a href=/people/f/felicity-wang/>Felicity Wang</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6105><div class="card-body p-3 small">Many architectures for multi-task learning (MTL) have been proposed to take advantage of transfer among tasks, often involving complex models and training procedures. In this paper, we ask if the sentence-level representations learned in previous approaches provide significant benefit beyond that provided by simply improving word-based representations. To investigate this question, we consider three techniques that ignore sequence information : a syntactically-oblivious pooling encoder, pre-trained non-contextual word embeddings, and unigram generative regularization. Compared to a state-of-the-art MTL approach to textual inference, the simple techniques we use yield similar performance on a universe of task combinations while reducing training time and model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6108 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6108/>Deep Bidirectional Transformers for Relation Extraction without Supervision</a></strong><br><a href=/people/y/yannis-papanikolaou/>Yannis Papanikolaou</a>
|
<a href=/people/i/ian-roberts/>Ian Roberts</a>
|
<a href=/people/a/andrea-pierleoni/>Andrea Pierleoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6108><div class="card-body p-3 small">We present a novel framework to deal with relation extraction tasks in cases where there is complete lack of supervision, either in the form of gold annotations, or relations from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Our approach leverages syntactic parsing and pre-trained word embeddings to extract few but precise relations, which are then used to annotate a larger corpus, in a manner identical to distant supervision. The resulting <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> is employed to fine tune a pre-trained BERT model in order to perform <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Empirical evaluation on four <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> from the biomedical domain shows that our method significantly outperforms two simple baselines for unsupervised relation extraction and, even if not using any supervision at all, achieves slightly worse results than the state-of-the-art in three out of four data sets. Importantly, we show that it is possible to successfully fine tune a large pretrained language model with noisy data, as opposed to previous works that rely on gold data for fine tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6111 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6111/>Fast Domain Adaptation of Semantic Parsers via Paraphrase Attention</a></strong><br><a href=/people/a/avik-ray/>Avik Ray</a>
|
<a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6111><div class="card-body p-3 small">Semantic parsers are used to convert user&#8217;s natural language commands to executable logical form in intelligent personal agents. Labeled datasets required to train such <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> are expensive to collect, and are never comprehensive. As a result, for effective post-deployment domain adaptation and personalization, semantic parsers are continuously retrained to learn new user vocabulary and paraphrase variety. However, state-of-the art attention based neural parsers are slow to retrain which inhibits real time domain adaptation. Secondly, these <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> do not leverage numerous <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> already present in the training dataset. Designing <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> which can simultaneously maintain high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and fast retraining time is challenging. In this paper, we present novel paraphrase attention based sequence-to-sequence / tree parsers which support fast near real time retraining. In addition, our <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> often boost <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by jointly modeling the semantic dependencies of paraphrases. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on benchmark datasets to demonstrate upto 9X speedup in retraining time compared to existing <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, as well as achieving state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6112 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6112/>Few-Shot and Zero-Shot Learning for Historical Text Normalization</a></strong><br><a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/n/natalia-korchagina/>Natalia Korchagina</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6112><div class="card-body p-3 small">Historical text normalization often relies on <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>small training datasets</a>. Recent work has shown that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> can lead to significant improvements by exploiting synergies with related datasets, but there has been no systematic study of different <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning architectures</a>. This paper evaluates 63 multi-task learning configurations for sequence-to-sequence-based historical text normalization across ten datasets from eight languages, using <a href=https://en.wikipedia.org/wiki/Autoencoding>autoencoding</a>, grapheme-to-phoneme mapping, and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> as auxiliary tasks. We observe consistent, significant improvements across languages when training data for the target task is limited, but minimal or no improvements when training data is abundant. We also show that zero-shot learning outperforms the simple, but relatively strong, identity baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6115 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-6115.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6115/>Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/s/sheng-zha/>Sheng Zha</a>
|
<a href=/people/h/haohan-wang/>Haohan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6115><div class="card-body p-3 small">Statistical natural language inference (NLI) models are susceptible to learning dataset bias : superficial cues that happen to associate with the label on a particular dataset, but are not useful in general, e.g., <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation words</a> indicate contradiction. As exposed by several recent challenge datasets, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> perform poorly when such association is absent, e.g., predicting that I love dogs. contradicts I do n&#8217;t love cats.. Our goal is to design <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithms</a> that guard against known dataset bias. We formalize the concept of dataset bias under the framework of distribution shift and present a simple debiasing algorithm based on residual fitting, which we call DRiFt. We first learn a biased model that only uses <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are known to relate to dataset bias. Then, we train a debiased model that fits to the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual</a> of the <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>biased model</a>, focusing on examples that can not be predicted well by biased features only. We use DRiFt to train three high-performing NLI models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve significant gains over baseline models on two challenge test sets, while maintaining reasonable performance on the original test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6116 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6116/>Metric Learning for Dynamic Text Classification</a></strong><br><a href=/people/j/jeremy-wohlwend/>Jeremy Wohlwend</a>
|
<a href=/people/e/ethan-r-elenberg/>Ethan R. Elenberg</a>
|
<a href=/people/s/sam-altschul/>Sam Altschul</a>
|
<a href=/people/s/shawn-henry/>Shawn Henry</a>
|
<a href=/people/t/tao-lei/>Tao Lei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6116><div class="card-body p-3 small">Traditional text classifiers are limited to predicting over a fixed set of labels. However, in many real-world applications the label set is frequently changing. For example, in intent classification, new intents may be added over time while others are removed. We propose to address the problem of dynamic text classification by replacing the traditional, fixed-size output layer with a learned, semantically meaningful <a href=https://en.wikipedia.org/wiki/Metric_space>metric space</a>. Here the distances between textual inputs are optimized to perform nearest-neighbor classification across overlapping label sets. Changing the label set does not involve removing parameters, but rather simply adding or removing support points in the <a href=https://en.wikipedia.org/wiki/Metric_space>metric space</a>. Then the learned <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> can be fine-tuned with only a few additional training examples. We demonstrate that this simple <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> is robust to changes in the label space. Furthermore, our results show that learning a non-Euclidean metric can improve performance in the low data regime, suggesting that further work on <a href=https://en.wikipedia.org/wiki/Metric_space>metric spaces</a> may benefit low-resource research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6118 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6118/>Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning : A Faroese Case Study<span class=acl-fixed-case>F</span>aroese Case Study</a></strong><br><a href=/people/j/james-barry/>James Barry</a>
|
<a href=/people/j/joachim-wagner/>Joachim Wagner</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6118><div class="card-body p-3 small">Cross-lingual dependency parsing involves transferring <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntactic knowledge</a> from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using <a href=https://en.wikipedia.org/wiki/Faroese_language>Faroese</a> as the target language, we compare two approaches using annotation projection : first, projecting from multiple monolingual source models ; second, projecting from a single polyglot model which is trained on the combination of all source languages. Furthermore, we reproduce multi-source projection (Tyers et al., 2018), in which dependency trees of multiple sources are combined. Finally, we apply multi-treebank modelling to the projected treebanks, in addition to or alternatively to polyglot modelling on the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6119 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6119/>Inject Rubrics into Short Answer Grading System</a></strong><br><a href=/people/t/tianqi-wang/>Tianqi Wang</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/t/tomoya-mizumoto/>Tomoya Mizumoto</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6119><div class="card-body p-3 small">Short Answer Grading (SAG) is a task of scoring students&#8217; answers in examinations. Most existing SAG systems predict scores based only on the answers, including the model used as base line in this paper, which gives the-state-of-the-art performance. But they ignore important evaluation criteria such as <a href=https://en.wikipedia.org/wiki/Rubric_(academic)>rubrics</a>, which play a crucial role for evaluating answers in real-world situations. In this paper, we present a method to inject information from rubrics into SAG systems. We implement our approach on top of word-level attention mechanism to introduce the rubric information, in order to locate information in each answer that are highly related to the score. Our experimental results demonstrate that injecting rubric information effectively contributes to the performance improvement and that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art SAG model on the widely used ASAP-SAS dataset under low-resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6124 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6124/>Reevaluating Argument Component Extraction in Low Resource Settings</a></strong><br><a href=/people/a/anirudh-joshi/>Anirudh Joshi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/r/richard-sinnott/>Richard Sinnott</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6124><div class="card-body p-3 small">Argument component extraction is a challenging and complex high-level semantic extraction task. As such, it is both expensive to annotate (meaning training data is limited and low-resource by nature), and hard for current-generation deep learning methods to model. In this paper, we reevaluate the performance of state-of-the-art approaches in both single- and multi-task learning settings using combinations of character-level, GloVe, ELMo, and BERT encodings using standard BiLSTM-CRF encoders. We use evaluation metrics that are more consistent with evaluation practice in <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> to understand how well current baselines address this challenge and compare their performance to lower-level semantic tasks such as CoNLL named entity recognition. We find that performance utilizing various pre-trained representations and training methodologies often leaves a lot to be desired as it currently stands, and suggest future pathways for improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6128 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6128/>Transductive Auxiliary Task Self-Training for Neural Multi-Task Models</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6128><div class="card-body p-3 small">Multi-task learning and self-training are two common ways to improve a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a>&#8217;s performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training : training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56 % over the pure multi-task model for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>dependency relation tagging</a> and by up to 13.03 % for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>semantic tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6129 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6129/>Weakly Supervised Attentional Model for Low Resource Ad-hoc Cross-lingual Information Retrieval</a></strong><br><a href=/people/l/lingjun-zhao/>Lingjun Zhao</a>
|
<a href=/people/r/rabih-zbib/>Rabih Zbib</a>
|
<a href=/people/z/zhuolin-jiang/>Zhuolin Jiang</a>
|
<a href=/people/d/damianos-karakos/>Damianos Karakos</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6129><div class="card-body p-3 small">We propose a weakly supervised neural model for Ad-hoc Cross-lingual Information Retrieval (CLIR) from low-resource languages. Low resource languages often lack relevance annotations for CLIR, and when available the training data usually has limited coverage for possible queries. In this paper, we design a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> which does not require relevance annotations, instead it is trained on samples extracted from translation corpora as weak supervision. This model relies on an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to learn spans in the foreign sentence that are relevant to the query. We report experiments on two low resource languages : <a href=https://en.wikipedia.org/wiki/Swahili_language>Swahili</a> and <a href=https://en.wikipedia.org/wiki/Tagalog_language>Tagalog</a>, trained on less that 100k parallel sentences each. The proposed model achieves 19 MAP points improvement compared to using CNNs for <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>, 12 points improvement from machine translation-based CLIR, and up to 6 points improvement compared to probabilistic CLIR models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6130 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6130" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6130/>X-WikiRE : A Large, Multilingual Resource for Relation Extraction as Machine Comprehension<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>RE</span>: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension</a></strong><br><a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/c/cezar-sas/>Cezar Sas</a>
|
<a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6130><div class="card-body p-3 small">Although the vast majority of knowledge bases (KBs) are heavily biased towards <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves (zero-shot) relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6132 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6132/>Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations</a></strong><br><a href=/people/k/ke-m-tran/>Ke Tran</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6132><div class="card-body p-3 small">We investigate whether off-the-shelf deep bidirectional sentence representations (Devlin et al., 2019) trained on a massively multilingual corpus (multilingual BERT) enable the development of an unsupervised universal dependency parser. This approach only leverages a mix of monolingual corpora in many languages and does not require any translation data making it applicable to low-resource languages. In our experiments we outperform the best CoNLL 2018 language-specific systems in all of the shared task&#8217;s six truly low-resource languages while using a single system. However, we also find that (i) parsing accuracy still varies dramatically when changing the training languages and (ii) in some target languages zero-shot transfer fails under all tested conditions, raising concerns on the &#8216;universality&#8217; of the whole approach.</div></div></div><hr><div id=d19-62><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-62.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-62/>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6200/>Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6203 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6203/>On the Effectiveness of the Pooling Methods for Biomedical Relation Extraction with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/t/tuan-ngo-nguyen/>Tuan Ngo Nguyen</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6203><div class="card-body p-3 small">Deep learning models have achieved state-of-the-art performances on many relation extraction datasets. A common element in these deep learning models involves the pooling mechanisms where a sequence of hidden vectors is aggregated to generate a single <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation vector</a>, serving as the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to perform prediction for RE. Unfortunately, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in the literature tend to employ different strategies to perform pooling for RE, leading to the challenge to determine the best pooling mechanism for this problem, especially in the <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical domain</a>. In order to answer this question, in this work, we conduct a comprehensive study to evaluate the effectiveness of different pooling mechanisms for the deep learning models in biomedical RE. The experimental results suggest that dependency-based pooling is the best pooling strategy for RE in the biomedical domain, yielding the state-of-the-art performance on two benchmark datasets for this problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6207 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6207/>Experiments with ad hoc ambiguous abbreviation expansion</a></strong><br><a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a>
|
<a href=/people/m/malgorzata-marciniak/>Malgorzata Marciniak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6207><div class="card-body p-3 small">The paper addresses experiments to expand ad hoc ambiguous abbreviations in medical notes on the basis of morphologically annotated texts, without using additional domain resources. We work on Polish data but the described approaches can be used for other languages too. We test two <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>methods</a> to select candidates for word abbreviation expansions. The first one automatically selects all words in text which might be an expansion of an abbreviation according to the <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>language rules</a>. The second method uses clustering of abbreviation occurrences to select representative elements which are manually annotated to determine lists of potential expansions. We then train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to assign expansions to <a href=https://en.wikipedia.org/wiki/Abbreviation>abbreviations</a> based on three <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training sets</a> : automatically obtained, consisting of manual annotation, and concatenation of the two previous ones. The results obtained for the manually annotated training data significantly outperform automatically obtained training data. Adding the automatically obtained training data to the manually annotated data improves the results, in particular for less frequent abbreviations. In this context the proposed a priori data driven selection of possible extensions turned out to be crucial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6209 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6209/>Extracting relevant information from physician-patient dialogues for automated clinical note taking</a></strong><br><a href=/people/s/serena-jeblee/>Serena Jeblee</a>
|
<a href=/people/f/faiza-khan-khattak/>Faiza Khan Khattak</a>
|
<a href=/people/n/noah-crampton/>Noah Crampton</a>
|
<a href=/people/m/muhammad-mamdani/>Muhammad Mamdani</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6209><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for automatically extracting pertinent medical information from dialogues between clinicians and patients. The <a href=https://en.wikipedia.org/wiki/System>system</a> parses each dialogue and extracts <a href=https://en.wikipedia.org/wiki/Legal_person>entities</a> such as medications and symptoms, using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> to predict which entities are relevant. We also classify the primary diagnosis for each conversation. In addition, we extract <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topic information</a> and identify relevant utterances. This serves as a baseline for a system that extracts information from dialogues and automatically generates a patient note, which can be reviewed and edited by the clinician.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6212 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6212/>What does the language of foods say about us?</a></strong><br><a href=/people/h/hoang-van/>Hoang Van</a>
|
<a href=/people/a/ahmad-musa/>Ahmad Musa</a>
|
<a href=/people/h/hang-chen/>Hang Chen</a>
|
<a href=/people/s/stephen-kobourov/>Stephen Kobourov</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6212><div class="card-body p-3 small">In this work we investigate the signal contained in the language of food on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We experiment with a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 24 million food-related tweets, and make several observations. First, thelanguageoffoodhaspredictive power. We are able to predict if states in the United States (US) are above the medianratesfortype2diabetesmellitus(T2DM), <a href=https://en.wikipedia.org/wiki/Income>income</a>, <a href=https://en.wikipedia.org/wiki/Poverty>poverty</a>, and <a href=https://en.wikipedia.org/wiki/Education>education</a> outperforming previous work by 418 %. Second, we investigate the effect of <a href=https://en.wikipedia.org/wiki/Socioeconomics>socioeconomic factors</a> (income, <a href=https://en.wikipedia.org/wiki/Poverty>poverty</a>, and education) on predicting state-level T2DM rates. Socioeconomic factors do improve T2DM prediction, with the greatestimprovementcomingfrompovertyinformation(6%),but, importantly, thelanguage of food adds distinct information that is not captured by <a href=https://en.wikipedia.org/wiki/Socioeconomics>socioeconomics</a>. Third, we analyze how the language of food has changed over a five-year period (2013 2017), which is indicative of the shift in eating habits in the US during that period. We find several food trends, and that the language of food is used differently by different groups such as differentgenders. Last, weprovideanonlinevisualization tool for real-time queries and <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6213 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-6213.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-6213/>Dreaddit : A Reddit Dataset for <a href=https://en.wikipedia.org/wiki/Stress&#8211;strain_analysis>Stress Analysis</a> in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>D</span>readdit: A <span class=acl-fixed-case>R</span>eddit Dataset for Stress Analysis in Social Media</a></strong><br><a href=/people/e/elsbeth-turcan/>Elsbeth Turcan</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6213><div class="card-body p-3 small">Stress is a nigh-universal human experience, particularly in the <a href=https://en.wikipedia.org/wiki/Online_and_offline>online world</a>. While <a href=https://en.wikipedia.org/wiki/Stress_(biology)>stress</a> can be a motivator, too much <a href=https://en.wikipedia.org/wiki/Stress_(biology)>stress</a> is associated with many negative health outcomes, making its identification useful across a range of domains. However, existing computational research typically only studies <a href=https://en.wikipedia.org/wiki/Stress_(biology)>stress</a> in domains such as <a href=https://en.wikipedia.org/wiki/Speech>speech</a>, or in short genres such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. We present Dreaddit, a new text corpus of lengthy multi-domain social media data for the identification of stress. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of 190 K posts from five different categories of <a href=https://en.wikipedia.org/wiki/Reddit>Reddit communities</a> ; we additionally label 3.5 K total segments taken from 3 K posts using <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a>. We present preliminary supervised learning methods for identifying <a href=https://en.wikipedia.org/wiki/Stress_(biology)>stress</a>, both neural and traditional, and analyze the complexity and diversity of the data and characteristics of each category.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6214 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6214/>Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation</a></strong><br><a href=/people/a/alexander-te-wei-shieh/>Alexander Te-Wei Shieh</a>
|
<a href=/people/y/yung-sung-chuang/>Yung-Sung Chuang</a>
|
<a href=/people/s/shang-yu-su/>Shang-Yu Su</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6214><div class="card-body p-3 small">Randomized controlled trials (RCTs) represent the paramount evidence of <a href=https://en.wikipedia.org/wiki/Medicine>clinical medicine</a>. Using <a href=https://en.wikipedia.org/wiki/Machine>machines</a> to interpret the massive amount of <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>RCTs</a> has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for <a href=https://en.wikipedia.org/wiki/Logical_consequence>conclusion generation</a>. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and <a href=https://en.wikipedia.org/wiki/Correctness_(computer_science)>correctness</a> in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6217 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6217/>Dilated LSTM with attention for Classification of Suicide Notes<span class=acl-fixed-case>LSTM</span> with attention for Classification of Suicide Notes</a></strong><br><a href=/people/a/annika-m-schoene/>Annika M Schoene</a>
|
<a href=/people/g/george-lacey/>George Lacey</a>
|
<a href=/people/a/alexander-p-turner/>Alexander P Turner</a>
|
<a href=/people/n/nina-dethlefs/>Nina Dethlefs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6217><div class="card-body p-3 small">In this paper we present a dilated LSTM with attention mechanism for document-level classification of suicide notes, last statements and depressed notes. We achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 87.34 % compared to competitive baselines of 80.35 % (Logistic Model Tree) and 82.27 % (Bi-directional LSTM with Attention). Furthermore, we provide an analysis of both the grammatical and thematic content of <a href=https://en.wikipedia.org/wiki/Suicide_note>suicide notes</a>, <a href=https://en.wikipedia.org/wiki/Suicide_note>last statements</a> and <a href=https://en.wikipedia.org/wiki/Suicide_note>depressed notes</a>. We find that the use of <a href=https://en.wikipedia.org/wiki/Personal_pronoun>personal pronouns</a>, <a href=https://en.wikipedia.org/wiki/Cognition>cognitive processes</a> and references to loved ones are most important. Finally, we show through visualisations of <a href=https://en.wikipedia.org/wiki/Attention>attention weights</a> that the Dilated LSTM with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is able to identify the same distinguishing features across documents as the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6218 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6218/>Writing habits and telltale neighbors : analyzing clinical concept usage patterns with sublanguage embeddings</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6218><div class="card-body p-3 small">Natural language processing techniques are being applied to increasingly diverse types of <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health records</a>, and can benefit from in-depth understanding of the distinguishing characteristics of medical document types. We present a method for characterizing the usage patterns of clinical concepts among different document types, in order to capture semantic differences beyond the lexical level. By training concept embeddings on clinical documents of different types and measuring the differences in their nearest neighborhood structures, we are able to measure divergences in concept usage while correcting for <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in embedding learning. Experiments on the MIMIC-III corpus demonstrate that our approach captures clinically-relevant differences in concept usage and provides an intuitive way to explore semantic characteristics of clinical document collections.</div></div></div><hr><div id=d19-63><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-63.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-63/>Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6300/>Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019)</a></strong><br><a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/a/anja-belz/>Anja Belz</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6307 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6307/>Surface Realization Shared Task 2019 (MSR19): The Team 6 Approach<span class=acl-fixed-case>MSR</span>19): The Team 6 Approach</a></strong><br><a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6307><div class="card-body p-3 small">This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR&#8217;19) (Mille et al., 2019). Based on Ferreira et al. (2017) and on our 2018 submission Ferreira et al. (2018), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rule-based and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6311 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6311/>The DipInfoUniTo Realizer at SRST’19 : Learning to Rank and Deep Morphology Prediction for Multilingual Surface Realization<span class=acl-fixed-case>D</span>ip<span class=acl-fixed-case>I</span>nfo<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>T</span>o Realizer at <span class=acl-fixed-case>SRST</span>’19: Learning to Rank and Deep Morphology Prediction for Multilingual Surface Realization</a></strong><br><a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6311><div class="card-body p-3 small">We describe the <a href=https://en.wikipedia.org/wiki/System>system</a> presented at the SR&#8217;19 shared task by the DipInfoUnito team. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is based on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a>. In particular, we divide the SR task into two independent subtasks, namely word order prediction and morphology inflection prediction. Two <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with different architectures run on the same input structure, each producing a partial output which is recombined in the final step in order to produce the predicted surface form. This work is a direct successor of the <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> presented at SR&#8217;19.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6312 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6312/>LORIA / Lorraine University at Multilingual Surface Realisation 2019<span class=acl-fixed-case>LORIA</span> / Lorraine University at Multilingual Surface Realisation 2019</a></strong><br><a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6312><div class="card-body p-3 small">This paper presents the LORIA / Lorraine University submission at the Multilingual Surface Realisation shared task 2019 for the shallow track. We outline our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> and evaluate <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> on 11 languages covered by the shared task. We provide a separate evaluation of each component of our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>, concluding on some difficulties and suggesting directions for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6313 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6313/>Back-Translation as Strategy to Tackle the Lack of Corpus in Natural Language Generation from Semantic Representations</a></strong><br><a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/t/thiago-pardo/>Thiago Pardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6313><div class="card-body p-3 small">This paper presents an exploratory study that aims to evaluate the usefulness of back-translation in Natural Language Generation (NLG) from semantic representations for non-English languages. Specifically, Abstract Meaning Representation and Brazilian Portuguese (BP) are chosen as semantic representation and language, respectively. Two methods (focused on Statistical and Neural Machine Translation) are evaluated on two datasets (one automatically generated and another one human-generated) to compare the performance in a real context. Also, several cuts according to quality measures are performed to evaluate the importance (or not) of the <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> in NLG. Results show that there are still many improvements to be made but <a href=https://en.wikipedia.org/wiki/This_(song)>this</a> is a promising approach.</div></div></div><hr><div id=d19-64><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-64.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-64/>Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6400/>Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></strong><br><a href=/people/a/aditya-mogadala/>Aditya Mogadala</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6403 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6403/>Big Generalizations with Small Data : Exploring the Role of Training Samples in Learning Adjectives of Size</a></strong><br><a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6403><div class="card-body p-3 small">In this paper, we experiment with a recently proposed visual reasoning task dealing with quantities modeling the multimodal, contextually-dependent meaning of size adjectives (&#8216;big&#8217;, &#8216;small&#8217;) and explore the impact of varying the training data on the learning behavior of a state-of-art system. In previous work, <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> have been shown to fail in generalizing to unseen adjective-noun combinations. Here, we investigate whether, and to what extent, seeing some of these cases during training helps a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> understand the rule subtending the task, i.e., that being big implies being not small, and vice versa. We show that relatively few examples are enough to understand this relationship, and that developing a specific, mutually exclusive representation of size adjectives is beneficial to the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6405 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6405/>On the Role of Scene Graphs in Image Captioning</a></strong><br><a href=/people/d/dalin-wang/>Dalin Wang</a>
|
<a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6405><div class="card-body p-3 small">Scene graphs represent semantic information in images, which can help <a href=https://en.wikipedia.org/wiki/Image>image captioning system</a> to produce more descriptive outputs versus using only the <a href=https://en.wikipedia.org/wiki/Image>image</a> as context. Recent captioning approaches rely on ad-hoc approaches to obtain <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graphs</a> for <a href=https://en.wikipedia.org/wiki/Image>images</a>. However, those <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graphs</a> introduce <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a> and it is unclear the effect of <a href=https://en.wikipedia.org/wiki/Parsing>parser errors</a> on captioning accuracy. In this work, we investigate to what extent <a href=https://en.wikipedia.org/wiki/Scene_graph>scene graphs</a> can help image captioning. Our results show that a state-of-the-art scene graph parser can boost performance almost as much as the ground truth graphs, showing that the bottleneck currently resides more on the captioning models than on the performance of the scene graph parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6406 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6406/>Understanding the Effect of Textual Adversaries in Multimodal Machine Translation</a></strong><br><a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6406><div class="card-body p-3 small">It is assumed that multimodal machine translation systems are better than text-only systems at translating phrases that have a direct correspondence in the image. This assumption has been challenged in experiments demonstrating that state-of-the-art multimodal systems perform equally well in the presence of randomly selected images, but, more recently, it has been shown that masking entities from the source language sentence during training can help to overcome this problem. In this paper, we conduct experiments with both visual and textual adversaries in order to understand the role of incorrect textual inputs to such systems. Our results show that when the source language sentence contains mistakes, multimodal translation systems do not leverage the additional visual signal to produce the correct <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We also find that the degradation of <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance caused by textual adversaries is significantly higher than by visual adversaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6407 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-6407.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-6407/>Learning to request guidance in emergent language</a></strong><br><a href=/people/b/benjamin-kolb/>Benjamin Kolb</a>
|
<a href=/people/l/leon-lang/>Leon Lang</a>
|
<a href=/people/h/henning-bartsch/>Henning Bartsch</a>
|
<a href=/people/a/arwin-gansekoele/>Arwin Gansekoele</a>
|
<a href=/people/r/raymond-koopmanschap/>Raymond Koopmanschap</a>
|
<a href=/people/l/leonardo-romor/>Leonardo Romor</a>
|
<a href=/people/d/david-speck/>David Speck</a>
|
<a href=/people/m/mathijs-mul/>Mathijs Mul</a>
|
<a href=/people/e/elia-bruni/>Elia Bruni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6407><div class="card-body p-3 small">Previous research into agent communication has shown that a pre-trained guide can speed up the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a> of an imitation learning agent. The guide achieves this by providing the agent with <a href=https://en.wikipedia.org/wiki/Message_passing>discrete messages</a> in an emerged language about how to solve the task. We extend this one-directional communication by a one-bit communication channel from the learner back to the guide : It is able to ask the guide for help, and we limit the guidance by penalizing the learner for these requests. During <a href=https://en.wikipedia.org/wiki/Training>training</a>, the agent learns to control this <a href=https://en.wikipedia.org/wiki/Gate>gate</a> based on its current observation. We find that the amount of requested guidance decreases over time and <a href=https://en.wikipedia.org/wiki/Advice_(opinion)>guidance</a> is requested in situations of high uncertainty. We investigate the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>&#8217;s performance in cases of open and closed gates and discuss potential motives for the observed gating behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6409 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6409/>Seeded self-play for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a></a></strong><br><a href=/people/a/abhinav-gupta/>Abhinav Gupta</a>
|
<a href=/people/r/ryan-lowe/>Ryan Lowe</a>
|
<a href=/people/j/jakob-foerster/>Jakob Foerster</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6409><div class="card-body p-3 small">How can we teach <a href=https://en.wikipedia.org/wiki/Intelligent_agent>artificial agents</a> to use <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> flexibly to solve problems in real-world environments? We have an example of this in nature : human babies eventually learn to use <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> to solve problems, and they are taught with an adult human-in-the-loop. Unfortunately, current <a href=https://en.wikipedia.org/wiki/List_of_machine_learning_methods>machine learning methods</a> (e.g. from deep reinforcement learning) are too data inefficient to learn <a href=https://en.wikipedia.org/wiki/Language>language</a> in this way. An outstanding goal is finding an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> with a suitable &#8216;language learning prior&#8217; that allows it to learn human language, while minimizing the number of on-policy human interactions. In this paper, we propose to learn such a prior in simulation using an approach we call, Learning to Learn to Communicate (L2C). Specifically, in L2C we train a meta-learning agent in simulation to interact with populations of pre-trained agents, each with their own distinct communication protocol. Once the meta-learning agent is able to quickly adapt to each population of agents, it can be deployed in new <a href=https://en.wikipedia.org/wiki/Population>populations</a>, including populations speaking human language. Our key insight is that such populations can be obtained via self-play, after pre-training agents with imitation learning on a small amount of off-policy human language data. We call this latter technique Seeded Self-Play (S2P). Our preliminary experiments show that agents trained with L2C and S2P need fewer on-policy samples to learn a compositional language in a <a href=https://en.wikipedia.org/wiki/Lewis_signaling_game>Lewis signaling game</a>.</div></div></div><hr><div id=d19-65><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-65.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-65/>Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6500/>Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</a></strong><br><a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6501 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6501/>Analysing Coreference in Transformer Outputs</a></strong><br><a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6501><div class="card-body p-3 small">We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric information. We compare <a href=https://en.wikipedia.org/wiki/System>system</a> performance on two different <a href=https://en.wikipedia.org/wiki/Genre>genres</a> : <a href=https://en.wikipedia.org/wiki/News>news</a> and <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a>. To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations. We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words. The features of coreference chains in automatic translations are also compared to those of the source texts and <a href=https://en.wikipedia.org/wiki/Translation>human translations</a>. The analysis shows stronger potential translationese effects in machine translated outputs than in human translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6503 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-6503.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-6503" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-6503/>When and Why is Document-level Context Useful in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>?</a></strong><br><a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/d/duc-thanh-tran/>Duc Thanh Tran</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6503><div class="card-body p-3 small">Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However, recent advances in document-level NMT focus on sophisticated integration of the context, explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets, clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. We also show that a minimal encoding is sufficient for the <a href=https://en.wikipedia.org/wiki/Context-free_grammar>context modeling</a> and very long context is not helpful for <a href=https://en.wikipedia.org/wiki/Context-free_grammar>NMT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6504 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6504/>Data augmentation using <a href=https://en.wikipedia.org/wiki/Back_translation>back-translation</a> for context-aware neural machine translation</a></strong><br><a href=/people/a/amane-sugiyama/>Amane Sugiyama</a>
|
<a href=/people/n/naoki-yoshinaga/>Naoki Yoshinaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6504><div class="card-body p-3 small">A single sentence does not always convey information that is enough to translate it into other languages. Some target languages need to add or specialize words that are omitted or ambiguous in the source languages (e.g, <a href=https://en.wikipedia.org/wiki/Zero_(linguistics)>zero pronouns</a> in translating Japanese to English or <a href=https://en.wikipedia.org/wiki/Epicene_pronoun>epicene pronouns</a> in translating English to French). To translate such ambiguous sentences, we need contexts beyond a single sentence, and have so far explored context-aware neural machine translation (NMT). However, a large amount of parallel corpora is not easily available to train accurate context-aware NMT models. In this study, we first obtain large-scale pseudo parallel corpora by back-translating monolingual data, and then investigate its impact on the translation accuracy of context-aware NMT models. We evaluated context-aware NMT models trained with small parallel corpora and the large-scale pseudo parallel corpora on English-Japanese and English-French datasets to demonstrate the large impact of the <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for context-aware NMT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6506 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6506/>Analysing concatenation approaches to document-level NMT in two different domains<span class=acl-fixed-case>NMT</span> in two different domains</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6506><div class="card-body p-3 small">In this paper, we investigate how different aspects of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>discourse context</a> affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore, we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context, arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are indeed affected by the manipulation of the test data, providing a different view on document-level translation quality than absolute sentence-level scores.</div></div></div><hr><div id=d19-66><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-66.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-66/>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6600/>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6603 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6603/>Neural Multi-Task Learning for Stance Prediction</a></strong><br><a href=/people/w/wei-fang/>Wei Fang</a>
|
<a href=/people/m/moin-nadeem/>Moin Nadeem</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6603><div class="card-body p-3 small">We present a multi-task learning model that leverages large amount of textual information from existing datasets to improve stance prediction. In particular, we utilize multiple NLP tasks under both unsupervised and supervised settings for the target stance prediction task. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains state-of-the-art performance on a public benchmark dataset, Fake News Challenge, outperforming current approaches by a wide margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6604 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-6604.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-6604/>GEM : Generative Enhanced Model for adversarial attacks<span class=acl-fixed-case>GEM</span>: Generative Enhanced Model for adversarial attacks</a></strong><br><a href=/people/p/piotr-niewinski/>Piotr Niewinski</a>
|
<a href=/people/m/maria-pszona/>Maria Pszona</a>
|
<a href=/people/m/maria-janicka/>Maria Janicka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6604><div class="card-body p-3 small">We present our Generative Enhanced Model (GEM) that we used to create samples awarded the first prize on the FEVER 2.0 Breakers Task. GEM is the extended language model developed upon GPT-2 architecture. The addition of novel target vocabulary input to the already existing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context input</a> enabled controlled text generation. The training procedure resulted in creating a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that inherited the knowledge of pretrained GPT-2, and therefore was ready to generate natural-like English sentences in the task domain with some additional control. As a result, <a href=https://en.wikipedia.org/wiki/Graphics_Environment_Manager>GEM</a> generated malicious claims that mixed facts from various articles, so it became difficult to classify their truthfulness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6609 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6609/>Unsupervised Question Answering for Fact-Checking</a></strong><br><a href=/people/m/mayank-jobanputra/>Mayank Jobanputra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6609><div class="card-body p-3 small">Recent <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning (DL) models</a> have succeeded in achieving human-level accuracy on various natural language tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference (NLI)</a>, and <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>. These <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> not only require the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual knowledge</a> but also the <a href=https://en.wikipedia.org/wiki/Reason>reasoning abilities</a> to be solved efficiently. In this paper, we propose an unsupervised question-answering based approach for a similar task, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a>. We transform the FEVER dataset into a Cloze-task by masking named entities provided in the claims. To predict the answer token, we utilize pre-trained Bidirectional Encoder Representations from Transformers (BERT). The classifier computes label based on the correctly answered questions and a threshold. Currently, the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> is able to classify the claims as SUPPORTS and MANUAL_REVIEW. This approach achieves a label accuracy of 80.2 % on the development set and 80.25 % on the test set of the transformed dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6610 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6610/>Improving Evidence Detection by Leveraging Warrants</a></strong><br><a href=/people/k/keshav-singh/>Keshav Singh</a>
|
<a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/p/pride-kavumba/>Pride Kavumba</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6610><div class="card-body p-3 small">Recognizing the implicit link between a claim and a piece of evidence (i.e. warrant) is the key to improving the performance of evidence detection. In this work, we explore the effectiveness of automatically extracted warrants for evidence detection. Given a claim and candidate evidence, our proposed method extracts multiple warrants via similarity search from an existing, structured corpus of arguments. We then attentively aggregate the extracted warrants, considering the consistency between the given argument and the acquired <a href=https://en.wikipedia.org/wiki/Warrant_(law)>warrants</a>. Although a qualitative analysis on the warrants shows that the extraction method needs to be improved, our results indicate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can still improve the performance of evidence detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6612 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6612/>Extract and Aggregate : A Novel Domain-Independent Approach to Factual Data Verification</a></strong><br><a href=/people/a/anton-chernyavskiy/>Anton Chernyavskiy</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6612><div class="card-body p-3 small">Triggered by Internet development, a large amount of information is published in online sources. However, it is a well-known fact that publications are inundated with inaccurate data. That is why <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a> has become a significant topic in the last 5 years. It is widely accepted that factual data verification is a challenge even for the experts. This paper presents a domain-independent fact checking system. It can solve the fact verification problem entirely or at the individual stages. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> combines various advanced methods of <a href=https://en.wikipedia.org/wiki/Text_mining>text data analysis</a>, such as BERT and Infersent. The theoretical and empirical study of the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>system features</a> is carried out. Based on FEVER and Fact Checking Challenge test-collections, experimental results demonstrate that our model can achieve the score on a par with state-of-the-art models designed by the specificity of particular datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6615 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6615/>FEVER Breaker’s Run of Team NbAuzDrLqg<span class=acl-fixed-case>FEVER</span> Breaker’s Run of Team <span class=acl-fixed-case>N</span>b<span class=acl-fixed-case>A</span>uz<span class=acl-fixed-case>D</span>r<span class=acl-fixed-case>L</span>qg</a></strong><br><a href=/people/y/youngwoo-kim/>Youngwoo Kim</a>
|
<a href=/people/j/james-allan/>James Allan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6615><div class="card-body p-3 small">We describe our submission for the Breaker phase of the second Fact Extraction and VERification (FEVER) Shared Task. Our adversarial data can be explained by two perspectives. First, we aimed at testing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to retrieve evidence, when appropriate query terms could not be easily generated from the claim. Second, we test model&#8217;s ability to precisely understand the implications of the texts, which we expect to be rare in FEVER 1.0 dataset. Overall, we suggested six types of <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial attacks</a>. The evaluation on the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> showed that the <a href=https://en.wikipedia.org/wiki/System>systems</a> were only able get both the evidence and label correct in 20 % of the data. We also demonstrate our adversarial run analysis in the data development process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-6617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-6617 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-6617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-6617/>Team GPLSI. Approach for automated fact checking<span class=acl-fixed-case>GPLSI</span>. Approach for automated fact checking</a></strong><br><a href=/people/a/aimee-alonso-reina/>Aimée Alonso-Reina</a>
|
<a href=/people/r/robiert-sepulveda-torres/>Robiert Sepúlveda-Torres</a>
|
<a href=/people/e/estela-saquete/>Estela Saquete</a>
|
<a href=/people/m/manuel-palomar/>Manuel Palomar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-6617><div class="card-body p-3 small">Fever Shared 2.0 Task is a challenge meant for developing automated fact checking systems. Our approach for the Fever 2.0 is based on a previous proposal developed by Team Athene UKP TU Darmstadt. Our proposal modifies the sentence retrieval phase, using statement extraction and representation in the form of triplets (subject, object, action). Triplets are extracted from the claim and compare to <a href=https://en.wikipedia.org/wiki/Multiple_birth>triplets</a> extracted from Wikipedia articles using <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. Our results are satisfactory but there is room for improvement.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>