<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Annual Meeting of the Association for Computational Linguistics (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Annual Meeting of the Association for Computational Linguistics (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#p19-1>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">297&nbsp;papers</span></li><li><a class=align-middle href=#p19-2>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">34&nbsp;papers</span></li><li><a class=align-middle href=#p19-3>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#p19-4>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w19-32>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w19-33>Proceedings of the First International Workshop on Designing Meaning Representations</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w19-34>Proceedings of the Second Workshop on Storytelling</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w19-35>Proceedings of the Third Workshop on Abusive Language Online</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w19-36>Proceedings of the 2019 Workshop on Widening NLP</a>
<span class="badge badge-info align-middle ml-1">57&nbsp;papers</span></li><li><a class=align-middle href=#w19-37>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-38>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-39>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w19-40>Proceedings of the 13th Linguistic Annotation Workshop</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w19-41>Proceedings of the First Workshop on NLP for Conversational AI</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w19-42>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w19-43>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class=align-middle href=#w19-44>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a>
<span class="badge badge-info align-middle ml-1">28&nbsp;papers</span></li><li><a class=align-middle href=#w19-45>Proceedings of the 6th Workshop on Argument Mining</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w19-46>Proceedings of the Fourth Arabic Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#w19-47>Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#w19-48>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#w19-49>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w19-50>Proceedings of the 18th BioNLP Workshop and Shared Task</a>
<span class="badge badge-info align-middle ml-1">24&nbsp;papers</span></li><li><a class=align-middle href=#w19-51>Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w19-52>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w19-53>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a>
<span class="badge badge-info align-middle ml-1">37&nbsp;papers</span></li><li><a class=align-middle href=#w19-54>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a>
<span class="badge badge-info align-middle ml-1">25&nbsp;papers</span></li></ul></div></div><div id=p19-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P19-1/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1000/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</a></strong><br><a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/d/david-traum/>David Traum</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1001/>One Time of Interaction May Not Be Enough : Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues</a></strong><br><a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1001><div class="card-body p-3 small">Currently, researchers have paid great attention to retrieval-based dialogues in <a href=https://en.wikipedia.org/wiki/Open_domain>open-domain</a>. In particular, people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets. State-of-the-art methods require a response to interact with each utterance in a context from the beginning, but the interaction is performed in a shallow way. In this work, we let utterance-response interaction go deep by proposing an interaction-over-interaction network (IoI). The model performs <a href=https://en.wikipedia.org/wiki/Matching_(statistics)>matching</a> by stacking multiple interaction blocks in which <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual information</a> from one time of interaction initiates the interaction process again. Thus, matching information within an utterance-response pair is extracted from the <a href=https://en.wikipedia.org/wiki/Interaction>interaction</a> of the pair in an iterative fashion, and the information flows along the chain of the blocks via <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. Evaluation results on three benchmark data sets indicate that IoI can significantly outperform state-of-the-art methods in terms of various matching metrics. Through further analysis, we also unveil how the depth of interaction affects the performance of IoI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383950369 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1002/>Incremental Transformer with Deliberation Decoder for Document Grounded Conversations</a></strong><br><a href=/people/z/zekang-li/>Zekang Li</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/q/qian-li/>Qian Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1002><div class="card-body p-3 small">Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously, document knowledge plays a critical role in Document Grounded Conversations, while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper, we propose a novel Transformer-based architecture for multi-turn document grounded conversations. In particular, we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents. Motivated by the human cognitive process, we design a two-pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383951307 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1003/>Improving Multi-turn Dialogue Modelling with Utterance ReWriter<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>W</span>riter</a></strong><br><a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/r/rongzhi-zhang/>Rongzhi Zhang</a>
|
<a href=/people/f/fei-sun/>Fei Sun</a>
|
<a href=/people/p/pengwei-hu/>Pengwei Hu</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1003><div class="card-body p-3 small">Recent research has achieved impressive results in single-turn dialogue modelling. In the multi-turn setting, however, current <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> are still far from satisfactory. One major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. In this paper, we propose rewriting the human utterance as a <a href=https://en.wikipedia.org/wiki/Pre-process>pre-process</a> to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. To properly train the utterance rewriter, we collect a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network. We show the proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> achieves remarkably good performance on the utterance rewriting task. The trained utterance rewriter can be easily integrated into <a href=https://en.wikipedia.org/wiki/Chatbot>online chatbots</a> and brings general improvement over different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383952222 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1004/>Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study</a></strong><br><a href=/people/c/chinnadhurai-sankar/>Chinnadhurai Sankar</a>
|
<a href=/people/s/sandeep-subramanian/>Sandeep Subramanian</a>
|
<a href=/people/c/christopher-pal/>Chris Pal</a>
|
<a href=/people/s/sarath-chandar/>Sarath Chandar</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1004><div class="card-body p-3 small">Neural generative models have been become increasingly popular when building <a href=https://en.wikipedia.org/wiki/Intelligent_agent>conversational agents</a>. They offer flexibility, can be easily adapted to <a href=https://en.wikipedia.org/wiki/Domain_of_unknown_function>new domains</a>, and require minimal domain engineering. A common criticism of these <a href=https://en.wikipedia.org/wiki/System>systems</a> is that they seldom understand or use the available dialog history effectively. In this paper, we take an empirical approach to understanding how these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> use the available dialog history by studying the sensitivity of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to artificially introduced unnatural changes or perturbations to their context at test time. We experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shuffling words, etc. Also, by open-sourcing our <a href=https://en.wikipedia.org/wiki/Source_code>code</a>, we believe that it will serve as a useful diagnostic tool for evaluating <a href=https://en.wikipedia.org/wiki/Dialog_(software)>dialog systems</a> in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383953490 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1005/>Boosting Dialog Response Generation</a></strong><br><a href=/people/w/wenchao-du/>Wenchao Du</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1005><div class="card-body p-3 small">Neural models have become one of the most important approaches to dialog response generation. However, they still tend to generate the most common and generic responses in the corpus all the time. To address this problem, we designed an iterative training process and <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> based on <a href=https://en.wikipedia.org/wiki/Boosting_(machine_learning)>boosting</a>. We combined our method with different training and decoding paradigms as the base model, including mutual-information-based decoding and reward-augmented maximum likelihood learning. Empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models, backed by objective measurements and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383955994 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1007/>Semantic Parsing with Dual Learning</a></strong><br><a href=/people/r/ruisheng-cao/>Ruisheng Cao</a>
|
<a href=/people/s/su-zhu/>Su Zhu</a>
|
<a href=/people/c/chen-liu/>Chen Liu</a>
|
<a href=/people/j/jieyu-li/>Jieyu Li</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1007><div class="card-body p-3 small">Semantic parsing converts <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language queries</a> into structured logical forms. The lack of training data is still one of the most serious problems in this area. In this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> to make full use of data (labeled and even unlabeled) through a dual-learning game. This game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. By utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on OVERNIGHT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383957151 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1009/>AMR Parsing as Sequence-to-Graph Transduction<span class=acl-fixed-case>AMR</span> Parsing as Sequence-to-Graph Transduction</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/x/xutai-ma/>Xutai Ma</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1009><div class="card-body p-3 small">We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3 % on LDC2017T10) and AMR 1.0 (70.2 % on LDC2014T12).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383957851 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1010/>Generating Logical Forms from Graph Representations of Text and Entities</a></strong><br><a href=/people/p/peter-shaw/>Peter Shaw</a>
|
<a href=/people/p/philip-massey/>Philip Massey</a>
|
<a href=/people/a/angelica-chen/>Angelica Chen</a>
|
<a href=/people/f/francesco-piccinno/>Francesco Piccinno</a>
|
<a href=/people/y/yasemin-altun/>Yasemin Altun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1010><div class="card-body p-3 small">Structured information about entities is critical for many semantic parsing tasks. We present an approach that uses a Graph Neural Network (GNN) architecture to incorporate information about relevant entities and their relations during <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Combined with a decoder copy mechanism, this approach provides a conceptually simple mechanism to generate <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a> with <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a>. We demonstrate that this approach is competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> across several tasks without pre-training, and outperforms existing approaches when combined with BERT pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383958512 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1011/>Learning Compressed Sentence Representations for On-Device Text Processing</a></strong><br><a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/p/pengyu-cheng/>Pengyu Cheng</a>
|
<a href=/people/d/dhanasekar-sundararaman/>Dhanasekar Sundararaman</a>
|
<a href=/people/x/xinyuan-zhang/>Xinyuan Zhang</a>
|
<a href=/people/q/qian-yang/>Qian Yang</a>
|
<a href=/people/m/meng-tang/>Meng Tang</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1011><div class="card-body p-3 small">Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP problems. The learned representations are generally assumed to be continuous and real-valued, giving rise to a large <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> and slow retrieval speed, which hinders their applicability to low-resource (memory and computation) platforms, such as <a href=https://en.wikipedia.org/wiki/Mobile_device>mobile devices</a>. In this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form, while preserving their rich semantic information. The introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence embeddings are demonstrated to degrade performance by only about 2 % relative to their continuous counterparts, while reducing the storage requirement by over 98 %. Moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their <a href=https://en.wikipedia.org/wiki/Hamming_distance>Hamming distance</a>, which is more computational efficient compared with the <a href=https://en.wikipedia.org/wiki/Inner_product_space>inner product operation</a> between continuous embeddings. Detailed analysis and case study further validate the effectiveness of proposed methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383962400 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1012/>The (Non-)Utility of Structural Features in BiLSTM-based Dependency Parsers<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-based Dependency Parsers</a></strong><br><a href=/people/a/agnieszka-falenska/>Agnieszka Falenska</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1012><div class="card-body p-3 small">Classical non-neural dependency parsers put considerable effort on the design of feature functions. Especially, they benefit from information coming from structural features, such as features drawn from neighboring tokens in the dependency tree. In contrast, their BiLSTM-based successors achieve state-of-the-art performance without explicit information about the structural context. In this paper we aim to answer the question : How much structural context are the BiLSTM representations able to capture implicitly? We show that <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> drawn from partial subtrees become redundant when the BiLSTMs are used. We provide a deep insight into <a href=https://en.wikipedia.org/wiki/Information_flow>information flow</a> in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> make their decisions. Finally, with model ablations we demonstrate that the structural context is not only present in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, but it significantly influences their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383966926 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1019/>An Effective Approach to Unsupervised Machine Translation</a></strong><br><a href=/people/m/mikel-artetxe/>Mikel Artetxe</a>
|
<a href=/people/g/gorka-labaka/>Gorka Labaka</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1019><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1020.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383968561 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1020/>Effective Adversarial Regularization for Neural Machine Translation</a></strong><br><a href=/people/m/motoki-sato/>Motoki Sato</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/s/shun-kiyono/>Shun Kiyono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1020><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization technique</a> based on adversarial perturbation, which was initially developed in the field of <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a>, has been successfully applied to text classification tasks and has yielded attractive improvements. We aim to further leverage this promising <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> into more sophisticated and critical neural models in the natural language processing field, i.e., <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT) models</a>. However, it is not trivial to apply this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Thus, this paper investigates the effectiveness of several possible configurations of applying the adversarial perturbation and reveals that the adversarial regularization technique can significantly and consistently improve the performance of widely used NMT models, such as LSTM-based and Transformer-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383992004 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1024/>Attention Guided Graph Convolutional Networks for Relation Extraction</a></strong><br><a href=/people/z/zhijiang-guo/>Zhijiang Guo</a>
|
<a href=/people/y/yan-zhang/>Yan Zhang</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1024><div class="card-body p-3 small">Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383993749 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1026/>Relation Embedding with <a href=https://en.wikipedia.org/wiki/Dihedral_group>Dihedral Group</a> in Knowledge Graph</a></strong><br><a href=/people/c/canran-xu/>Canran Xu</a>
|
<a href=/people/r/ruijiang-li/>Ruijiang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1026><div class="card-body p-3 small">Link prediction is critical for the application of incomplete knowledge graph (KG) in the downstream tasks. As a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the <a href=https://en.wikipedia.org/wiki/Bilinear_form>bilinear form</a> defined therein is a well-behaved scoring function. Despite of their successful performances, existing <a href=https://en.wikipedia.org/wiki/Bilinear_form>bilinear forms</a> overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on <a href=https://en.wikipedia.org/wiki/Key_(music)>KG</a>. To fulfill this gap, we propose a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> called DihEdral, named after <a href=https://en.wikipedia.org/wiki/Dihedral_group>dihedral symmetry group</a>. This new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns knowledge graph embeddings that can capture <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation compositions</a> by nature. Furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the <a href=https://en.wikipedia.org/wiki/Solution_space>solution space</a> drastically. Our experiments show that DihEdral is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) Abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383997156 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1028" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1028/>Augmenting Neural Networks with <a href=https://en.wikipedia.org/wiki/First-order_logic>First-order Logic</a></a></strong><br><a href=/people/t/tao-li/>Tao Li</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1028><div class="card-body p-3 small">Today, the dominant paradigm for training <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> involves minimizing task loss on a large dataset. Using <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> to inform a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> systematically compiles logical statements into computation graphs that augment a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks : machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, especially in low-data regimes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/383997816 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1029" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1029/>Self-Regulated Interactive Sequence-to-Sequence Learning</a></strong><br><a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/s/stefan-riezler/>Stefan Riezler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1029><div class="card-body p-3 small">Not all types of supervision signals are created equal : Different types of <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> have different costs and effects on <a href=https://en.wikipedia.org/wiki/Learning>learning</a>. We show how self-regulation strategies that decide when to ask for which kind of <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> from a teacher (or from oneself) can be cast as a learning-to-learn problem leading to improved cost-aware sequence-to-sequence learning. In experiments on interactive neural machine translation, we find that the self-regulator discovers an -greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections, error markups, and self-supervision. Furthermore, we demonstrate its <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> under domain shift and identify it as a promising alternative to <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>.<tex-math>\\epsilon</tex-math>-greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections, error markups, and self-supervision. Furthermore, we demonstrate its robustness under domain shift and identify it as a promising alternative to active learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384000960 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1030/>You Only Need Attention to Traverse Trees</a></strong><br><a href=/people/m/mahtab-ahmed/>Mahtab Ahmed</a>
|
<a href=/people/m/muhammad-rifayat-samee/>Muhammad Rifayat Samee</a>
|
<a href=/people/r/robert-e-mercer/>Robert E. Mercer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1030><div class="card-body p-3 small">In recent NLP research, a topic of interest is universal sentence encoding, sentence representations that can be used in any <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised task</a>. At the word sequence level, fully attention-based models suffer from two problems : a quadratic increase in memory consumption with respect to the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> and an inability to capture and use <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. Recursive neural nets can extract very good <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> by traversing a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree structure</a>. To this end, we propose Tree Transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. Evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and LSTM-based models as well as tree-structured LSTMs. Ablation studies to find whether positional information is inherently encoded in the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> and which type of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is suitable for doing the recursive traversal are provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385244938 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1031" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1031/>Cross-Domain Generalization of Neural Constituency Parsers</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1031><div class="card-body p-3 small">Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsingbut to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting : training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384007585 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1032" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1032/>Adaptive Attention Span in Transformers</a></strong><br><a href=/people/s/sainbayar-sukhbaatar/>Sainbayar Sukhbaatar</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a>
|
<a href=/people/p/piotr-bojanowski/>Piotr Bojanowski</a>
|
<a href=/people/a/armand-joulin/>Armand Joulin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1032><div class="card-body p-3 small">We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their <a href=https://en.wikipedia.org/wiki/Memory_footprint>memory footprint</a> and <a href=https://en.wikipedia.org/wiki/Time_complexity>computational time</a>. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1033/>Neural News Recommendation with Long- and Short-term User Representations</a></strong><br><a href=/people/m/mingxiao-an/>Mingxiao An</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/k/kun-zhang/>Kun Zhang</a>
|
<a href=/people/z/zheng-liu/>Zheng Liu</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1033><div class="card-body p-3 small">Personalized news recommendation is important to help users find their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufficient. In this paper, we propose a neural news recommendation approach which can learn both long- and short-term user representations. The core of our approach is a news encoder and a <a href=https://en.wikipedia.org/wiki/User-generated_content>user encoder</a>. In the news encoder, we learn representations of news from their titles and topic categories, and use attention network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to combine long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both long- and short-term user representations as a unified user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1035.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1035.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1035" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1035/>Manipulating the Difficulty of C-Tests<span class=acl-fixed-case>C</span>-Tests</a></strong><br><a href=/people/j/ji-ung-lee/>Ji-Ung Lee</a>
|
<a href=/people/e/erik-schwan/>Erik Schwan</a>
|
<a href=/people/c/christian-m-meyer/>Christian M. Meyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1035><div class="card-body p-3 small">We propose two novel manipulation strategies for increasing and decreasing the difficulty of C-tests automatically. This is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing <a href=https://en.wikipedia.org/wiki/Language_assessment>language assessment tests</a>. To reach the desired difficulty level, we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. We evaluate our approach in corpus-based experiments and in a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> with 60 participants. We find that both <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> are able to generate C-tests with the desired <a href=https://en.wikipedia.org/wiki/Degree_of_difficulty>difficulty level</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1041/>Disentangled Representation Learning for Non-Parallel Text Style Transfer</a></strong><br><a href=/people/v/vineet-john/>Vineet John</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/h/hareesh-bahuleyan/>Hareesh Bahuleyan</a>
|
<a href=/people/o/olga-vechtomova/>Olga Vechtomova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1041><div class="card-body p-3 small">This paper tackles the problem of disentangling the latent representations of style and content in <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, <a href=https://en.wikipedia.org/wiki/Preservation_(library_and_archival_science)>content preservation</a>, and <a href=https://en.wikipedia.org/wiki/Fluency>language fluency</a>, in comparison to various previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1043.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1043" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1043/>This Email Could Save Your Life : Introducing the Task of Email Subject Line Generation</a></strong><br><a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1043><div class="card-body p-3 small">Given the overwhelming number of emails, an effective subject line becomes essential to better inform the recipient of the email&#8217;s content. In this paper, we propose and study the task of email subject line generation : automatically generating an email subject line from the <a href=https://en.wikipedia.org/wiki/Email>email body</a>. We create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation.<i>email subject line generation</i>: automatically generating an email subject line from the email body. We create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1044/>Time-Out : Temporal Referencing for Robust Modeling of Lexical Semantic Change</a></strong><br><a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/s/simon-hengchen/>Simon Hengchen</a>
|
<a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1044><div class="card-body p-3 small">State-of-the-art models of lexical semantic change detection suffer from <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> stemming from vector space alignment. We have empirically tested the Temporal Referencing method for lexical semantic change and show that, by avoiding alignment, it is less affected by this <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>. We show that, trained on a diachronic corpus, the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset. We introduce a principled way to simulate lexical semantic change and systematically control for possible biases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1046/>Divide, Conquer and Combine : Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing</a></strong><br><a href=/people/s/sijie-mai/>Sijie Mai</a>
|
<a href=/people/h/haifeng-hu/>Haifeng Hu</a>
|
<a href=/people/s/songlong-xing/>Songlong Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1046><div class="card-body p-3 small">We propose a general <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> named &#8216;divide, conquer and combine&#8217; for multimodal fusion. Instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. In the &#8216;divide&#8217; and &#8216;conquer&#8217; stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are explored sufficiently. On its basis, global fusion is conducted in the &#8216;combine&#8217; stage to explore the interconnection across local interactions, via an Attentive Bi-directional Skip-connected LSTM that directly connects distant local interactions and integrates two levels of attention mechanism. In this way, local interactions can exchange information sufficiently and thus obtain an overall view of multimodal information. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves state-of-the-art performance on multimodal affective computing with higher efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1050" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1050/>MELD : A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations<span class=acl-fixed-case>MELD</span>: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</a></strong><br><a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/g/gautam-naik/>Gautam Naik</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1050><div class="card-body p-3 small">Emotion recognition in conversations is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http://affective-meld.github.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1051" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1051/>Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification</a></strong><br><a href=/people/m/minghao-hu/>Minghao Hu</a>
|
<a href=/people/y/yuxing-peng/>Yuxing Peng</a>
|
<a href=/people/z/zhen-huang/>Zhen Huang</a>
|
<a href=/people/d/dongsheng-li/>Dongsheng Li</a>
|
<a href=/people/y/yiwei-lv/>Yiwei Lv</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1051><div class="card-body p-3 small">Open-domain targeted sentiment analysis aims to detect <a href=https://en.wikipedia.org/wiki/Opinion_poll>opinion targets</a> along with their <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment polarities</a> from a sentence. Prior work typically formulates this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as a sequence tagging problem. However, such <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a> suffers from problems such as <a href=https://en.wikipedia.org/wiki/Big_data>huge search space</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment inconsistency</a>. To address these problems, we propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations. We further investigate three approaches under this framework, namely the pipeline, joint, and collapsed models. Experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline. Moreover, we find that the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline model</a> achieves the best performance compared with the other two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1053/>Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis</a></strong><br><a href=/people/j/jialong-tang/>Jialong Tang</a>
|
<a href=/people/z/ziyao-lu/>Ziyao Lu</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/y/yubin-ge/>Yubin Ge</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/j/jiebo-luo/>Jiebo Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1053><div class="card-body p-3 small">In aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a <a href=https://en.wikipedia.org/wiki/Mechanism_(sociology)>mechanism</a> tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. In this paper, we propose a progressive self-supervised attention learning approach for neural ASC models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. Specifically, we iteratively conduct <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment predictions</a> on all training instances. Particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active / misleading influence on the correct / incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. Finally, we augment the conventional training objective with a regularization term, which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two state-of-the-art neural ASC models. Source code and trained models are available at https://github.com/DeepLearnXMU/PSSAttention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1055.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1055" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1055/>Sentiment Tagging with Partial Labels using Modular Architectures</a></strong><br><a href=/people/x/xiao-zhang/>Xiao Zhang</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1055><div class="card-body p-3 small">Many NLP learning tasks can be decomposed into several distinct sub-tasks, each associated with a partial label. In this paper we focus on a popular class of learning problems, sequence prediction applied to several sentiment analysis tasks, and suggest a modular learning approach in which different sub-tasks are learned using separate functional modules, combined to perform the final task while sharing information. Our experiments show this approach helps constrain the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a> and can alleviate some of the supervision efforts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1057/>A Corpus for Modeling User and Language Effects in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>Argumentation</a> on Online Debating</a></strong><br><a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1057><div class="card-body p-3 small">Existing argumentation datasets have succeeded in allowing researchers to develop <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational methods</a> for analyzing the content, structure and linguistic features of argumentative text. They have been much less successful in fostering studies of the effect of user traits characteristics and beliefs of the participants on the debate / argument outcome as this type of user information is generally not available. This paper presents a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 78,376 <a href=https://en.wikipedia.org/wiki/Debate>debates</a> generated over a 10-year period along with surprisingly comprehensive participant profiles. We also complete an example study using the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to analyze the effect of selected user traits on the debate outcome in comparison to the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> typically employed in studies of this kind.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1061/>Data Programming for Learning Discourse Structure</a></strong><br><a href=/people/s/sonia-badene/>Sonia Badene</a>
|
<a href=/people/k/kate-thompson/>Kate Thompson</a>
|
<a href=/people/j/jean-pierre-lorre/>Jean-Pierre Lorré</a>
|
<a href=/people/n/nicholas-asher/>Nicholas Asher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1061><div class="card-body p-3 small">This paper investigates the advantages and limits of data programming for the task of learning discourse structure. The data programming paradigm implemented in the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the generative step into probability distributions of the class labels given the training candidates. These results are later generalized using a <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a>. Snorkel&#8217;s attractive promise to create a large amount of annotated data from a smaller set of training data by unifying the output of a set of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> has yet to be used for computationally difficult tasks, such as that of discourse attachment, in which one must decide where a given discourse unit attaches to other units in a text in order to form a coherent discourse structure. Although approaching this problem using <a href=https://en.wikipedia.org/wiki/Snorkeling>Snorkel</a> requires significant modifications to the structure of the <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>, we show that weak supervision methods can be more than competitive with classical supervised learning approaches to the attachment problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1062.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1062" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1062/>Evaluating Discourse in Structured Text Representations</a></strong><br><a href=/people/e/elisa-ferracane/>Elisa Ferracane</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1062><div class="card-body p-3 small">Discourse structure is integral to understanding a text and is helpful in many NLP tasks. Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data. Liu and Lapata (2018) propose a structured attention mechanism for text classification that derives a <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> over a text, akin to an RST discourse tree. We examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the structured attention improves performance on the end task and whether it captures a text&#8217;s discourse structure. We find the learned latent trees have little to no structure and instead focus on lexical cues ; even after obtaining more structured trees with proposed model modifications, the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1068" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1068/>MOROCO : The Moldavian and Romanian Dialectal Corpus<span class=acl-fixed-case>MOROCO</span>: The <span class=acl-fixed-case>M</span>oldavian and <span class=acl-fixed-case>R</span>omanian Dialectal Corpus</a></strong><br><a href=/people/a/andrei-butnaru/>Andrei Butnaru</a>
|
<a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1068><div class="card-body p-3 small">In this work, we introduce the MOldavian and ROmanian Dialectal COrpus (MOROCO), which is freely available for download at https://github.com/butnaruandrei/MOROCO. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains 33564 samples of text (with over 10 million tokens) collected from the <a href=https://en.wikipedia.org/wiki/News_media>news domain</a>. The samples belong to one of the following six topics : <a href=https://en.wikipedia.org/wiki/Culture>culture</a>, <a href=https://en.wikipedia.org/wiki/Finance>finance</a>, <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, <a href=https://en.wikipedia.org/wiki/Science>science</a>, sports and tech. The <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> is divided into 21719 samples for training, 5921 samples for validation and another 5924 samples for testing. For each sample, we provide corresponding dialectal and category labels. This allows us to perform empirical studies on several classification tasks such as (i) binary discrimination of Moldavian versus Romanian text samples, (ii) intra-dialect multi-class categorization by topic and (iii) cross-dialect multi-class categorization by topic. We perform experiments using a shallow approach based on <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a>, as well as a novel deep approach based on character-level convolutional neural networks containing Squeeze-and-Excitation blocks. We also present and analyze the most discriminative features of our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, before and after named entity removal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1069/>Just OneSeC for Producing Multilingual Sense-Annotated Data<span class=acl-fixed-case>O</span>ne<span class=acl-fixed-case>S</span>e<span class=acl-fixed-case>C</span>” for Producing Multilingual Sense-Annotated Data</a></strong><br><a href=/people/b/bianca-scarlini/>Bianca Scarlini</a>
|
<a href=/people/t/tommaso-pasini/>Tommaso Pasini</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1069><div class="card-body p-3 small">The well-known problem of <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a> is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automatically-generated data consistently lead a supervised WSD model to state-of-the-art performance when compared with other automatic and semi-automatic methods. Moreover, our approach outperforms its competitors on multilingual and domain-specific settings, where it beats the existing state of the art on all languages and most domains. All the training data are available for research purposes at http://trainomatic.org/onesec.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1070.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1070" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1070/>How to (Properly) Evaluate Cross-Lingual Word Embeddings : On Strong Baselines, Comparative Analyses, and Some Misconceptions</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/r/robert-litschko/>Robert Litschko</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1070><div class="card-body p-3 small">Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models : we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1076/>Automatic Evaluation of Local Topic Quality</a></strong><br><a href=/people/j/jeffrey-lund/>Jeffrey Lund</a>
|
<a href=/people/p/piper-armstrong/>Piper Armstrong</a>
|
<a href=/people/w/wilson-fearn/>Wilson Fearn</a>
|
<a href=/people/s/stephen-cowley/>Stephen Cowley</a>
|
<a href=/people/c/courtni-byun/>Courtni Byun</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/k/kevin-seppi/>Kevin Seppi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1076><div class="card-body p-3 small">Topic models are typically evaluated with respect to the global topic distributions that they generate, using <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> such as <a href=https://en.wikipedia.org/wiki/Coherence_(statistics)>coherence</a>, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Even recent <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, which aim to improve the quality of these token-level topic assignments, have been evaluated only with respect to <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>global metrics</a>. We propose a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> designed to elicit human judgments of token-level topic assignments. We use a variety of <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a> types and parameters and discover that global metrics agree poorly with human assignments. Since <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> is expensive we propose a variety of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automated metrics</a> to evaluate <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> at a local level. Finally, we correlate our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> from the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> on several datasets. We show that an <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> based on the percent of topic switches correlates most strongly with human judgment of local topic quality. We suggest that this new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, which we call <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a>, be adopted alongside global metrics such as topic coherence when evaluating new topic models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384011409 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1078" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1078/>Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems</a></strong><br><a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/e/ehsan-hosseini-asl/>Ehsan Hosseini-Asl</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1078><div class="card-body p-3 small">Over-dependence on <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a> and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short when tracking unknown slot values during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art 48.62 % joint goal accuracy for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58 % joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384538335 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1079/>Multi-Task Networks with Universe, Group, and Task Feature Learning</a></strong><br><a href=/people/s/shiva-pentyala/>Shiva Pentyala</a>
|
<a href=/people/m/mengwen-liu/>Mengwen Liu</a>
|
<a href=/people/m/markus-dreyer/>Markus Dreyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1079><div class="card-body p-3 small">We present methods for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>, such as task domain or language. Such task groups represent <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised information</a> at the inter-task level and can be encoded into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks : (1) parallel architectures encode each input simultaneously into feature spaces at different levels ; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on <a href=https://en.wikipedia.org/wiki/Automatic_terminal_information_service>ATIS</a>, Snips, and a large in-house dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1084.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384034160 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1084" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1084/>Do n’t Take the Premise for Granted : Mitigating Artifacts in Natural Language Inference</a></strong><br><a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/s/stuart-m-shieber/>Stuart Shieber</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1084><div class="card-body p-3 small">Natural Language Inference (NLI) datasets often contain hypothesis-only biasesartifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that are more robust to such <a href=https://en.wikipedia.org/wiki/Bias>biases</a> and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> containing biases and testing on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore <a href=https://en.wikipedia.org/wiki/Bias>biases</a> and fine-tuning on target datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384465038 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1088/>What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/x/xinyi-wu/>Xinyi Wu</a>
|
<a href=/people/k/kenneth-resnicow/>Kenneth Resnicow</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1088><div class="card-body p-3 small">The quality of a <a href=https://en.wikipedia.org/wiki/Intervention_(counseling)>counseling intervention</a> relies highly on the active collaboration between clients and counselors. In this paper, we explore several <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic aspects</a> of the <a href=https://en.wikipedia.org/wiki/Collaboration>collaboration process</a> occurring during <a href=https://en.wikipedia.org/wiki/List_of_counseling_topics>counseling conversations</a>. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants&#8217; turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are then used to build <a href=https://en.wikipedia.org/wiki/Statistical_classification>automatic classifiers</a> that can predict <a href=https://en.wikipedia.org/wiki/Counseling_psychology>counseling quality</a> with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracies</a> of up to 88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384467269 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1089/>Finding Your Voice : The Linguistic Development of Mental Health Counselors</a></strong><br><a href=/people/j/justine-zhang/>Justine Zhang</a>
|
<a href=/people/r/robert-filbin/>Robert Filbin</a>
|
<a href=/people/c/christine-morrison/>Christine Morrison</a>
|
<a href=/people/j/jaclyn-weiser/>Jaclyn Weiser</a>
|
<a href=/people/c/cristian-danescu-niculescu-mizil/>Cristian Danescu-Niculescu-Mizil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1089><div class="card-body p-3 small">Mental health counseling is an enterprise with profound societal importance where <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> play a primary role. In order to acquire the conversational skills needed to face a challenging range of situations, mental health counselors must rely on training and on continued experience with actual clients. However, in the absence of large scale longitudinal studies, the nature and significance of this developmental process remain unclear. For example, prior literature suggests that experience might not translate into consequential changes in counselor behavior. This has led some to even argue that <a href=https://en.wikipedia.org/wiki/List_of_counseling_topics>counseling</a> is a profession without expertise. In this work, we develop a computational framework to quantify the extent to which individuals change their linguistic behavior with experience and to study the nature of this evolution. We use our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to conduct a large longitudinal study of mental health counseling conversations, tracking over 3,400 counselors across their tenure. We reveal that overall, counselors do indeed change their conversational behavior to become more diverse across interactions, developing an individual voice that distinguishes them from other counselors. Furthermore, a finer-grained investigation shows that the rate and nature of this diversification vary across functionally different conversational components.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384469154 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1093/>Are You Convinced? Choosing the More Convincing Evidence with a Siamese Network<span class=acl-fixed-case>S</span>iamese Network</a></strong><br><a href=/people/m/martin-gleize/>Martin Gleize</a>
|
<a href=/people/e/eyal-shnarch/>Eyal Shnarch</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/l/lena-dankin/>Lena Dankin</a>
|
<a href=/people/g/guy-moshkowich/>Guy Moshkowich</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1093><div class="card-body p-3 small">With the advancement in argument detection, we suggest to pay more attention to the challenging task of identifying the more convincing arguments. Machines capable of responding and interacting with humans in helpful ways have become ubiquitous. We now expect them to discuss with us the more delicate questions in our world, and they should do so armed with effective arguments. But what makes an argument more persuasive? What will convince you? In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, IBM-EviConv, of pairs of evidence labeled for convincingness, designed to be more challenging than existing alternatives. We also propose a Siamese neural network architecture shown to outperform several baselines on both a prior convincingness data set and our own. Finally, we provide insights into our experimental results and the various kinds of argumentative value our method is capable of detecting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384469239 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1094/>From Surrogacy to Adoption ; From <a href=https://en.wikipedia.org/wiki/Bitcoin>Bitcoin</a> to <a href=https://en.wikipedia.org/wiki/Cryptocurrency>Cryptocurrency</a> : Debate Topic Expansion</a></strong><br><a href=/people/r/roy-bar-haim/>Roy Bar-Haim</a>
|
<a href=/people/d/dalia-krieger/>Dalia Krieger</a>
|
<a href=/people/o/orith-toledo-ronen/>Orith Toledo-Ronen</a>
|
<a href=/people/l/lilach-edelstein/>Lilach Edelstein</a>
|
<a href=/people/y/yonatan-bilu/>Yonatan Bilu</a>
|
<a href=/people/a/alon-halfon/>Alon Halfon</a>
|
<a href=/people/y/yoav-katz/>Yoav Katz</a>
|
<a href=/people/a/amir-menczel/>Amir Menczel</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1094><div class="card-body p-3 small">When debating a controversial topic, it is often desirable to expand the boundaries of discussion. For example, we may consider the pros and cons of possible alternatives to the debate topic, make generalizations, or give specific examples. We introduce the task of Debate Topic Expansion-finding such related topics for a given debate topic, along with a novel annotated dataset for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We focus on relations between Wikipedia concepts, and show that they differ from well-studied lexical-semantic relations such as <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernyms</a>, <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponyms</a> and <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a>. We present <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for finding both consistent and contrastive expansions and demonstrate their effectiveness empirically. We suggest that debate topic expansion may have various use cases in argumentation mining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1096/>Emotion-Cause Pair Extraction : A New Task to Emotion Analysis in Texts</a></strong><br><a href=/people/r/rui-xia/>Rui Xia</a>
|
<a href=/people/z/zixiang-ding/>Zixiang Ding</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1096><div class="card-body p-3 small">Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings : 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios ; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task : emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384475549 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1099/>Global Optimization under Length Constraint for Neural Text Summarization</a></strong><br><a href=/people/t/takuya-makino/>Takuya Makino</a>
|
<a href=/people/t/tomoya-iwakura/>Tomoya Iwakura</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1099><div class="card-body p-3 small">We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN / Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest <a href=https://en.wikipedia.org/wiki/Time_complexity>processing speed</a> ; only 6.70 % overlength summaries on CNN / Daily and 7.8 % on long summary of Mainichi, compared to the approximately 20 % to 50 % on CNN / Daily Mail and 10 % to 30 % on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a> with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30 % to 40 % improved post-editing time by use of in-length summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1102.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384478403 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1102/>Multi-News : A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model</a></strong><br><a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/i/irene-li/>Irene Li</a>
|
<a href=/people/t/tianwei-she/>Tianwei She</a>
|
<a href=/people/s/suyi-li/>Suyi Li</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1102><div class="card-body p-3 small">Automatic generation of summaries from multiple <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> is a valuable tool as the number of <a href=https://en.wikipedia.org/wiki/Online_newspaper>online publications</a> grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> has been limited to <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384478489 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1103/>Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency</a></strong><br><a href=/people/s/shuhuai-ren/>Shuhuai Ren</a>
|
<a href=/people/y/yihe-deng/>Yihe Deng</a>
|
<a href=/people/k/kun-he/>Kun He</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1103><div class="card-body p-3 small">We address the problem of adversarial attacks on <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, which is rarely studied comparing to attacks on <a href=https://en.wikipedia.org/wiki/Image_classification>image classification</a>. The challenge of this task is to generate adversarial examples that maintain <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical correctness</a>, <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical correctness</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithm</a> called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using <a href=https://en.wikipedia.org/wiki/Convolution>convolutional</a> as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. At last, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> also exhibits a good transferability on the generated adversarial examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384478577 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1104/>Heuristic Authorship Obfuscation</a></strong><br><a href=/people/j/janek-bevendorff/>Janek Bevendorff</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1104><div class="card-body p-3 small">Authorship verification is the task of determining whether two texts were written by the same author. We deal with the adversary task, called authorship obfuscation : preventing verification by altering a to-be-obfuscated text. Our new obfuscation approach (1) models writing style difference as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author&#8217;s subconsciously encoded writing style in a sophisticated manner using heuristic search. To obfuscate, we analyze the huge space of textual variants for a paraphrased version of the to-be-obfuscated text that has a sufficient Jensen-Shannon distance at minimal costs in terms of text quality. We analyze, quantify, and illustrate the rationale of this approach, define paraphrasing operators, derive obfuscation thresholds, and develop an effective obfuscation framework. Our authorship obfuscation approach defeats state-of-the-art verification approaches, including unmasking and compression models, while keeping text changes at a minimum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1108/>Figurative Usage Detection of Symptom Words to Improve Personal Health Mention Detection</a></strong><br><a href=/people/a/adith-iyer/>Adith Iyer</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/r/ross-sparks/>Ross Sparks</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1108><div class="card-body p-3 small">Personal health mention detection deals with predicting whether or not a given sentence is a report of a health condition. Past work mentions errors in this prediction when symptom words, i.e., names of symptoms of interest, are used in a <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative sense</a>. Therefore, we combine a state-of-the-art figurative usage detection with CNN-based personal health mention detection. To do so, we present two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> : a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline-based approach</a> and a feature augmentation-based approach. The introduction of figurative usage detection results in an average improvement of 2.21 % F-score of personal health mention detection, in the case of the feature augmentation-based approach. This paper demonstrates the promise of using figurative usage detection to improve personal health mention detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1110/>Neural News Recommendation with Topic-Aware News Representation</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/m/mingxiao-an/>Mingxiao An</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1110><div class="card-body p-3 small">News recommendation can help users find interested news and alleviate <a href=https://en.wikipedia.org/wiki/Information_overload>information overload</a>. The topic information of news is critical for learning accurate news and user representations for <a href=https://en.wikipedia.org/wiki/News_aggregator>news recommendation</a>. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is not considered in many existing news recommendation methods. In this paper, we propose a neural news recommendation approach with topic-aware news representations. The core of our approach is a topic-aware news encoder and a user encoder. In the news encoder we learn representations of news from their titles via CNN networks and apply attention networks to select important words. In addition, we propose to learn topic-aware news representations by jointly training the news encoder with an auxiliary topic classification task. In the user encoder we learn the representations of users from their browsed news and use attention networks to select informative news for user representation learning. Extensive experiments on a real-world dataset validate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1115/>Self-Attentional Models for <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>Lattice Inputs</a></a></strong><br><a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1115><div class="card-body p-3 small">Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle <a href=https://en.wikipedia.org/wiki/Lattice_(group)>lattices</a>, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattice structures</a>. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1117/>A Compact and Language-Sensitive Multilingual Translation Method</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/f/feifei-zhai/>Feifei Zhai</a>
|
<a href=/people/j/jingfang-xu/>Jingfang Xu</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1117><div class="card-body p-3 small">Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of <a href=https://en.wikipedia.org/wiki/Language_family>language commonality</a> and parameter sharing between encoder and decoder. Furthermore, this kind of <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> can not outperform the individual <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, and <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is especially helpful in low-resource and zero-shot translation scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1119/>Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation</a></strong><br><a href=/people/h/haipeng-sun/>Haipeng Sun</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1119><div class="card-body p-3 small">Unsupervised bilingual word embedding (UBWE), together with other technologies such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and <a href=https://en.wikipedia.org/wiki/Noise_reduction>denoising</a>, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and <a href=https://en.wikipedia.org/wiki/UNMT>UNMT</a>. The empirical findings show that the performance of <a href=https://en.wikipedia.org/wiki/Unmanned_combat_aerial_vehicle>UNMT</a> is significantly affected by the performance of <a href=https://en.wikipedia.org/wiki/Unmanned_combat_aerial_vehicle>UBWE</a>. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1120 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1120/>Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies</a></strong><br><a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1120><div class="card-body p-3 small">Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> do not require restructuring the vocabulary or retraining the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. We improve plain NMT transfer by up to +5.1 % BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1121/>Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations</a></strong><br><a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/y/yong-wang/>Yong Wang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/v/victor-o-k-li/>Victor O.K. Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1121><div class="card-body p-3 small">Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>system</a> in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches : (1) decoder pre-training ; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1122.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1122/>Syntactically Supervised Transformers for Faster Neural Machine Translation</a></strong><br><a href=/people/n/nader-akoury/>Nader Akoury</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1122><div class="card-body p-3 small">Standard decoders for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation autoregressively</a> generate a single target token per timestep, which slows <a href=https://en.wikipedia.org/wiki/Inference>inference</a> especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, <a href=https://en.wikipedia.org/wiki/Inference>inference</a> still proceeds sequentially. Recent developments in non- and semi-autoregressive decoding produce multiple tokens per timestep independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1123/>Dynamically Composing Domain-Data Selection with Clean-Data Selection by Co-Curricular Learning for Neural Machine Translation</a></strong><br><a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/i/isaac-caswell/>Isaac Caswell</a>
|
<a href=/people/c/ciprian-chelba/>Ciprian Chelba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1123><div class="card-body p-3 small">Noise and domain are important aspects of <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a co-curricular learning method to compose dynamic domain-data selection with dynamic clean-data selection, for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> across both capabilities. We apply an EM-style optimization procedure to further refine the co-curriculum. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1125 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1125/>Imitation Learning for Non-Autoregressive Neural Machine Translation</a></strong><br><a href=/people/b/bingzhen-wei/>Bingzhen Wei</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1125><div class="card-body p-3 small">Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>, while keeping the translation quality comparable to the <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>. By sampling <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> in parallel at <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a>, we achieve the performance of 31.85 BLEU on WMT16 RoEn and 30.68 BLEU on IWSLT16 EnDe.<tex-math>\\rightarrow</tex-math>En and 30.68 BLEU on IWSLT16 En<tex-math>\\rightarrow</tex-math>De.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1126.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1126/>Monotonic Infinite Lookback Attention for Simultaneous Machine Translation</a></strong><br><a href=/people/n/naveen-arivazhagan/>Naveen Arivazhagan</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/w/wolfgang-macherey/>Wolfgang Macherey</a>
|
<a href=/people/c/chung-cheng-chiu/>Chung-Cheng Chiu</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/r/ruoming-pang/>Ruoming Pang</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/c/colin-raffel/>Colin Raffel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1126><div class="card-body p-3 small">Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> against <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a>. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk&#8217;s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1130/>Exploiting Entity BIO Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data<span class=acl-fixed-case>BIO</span> Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data</a></strong><br><a href=/people/w/wei-ye/>Wei Ye</a>
|
<a href=/people/b/bo-li/>Bo Li</a>
|
<a href=/people/r/rui-xie/>Rui Xie</a>
|
<a href=/people/z/zhonghao-sheng/>Zhonghao Sheng</a>
|
<a href=/people/l/long-chen/>Long Chen</a>
|
<a href=/people/s/shikun-zhang/>Shikun Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1130><div class="card-body p-3 small">In practical scenario, <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> needs to first identify entity pairs that have relation and then assign a correct <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation class</a>. However, the number of non-relation entity pairs in context (negative instances) usually far exceeds the others (positive instances), which negatively affects a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance. To mitigate this problem, we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss. Meanwhile, we observe that a sentence may have multiple entities and relation mentions, and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances. Thus we further incorporate the embeddings of character-wise / word-wise BIO tag from the named entity recognition task into character / word embeddings to enrich the input representation. Experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10 % absolute increase in F1-score, and outperform the state-of-the-art models on ACE 2005 Chinese and English corpus. Moreover, BIO tag embeddings are particularly effective and can be used to improve other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1132" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1132/>Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers</a></strong><br><a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/m/ming-tan/>Ming Tan</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/d/dakuo-wang/>Dakuo Wang</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/x/xiaoxiao-guo/>Xiaoxiao Guo</a>
|
<a href=/people/s/saloni-potdar/>Saloni Potdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1132><div class="card-body p-3 small">Many approaches to extract multiple relations from a paragraph require multiple passes over the paragraph. In practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>. In this work, we focus on the task of multiple relation extractions by encoding the paragraph only once. We build our solution upon the pre-trained self-attentive models (Transformer), where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with entity-aware attention. We show that our approach is not only scalable but can also perform <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on the standard benchmark ACE 2005.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1134 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1134" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1134/>Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction</a></strong><br><a href=/people/c/christoph-alt/>Christoph Alt</a>
|
<a href=/people/m/marc-hubner/>Marc Hübner</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1134><div class="card-body p-3 small">Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to be biased towards recognizing a limited set of relations with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) (Radford et al., 2018). The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of common-sense knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> shows that it achieves a state-of-the-art <a href=https://en.wikipedia.org/wiki/Receiver_operating_characteristic>AUC score</a> of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1135 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1135/>ARNOR : Attention Regularization based Noise Reduction for Distant Supervision Relation Classification<span class=acl-fixed-case>ARNOR</span>: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification</a></strong><br><a href=/people/w/wei-jia/>Wei Jia</a>
|
<a href=/people/d/dai-dai/>Dai Dai</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1135><div class="card-body p-3 small">Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with an unlabeled corpus. However, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. In this paper, we propose ARNOR, a novel Attention Regularization based NOise Reduction framework for distant supervision relation classification. ARNOR assumes that a trustable relation label should be explained by the neural attention model. Specifically, our ARNOR framework iteratively learns an interpretable model and utilizes it to select trustable instances. We first introduce attention regularization to force the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to pay attention to the patterns which explain the relation labels, so as to make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> more interpretable. Then, if the learned model can clearly locate the relation patterns of a candidate instance in the training set, we will select it as a trustable instance for further training step. According to the experiments on NYT data, our ARNOR framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1136/>GraphRel : Modeling Text as Relational Graphs for Joint Entity and Relation Extraction<span class=acl-fixed-case>G</span>raph<span class=acl-fixed-case>R</span>el: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction</a></strong><br><a href=/people/t/tsu-jui-fu/>Tsu-Jui Fu</a>
|
<a href=/people/p/peng-hsuan-li/>Peng-Hsuan Li</a>
|
<a href=/people/w/wei-yun-ma/>Wei-Yun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1136><div class="card-body p-3 small">In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets : <a href=https://en.wikipedia.org/wiki/The_New_York_Times>NYT</a> and WebNLG. Results show that GraphRel maintains high <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> while increasing <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>recall</a> substantially. Also, GraphRel outperforms previous <a href=https://en.wikipedia.org/wiki/Work_(thermodynamics)>work</a> by 3.2 % and 5.8 % (F1 score), achieving a new state-of-the-art for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1137/>DIAG-NRE : A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction<span class=acl-fixed-case>DIAG</span>-<span class=acl-fixed-case>NRE</span>: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction</a></strong><br><a href=/people/s/shun-zheng/>Shun Zheng</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peilin-yu/>Peilin Yu</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/l/ling-huang/>Ling Huang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1137><div class="card-body p-3 small">Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1139/>ERNIE : Enhanced Language Representation with Informative Entities<span class=acl-fixed-case>ERNIE</span>: Enhanced Language Representation with Informative Entities</a></strong><br><a href=/people/z/zhengyan-zhang/>Zhengyan Zhang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1139><div class="card-body p-3 small">Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. We argue that informative entities in KGs can enhance <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>language representation</a> with <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a>. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1140/>Multi-Channel Graph Neural Network for Entity Alignment</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1140><div class="card-body p-3 small">Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5 % Hits@1 up on average). Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1141.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1141.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1141" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1141/>A Neural Multi-digraph Model for Chinese NER with Gazetteers<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NER</span> with Gazetteers</a></strong><br><a href=/people/r/ruixue-ding/>Ruixue Ding</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/x/xiaoyan-zhang/>Xiaoyan Zhang</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1141><div class="card-body p-3 small">Gazetteers were shown to be useful resources for named entity recognition (NER). Many existing approaches to incorporating <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteers</a> into machine learning based NER systems rely on manually defined selection strategies or handcrafted templates, which may not always lead to optimal effectiveness, especially when multiple gazetteers are involved. This is especially the case for the task of Chinese NER, where the words are not naturally tokenized, leading to additional <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguities</a>. To automatically learn how to incorporate multiple <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteers</a> into an NER system, we propose a novel approach based on graph neural networks with a multi-digraph structure that captures the information that the <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteers</a> offer. Experiments on various datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective in incorporating rich gazetteer information while resolving <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguities</a>, outperforming previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1153/>Towards Lossless Encoding of Sentences</a></strong><br><a href=/people/g/gabriele-prato/>Gabriele Prato</a>
|
<a href=/people/m/mathieu-duchesneau/>Mathieu Duchesneau</a>
|
<a href=/people/s/sarath-chandar/>Sarath Chandar</a>
|
<a href=/people/a/alain-tapp/>Alain Tapp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1153><div class="card-body p-3 small">A lot of work has been done in the field of <a href=https://en.wikipedia.org/wiki/Image_compression>image compression</a> via <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, but not much attention has been given to the compression of natural language. Compressing text into lossless representations while making <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> easily retrievable is not a trivial task, yet has huge benefits. Most methods designed to produce feature rich sentence embeddings focus solely on performing well on downstream tasks and are unable to properly reconstruct the original sequence from the learned embedding. In this work, we propose a near lossless method for encoding long sequences of texts as well as all of their sub-sequences into feature rich representations. We test our method on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and show good performance across all sub-sentence and sentence embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1157/>Historical Text Normalization with Delayed Rewards</a></strong><br><a href=/people/s/simon-flachs/>Simon Flachs</a>
|
<a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1157><div class="card-body p-3 small">Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrase-based models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>from-scratch reinforcement learning</a>, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalizations</a> for long or unseen words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384482232 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1160/>Gender-preserving Debiasing for Pre-trained Word Embeddings</a></strong><br><a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1160><div class="card-body p-3 small">Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information : feminine, masculine, gender-neutral and stereotypical, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.<i>feminine</i>, <i>masculine</i>, <i>gender-neutral</i> and <i>stereotypical</i>, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384489801 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1165/>LSTMEmbed : Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories<span class=acl-fixed-case>LSTME</span>mbed: Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories</a></strong><br><a href=/people/i/ignacio-iacobacci/>Ignacio Iacobacci</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1165><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are now a de facto standard representation of words in most NLP tasks, recently the attention has been shifting towards <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> which capture the different meanings, i.e., senses, of words. In this paper we explore the capabilities of a bidirectional LSTM model to learn representations of word senses from semantically annotated corpora. We show that the utilization of an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that is aware of <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>, like an LSTM, enables us to create better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. We assess our proposed model on various standard benchmarks for evaluating semantic representations, reaching state-of-the-art performance on the SemEval-2014 word-to-sense similarity task. We release the <a href=https://en.wikipedia.org/wiki/Source_code>code</a> and the resulting <a href=https://en.wikipedia.org/wiki/Word_embedding>word and sense embeddings</a> at http://lcl.uniroma1.it/LSTMEmbed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384490216 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1166/>Understanding Undesirable Word Embedding Associations</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a>
|
<a href=/people/d/david-duvenaud/>David Duvenaud</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1166><div class="card-body p-3 small">Word embeddings are often criticized for capturing undesirable word associations such as <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>gender stereotypes</a>. However, <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for measuring and removing such <a href=https://en.wikipedia.org/wiki/Bias>biases</a> remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common <a href=https://en.wikipedia.org/wiki/Association_test>association test</a> for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1167 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1167.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1167/>Unsupervised Discovery of Gendered Language through <a href=https://en.wikipedia.org/wiki/Latent-variable_model>Latent-Variable Modeling</a></a></strong><br><a href=/people/a/alexander-miserlis-hoyle/>Alexander Miserlis Hoyle</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1167><div class="card-body p-3 small">Studying the ways in which language is gendered has long been an area of interest in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a>. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes : Positive adjectives used to describe women are more often related to their bodies than <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> used to describe men.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384491244 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1169/>SphereRE : Distinguishing <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Relations</a> with Hyperspherical Relation Embeddings<span class=acl-fixed-case>S</span>phere<span class=acl-fixed-case>RE</span>: Distinguishing Lexical Relations with Hyperspherical Relation Embeddings</a></strong><br><a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/x/xiaofeng-he/>Xiaofeng He</a>
|
<a href=/people/a/aoying-zhou/>Aoying Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1169><div class="card-body p-3 small">Lexical relations describe how meanings of terms relate to each other. Typical examples include <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a>, <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a>, <a href=https://en.wikipedia.org/wiki/Meronymy>meronymy</a>, etc. Automatic distinction of lexical relations is vital for NLP applications, and also challenging due to the lack of contextual signals to discriminate between such <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. In this work, we present a neural representation learning model to distinguish lexical relations among term pairs based on Hyperspherical Relation Embeddings (SphereRE). Rather than learning embeddings for individual terms, the model learns representations of relation triples by mapping them to the hyperspherical embedding space, where relation triples of different lexical relations are well separated. Experiments over several benchmarks confirm SphereRE outperforms <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-arts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1170.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384494210 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1170/>Multilingual Factor Analysis</a></strong><br><a href=/people/f/francisco-vargas/>Francisco Vargas</a>
|
<a href=/people/k/kamen-brestnichki/>Kamen Brestnichki</a>
|
<a href=/people/a/alex-papadopoulos-korfiatis/>Alex Papadopoulos Korfiatis</a>
|
<a href=/people/n/nils-hammerla/>Nils Hammerla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1170><div class="card-body p-3 small">In this work we approach the task of learning multilingual word representations in an offline manner by fitting a <a href=https://en.wikipedia.org/wiki/Generative_model>generative latent variable model</a> to a <a href=https://en.wikipedia.org/wiki/Multilingual_dictionary>multilingual dictionary</a>. We model equivalent words in different languages as different views of the same word generated by a common <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> representing their latent lexical meaning. We explore the task of alignment by querying the fitted <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in the embedding space making it a suitable method for <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> learned from noisy corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384512599 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1173/>Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1173><div class="card-body p-3 small">Morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> to minimize model sparsity. Dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources. In this paper we explore the use of <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging. We use <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> for joint morphological modeling for the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> within two dialects, and as a <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge-transfer scheme</a> for cross-dialectal modeling. We use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants. We work with two dialectal variants : Modern Standard Arabic (high-resource dialect&#8217;) and Egyptian Arabic (low-resource dialect) as a case study. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve state-of-the-art results for both. Furthermore, adversarial training provides more significant improvement when using smaller training datasets in particular.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384527233 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1174/>Neural Machine Translation with Reordering Embeddings</a></strong><br><a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1174><div class="card-body p-3 small">The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The reordering mechanism can be easily integrated into both the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> in the Transformer translation system. Experimental results on WMT&#8217;14 English-to-German, NIST Chinese-to-English, and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384527378 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1175/>Neural Fuzzy Repair : Integrating Fuzzy Matches into Neural Machine Translation</a></strong><br><a href=/people/b/bram-bulte/>Bram Bulte</a>
|
<a href=/people/a/arda-tezcan/>Arda Tezcan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1175><div class="card-body p-3 small">We present a simple yet powerful data augmentation method for boosting Neural Machine Translation (NMT) performance by leveraging information retrieved from a Translation Memory (TM). We propose and test two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for augmenting <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>NMT training data</a> with fuzzy TM matches. Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems. The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a> across translations is to be expected, especially considering its ease of implementation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384515284 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1178/>Self-Supervised Neural Machine Translation</a></strong><br><a href=/people/d/dana-ruiter/>Dana Ruiter</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1178><div class="card-body p-3 small">We present a simple new method where an emergent NMT system is used for simultaneously selecting training data and learning internal NMT representations. This is done in a self-supervised way without parallel data, in such a way that both tasks enhance each other during training. The method is language independent, introduces no additional hyper-parameters, and achieves BLEU scores of 29.21 (en2fr) and 27.36 (fr2en) on newstest2014 using English and French Wikipedia data for training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384515395 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1179/>Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation</a></strong><br><a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1179><div class="card-body p-3 small">Previous work on end-to-end translation from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> has primarily used frame-level features as <a href=https://en.wikipedia.org/wiki/Speech>speech representations</a>, which creates longer, sparser sequences than <a href=https://en.wikipedia.org/wiki/Written_language>text</a>. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We see improvements of up to 5 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on both our high and low resource language pairs, with a reduction in training time of 60 %. Our improvements hold across multiple data sizes and two language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384518803 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1181/>Stay on the Path : Instruction Fidelity in Vision-and-Language Navigation</a></strong><br><a href=/people/v/vihan-jain/>Vihan Jain</a>
|
<a href=/people/g/gabriel-magalhaes/>Gabriel Magalhaes</a>
|
<a href=/people/a/alexander-ku/>Alexander Ku</a>
|
<a href=/people/a/ashish-vaswani/>Ashish Vaswani</a>
|
<a href=/people/e/eugene-ie/>Eugene Ie</a>
|
<a href=/people/j/jason-baldridge/>Jason Baldridge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1181><div class="card-body p-3 small">Advances in learning and representations have reinvigorated work that connects <a href=https://en.wikipedia.org/wiki/Language>language</a> to other modalities. A particularly exciting direction is Vision-and-Language Navigation(VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language under-standing plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for the Room-to-Room dataset (Anderson et al.,2018b) and propose a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, Coverage weighted by Length Score (CLS). We also show that the existing <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>paths</a> in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, Room-for-Room (R4R). Using R4R and CLS, we show that <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> that receive rewards for instruction fidelity outperform <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> that focus on goal completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384520109 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1182/>Expressing Visual Relationships via Language</a></strong><br><a href=/people/h/hao-tan/>Hao Tan</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/z/zhe-lin/>Zhe Lin</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1182><div class="card-body p-3 small">Describing images with text is a fundamental problem in vision-language research. Current studies in this domain mostly focus on single image captioning. However, in various real applications (e.g., <a href=https://en.wikipedia.org/wiki/Image_editing>image editing</a>, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. This important <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> has not been explored mostly due to lack of datasets and effective <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. To push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. We also extend the model with dynamic relational attention, which calculates visual alignment while decoding. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. Experimental results, based on both automatic and human evaluation, demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms all baselines and existing methods on all the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1189" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1189/>Reinforced Training Data Selection for Domain Adaptation</a></strong><br><a href=/people/m/miaofeng-liu/>Miaofeng Liu</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/h/hongbin-zou/>Hongbin Zou</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1189><div class="card-body p-3 small">Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>TDS</a> self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, dependency parsing, and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, as well as <a href=https://en.wikipedia.org/wiki/Ablation>ablation studies</a>, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1192/>Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation<span class=acl-fixed-case>M</span>odern <span class=acl-fixed-case>C</span>hinese Poetry Generation</a></strong><br><a href=/people/z/zhiqiang-liu/>Zhiqiang Liu</a>
|
<a href=/people/z/zuohui-fu/>Zuohui Fu</a>
|
<a href=/people/j/jie-cao/>Jie Cao</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a>
|
<a href=/people/y/yik-cheung-tam/>Yik-Cheung Tam</a>
|
<a href=/people/c/cheng-niu/>Cheng Niu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1192><div class="card-body p-3 small">Rhetoric is a vital element in modern <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>, and plays an essential role in improving its <a href=https://en.wikipedia.org/wiki/Aesthetics>aesthetics</a>. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a <a href=https://en.wikipedia.org/wiki/Latent_variable_model>continuous latent variable</a> as a rhetoric controller to capture various rhetorical patterns in an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, and then incorporates rhetoric-based mixtures while generating <a href=https://en.wikipedia.org/wiki/Modern_Chinese_poetry>modern Chinese poetry</a>. For metaphor and personification, an automated evaluation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1196 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1196/>Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation</a></strong><br><a href=/people/j/jiangjie-chen/>Jiangjie Chen</a>
|
<a href=/people/a/ao-wang/>Ao Wang</a>
|
<a href=/people/h/haiyun-jiang/>Haiyun Jiang</a>
|
<a href=/people/s/suo-feng/>Suo Feng</a>
|
<a href=/people/c/chenguang-li/>Chenguang Li</a>
|
<a href=/people/y/yanghua-xiao/>Yanghua Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1196><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Type_description>type description</a> is a succinct noun compound which helps human and machines to quickly grasp the informative and distinctive information of an entity. Entities in most knowledge graphs (KGs) still lack such descriptions, thus calling for automatic methods to supplement such information. However, existing <a href=https://en.wikipedia.org/wiki/Generative_grammar>generative methods</a> either overlook the <a href=https://en.wikipedia.org/wiki/Grammar>grammatical structure</a> or make factual mistakes in generated texts. To solve these problems, we propose a head-modifier template based method to ensure the readability and data fidelity of generated type descriptions. We also propose a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and two <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Experiments show that our method improves substantially compared with <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> and achieves state-of-the-art performance on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1197/>Key Fact as Pivot : A Two-Stage Model for Low Resource Table-to-Text Generation</a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1197><div class="card-body p-3 small">Table-to-text generation aims to translate the <a href=https://en.wikipedia.org/wiki/Data_structure>structured data</a> into the <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. Most existing methods adopt the encoder-decoder framework to learn the <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a>, which requires large-scale training samples. However, the lack of large parallel data is a major practical problem for many domains. In this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available. We propose a novel <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to separate the generation into two stages : key fact prediction and surface realization. It first predicts the key facts from the tables, and then generates the text with the key facts. The training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a biography generation dataset. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve 27.34 BLEU score with only 1,000 <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>, while the baseline model only obtain the performance of 9.71 BLEU score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1200.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1200/>Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models</a></strong><br><a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/l/liqun-chen/>Liqun Chen</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1200><div class="card-body p-3 small">Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a> with <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-VAEs</a>. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1205/>Improving Abstractive Document Summarization with Salient Information Modeling</a></strong><br><a href=/people/y/yongjian-you/>Yongjian You</a>
|
<a href=/people/w/weijia-jia/>Weijia Jia</a>
|
<a href=/people/t/tianyi-liu/>Tianyi Liu</a>
|
<a href=/people/w/wenmian-yang/>Wenmian Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1205><div class="card-body p-3 small">Comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information. To tackle the above difficulties, we propose a Transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization. Specifically, (1) to encode the documents comprehensively, we design a focus-attention mechanism and incorporate it into the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. This mechanism models a Gaussian focal bias on attention scores to enhance the perception of local context, which contributes to producing salient and informative summaries. (2) To distinguish <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salient information</a> precisely, we design an independent saliency-selection network which manages the <a href=https://en.wikipedia.org/wiki/Information_flow>information flow</a> from <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to <a href=https://en.wikipedia.org/wiki/Code>decoder</a>. This <a href=https://en.wikipedia.org/wiki/Social_network>network</a> effectively reduces the influences of secondary information on the generated summaries. Experimental results on the popular CNN / Daily Mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the ROUGE metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1206.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1206/>Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</a></strong><br><a href=/people/m/masaru-isonuma/>Masaru Isonuma</a>
|
<a href=/people/j/junichiro-mori/>Junichiro Mori</a>
|
<a href=/people/i/ichiro-sakata/>Ichiro Sakata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1206><div class="card-body p-3 small">This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. By recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that ranks the importance of each sentence on the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> to support summary generation focusing on the main review point. The experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is competitive with or outperforms other unsupervised approaches. In particular, for relatively long reviews, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves a competitive or better performance than <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a>. The induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1211/>Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation</a></strong><br><a href=/people/f/francine-chen/>Francine Chen</a>
|
<a href=/people/y/yan-ying-chen/>Yan-Ying Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1211><div class="card-body p-3 small">A common issue in training a deep learning, abstractive summarization model is lack of a large set of training summaries. This paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation. In addition to adversarial domain adaptation (ADA), we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain. Evaluation on adapting to / from news articles and Stack Exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> with limited target data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1212/>BIGPATENT : A Large-Scale Dataset for Abstractive and Coherent Summarization<span class=acl-fixed-case>BIGPATENT</span>: A Large-Scale Dataset for Abstractive and Coherent Summarization</a></strong><br><a href=/people/e/eva-sharma/>Eva Sharma</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1212><div class="card-body p-3 small">Most existing text summarization datasets are compiled from the <a href=https://en.wikipedia.org/wiki/News_media>news domain</a>, where summaries have a flattened discourse structure. In such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article&#8217;s global content structure as well as produce abstractive summaries with high <a href=https://en.wikipedia.org/wiki/Compression_ratio>compression ratio</a>. In this work, we present a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, BIGPATENT, consisting of 1.3 million records of <a href=https://en.wikipedia.org/wiki/United_States_patent_law>U.S. patent documents</a> along with <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>human written abstractive summaries</a>. Compared to existing summarization datasets, BIGPATENT has the following properties : i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1213/>Ranking Generated Summaries by Correctness : An Interesting but Challenging Application for Natural Language Inference</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/l/leonardo-f-r-ribeiro/>Leonardo F. R. Ribeiro</a>
|
<a href=/people/p/prasetya-ajie-utama/>Prasetya Ajie Utama</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1213><div class="card-body p-3 small">While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> and show that such <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>errors</a> occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment models</a>. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1214/>Self-Supervised Learning for Contextualized Extractive Summarization</a></strong><br><a href=/people/h/hong-wang/>Hong Wang</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/x/xiaoxiao-guo/>Xiaoxiao Guo</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1214><div class="card-body p-3 small">Existing models for extractive summarization are usually trained from scratch with a cross-entropy loss, which does not explicitly capture the global context at the document level. In this paper, we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion. Experiments on the widely-used CNN / DM dataset validate the effectiveness of the proposed auxiliary tasks. Furthermore, we show that after pre-training, a clean model with simple building blocks is able to outperform previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> that are carefully designed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1215" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1215/>On the Summarization of Consumer Health Questions</a></strong><br><a href=/people/a/asma-ben-abacha/>Asma Ben Abacha</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1215><div class="card-body p-3 small">Question understanding is one of the main challenges in <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. In real world applications, users often submit <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> that are longer than needed and include peripheral information that increases the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the question, leading to substantially more false positives in answer retrieval. In this paper, we study neural abstractive models for medical question summarization. We introduce the MeQSum corpus of 1,000 summarized consumer health questions. We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. In particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a ROUGE-1 score of 44.16 %. We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1216.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1216/>Unsupervised Rewriter for Multi-Sentence Compression</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1216><div class="card-body p-3 small">Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for <a href=https://en.wikipedia.org/wiki/Microsoft_SQL_Server>MSC</a> is the extraction-based word graph approach. A few variants further leveraged <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> is often inappropriate without the consideration of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a>. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> of compression based on human evaluation. A <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1219/>Explicit Utilization of General Knowledge in Machine Reading Comprehension</a></strong><br><a href=/people/c/chao-wang/>Chao Wang</a>
|
<a href=/people/h/hui-jiang/>Hui Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1219><div class="card-body p-3 small">To bridge the gap between Machine Reading Comprehension (MRC) models and human beings, which is mainly reflected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neural networks of MRC models with the general knowledge of human beings. On the one hand, we propose a data enrichment method, which uses <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> to extract inter-word semantic connections as <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> from each given passage-question pair. On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> than them. When only a subset (20%-80 %) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1220 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1220.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1220/>Multi-style Generative Reading Comprehension</a></strong><br><a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/i/itsumi-saito/>Itsumi Saito</a>
|
<a href=/people/k/kosuke-nishida/>Kosuke Nishida</a>
|
<a href=/people/k/kazutoshi-shinoda/>Kazutoshi Shinoda</a>
|
<a href=/people/a/atsushi-otsuka/>Atsushi Otsuka</a>
|
<a href=/people/h/hisako-asano/>Hisako Asano</a>
|
<a href=/people/j/junji-tomita/>Junji Tomita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1220><div class="card-body p-3 small">This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, called <a href=https://en.wikipedia.org/wiki/Masque>Masque</a>. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to improve the NLG capability for all styles involved. This also enables our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to give an answer in the target style. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1223.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1223/>E3 : Entailment-driven Extracting and Editing for Conversational Machine Reading<span class=acl-fixed-case>E</span>3: Entailment-driven Extracting and Editing for Conversational Machine Reading</a></strong><br><a href=/people/v/victor-zhong/>Victor Zhong</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1223><div class="card-body p-3 small">Conversational machine reading systems help users answer high-level questions (e.g. determine if they qualify for particular government benefits) when they do not know the exact <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> by which the determination is made (e.g. whether they need certain <a href=https://en.wikipedia.org/wiki/Income_inequality_in_the_United_States>income levels</a> or veteran status). The key challenge is that these <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> are only provided in the form of a procedural text (e.g. guidelines from government website) which the system must read to figure out what to ask the user. We present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational history and which still need to be edited to create questions for the user. On the recently introduced ShARC conversational machine reading dataset, our Entailment-driven Extract and Edit network (E3) achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In addition, by explicitly highlighting which information still needs to be gathered, E3 provides a more explainable alternative to prior work. We release source code for our models and experiments at https://github.com/vzhong/e3.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1224.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1224.Note.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1224/>Generating Question-Answer Hierarchies</a></strong><br><a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1224><div class="card-body p-3 small">The process of <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a> can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (Hintikka, 1981 ; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a new way of representing documents. In this paper, we present SQUASH (Specificity-controlled Question-Answer Hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. Users can click on high-level questions (e.g., Why did Frodo leave the Fellowship?) to reveal related but more specific questions (e.g., Who did Frodo leave with?). Using a question taxonomy loosely based on Lehnert (1978), we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC. We then use these labels as input to a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipelined system</a> centered around a conditional neural language model. We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1226 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1226/>Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a></strong><br><a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/k/kai-liu/>Kai Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/q/qiaoqiao-she/>Qiaoqiao She</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1226><div class="card-body p-3 small">Machine reading comprehension (MRC) is a crucial and challenging task in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Machine_learning>MRC</a>. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for <a href=https://en.wikipedia.org/wiki/Medical_record>MRC</a>. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>baselines</a> on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single <a href=https://en.wikipedia.org/wiki/Computer_model>model</a> on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1229 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1229/>Semi-supervised Domain Adaptation for Dependency Parsing</a></strong><br><a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/x/xue-peng/>Xue Peng</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1229><div class="card-body p-3 small">During the past decades, due to the lack of sufficient labeled data, most studies on cross-domain parsing focus on unsupervised domain adaptation, assuming there is no target-domain training data. However, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approaches</a> make limited progress so far due to the intrinsic difficulty of both <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. This paper tackles the semi-supervised domain adaptation problem for Chinese dependency parsing, based on two newly-annotated large-scale domain-aware datasets. We propose a simple domain embedding approach to merge the source- and target-domain training data, which is shown to be more effective than both direct corpus concatenation and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. In order to utilize unlabeled target-domain data, we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1230.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1230/>Head-Driven Phrase Structure Grammar Parsing on <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a><span class=acl-fixed-case>H</span>ead-<span class=acl-fixed-case>D</span>riven <span class=acl-fixed-case>P</span>hrase <span class=acl-fixed-case>S</span>tructure <span class=acl-fixed-case>G</span>rammar Parsing on <span class=acl-fixed-case>P</span>enn <span class=acl-fixed-case>T</span>reebank</a></strong><br><a href=/people/j/junru-zhou/>Junru Zhou</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1230><div class="card-body p-3 small">Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified <a href=https://en.wikipedia.org/wiki/Head-driven_phrase_structure_grammar>HPSG</a> by integrating constituent and dependency formal representations into <a href=https://en.wikipedia.org/wiki/Head-driven_phrase_structure_grammar>head-driven phrase structure</a>. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent parsing</a> and 97.00 % UAS of dependency parsing on <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>PTB</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1237 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1237" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1237/>Graph-based Dependency Parsing with Graph Neural Networks</a></strong><br><a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/m/man-lan/>Man Lan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1237><div class="card-body p-3 small">We investigate the problem of efficiently incorporating <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>high-order features</a> into neural graph-based dependency parsing. Instead of explicitly extracting high-order features from intermediate parse trees, we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently. We use graph neural networks (GNNs) to learn the representations and discuss several new configurations of GNN&#8217;s updating and aggregation functions. Experiments on <a href=https://en.wikipedia.org/wiki/Parsing>PTB</a> show that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves the best UAS and LAS on <a href=https://en.wikipedia.org/wiki/Parsing>PTB</a> (96.0 %, 94.3 %) among systems without using any external resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1240/>Topic-Aware Neural Keyphrase Generation for Social Media Language</a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/h/hou-pong-chan/>Hou Pong Chan</a>
|
<a href=/people/i/irwin-king/>Irwin King</a>
|
<a href=/people/m/michael-r-lyu/>Michael R. Lyu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1240><div class="card-body p-3 small">A huge volume of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> is daily produced on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. To facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. While most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. Moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. Further discussions show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns meaningful topics, which interprets its superiority in social media keyphrase generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1241 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1241" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1241/># YouToo? Detection of Personal Recollections of Sexual Harassment on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>oo? Detection of Personal Recollections of Sexual Harassment on Social Media</a></strong><br><a href=/people/a/arijit-ghosh-chowdhury/>Arijit Ghosh Chowdhury</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a>
|
<a href=/people/d/debanjan-mahata/>Debanjan Mahata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1241><div class="card-body p-3 small">The availability of large-scale online social data, coupled with computational methods can help us answer fundamental questions relat- ing to our social lives, particularly our health and well-being. The # MeToo trend has led to people talking about personal experiences of harassment more openly. This work at- tempts to aggregate such experiences of <a href=https://en.wikipedia.org/wiki/Sexual_abuse>sex- ual abuse</a> to facilitate a better understanding of <a href=https://en.wikipedia.org/wiki/Social_constructionism>social media constructs</a> and to bring about <a href=https://en.wikipedia.org/wiki/Social_change>social change</a>. It has been found that disclo- sure of abuse has positive psychological im- pacts. Hence, we contend that such informa- tion can leveraged to create better campaigns for <a href=https://en.wikipedia.org/wiki/Social_change>social change</a> by analyzing how users react to these stories and to obtain a better insight into the consequences of <a href=https://en.wikipedia.org/wiki/Sexual_abuse>sexual abuse</a>. We use a three part Twitter-Specific Social Media Lan- guage Model to segregate personal recollec- tions of sexual harassment from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter posts</a>. An extensive comparison with state-of-the-art generic and specific models along with a de- tailed error analysis explores the merit of our proposed model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1244 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1244/>Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks</a></strong><br><a href=/people/j/jing-ma/>Jing Ma</a>
|
<a href=/people/w/wei-gao/>Wei Gao</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1244><div class="card-body p-3 small">Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. Our model consists of three main components : 1) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles ; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention ; and 3) An output layer predicts the verdict based on the embedded evidence. Experimental results on three public benchmark datasets show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms a set of state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1246/>You Write like You Eat : Stylistic Variation as a Predictor of <a href=https://en.wikipedia.org/wiki/Social_stratification>Social Stratification</a></a></strong><br><a href=/people/a/angelo-basile/>Angelo Basile</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1246><div class="card-body p-3 small">Inspired by Labov&#8217;s seminal work on stylisticvariation as a function of <a href=https://en.wikipedia.org/wiki/Social_stratification>social stratification</a>, we develop and compare neural models thatpredict a person&#8217;s presumed socio-economicstatus, obtained through distant supervision, from their writing style on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict <a href=https://en.wikipedia.org/wiki/Socioeconomic_status>socio-economic group</a>. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1249 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1249" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1249/>Celebrity Profiling</a></strong><br><a href=/people/m/matti-wiegmann/>Matti Wiegmann</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1249><div class="card-body p-3 small">Celebrities are among the most prolific users of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, promoting their personas and rallying followers. This activity is closely tied to genuine writing samples, which makes them worthy research subjects in many respects, not least <a href=https://en.wikipedia.org/wiki/Profiling_(information_science)>profiling</a>. With this paper we introduce the Webis Celebrity Corpus 2019. For its construction the Twitter feeds of 71,706 verified accounts have been carefully linked with their respective Wikidata items, crawling both. After cleansing, the resulting profiles contain an average of 29,968 words per <a href=https://en.wikipedia.org/wiki/User_profile>profile</a> and up to 239 pieces of <a href=https://en.wikipedia.org/wiki/Personal_data>personal information</a>. A <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross-evaluation</a> that checked the correct association of Twitter account and Wikidata item revealed an <a href=https://en.wikipedia.org/wiki/Error_rate>error rate</a> of only 0.6 %, rendering the <a href=https://en.wikipedia.org/wiki/User_profile>profiles</a> highly reliable. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises a wide cross-section of local and global celebrities, forming a unique combination of <a href=https://en.wikipedia.org/wiki/Scale_(social_sciences)>scale</a>, profile comprehensiveness, and label reliability. We further establish the state of the art&#8217;s profiling performance by evaluating the winning approaches submitted to the PAN gender prediction tasks in a transfer learning experiment. They are only outperformed by our own deep learning approach, which we also use to exemplify celebrity occupation prediction for the first time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1252 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1252.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1252" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1252/>Twitter Homophily : Network Based Prediction of User’s Occupation<span class=acl-fixed-case>T</span>witter Homophily: Network Based Prediction of User’s Occupation</a></strong><br><a href=/people/j/jiaqi-pan/>Jiaqi Pan</a>
|
<a href=/people/r/rishabh-bhardwaj/>Rishabh Bhardwaj</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/h/hai-leong-chieu/>Hai Leong Chieu</a>
|
<a href=/people/x/xinghao-pan/>Xinghao Pan</a>
|
<a href=/people/n/ni-yi-puay/>Ni Yi Puay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1252><div class="card-body p-3 small">In this paper, we investigate the importance of <a href=https://en.wikipedia.org/wiki/Social_network>social network information</a> compared to <a href=https://en.wikipedia.org/wiki/Content_(media)>content information</a> in the prediction of a Twitter user&#8217;s occupational class. We show that the content information of a user&#8217;s tweets, the profile descriptions of a user&#8217;s follower / following community, and the user&#8217;s social network provide useful information for classifying a user&#8217;s occupational group. In our study, we extend an existing <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, and we achieve significantly better performance by using social network homophily that has not been fully exploited in previous work. In our analysis, we found that by using the graph convolutional network to exploit social homophily, we can achieve competitive performance on this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> with just a small fraction of the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1254 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384728593 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1254/>Strategies for Structuring Story Generation</a></strong><br><a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/y/yann-dauphin/>Yann Dauphin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1254><div class="card-body p-3 small">Writers often rely on plans or sketches to write long stories, but most current <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> generate word by word from left to right. We explore coarse-to-fine models for creating <a href=https://en.wikipedia.org/wiki/Narrative>narrative texts</a> of several hundred words, and introduce new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> which decompose stories by abstracting over actions and entities. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384728654 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1255/>Argument Generation with Retrieval, <a href=https://en.wikipedia.org/wiki/Planning>Planning</a>, and Realization</a></strong><br><a href=/people/x/xinyu-hua/>Xinyu Hua</a>
|
<a href=/people/z/zhe-hu/>Zhe Hu</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1255><div class="card-body p-3 small">Automatic argument generation is an appealing but challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we study the specific problem of counter-argument generation, and present a novel framework, CANDELA. It consists of a powerful retrieval system and a novel two-step generation model, where a text planning decoder first decides on the main talking points and a proper language style for each sentence, then a content realization decoder reflects the decisions and constructs an informative paragraph-level argument. Furthermore, our generation model is empowered by a retrieval system indexed with 12 million articles collected from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and popular English news media, which provides access to high-quality content with diversity. Automatic evaluation on a large-scale dataset collected from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> shows that our model yields significantly higher BLEU, ROUGE, and METEOR scores than the state-of-the-art and non-trivial comparisons. Human evaluation further indicates that our system arguments are more appropriate for refutation and richer in content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384728744 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1256/>A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation</a></strong><br><a href=/people/f/feng-nie/>Feng Nie</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a>
|
<a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/r/rong-pan/>Rong Pan</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1256><div class="card-body p-3 small">Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50 % relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator.<i>hallucinate</i> contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384731731 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1257" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1257/>Cross-Modal Commentator : Automatic Machine Commenting Based on Cross-Modal Information</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/z/zhihan-zhang/>Zhihan Zhang</a>
|
<a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/c/chengyang-huang/>Chengyang Huang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1257><div class="card-body p-3 small">Automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. Previous work focuses on automatic commenting based solely on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual content</a>. However, in real-scenarios, online articles usually contain multiple modal contents. For instance, graphic news contains plenty of images in addition to text. Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. To remedy this, we propose a new task : cross-model automatic commenting (CMAC), which aims to make comments by integrating multiple modal contents. We construct a <a href=https://en.wikipedia.org/wiki/Data_set>large-scale dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and explore several representative methods. Going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. Evaluation results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve better performance than competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1259 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384732092 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1259" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1259/>Cognitive Graph for Multi-Hop Reading Comprehension at Scale</a></strong><br><a href=/people/m/ming-ding/>Ming Ding</a>
|
<a href=/people/c/chang-zhou/>Chang Zhou</a>
|
<a href=/people/q/qibin-chen/>Qibin Chen</a>
|
<a href=/people/h/hongxia-yang/>Hongxia Yang</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1259><div class="card-body p-3 small">We propose a new CogQA framework for multi-hop reading comprehension question answering in web-scale documents. Founded on the <a href=https://en.wikipedia.org/wiki/Dual_process_theory>dual process theory</a> in <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>, the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint F_1 score of 34.9 on the leaderboard, compared to 23.1 of the best competitor.<i>cognitive graph</i> in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our framework further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint <tex-math>F_1</tex-math> score of 34.9 on the leaderboard, compared to 23.1 of the best competitor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1262 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384736016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1262" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1262/>Avoiding Reasoning Shortcuts : Adversarial Evaluation, Training, and Model Development for Multi-Hop QA<span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1262><div class="card-body p-3 small">Multi-hop question answering requires a <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> to connect multiple pieces of evidence scattered in a long context to answer the question. In this paper, we show that in the multi-hop HotpotQA (Yang et al., 2018) dataset, the examples often contain reasoning shortcuts through which models can directly locate the answer by word-matching the question with a sentence in the context. We demonstrate this issue by constructing adversarial documents that create contradicting answers to the shortcut but do not affect the validity of the original answer. The performance of strong baseline models drops significantly on our adversarial test, indicating that they are indeed exploiting the <a href=https://en.wikipedia.org/wiki/Shortcut_(computing)>shortcuts</a> rather than performing multi-hop reasoning. After adversarial training, the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>&#8217;s performance improves but is still limited on the adversarial test. Hence, we use a <a href=https://en.wikipedia.org/wiki/Control_unit>control unit</a> that dynamically attends to the question at different reasoning hops to guide the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s multi-hop reasoning. We show that our 2-hop model trained on the regular data is more robust to the adversaries than the baseline. After adversarial training, it not only achieves significant improvements over its counterpart trained on regular data, but also outperforms the adversarially-trained baseline significantly. Finally, we sanity-check that these improvements are not obtained by exploiting potential new shortcuts in the adversarial data, but indeed due to robust multi-hop reasoning skills of the models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1263 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384736068 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1263" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1263/>Exploiting Explicit Paths for Multi-hop Reading Comprehension</a></strong><br><a href=/people/s/souvik-kundu/>Souvik Kundu</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1263><div class="card-body p-3 small">We propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. Although inspired by multi-hop reasoning over <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>, our proposed approach operates directly over <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. It generates potential paths through passages and scores them without any direct path supervision. The proposed model, named PathNet, attempts to extract implicit relations from text through <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity pair representations</a>, and compose them to encode each path. To capture additional context, PathNet also composes the passage representations along each path to compute a passage-based representation. Unlike previous approaches, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is then able to explain its reasoning via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384738334 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1267/>We Need to Talk about Standard Splits</a></strong><br><a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1267><div class="card-body p-3 small">It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>statistical tests</a> to determine whether differences in performance are likely to arise by chance, and few examine the stability of <a href=https://en.wikipedia.org/wiki/Ranking>system ranking</a> across multiple <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training-testing splits</a>. We conduct replication and reproduction experiments with nine <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech taggers</a> published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used standard split. While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1270 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384738763 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1270" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1270/>Joint Effects of Context and User History for Predicting Online Conversation Re-entries</a></strong><br><a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1270><div class="card-body p-3 small">As the <a href=https://en.wikipedia.org/wiki/Online_and_offline>online world</a> continues its exponential growth, <a href=https://en.wikipedia.org/wiki/Interpersonal_communication>interpersonal communication</a> has come to play an increasingly central role in opinion formation and <a href=https://en.wikipedia.org/wiki/Social_change>change</a>. In order to help users better engage with each other online, we study a challenging problem of re-entry prediction foreseeing whether a user will come back to a conversation they once participated in. We hypothesize that both the context of the ongoing conversations and the users&#8217; previous chatting history will affect their continued interests in future engagement. Specifically, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting history jointly result in their re-entry behavior. We experiment with two <a href=https://en.wikipedia.org/wiki/Data_set>large-scale datasets</a> collected from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Results show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> with bi-attention achieves an F1 score of 61.1 on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter conversations</a>, outperforming the state-of-the-art methods from previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1271 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384740828 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1271/>CONAN-COunter NArratives through Nichesourcing : a Multilingual Dataset of Responses to Fight Online Hate Speech<span class=acl-fixed-case>CONAN</span> - <span class=acl-fixed-case>CO</span>unter <span class=acl-fixed-case>NA</span>rratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech</a></strong><br><a href=/people/y/yi-ling-chung/>Yi-Ling Chung</a>
|
<a href=/people/e/elizaveta-kuzmenko/>Elizaveta Kuzmenko</a>
|
<a href=/people/s/serra-sinem-tekiroglu/>Serra Sinem Tekiroglu</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1271><div class="card-body p-3 small">Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>, dealing with hatred online is still a tough problem. Tackling <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> in the standard way of content deletion or <a href=https://en.wikipedia.org/wiki/Suspension_(punishment)>user suspension</a> may be charged with <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a> and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, we describe the creation of the first large-scale, multilingual, expert-based dataset of hate-speech / counter-narrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>. Finally, we provide initial experiments to assess the quality of our <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1275 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384744638 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1275/>Exploring Author Context for Detecting Intended vs Perceived Sarcasm</a></strong><br><a href=/people/s/silviu-oprea/>Silviu Oprea</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1275><div class="card-body p-3 small">We investigate the impact of using author context on textual sarcasm detection. We define author context as the embedded representation of their historical posts on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and suggest neural models that extract these representations. We experiment with two tweet datasets, one labelled manually for <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and the other via tag-based distant supervision. We achieve state-of-the-art performance on the second <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, but not on the one labelled manually, indicating a difference between <a href=https://en.wikipedia.org/wiki/Sarcasm>intended sarcasm</a>, captured by distant supervision, and <a href=https://en.wikipedia.org/wiki/Sarcasm>perceived sarcasm</a>, captured by manual labelling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1276 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384744763 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1276" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1276/>Open Domain Event Extraction Using Neural Latent Variable Models</a></strong><br><a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1276><div class="card-body p-3 small">We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> gives better performance compared to the state-of-the-art method for event schema induction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1277 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384744882 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1277" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1277/>Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification</a></strong><br><a href=/people/z/zhi-xiu-ye/>Zhi-Xiu Ye</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1277><div class="card-body p-3 small">This paper presents a multi-level matching and aggregation network (MLMAN) for few-shot relation classification. Previous studies on this topic adopt prototypical networks, which calculate the embedding vector of a query instance and the prototype vector of the support set for each relation candidate independently. On the contrary, our proposed MLMAN model encodes the query instance and each support set in an interactive way by considering their matching information at both local and instance levels. The final class prototype for each support set is obtained by attentive aggregation over the representations of support instances, where the weights are calculated using the query instance. Experimental results demonstrate the effectiveness of our proposed methods, which achieve a new state-of-the-art performance on the FewRel dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1281 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384794970 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1281" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1281/>FIESTA : Fast IdEntification of State-of-The-Art models using adaptive bandit algorithms<span class=acl-fixed-case>FIESTA</span>: Fast <span class=acl-fixed-case>I</span>d<span class=acl-fixed-case>E</span>ntification of State-of-The-Art models using adaptive bandit algorithms</a></strong><br><a href=/people/h/henry-moss/>Henry Moss</a>
|
<a href=/people/a/andrew-moore/>Andrew Moore</a>
|
<a href=/people/d/david-leslie/>David Leslie</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1281><div class="card-body p-3 small">We present FIESTA, a model selection approach that significantly reduces the computational resources required to reliably identify state-of-the-art performance from large collections of candidate models. Despite being known to produce unreliable comparisons, it is still common practice to compare model evaluations based on single choices of <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a>. We show that reliable <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a> also requires evaluations based on multiple train-test splits (contrary to common practice in many shared tasks). Using bandit theory from the statistics literature, we are able to adaptively determine appropriate numbers of data splits and random seeds used to evaluate each <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, focusing computational resources on the evaluation of promising <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> whilst avoiding wasting evaluations on models with lower performance. Furthermore, our user-friendly Python implementation produces <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence guarantees</a> of correctly selecting the optimal model. We evaluate our algorithms by selecting between 8 target-dependent sentiment analysis methods using dramatically fewer model evaluations than current model selection approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1286.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1286/>Domain Adaptation of Neural Machine Translation by Lexicon Induction</a></strong><br><a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/m/mengzhou-xia/>Mengzhou Xia</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1286><div class="card-body p-3 small">It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1287 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1287/>Reference Network for Neural Machine Translation</a></strong><br><a href=/people/h/han-fu/>Han Fu</a>
|
<a href=/people/c/chenghao-liu/>Chenghao Liu</a>
|
<a href=/people/j/jianling-sun/>Jianling Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1287><div class="card-body p-3 small">Neural Machine Translation (NMT) has achieved notable success in recent years. Such a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> usually generates <a href=https://en.wikipedia.org/wiki/Translation>translations</a> in isolation. In contrast, <a href=https://en.wikipedia.org/wiki/Translation>human translators</a> often refer to <a href=https://en.wikipedia.org/wiki/Reference_data>reference data</a>, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a <a href=https://en.wikipedia.org/wiki/Reference_work>reference book</a>, an intuitive way is to store the detailed translation history with extra <a href=https://en.wikipedia.org/wiki/Computer_memory>memory</a>, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1290 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1290/>Look Harder : A Neural Machine Translation Model with Hard Attention</a></strong><br><a href=/people/s/sathish-reddy-indurthi/>Sathish Reddy Indurthi</a>
|
<a href=/people/i/insoo-chung/>Insoo Chung</a>
|
<a href=/people/s/sangha-kim/>Sangha Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1290><div class="card-body p-3 small">Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning algorithm</a> coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (ENFR) translation tasks compared to the soft attention based NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1292.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1292 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1292 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1292" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1292/>A Simple and Effective Approach to Automatic Post-Editing with <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a></a></strong><br><a href=/people/g/goncalo-m-correia/>Gonçalo M. Correia</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1292><div class="card-body p-3 small">Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training a MT system from scratch. in this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 23 K sentences for 3 hours on a single GPU we obtain results that are competitive with <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>systems</a> that were trained on 5 M artificial sentences. When we add this <a href=https://en.wikipedia.org/wiki/Data_(computing)>artificial data</a> our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> obtains state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1294 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1294" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1294/>Training Neural Machine Translation to Apply Terminology Constraints</a></strong><br><a href=/people/g/georgiana-dinu/>Georgiana Dinu</a>
|
<a href=/people/p/prashant-mathur/>Prashant Mathur</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/y/yaser-al-onaizan/>Yaser Al-Onaizan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1294><div class="card-body p-3 small">This paper proposes a novel method to inject custom terminology into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a> to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1295 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1295" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1295/>Leveraging Local and Global Patterns for Self-Attention Networks</a></strong><br><a href=/people/m/mingzhou-xu/>Mingzhou Xu</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/l/lidia-s-chao/>Lidia S. Chao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1295><div class="card-body p-3 small">Self-attention networks have received increasing research attention. By default, the hidden states of each word are hierarchically calculated by attending to all words in the sentence, which assembles global information. However, several studies pointed out that taking all signals into account may lead to overlooking <a href=https://en.wikipedia.org/wiki/Entropy_(information_theory)>neighboring information</a> (e.g. phrase pattern). To address this argument, we propose a hybrid attention mechanism to dynamically leverage both of the local and global information. Specifically, our approach uses a gating scalar for integrating both sources of the information, which is also convenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1296 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1296/>Sentence-Level Agreement for Neural Machine Translation</a></strong><br><a href=/people/m/mingming-yang/>Mingming Yang</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1296><div class="card-body p-3 small">The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training objective</a> is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1297 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1297/>Multilingual Unsupervised NMT using Shared Encoder and Language-Specific Decoders<span class=acl-fixed-case>NMT</span> using Shared Encoder and Language-Specific Decoders</a></strong><br><a href=/people/s/sukanta-sen/>Sukanta Sen</a>
|
<a href=/people/k/kamal-kumar-gupta/>Kamal Kumar Gupta</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1297><div class="card-body p-3 small">In this paper, we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders. Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages. This results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> for all possible translation directions, the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is still able to translate in a many-to-many fashion leveraging encoder&#8217;s ability to generate interlingual representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1301/>Choosing <a href=https://en.wikipedia.org/wiki/Language_transfer>Transfer Languages</a> for Cross-Lingual Learning</a></strong><br><a href=/people/y/yu-hsiang-lin/>Yu-Hsiang Lin</a>
|
<a href=/people/c/chian-yu-chen/>Chian-Yu Chen</a>
|
<a href=/people/j/jean-lee/>Jean Lee</a>
|
<a href=/people/z/zirui-li/>Zirui Li</a>
|
<a href=/people/y/yuyan-zhang/>Yuyan Zhang</a>
|
<a href=/people/m/mengzhou-xia/>Mengzhou Xia</a>
|
<a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1301><div class="card-body p-3 small">Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including <a href=https://en.wikipedia.org/wiki/Phylogenetic_tree>phylogenetic similarity</a>, <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typological properties</a>, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that consider the aforementioned <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1302.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1302/>CogNet : A Large-Scale Cognate Database<span class=acl-fixed-case>C</span>og<span class=acl-fixed-case>N</span>et: A Large-Scale Cognate Database</a></strong><br><a href=/people/k/khuyagbaatar-batsuren/>Khuyagbaatar Batsuren</a>
|
<a href=/people/g/gabor-bella/>Gabor Bella</a>
|
<a href=/people/f/fausto-giunchiglia/>Fausto Giunchiglia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1302><div class="card-body p-3 small">This paper introduces CogNet, a new, large-scale lexical database that provides cognates -words of common origin and meaning- across languages. The <a href=https://en.wikipedia.org/wiki/Database>database</a> currently contains 3.1 million <a href=https://en.wikipedia.org/wiki/Cognate>cognate pairs</a> across 338 languages using 35 <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a>. The paper also describes the automated method by which <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> were computed from publicly available <a href=https://en.wikipedia.org/wiki/Wordnet>wordnets</a>, with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> evaluated to 94 %. Finally, it presents statistics about the cognate data and some initial insights into it, hinting at a possible future exploitation of the <a href=https://en.wikipedia.org/wiki/Resource>resource</a> by various fields of <a href=https://en.wikipedia.org/wiki/Lingustics>lingustics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1303/>Neural Decipherment via Minimum-Cost Flow : From Ugaritic to Linear B<span class=acl-fixed-case>U</span>garitic to <span class=acl-fixed-case>L</span>inear <span class=acl-fixed-case>B</span></a></strong><br><a href=/people/j/jiaming-luo/>Jiaming Luo</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1303><div class="card-body p-3 small">In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in unsupervised manner, we innovate the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a> by formalizing it as a minimum-cost flow problem. When applied to decipherment of Ugaritic, we achieve 5 % absolute improvement over state-of-the-art results. We also report first automatic results in deciphering Linear B, a <a href=https://en.wikipedia.org/wiki/Syllabary>syllabic language</a> related to <a href=https://en.wikipedia.org/wiki/Ancient_Greek>ancient Greek</a>, where our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> correctly translates 67.3 % of <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1306/>Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations</a></strong><br><a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/c/caitlin-westerfield/>Caitlin Westerfield</a>
|
<a href=/people/s/sungrok-shim/>Sungrok Shim</a>
|
<a href=/people/g/garrett-bingham/>Garrett Bingham</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/w/william-hu/>William Hu</a>
|
<a href=/people/n/neha-verma/>Neha Verma</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1306><div class="card-body p-3 small">In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can also be directly applied to another language pair without any <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training label</a>. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1307 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1307/>Are Girls Neko or Shjo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization</a></strong><br><a href=/people/m/mozhi-zhang/>Mozhi Zhang</a>
|
<a href=/people/k/keyulu-xu/>Keyulu Xu</a>
|
<a href=/people/k/ken-ichi-kawarabayashi/>Ken-ichi Kawarabayashi</a>
|
<a href=/people/s/stefanie-jegelka/>Stefanie Jegelka</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1307><div class="card-body p-3 small">Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language&#8217;s average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2 % to 44 % test accuracy).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1311 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1311/>Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections</a></strong><br><a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1311><div class="card-body p-3 small">Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. However, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> are available. In this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> with a structured prior that utilizes labeled source data and unlabeled target data jointly. The parameters of <a href=https://en.wikipedia.org/wiki/Statistical_model>source model</a> and <a href=https://en.wikipedia.org/wiki/Statistical_model>target model</a> are softly shared through a regularized log likelihood objective. An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. We evaluate our method on two syntactic tasks : part-of-speech (POS) tagging and dependency parsing. On the Universal Dependency Treebanks, we use <a href=https://en.wikipedia.org/wiki/English_language>English</a> as the only source corpus and transfer to a wide range of target languages. On the 10 languages in this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that are distant from <a href=https://en.wikipedia.org/wiki/English_language>English</a>, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> yields an average of 5.2 % absolute improvement on <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> and 8.3 % absolute improvement on dependency parsing over a direct transfer method using state-of-the-art <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1312 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1312/>Unsupervised Joint Training of Bilingual Word Embeddings</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1312><div class="card-body p-3 small">State-of-the-art methods for unsupervised bilingual word embeddings (BWE) train a <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>mapping function</a> that maps pre-trained monolingual word embeddings into a bilingual space. Despite its remarkable results, unsupervised mapping is also well-known to be limited by the original dissimilarity between the word embedding spaces to be mapped. In this work, we propose a new approach that trains unsupervised BWE jointly on synthetic parallel data generated through unsupervised machine translation. We demonstrate that existing algorithms that jointly train BWE are very robust to noisy training data and show that unsupervised BWE jointly trained significantly outperform unsupervised mapped BWE in several cross-lingual NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1314/>Is <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a> Necessary for Deep Learning of Chinese Representations?<span class=acl-fixed-case>C</span>hinese Representations?</a></strong><br><a href=/people/x/xiaoya-li/>Xiaoya Li</a>
|
<a href=/people/y/yuxian-meng/>Yuxian Meng</a>
|
<a href=/people/x/xiaofei-sun/>Xiaofei Sun</a>
|
<a href=/people/q/qinghong-han/>Qinghong Han</a>
|
<a href=/people/a/arianna-yuan/>Arianna Yuan</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1314><div class="card-body p-3 small">Segmenting a chunk of text into words is usually the first step of processing <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese text</a>, but its necessity has rarely been explored. In this paper, we ask the fundamental question of whether Chinese word segmentation (CWS) is necessary for deep learning-based Chinese Natural Language Processing. We benchmark neural word-based models which rely on <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> against neural char-based models which do not involve <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> in four end-to-end NLP benchmark tasks : language modeling, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, sentence matching / paraphrase and text classification. Through direct comparisons between these two types of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, we find that char-based models consistently outperform word-based models. Based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. We hope this paper could encourage researchers in the community to rethink the necessity of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> in deep learning-based Chinese Natural Language Processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1316/>On the Compositionality Prediction of Noun Phrases using Poincar Embeddings</a></strong><br><a href=/people/a/abhik-jana/>Abhik Jana</a>
|
<a href=/people/d/dima-puzyrev/>Dima Puzyrev</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1316><div class="card-body p-3 small">The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their <a href=https://en.wikipedia.org/wiki/Grammatical_relation>grammatical relations</a>. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional information</a> for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincar embeddings in addition to the distributional information to detect compositionality for <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a>. Using a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average</a> of the distributional similarity and a Poincar similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a>, we have also framed the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised task</a>, obtaining comparable improvements. Further, we publicly release our Poincar embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1317 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1317/>Robust Representation Learning of Biomedical Names</a></strong><br><a href=/people/m/minh-c-phan/>Minh C. Phan</a>
|
<a href=/people/a/aixin-sun/>Aixin Sun</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1317><div class="card-body p-3 small">Biomedical concepts are often mentioned in <a href=https://en.wikipedia.org/wiki/Medical_record>medical documents</a> under different name variations (synonyms). This mismatch between surface forms is problematic, resulting in difficulties pertaining to learning effective <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. Consequently, this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable. This paper proposes a new <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for learning robust representations of biomedical names and terms. The idea behind our approach is to consider and encode <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual meaning</a>, conceptual meaning, and the <a href=https://en.wikipedia.org/wiki/Synonym>similarity between synonyms</a> during the representation learning process. Via extensive experiments, we show that our proposed method outperforms other baselines on a battery of retrieval, similarity and relatedness benchmarks. Moreover, our proposed method is also able to compute meaningful representations for unseen names, resulting in high practical utility in real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1320 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1320.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1320" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1320/>Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks</a></strong><br><a href=/people/s/shikhar-vashishth/>Shikhar Vashishth</a>
|
<a href=/people/m/manik-bhandari/>Manik Bhandari</a>
|
<a href=/people/p/prateek-yadav/>Prateek Yadav</a>
|
<a href=/people/p/piyush-rai/>Piyush Rai</a>
|
<a href=/people/c/chiranjib-bhattacharyya/>Chiranjib Bhattacharyya</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1320><div class="card-body p-3 small">Word embeddings have been widely adopted across several NLP applications. Most existing word embedding methods utilize sequential context of a word to learn its embedding. While there have been some attempts at utilizing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>syntactic context</a> of a word, such methods result in an explosion of the <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary size</a>. In this paper, we overcome this problem by proposing SynGCN, a flexible Graph Convolution based method for learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. SynGCN utilizes the <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency context</a> of a word without increasing the <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>vocabulary size</a>. Word embeddings learned by SynGCN outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with ELMo. We also propose SemGCN, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations. We make the source code of both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> available to encourage reproducible research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1325/>Making Fast Graph-based Algorithms with Graph Metric Embeddings</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/m/mohammad-dorgham/>Mohammad Dorgham</a>
|
<a href=/people/o/oleksiy-oliynyk/>Oleksiy Oliynyk</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1325><div class="card-body p-3 small">Graph measures, such as node distances, are inefficient to compute. We explore dense vector representations as an effective way to approximate the same information. We introduce a simple yet efficient and effective approach for learning <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a>. Instead of directly operating on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a> into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1328 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1328.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1328/>BERT-based Lexical Substitution<span class=acl-fixed-case>BERT</span>-based Lexical Substitution</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1328><div class="card-body p-3 small">Previous studies on <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> tend to obtain substitute candidates by finding the target word&#8217;s synonyms from lexical resources (e.g., WordNet) and then rank the candidates based on its contexts. These approaches have two limitations : (1) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources ; (2) They fail to take into account the substitution&#8217;s influence on the global context of the sentence. To address these issues, we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. Our approach first applies dropout to the target word&#8217;s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word&#8217;s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution&#8217;s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1330.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384768559 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1330" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1330/>HighRES : Highlight-based Reference-less Evaluation of Summarization<span class=acl-fixed-case>H</span>igh<span class=acl-fixed-case>RES</span>: Highlight-based Reference-less Evaluation of Summarization</a></strong><br><a href=/people/h/hardy-hardy/>Hardy Hardy</a>
|
<a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1330><div class="card-body p-3 small">There has been substantial progress in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1331 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384771870 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1331" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1331/>EditNTS : An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing<span class=acl-fixed-case>E</span>dit<span class=acl-fixed-case>NTS</span>: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing</a></strong><br><a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/z/zichao-li/>Zichao Li</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1331><div class="card-body p-3 small">We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform <a href=https://en.wikipedia.org/wiki/Simplification>simplification</a> and revision. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1332 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384795570 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1332/>Decomposable Neural Paraphrase Generation</a></strong><br><a href=/people/z/zichao-li/>Zichao Li</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1332><div class="card-body p-3 small">Paraphrasing exists at different granularity levels, such as <a href=https://en.wikipedia.org/wiki/Lexicon>lexical level</a>, <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrasal level</a> and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentential level</a>. This paper presents Decomposable Neural Paraphrase Generator (DNPG), a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way. Specifically, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is composed of multiple encoders and decoders with different structures, each of which corresponds to a specific granularity. The empirical study shows that the decomposition mechanism of DNPG makes <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> more interpretable and controllable. Based on DNPG, we further develop an unsupervised domain adaptation method for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves competitive in-domain performance compared to state-of-the-art neural models, and significantly better performance when adapting to a new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1334 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384776891 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1334" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1334/>Right for the Wrong Reasons : Diagnosing Syntactic Heuristics in Natural Language Inference</a></strong><br><a href=/people/t/tom-mccoy/>Tom McCoy</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1334><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning system</a> can score well on a given test set by relying on <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics : the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on <a href=https://en.wikipedia.org/wiki/HANS>HANS</a>, suggesting that they have indeed adopted these <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1336 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384782139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1336/>Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition</a></strong><br><a href=/people/j/joey-tianyi-zhou/>Joey Tianyi Zhou</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/h/hongyuan-zhu/>Hongyuan Zhu</a>
|
<a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/r/rick-siow-mong-goh/>Rick Siow Mong Goh</a>
|
<a href=/people/k/kenneth-kwok/>Kenneth Kwok</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1336><div class="card-body p-3 small">We propose a new neural transfer method termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are investigated to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. In experiments, we examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1338 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384800134 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1338" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1338/>An Imitation Learning Approach to Unsupervised Parsing</a></strong><br><a href=/people/b/bowen-li/>Bowen Li</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1338><div class="card-body p-3 small">Recently, there has been an increasing interest in <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised parsers</a> that optimize semantically oriented objectives, typically using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Unfortunately, the learned trees often do not match actual <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>syntax trees</a> well. Shen et al. (2018) propose a structured attention mechanism for language modeling (PRPN), which induces better syntactic structures but relies on ad hoc heuristics. Also, their <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> lacks interpretability as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is not grounded in parsing actions. In our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by PRPN to a Tree-LSTM model with discrete parsing actions. Its <a href=https://en.wikipedia.org/wiki/Policy>policy</a> is then refined by Gumbel-Softmax training towards a semantically oriented objective. We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing F-score, outperforming our base models, including PRPN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1339 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384801759 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1339/>Women’s Syntactic Resilience and Men’s Grammatical Luck : Gender-Bias in Part-of-Speech Tagging and Dependency Parsing</a></strong><br><a href=/people/a/aparna-garimella/>Aparna Garimella</a>
|
<a href=/people/c/carmen-banea/>Carmen Banea</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1339><div class="card-body p-3 small">Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parsing</a> have still not adapted to account for these differences. To address this, we annotate the <a href=https://en.wikipedia.org/wiki/The_Wall_Street_Journal>Wall Street Journal</a> part of the Penn Treebank with the gender information of the articles&#8217; authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tags</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic relations</a> whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1340 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384782901 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1340/>Multilingual Constituency Parsing with Self-Attention and Pre-Training</a></strong><br><a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/s/steven-cao/>Steven Cao</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1340><div class="card-body p-3 small">We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, <a href=https://en.wikipedia.org/wiki/FastText>fastText</a>, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested ; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2 % relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 <a href=https://en.wikipedia.org/wiki/Language>languages</a>, including <a href=https://en.wikipedia.org/wiki/English_language>English</a> (95.8 F1) and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> (91.8 F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1341 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384801834 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1341/>A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction<span class=acl-fixed-case>BPE</span> Embedding Space for Universal Sentiment Lexicon Induction</a></strong><br><a href=/people/m/mengjie-zhao/>Mengjie Zhao</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1341><div class="card-body p-3 small">We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world&#8217;s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, we then generalize the domain-specific lexicon to a general one. We show across typologically diverse languages in PBC+ good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1343 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384803075 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1343" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1343/>Improved Sentiment Detection via Label Transfer from <a href=https://en.wikipedia.org/wiki/Monolingualism>Monolingual</a> to Synthetic Code-Switched Text</a></strong><br><a href=/people/b/bidisha-samanta/>Bidisha Samanta</a>
|
<a href=/people/n/niloy-ganguly/>Niloy Ganguly</a>
|
<a href=/people/s/soumen-chakrabarti/>Soumen Chakrabarti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1343><div class="card-body p-3 small">Multilingual writers and speakers often alternate between two languages in a single discourse. This practice is called <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text. Manually labeled code-switched text, especially involving <a href=https://en.wikipedia.org/wiki/Minority_language>minority languages</a>, is extremely rare. Consequently, the best monolingual methods perform relatively poorly on code-switched text. We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is relatively readily available. The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language. By augmenting the scarce labeled code-switched text with plentiful synthetic labeled code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5 %, 5.11 % 7.20 %) for three different language pairs (English-Hindi, English-Spanish and English-Bengali). The improvement is even significant in hatespeech detection whereby we achieve a 4 % improvement using only synthetic code-switched data (6 % with data augmentation).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1346 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1346.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384783066 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1346" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1346/>ELI5 : Long Form Question Answering<span class=acl-fixed-case>ELI</span>5: Long Form Question Answering</a></strong><br><a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a>
|
<a href=/people/e/ethan-perez/>Ethan Perez</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1346><div class="card-body p-3 small">We introduce the first <a href=https://en.wikipedia.org/wiki/Text_corpus>large-scale corpus</a> for long form question answering, a task requiring elaborate and in-depth answers to <a href=https://en.wikipedia.org/wiki/Open-ended_question>open-ended questions</a>. The dataset comprises 270 K threads from the Reddit forum Explain Like I&#8217;m Five (ELI5) where an <a href=https://en.wikipedia.org/wiki/Online_community>online community</a> provides answers to questions which are comprehensible by five year olds. Compared to existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of <a href=https://en.wikipedia.org/wiki/Web_page>web documents</a> to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, as well as a strong extractive baseline. However, our best model is still far from human performance since raters prefer gold responses in over 86 % of cases, leaving ample opportunity for future improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1347 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1347/>Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension</a></strong><br><a href=/people/d/daesik-kim/>Daesik Kim</a>
|
<a href=/people/s/seonhoon-kim/>Seonhoon Kim</a>
|
<a href=/people/n/nojun-kwak/>Nojun Kwak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1347><div class="card-body p-3 small">In this work, we introduce a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multi-modal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called &#8216;out-of-domain&#8217; issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1350 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1350.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384787273 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1350/>Psycholinguistics Meets Continual Learning : Measuring Catastrophic Forgetting in Visual Question Answering</a></strong><br><a href=/people/c/claudio-greco/>Claudio Greco</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1350><div class="card-body p-3 small">We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a>. Our results show that dramatic forgetting is at play and that task difficulty and <a href=https://en.wikipedia.org/wiki/Order_and_disorder>order</a> matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1354.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1354 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1354 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384961600 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1354" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1354/>Assessing the Ability of Self-Attention Networks to Learn Word Order</a></strong><br><a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/l/lidia-s-chao/>Lidia S. Chao</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1354><div class="card-body p-3 small">Self-attention networks (SAN) have attracted a lot of interests due to their high <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallelization</a> and strong performance on a variety of NLP tasks, e.g. machine translation. Due to the lack of <a href=https://en.wikipedia.org/wiki/Recurrence_relation>recurrence structure</a> such as <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a>, SAN is ascribed to be weak at learning positional information of words for sequence modeling. However, neither this speculation has been empirically confirmed, nor explanations for their strong performances on machine translation tasks when lacking positional information have been explored. To this end, we propose a novel word reordering detection task to quantify how well the word order information learned by <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>SAN</a> and <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>RNN</a>. Specifically, we randomly move one word to another position, and examine whether a trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can detect both the original and inserted positions. Experimental results reveal that : 1) SAN trained on word reordering detection indeed has difficulty learning the positional information even with the position embedding ; and 2) SAN trained on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> learns better positional information than its RNN counterpart, in which position embedding plays a critical role. Although <a href=https://en.wikipedia.org/wiki/Recurrence_relation>recurrence structure</a> make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> more universally-effective on learning <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>, learning objectives matter more in the downstream tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1355 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/384787604 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1355" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1355/>Energy and Policy Considerations for <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/a/ananya-ganesh/>Ananya Ganesh</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1355><div class="card-body p-3 small">Recent progress in hardware and methodology for training <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> has ushered in a new generation of large networks trained on abundant data. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have obtained notable gains in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> across many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. However, these <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. As a result these models are costly to train and develop, both financially, due to the cost of <a href=https://en.wikipedia.org/wiki/Computer_hardware>hardware</a> and electricity or cloud compute time, and environmentally, due to the <a href=https://en.wikipedia.org/wiki/Carbon_footprint>carbon footprint</a> required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1359.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1359 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1359 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1359/>Generating Responses with a Specific Emotion in Dialog</a></strong><br><a href=/people/z/zhenqiao-song/>Zhenqiao Song</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/l/lu-liu/>Lu Liu</a>
|
<a href=/people/m/mu-xu/>Mu Xu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1359><div class="card-body p-3 small">It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their <a href=https://en.wikipedia.org/wiki/Usability>usability</a> and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words ; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, diversity and the quality of emotional expression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1360 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1360" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1360/>Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/p/pengda-qin/>Pengda Qin</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1360><div class="card-body p-3 small">Semantically controlled neural response generation on <a href=https://en.wikipedia.org/wiki/Domain-specific_language>limited-domain</a> has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph</a>. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1365 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1365" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1365/>Comparison of Diverse Decoding Methods from <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>Conditional Language Models</a></a></strong><br><a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/r/reno-kriz/>Reno Kriz</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/m/maria-kustikova/>Maria Kustikova</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1365><div class="card-body p-3 small">While conditional language models have greatly improved in their ability to output high quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that rerank and combine candidate outputs. Standard decoding methods, such as <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from a <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>conditional language model</a>. In addition, we present a novel method where we over-sample candidates, then use <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> to remove similar sequences, thus achieving high diversity without sacrificing quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1366 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1366/>Retrieval-Enhanced Adversarial Training for Neural Response Generation</a></strong><br><a href=/people/q/qingfu-zhu/>Qingfu Zhu</a>
|
<a href=/people/l/lei-cui/>Lei Cui</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1366><div class="card-body p-3 small">Dialogue systems are usually built on either generation-based or retrieval-based approaches, yet they do not benefit from the advantages of different models. In this paper, we propose a Retrieval-Enhanced Adversarial Training (REAT) method for neural response generation. Distinct from existing approaches, the REAT method leverages an encoder-decoder framework in terms of an adversarial training paradigm, while taking advantage of N-best response candidates from a retrieval-based system to construct the discriminator. An empirical study on a large scale public available benchmark dataset shows that the REAT method significantly outperforms the vanilla Seq2Seq model as well as the conventional adversarial training approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1367 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1367/>Vocabulary Pyramid Network : Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation</a></strong><br><a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1367><div class="card-body p-3 small">We study the task of response generation. Conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence. To tackle the above two problems, we present a Vocabulary Pyramid Network (VPN) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation. Specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words. Then, multi-pass encoding and decoding are conducted on the multi-level vocabularies. Since <a href=https://en.wikipedia.org/wiki/Virtual_private_network>VPN</a> is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses. Experiments on English Twitter and Chinese Weibo datasets demonstrate that <a href=https://en.wikipedia.org/wiki/Virtual_private_network>VPN</a> remarkably outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1368 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1368/>On-device Structured and Context Partitioned Projection Networks</a></strong><br><a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1368><div class="card-body p-3 small">A challenging problem in on-device text classification is to build highly accurate neural models that can fit in small memory footprint and have <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>low latency</a>. To address this challenge, we propose an on-device neural network SGNN++ which dynamically learns compact projection vectors from raw text using structured and context-dependent partition projections. We show that this results in accelerated inference and performance improvements. We conduct extensive evaluation on multiple conversational tasks and languages such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. Our SGNN++ model significantly outperforms all baselines, improves upon existing on-device neural models and even surpasses <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNN</a>, CNN and BiLSTM models on dialog act and intent prediction. Through a series of ablation studies we show the impact of the partitioned projections and structured information leading to 10 % improvement. We study the impact of the model size on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and introduce quatization-aware training for SGNN++ to further reduce the model size while preserving the same quality. Finally, we show fast <a href=https://en.wikipedia.org/wiki/Inference>inference</a> on <a href=https://en.wikipedia.org/wiki/Mobile_phone>mobile phones</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1369.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1369 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1369 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1369/>Proactive Human-Machine Conversation with Explicit Conversation Goal</a></strong><br><a href=/people/w/wenquan-wu/>Wenquan Wu</a>
|
<a href=/people/z/zhen-guo/>Zhen Guo</a>
|
<a href=/people/x/xiangyang-zhou/>Xiangyang Zhou</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/x/xiyuan-zhang/>Xiyuan Zhang</a>
|
<a href=/people/r/rongzhong-lian/>Rongzhong Lian</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1369><div class="card-body p-3 small">Though great progress has been made for human-machine conversation, current dialogue system is still in its infancy : it usually converses passively and utters words more as a matter of response, rather than on its own initiatives. In this paper, we take a radical step towards building a human-like conversational agent : endowing it with the ability of proactively leading the conversation (introducing a new topic or maintaining the current topic). To facilitate the development of such conversation systems, we create a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> named Konv where one acts as a conversation leader and the other acts as the follower. The leader is provided with a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. Konv enables a very challenging task as the model needs to both understand dialogue and plan over the given <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. We establish baseline results on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> (about 270 K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> along with the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1371.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1371 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1371 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1371" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1371/>Learning to Abstract for Memory-augmented Conversational Response Generation</a></strong><br><a href=/people/z/zhiliang-tian/>Zhiliang Tian</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/x/xiaopeng-li/>Xiaopeng Li</a>
|
<a href=/people/n/nevin-l-zhang/>Nevin L. Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1371><div class="card-body p-3 small">Neural generative models for open-domain chit-chat conversations have become an active area of research in recent years. A critical issue with most existing <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> is that the generated responses lack informativeness and <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a>. A few researchers attempt to leverage the results of retrieval models to strengthen the generative models, but these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are limited by the quality of the retrieval results. In this work, we propose a memory-augmented generative model, which learns to abstract from the training corpus and saves the useful information to the <a href=https://en.wikipedia.org/wiki/Memory>memory</a> to assist the response generation. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> clusters query-response samples, extracts characteristics of each cluster, and learns to utilize these characteristics for response generation. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other competitive baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1374 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1374.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1374.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1374" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1374/>A Large-Scale Corpus for Conversation Disentanglement</a></strong><br><a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/s/sai-r-gouravajhala/>Sai R. Gouravajhala</a>
|
<a href=/people/j/joseph-j-peper/>Joseph J. Peper</a>
|
<a href=/people/v/vignesh-athreya/>Vignesh Athreya</a>
|
<a href=/people/c/chulaka-gunasekara/>Chulaka Gunasekara</a>
|
<a href=/people/j/jatin-ganhotra/>Jatin Ganhotra</a>
|
<a href=/people/s/siva-sankalp-patel/>Siva Sankalp Patel</a>
|
<a href=/people/l/lazaros-c-polymenakos/>Lazaros C Polymenakos</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1374><div class="card-body p-3 small">Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our <a href=https://en.wikipedia.org/wiki/Data>data</a> is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. We use our data to re-examine prior work, in particular, finding that 89 % of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1375.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1375 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1375 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1375/>Self-Supervised Dialogue Learning</a></strong><br><a href=/people/j/jiawei-wu/>Jiawei Wu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1375><div class="card-body p-3 small">The sequential order of utterances is often meaningful in coherent dialogues, and the order changes of utterances could lead to low-quality and incoherent conversations. We consider the order information as a crucial supervised signal for dialogue learning, which, however, has been neglected by many previous <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. Therefore, in this paper, we introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the flow of conversation in dialogues. Given a sampled utterance pair triple, the task is to predict whether it is ordered or misordered. Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history. Furthermore, we design a joint learning framework where <a href=https://en.wikipedia.org/wiki/Social_security_number>SSN</a> can guide the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1376 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1376/>Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection<span class=acl-fixed-case>E</span>nglish past tense inflection</a></strong><br><a href=/people/m/maria-corkery/>Maria Corkery</a>
|
<a href=/people/y/yevgen-matusevych/>Yevgen Matusevych</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1376><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Cognition>cognitive mechanisms</a> needed to account for the English past tense have long been a subject of debate in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> and <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive science</a>. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in this task. We find that (1) the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strongworse than an older rule-based model. These findings hold up through several alternative training regimes and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation measures</a>. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> are a good <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive model</a> for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1377.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1377 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1377 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1377/>A Spreading Activation Framework for Tracking Conceptual Complexity of Texts</a></strong><br><a href=/people/i/ioana-hulpus/>Ioana Hulpuș</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/h/heiner-stuckenschmidt/>Heiner Stuckenschmidt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1377><div class="card-body p-3 small">We propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for assessing conceptual complexity of texts, based on <a href=https://en.wikipedia.org/wiki/Spreading_activation>spreading activation</a>. Using DBpedia knowledge graph as a proxy to <a href=https://en.wikipedia.org/wiki/Long-term_memory>long-term memory</a>, mentioned concepts become activated and trigger further activation as the text is sequentially traversed. Drawing inspiration from psycholinguistic theories of reading comprehension, we model memory processes such as <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>semantic priming</a>, sentence wrap-up, and <a href=https://en.wikipedia.org/wiki/Forgetting>forgetting</a>. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capture various aspects of conceptual text complexity and significantly outperform current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1379 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1379/>Diachronic Sense Modeling with Deep Contextualized Word Embeddings : An Ecological View</a></strong><br><a href=/people/r/renfen-hu/>Renfen Hu</a>
|
<a href=/people/s/shen-li/>Shen Li</a>
|
<a href=/people/s/shichen-liang/>Shichen Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1379><div class="card-body p-3 small">Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of <a href=https://en.wikipedia.org/wiki/Evolutionary_linguistics>language evolution</a>, i.e. sense competition and sense cooperation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1380 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1380.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1380/>Miss Tools and Mr Fruit : Emergent Communication in Agents Learning about Object Affordances</a></strong><br><a href=/people/d/diane-bouchacourt/>Diane Bouchacourt</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1380><div class="card-body p-3 small">Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> solve the shared task through genuine bilateral, referential communication. However, the <a href=https://en.wikipedia.org/wiki/Agency_(philosophy)>agents</a> develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a <a href=https://en.wikipedia.org/wiki/Natural_language>common language</a> to emerge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1381 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1381/>CNNs found to jump around more skillfully than RNNs : Compositional Generalization in Seq2seq Convolutional Networks<span class=acl-fixed-case>CNN</span>s found to jump around more skillfully than <span class=acl-fixed-case>RNN</span>s: Compositional Generalization in Seq2seq Convolutional Networks</a></strong><br><a href=/people/r/roberto-dessi/>Roberto Dessì</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1381><div class="card-body p-3 small">Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of jump around 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging <a href=https://en.wikipedia.org/wiki/Generalization>generalization cases</a>. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1385 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1385" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1385/>Attention-based Conditioning Methods for External Knowledge Integration</a></strong><br><a href=/people/k/katerina-margatina/>Katerina Margatina</a>
|
<a href=/people/c/christos-baziotis/>Christos Baziotis</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1385><div class="card-body p-3 small">In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distribution</a>, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and <a href=https://en.wikipedia.org/wiki/Affine_transformation>affine transformation</a>. Experiments on six <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1391 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1391.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1391/>Crowdsourcing and Validating Event-focused Emotion Corpora for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and English<span class=acl-fixed-case>G</span>erman and <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/e/enrica-troiano/>Enrica Troiano</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1391><div class="card-body p-3 small">Sentiment analysis has a range of corpora available across multiple languages. For emotion analysis, the situation is more limited, which hinders potential research on crosslingual modeling and the development of <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> for other languages. In this paper, we fill this gap for <a href=https://en.wikipedia.org/wiki/German_language>German</a> by constructing deISEAR, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> designed in analogy to the well-established English ISEAR emotion dataset. Motivated by Scherer&#8217;s appraisal theory, we implement a crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of <a href=https://en.wikipedia.org/wiki/Emotion>emotional events</a> for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> does not, on average, cause a performance drop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1393 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1393" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1393/>Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation</a></strong><br><a href=/people/c/cunxiang-wang/>Cunxiang Wang</a>
|
<a href=/people/s/shuailong-liang/>Shuailong Liang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/x/xiaonan-li/>Xiaonan Li</a>
|
<a href=/people/t/tian-gao/>Tian Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1393><div class="card-body p-3 small">Introducing <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a> to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding systems</a> has received increasing research attention. It remains a fundamental question on how to evaluate whether a <a href=https://en.wikipedia.org/wiki/System>system</a> has the sense-making capability. Existing <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> measure <a href=https://en.wikipedia.org/wiki/Common_sense>common sense knowledge</a> indirectly or without <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. In this paper, we release a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> to directly test whether a <a href=https://en.wikipedia.org/wiki/System>system</a> can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1397 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1397/>Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning</a></strong><br><a href=/people/s/shuai-tang/>Shuai Tang</a>
|
<a href=/people/v/virginia-r-de-sa/>Virginia R. de Sa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1397><div class="card-body p-3 small">Encoder-decoder models for unsupervised sentence representation learning using the <a href=https://en.wikipedia.org/wiki/Distributional_hypothesis>distributional hypothesis</a> effectively constrain the learnt representation of a sentence to only that needed to reproduce the next sentence. While the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a> is important to constrain the representation, these models tend to discard the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a> after training since only the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> is needed to map the input sentence into a <a href=https://en.wikipedia.org/wiki/Vector_space>vector representation</a>. However, parameters learnt in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> also contain useful information about the language. In order to utilise the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> after learning, we present two types of decoding functions whose inverse can be easily derived without expensive inverse calculation. Therefore, the inverse of the decoding function serves as another <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> that produces <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a>. We show that, with careful design of the decoding functions, the model learns good sentence representations, and the ensemble of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> produced from the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and the inverse of the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a> demonstrate even better generalisation ability and solid transferability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1402 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1402" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1402/>Few-Shot Representation Learning for Out-Of-Vocabulary Words</a></strong><br><a href=/people/z/ziniu-hu/>Ziniu Hu</a>
|
<a href=/people/t/ting-chen/>Ting Chen</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/y/yizhou-sun/>Yizhou Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1402><div class="card-body p-3 small">Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> for OOV words and improves downstream tasks when the <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> is utilized.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1407 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1407/>Keeping Notes : Conditional Natural Language Generation with a Scratchpad Encoder</a></strong><br><a href=/people/r/ryan-benmalek/>Ryan Benmalek</a>
|
<a href=/people/m/madian-khabsa/>Madian Khabsa</a>
|
<a href=/people/s/suma-desu/>Suma Desu</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a>
|
<a href=/people/m/michele-banko/>Michele Banko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1407><div class="card-body p-3 small">We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder output layers</a>, <a href=https://en.wikipedia.org/wiki/Scratchpad>Scratchpad</a> can employ the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> as a <a href=https://en.wikipedia.org/wiki/Scratchpad_memory>scratchpad memory</a> to keep track of what has been generated so far and thereby guide future generation. We evaluate <a href=https://en.wikipedia.org/wiki/Scratchpad>Scratchpad</a> in the context of three well-studied natural language generation tasks <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, Question Generation, and Text Summarization and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of <a href=https://en.wikipedia.org/wiki/Scratchpad>Scratchpad</a> to generate fluent and expressive output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1408 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385196786 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1408" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1408/>Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/l/leo-born/>Leo Born</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1408><div class="card-body p-3 small">The common practice in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> is to identify and evaluate the maximum span of mentions. The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. To address this problem, minimum spans are manually annotated in smaller corpora. However, this additional annotation is costly and therefore, this solution does not scale to large corpora. In this paper, we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts. Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1410 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1410.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385254497 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1410" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1410/>A Unified Linear-Time Framework for Sentence-Level Discourse Parsing</a></strong><br><a href=/people/x/xiang-lin/>Xiang Lin</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/p/prathyusha-jwalapuram/>Prathyusha Jwalapuram</a>
|
<a href=/people/m/m-saiful-bari/>M Saiful Bari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1410><div class="card-body p-3 small">We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. Both the segmenter and the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> are based on Pointer Networks and operate in <a href=https://en.wikipedia.org/wiki/Time_complexity>linear time</a>. Our segmenter yields an F1 score of 95.4 %, and our parser achieves an F1 score of 81.7 % on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1411 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385254810 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1411/>Employing the Correspondence of Relations and Connectives to Identify Implicit Discourse Relations via Label Embeddings</a></strong><br><a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/l/linh-van-ngo/>Linh Van Ngo</a>
|
<a href=/people/k/khoat-than/>Khoat Than</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1411><div class="card-body p-3 small">It has been shown that implicit connectives can be exploited to improve the performance of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relations</a> conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> and the <a href=https://en.wikipedia.org/wiki/Binary_relation>connectives</a> are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (i.e., the Penn Discourse Treebank dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1417 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385198664 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1417" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1417/>Improving <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> over Incomplete KBs with Knowledge-Aware Reader<span class=acl-fixed-case>KB</span>s with Knowledge-Aware Reader</a></strong><br><a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/x/xiaoxiao-guo/>Xiaoxiao Guo</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1417><div class="card-body p-3 small">We propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (KB) and a set of retrieved text snippets. Under the assumptions that structured data is easier to query and the acquired knowledge can help the understanding of unstructured text, our model first accumulates knowledge ofKB entities from a question-related KB sub-graph ; then reformulates the question in the latent space and reads the text with the accumulated entity knowledge at hand. The evidence from KB and <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> are finally aggregated to predict answers. On the widely-used KBQA benchmark WebQSP, our model achieves consistent improvements across settings with different extents of KB incompleteness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1418.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1418/>AdaNSP : Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing<span class=acl-fixed-case>A</span>da<span class=acl-fixed-case>NSP</span>: Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing</a></strong><br><a href=/people/x/xiang-zhang/>Xiang Zhang</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1418><div class="card-body p-3 small">Neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation. To keep the model aware of the underlying grammar in target sequences, many constrained decoders were devised in a multi-stage paradigm, which decode to the sketches or abstract syntax trees first, and then decode to target semantic tokens. We instead to propose an adaptive decoding method to avoid such intermediate representations. The <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> is guided by model uncertainty and automatically uses deeper computations when necessary. Thus <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can predict tokens adaptively. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or <a href=https://en.wikipedia.org/wiki/Sketch_(drawing)>sketches</a> in the meantime.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1419 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1419.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1419.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385198774 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1419" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1419/>The Language of Legal and Illegal Activity on the <a href=https://en.wikipedia.org/wiki/Darknet>Darknet</a><span class=acl-fixed-case>D</span>arknet</a></strong><br><a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/d/dan-eldad/>Dan Eldad</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/e/elior-sulem/>Elior Sulem</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1419><div class="card-body p-3 small">The non-indexed parts of the Internet (the Darknet) have become a haven for both legal and illegal anonymous activity. Given the magnitude of these <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a>, scalably monitoring their activity necessarily relies on automated tools, and notably on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tools</a>. However, little is known about what characteristics texts communicated through the <a href=https://en.wikipedia.org/wiki/Darknet>Darknet</a> have, and how well do off-the-shelf NLP tools do on this domain. This paper tackles this gap and performs an in-depth investigation of the characteristics of legal and illegal text in the <a href=https://en.wikipedia.org/wiki/Darknet>Darknet</a>, comparing it to a clear net website with similar content as a control condition. Taking drugs-related websites as a test case, we find that texts for selling legal and illegal drugs have several linguistic characteristics that distinguish them from one another, as well as from the control condition, among them the distribution of POS tags, and the coverage of their named entities in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1425 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1425/>Robust Neural Machine Translation with Doubly Adversarial Inputs</a></strong><br><a href=/people/y/yong-cheng/>Yong Cheng</a>
|
<a href=/people/l/lu-jiang/>Lu Jiang</a>
|
<a href=/people/w/wolfgang-macherey/>Wolfgang Macherey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1425><div class="card-body p-3 small">Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of NMT models, which consists of two parts : (1) attack the translation model with adversarial source examples ; (2) defend the translation model with adversarial target inputs to improve its <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> against the adversarial source inputs. For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs. Experimental results on Chinese-English and English-German translation tasks demonstrate that our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> achieves significant improvements (2.8 and 1.6 BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> on noisy data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1431 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385264668 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1431/>A2N : Attending to Neighbors for Knowledge Graph Inference<span class=acl-fixed-case>A</span>2<span class=acl-fixed-case>N</span>: Attending to Neighbors for Knowledge Graph Inference</a></strong><br><a href=/people/t/trapit-bansal/>Trapit Bansal</a>
|
<a href=/people/d/da-cheng-juan/>Da-Cheng Juan</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1431><div class="card-body p-3 small">State-of-the-art models for knowledge graph completion aim at learning a fixed embedding representation of entities in a multi-relational graph which can generalize to infer unseen entity relationships at test time. This can be sub-optimal as it requires memorizing and generalizing to all possible entity relationships using these fixed representations. We thus propose a novel attention-based method to learn query-dependent representation of entities which adaptively combines the relevant graph neighborhood of an entity leading to more accurate KG completion. The proposed method is evaluated on two benchmark datasets for knowledge graph completion, and experimental results show that the proposed model performs competitively or better than existing <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, including recent methods for explicit multi-hop reasoning. Qualitative probing offers insight into how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can reason about facts involving multiple hops in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>, through the use of neighborhood attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1432 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385264738 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1432/>Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures</a></strong><br><a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1432><div class="card-body p-3 small">Event factuality prediction (EFP) is the task of assessing the degree to which an event mentioned in a sentence has happened. For this task, both syntactic and semantic information are crucial to identify the important <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context words</a>. The previous work for EFP has only combined these information in a simple way that can not fully exploit their coordination. In this work, we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively. Our experiments demonstrate the advantage of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for EFP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1433 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385203861 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1433" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1433/>Embedding Time Expressions for Deep Temporal Ordering Models</a></strong><br><a href=/people/t/tanya-goyal/>Tanya Goyal</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1433><div class="card-body p-3 small">Data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often overlook explicit temporal signals, such as <a href=https://en.wikipedia.org/wiki/Calendar_date>dates</a> and time windows. Rule-based methods can be used to identify the temporal links between these time expressions (timexes), but they fail to capture timexes&#8217; interactions with events and are hard to integrate with the distributed representations of neural net models. In this paper, we introduce a framework to infuse temporal awareness into such <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> by learning a pre-trained model to embed timexes. We generate synthetic data consisting of pairs of <a href=https://en.wikipedia.org/wiki/Ternary_numeral_system>timexes</a>, then train a character LSTM to learn embeddings and classify the <a href=https://en.wikipedia.org/wiki/Ternary_numeral_system>timexes&#8217; temporal relation</a>. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1440 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1440" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1440/>Complex Question Decomposition for Semantic Parsing</a></strong><br><a href=/people/h/haoyu-zhang/>Haoyu Zhang</a>
|
<a href=/people/j/jingjing-cai/>Jingjing Cai</a>
|
<a href=/people/j/jianjun-xu/>Jianjun Xu</a>
|
<a href=/people/j/ji-wang/>Ji Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1440><div class="card-body p-3 small">In this work, we focus on complex question semantic parsing and propose a novel Hierarchical Semantic Parsing (HSP) method, which utilizes the decompositionality of complex questions for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is designed within a three-stage parsing architecture based on the idea of decomposition-integration. In the first stage, we propose a question decomposer which decomposes a complex question into a sequence of sub-questions. In the second stage, we design an <a href=https://en.wikipedia.org/wiki/Information_extraction>information extractor</a> to derive the type and predicate information of these questions. In the last stage, we integrate the generated information from previous stages and generate a <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> for the complex question. We conduct experiments on COMPLEXWEBQUESTIONS which is a large scale complex question semantic parsing dataset, results show that our model achieves significant improvement compared to state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1444 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1444.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1444" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1444/>Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation<span class=acl-fixed-case>SQL</span> in Cross-Domain Database with Intermediate Representation</a></strong><br><a href=/people/j/jiaqi-guo/>Jiaqi Guo</a>
|
<a href=/people/z/zecheng-zhan/>Zecheng Zhan</a>
|
<a href=/people/y/yan-gao/>Yan Gao</a>
|
<a href=/people/y/yan-xiao/>Yan Xiao</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1444><div class="card-body p-3 small">We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges : 1) the mismatch between intents expressed in natural language (NL) and the implementation details in <a href=https://en.wikipedia.org/wiki/SQL>SQL</a> ; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a>, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a <a href=https://en.wikipedia.org/wiki/Database_schema>database schema</a>. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NL</a> and <a href=https://en.wikipedia.org/wiki/SQL>SQL</a>. Finally, IRNet deterministically infers a <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a> from the synthesized SemQL query with <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, obtaining 19.5 % absolute improvement over previous <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>state-of-the-art approaches</a>. At the time of writing, IRNet achieves the first position on the Spider leaderboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1447 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1447/>Reranking for Neural Semantic Parsing</a></strong><br><a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1447><div class="card-body p-3 small">Semantic parsing considers the task of transducing natural language (NL) utterances into machine executable meaning representations (MRs). While neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs. This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are designed to fix observed problems with baseline models. We implement our reranker in a competitive neural semantic parser and test on four semantic parsing (GEO, ATIS) and Python code generation (Django, CoNaLa) tasks, improving the strong baseline parser by up to 5.7 % absolute in BLEU (CoNaLa) and 2.9 % in accuracy (Django), outperforming the best published neural parser results on all four datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1448 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1448.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1448" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1448/>Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing<span class=acl-fixed-case>SQL</span> Parsing</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1448><div class="card-body p-3 small">Research on <a href=https://en.wikipedia.org/wiki/Parsing>parsing language</a> to <a href=https://en.wikipedia.org/wiki/SQL>SQL</a> has largely ignored the structure of the <a href=https://en.wikipedia.org/wiki/Database_schema>database (DB) schema</a>, either because the <a href=https://en.wikipedia.org/wiki/Database>DB</a> was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the <a href=https://en.wikipedia.org/wiki/Database_schema>DB schema</a> can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the <a href=https://en.wikipedia.org/wiki/Database_schema>DB schema</a> is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our <a href=https://en.wikipedia.org/wiki/Parsing>parser accuracy</a> from 33.8 % to 39.4 %, dramatically above the current state of the art, which is at 19.7 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1449 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1449/>Human vs. Muppet : A Conservative Estimate of Human Performance on the GLUE Benchmark<span class=acl-fixed-case>GLUE</span> Benchmark</a></strong><br><a href=/people/n/nikita-nangia/>Nikita Nangia</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1449><div class="card-body p-3 small">The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a>, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> : Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these <a href=https://en.wikipedia.org/wiki/Annotation>annotators</a> robustly outperform the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1450 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1450.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1450" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1450/>Compositional Semantic Parsing across Graphbanks</a></strong><br><a href=/people/m/matthias-lindemann/>Matthias Lindemann</a>
|
<a href=/people/j/jonas-groschwitz/>Jonas Groschwitz</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1450><div class="card-body p-3 small">Most semantic parsers that map sentences to graph-based meaning representations are hand-designed for specific graphbanks. We present a compositional neural semantic parser which achieves, for the first time, competitive accuracies across a diverse range of graphbanks. Incorporating BERT embeddings and multi-task learning improves the accuracy further, setting new states of the art on DM, PAS, PSD, AMR 2015 and EDS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1452.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1452 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1452 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1452/>BERT Rediscovers the Classical NLP Pipeline<span class=acl-fixed-case>BERT</span> Rediscovers the Classical <span class=acl-fixed-case>NLP</span> Pipeline</a></strong><br><a href=/people/i/ian-tenney/>Ian Tenney</a>
|
<a href=/people/d/dipanjan-das/>Dipanjan Das</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1452><div class="card-body p-3 small">Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, BERT, and aim to quantify where linguistic information is captured within the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a>. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence : POS tagging, <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, NER, semantic roles, then <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>. Qualitative analysis reveals that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can and often does adjust this <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1454.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1454 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1454 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1454" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1454/>Second-Order Semantic Dependency Parsing with End-to-End Neural Networks</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/j/jingxian-huang/>Jingxian Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1454><div class="card-body p-3 small">Semantic dependency parsing aims to identify semantic relationships between words in a sentence that form a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. In this paper, we propose a second-order semantic dependency parser, which takes into consideration not only individual dependency edges but also interactions between pairs of <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a>. We show that second-order parsing can be approximated using mean field (MF) variational inference or loopy belief propagation (LBP). We can unfold both <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> as recurrent layers of a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and therefore can train the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> in an end-to-end manner. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> achieves state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1455 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1455.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1455" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1455/>Towards Multimodal Sarcasm Detection (An _ Obviously _ Perfect Paper)<span class=acl-fixed-case>O</span>bviously_ Perfect Paper)</a></strong><br><a href=/people/s/santiago-castro/>Santiago Castro</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1455><div class="card-body p-3 small">Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a <a href=https://en.wikipedia.org/wiki/Tone_(linguistics)>change of tone</a>, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual data</a>. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD), compiled from popular TV shows. MUStARD consists of <a href=https://en.wikipedia.org/wiki/Audiovisual>audiovisual utterances</a> annotated with <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm labels</a>. Each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9 % in <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> when compared to the use of individual modalities. The full dataset is publicly available for use at https://github.com/soujanyaporia/MUStARD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1458.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1458 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1458 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1458/>An Investigation of Transfer Learning-Based Sentiment Analysis in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a><span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/e/enkhbold-bataa/>Enkhbold Bataa</a>
|
<a href=/people/j/joshua-wu/>Joshua Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1458><div class="card-body p-3 small">Text classification approaches have usually required task-specific model architectures and huge labeled datasets. Recently, thanks to the rise of text-based transfer learning techniques, it is possible to pre-train a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> in an unsupervised manner and leverage them to perform effective on downstream tasks. In this work we focus on <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and show the potential use of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning techniques</a> in <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Specifically, we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets. We show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data. Furthermore, these approaches perform just as well for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> pre-trained on only 1/30 of the data. We release our <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-trained models</a> and code as open source.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1459 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1459" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1459/>Probing Neural Network Comprehension of Natural Language Arguments</a></strong><br><a href=/people/t/timothy-niven/>Timothy Niven</a>
|
<a href=/people/h/hung-yu-kao/>Hung-Yu Kao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1459><div class="card-body p-3 small">We are surprised to find that BERT&#8217;s peak performance of 77 % on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We analyze the nature of these <a href=https://en.wikipedia.org/wiki/Sensory_cue>cues</a> and demonstrate that a range of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> all exploit them. This analysis informs the construction of an adversarial dataset on which all <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1461 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1461.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1461" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1461/>Toward Comprehensive Understanding of a Sentiment Based on Human Motives</a></strong><br><a href=/people/n/naoki-otani/>Naoki Otani</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1461><div class="card-body p-3 small">In sentiment detection, the natural language processing community has focused on determining holders, <a href=https://en.wikipedia.org/wiki/Facet_(psychology)>facets</a>, and <a href=https://en.wikipedia.org/wiki/Valence_(linguistics)>valences</a>, but has paid little attention to the reasons for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment decisions</a>. Our work considers <a href=https://en.wikipedia.org/wiki/Motivation>human motives</a> as the driver for <a href=https://en.wikipedia.org/wiki/Sentimentality>human sentiments</a> and addresses the problem of motive detection as the first step. Following a study in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a>, we define six basic motives that cover a wide range of topics appearing in review texts, annotate 1,600 texts in restaurant and laptop domains with the motives, and report the performance of baseline methods on this new dataset. We also show that cross-domain transfer learning boosts <a href=https://en.wikipedia.org/wiki/Detection_theory>detection</a> performance, which indicates that these universal motives exist across different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1462.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1462 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1462 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1462/>Context-aware Embedding for Targeted Aspect-based Sentiment Analysis</a></strong><br><a href=/people/b/bin-liang/>Bin Liang</a>
|
<a href=/people/j/jiachen-du/>Jiachen Du</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a>
|
<a href=/people/b/binyang-li/>Binyang Li</a>
|
<a href=/people/h/hejiao-huang/>Hejiao Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1462><div class="card-body p-3 small">Attention-based neural models were employed to detect the different aspects and sentiment polarities of the same target in targeted aspect-based sentiment analysis (TABSA). However, existing methods do not specifically pre-train reasonable <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for targets and aspects in TABSA. This may result in targets or aspects having the same <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> in different contexts and losing the context-dependent information. To address this problem, we propose a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to refine the embeddings of targets and aspects. Such pivotal embedding refinement utilizes a sparse coefficient vector to adjust the embeddings of target and aspect from the context. Hence the embeddings of targets and aspects can be refined from the highly correlative words instead of using context-independent or randomly initialized vectors. Experiment results on two benchmark datasets show that our approach yields the state-of-the-art performance in TABSA task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1466 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1466" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1466/>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</a></strong><br><a href=/people/d/deepak-nathani/>Deepak Nathani</a>
|
<a href=/people/j/jatin-chauhan/>Jatin Chauhan</a>
|
<a href=/people/c/charu-sharma/>Charu Sharma</a>
|
<a href=/people/m/manohar-kaul/>Manohar Kaul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1466><div class="card-body p-3 small">The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the <a href=https://en.wikipedia.org/wiki/Neighbourhood_(mathematics)>local neighborhood</a> surrounding a triple. To this effect, our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity&#8217;s neighborhood. Additionally, we also encapsulate relation clusters and multi-hop relations in our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1467 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1467/>Neural Network Alignment for Sentential Paraphrases</a></strong><br><a href=/people/j/jessica-ouyang/>Jessica Ouyang</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1467><div class="card-body p-3 small">We present a monolingual alignment system for long, sentence- or clause-level alignments, and demonstrate that systems designed for word- or short phrase-based alignment are ill-suited for these longer alignments. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is capable of aligning semantically similar spans of arbitrary length. We achieve significantly higher <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> on aligning phrases of four or more words and outperform state-of-the- art aligners on the long alignments in the MSR RTE corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1468 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1468" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1468/>Duality of Link Prediction and Entailment Graph Induction</a></strong><br><a href=/people/m/mohammad-javad-hosseini/>Mohammad Javad Hosseini</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1468><div class="card-body p-3 small">Link prediction and entailment graph induction are often treated as different problems. In this paper, we show that these two <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a> are actually complementary. We train a link prediction model on a knowledge graph of assertions extracted from raw text. We propose an <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment score</a> that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. We further use the learned <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailments</a> to predict improved link prediction scores. Our results show that the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> can benefit from each other. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1470.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1470 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1470 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1470" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1470/>COMET : Commonsense Transformers for Automatic Knowledge Graph Construction<span class=acl-fixed-case>COMET</span>: Commonsense Transformers for Automatic Knowledge Graph Construction</a></strong><br><a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1470><div class="card-body p-3 small">We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs : <a href=https://en.wikipedia.org/wiki/ATOMIC>ATOMIC</a> (Sap et al., 2019) and <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> (Speer et al., 2017). Contrary to many conventional <a href=https://en.wikipedia.org/wiki/Knowledge_base>KBs</a> that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when <a href=https://en.wikipedia.org/wiki/Implicit_knowledge>implicit knowledge</a> from deep pre-trained language models is transferred to generate <a href=https://en.wikipedia.org/wiki/Explicit_knowledge>explicit knowledge</a> in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5 % (ATOMIC) and 91.7 % (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1476.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1476 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1476 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1476" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1476/>Generalized Tuning of Distributional Word Vectors for Monolingual and Cross-Lingual Lexical Entailment</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1476><div class="card-body p-3 small">Lexical entailment (LE ; also known as <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy-hypernymy</a> or is-a relation) is a core asymmetric lexical relation that supports tasks like <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy induction</a> and text generation. In this work, we propose a simple and effective method for fine-tuning distributional word vectors for LE. Our Generalized Lexical ENtailment model (GLEN) is decoupled from the word embedding model and applicable to any distributional vector space. Yet unlike existing retrofitting models it captures a general specialization function allowing for LE-tuning of the entire distributional space and not only the vectors of words seen in lexical constraints. Coupled with a multilingual embedding space, GLEN seamlessly enables cross-lingual LE detection. We demonstrate the effectiveness of GLEN in graded LE and report large improvements (over 20 % in accuracy) over <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in cross-lingual LE detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1478.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1478 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1478 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1478/>A Surprisingly Robust Trick for the Winograd Schema Challenge<span class=acl-fixed-case>W</span>inograd Schema Challenge</a></strong><br><a href=/people/v/vid-kocijan/>Vid Kocijan</a>
|
<a href=/people/a/ana-maria-cretu/>Ana-Maria Cretu</a>
|
<a href=/people/o/oana-maria-camburu/>Oana-Maria Camburu</a>
|
<a href=/people/y/yordan-yordanov/>Yordan Yordanov</a>
|
<a href=/people/t/thomas-lukasiewicz/>Thomas Lukasiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1478><div class="card-body p-3 small">The Winograd Schema Challenge (WSC) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. In this paper, we show that the performance of three language models on WSC273 consistently and robustly improves when fine-tuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSC-like dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.5 % and 74.7 % on WSC273 and WNLI, improving the previous state-of-the-art solutions by 8.8 % and 9.6 %, respectively. Furthermore, our fine-tuned models are also consistently more accurate on the complex subsets of WSC273, introduced by Trichelair et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1482 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385265051 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1482/>A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer</a></strong><br><a href=/people/c/chen-wu/>Chen Wu</a>
|
<a href=/people/x/xuancheng-ren/>Xuancheng Ren</a>
|
<a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1482><div class="card-body p-3 small">Unsupervised text style transfer aims to alter <a href=https://en.wikipedia.org/wiki/Style_guide>text styles</a> while preserving the content, without aligned data for <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. Existing seq2seq methods face three challenges : 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence operation method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental results on two text style transfer datasets show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly outperforms recent methods and effectively addresses the aforementioned challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1487 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385213801 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1487/>Explain Yourself ! Leveraging <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Commonsense Reasoning</a></strong><br><a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a>
|
<a href=/people/b/bryan-mccann/>Bryan McCann</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1487><div class="card-body p-3 small">Deep learning models perform poorly on tasks that require <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> in the form of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language sequences</a> and highlighted annotations in a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10 % on the challenging CommonsenseQA task. We further study <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> for <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1488 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385215761 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1488/>Interpretable Question Answering on <a href=https://en.wikipedia.org/wiki/Knowledge_base>Knowledge Bases</a> and Text</a></strong><br><a href=/people/a/alona-sydorova/>Alona Sydorova</a>
|
<a href=/people/n/nina-poerner/>Nina Poerner</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1488><div class="card-body p-3 small">Interpretability of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning (ML) models</a> becomes more relevant with their increasing adoption. In this work, we address the <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> of <a href=https://en.wikipedia.org/wiki/Question_answering>ML based question answering (QA) models</a> on a combination of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases (KB)</a> and text documents. We adapt post hoc explanation methods such as LIME and input perturbation (IP) and compare them with the self-explanatory attention mechanism of the model. For this purpose, we propose an automatic evaluation paradigm for explanation methods in the context of <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>. We also conduct a study with human annotators to evaluate whether explanations help them identify better QA models. Our results suggest that IP provides better explanations than LIME or <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, according to both automatic and human evaluation. We obtain the same ranking of methods in both experiments, which supports the validity of our automatic evaluation paradigm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1490.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1490 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1490 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1490.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385216016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1490/>Multilingual and Cross-Lingual Graded Lexical Entailment</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1490><div class="card-body p-3 small">Grounded in <a href=https://en.wikipedia.org/wiki/Cognitive_linguistics>cognitive linguistics</a>, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a>, CLEAR leverages <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomic LE knowledge</a> in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on binary cross-lingual LE detection by a wide margin for diverse language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1492.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1492 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1492 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385218856 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1492/>Analyzing the Limitations of Cross-lingual Word Embedding Mappings</a></strong><br><a href=/people/a/aitor-ormazabal/>Aitor Ormazabal</a>
|
<a href=/people/m/mikel-artetxe/>Mikel Artetxe</a>
|
<a href=/people/g/gorka-labaka/>Gorka Labaka</a>
|
<a href=/people/a/aitor-soroa/>Aitor Soroa</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1492><div class="card-body p-3 small">Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in different languages and map them to a shared space through <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a>. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more <a href=https://en.wikipedia.org/wiki/Isomorphism>isomorphic embeddings</a>, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1494.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1494 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1494 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385219132 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1494" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1494/>Bilingual Lexicon Induction through Unsupervised Machine Translation</a></strong><br><a href=/people/m/mikel-artetxe/>Mikel Artetxe</a>
|
<a href=/people/g/gorka-labaka/>Gorka Labaka</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1494><div class="card-body p-3 small">A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. In this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation. This way, instead of directly inducing a <a href=https://en.wikipedia.org/wiki/Bilingual_lexicon>bilingual lexicon</a> from cross-lingual embeddings, we use them to build a phrase-table, combine it with a language model, and use the resulting machine translation system to generate a synthetic parallel corpus, from which we extract the <a href=https://en.wikipedia.org/wiki/Bilingual_lexicon>bilingual lexicon</a> using statistical word alignment techniques. As such, our method can work with any <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> and cross-lingual mapping technique, and it does not require any additional resource besides the monolingual corpus used to train the embeddings. When evaluated on the exact same cross-lingual embeddings, our proposed method obtains an average improvement of 6 accuracy points over <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor</a> and 4 points over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1496.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1496 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1496 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385272712 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1496/>TWEETQA : A Social Media Focused Question Answering Dataset<span class=acl-fixed-case>TWEETQA</span>: A Social Media Focused Question Answering Dataset</a></strong><br><a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/j/jiawei-wu/>Jiawei Wu</a>
|
<a href=/people/h/hong-wang/>Hong Wang</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/x/xiaoxiao-guo/>Xiaoxiao Guo</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1496><div class="card-body p-3 small">With <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets have concentrated on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering (QA)</a> for formal text like <a href=https://en.wikipedia.org/wiki/News>news</a> and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, we present the first large-scale dataset for QA over <a href=https://en.wikipedia.org/wiki/Social_media>social media data</a>. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. We then ask human annotators to write questions and answers upon these tweets. Unlike otherQA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive. We show that two recently proposed neural models that perform well on <a href=https://en.wikipedia.org/wiki/Formal_language>formal texts</a> are limited in their performance when applied to our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. In addition, even the fine-tuned BERT model is still lagging behind human performance with a large margin. Our results thus point to the need of improved <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a> targeting <a href=https://en.wikipedia.org/wiki/Social_media>social media text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1498.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1498 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1498 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385272871 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1498/>Tree LSTMs with Convolution Units to Predict Stance and Rumor Veracity in Social Media Conversations<span class=acl-fixed-case>LSTM</span>s with Convolution Units to Predict Stance and Rumor Veracity in Social Media Conversations</a></strong><br><a href=/people/s/sumeet-kumar/>Sumeet Kumar</a>
|
<a href=/people/k/kathleen-m-carley/>Kathleen Carley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1498><div class="card-body p-3 small">Learning from social-media conversations has gained significant attention recently because of its applications in areas like rumor detection. In this research, we propose a new way to represent social-media conversations as binarized constituency trees that allows comparing features in source-posts and their replies effectively. Moreover, we propose to use convolution units in Tree LSTMs that are better at learning patterns in <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> obtained from the source and reply posts. Our Tree LSTM models employ multi-task (stance + rumor) learning and propagate the useful stance signal up in the tree for rumor classification at the root node. The proposed models achieve state-of-the-art performance, outperforming the current best model by 12 % and 15 % on F1-macro for rumor-veracity classification and stance classification tasks respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1503 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385219323 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1503" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1503/>Simple Unsupervised Summarization by Contextual Matching</a></strong><br><a href=/people/j/jiawei-zhou/>Jiawei Zhou</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1503><div class="card-body p-3 small">We propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> for sentence summarization using only <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. The approach employs two <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, <a href=https://en.wikipedia.org/wiki/Monotonic_function>one</a> that is generic (i.e. pretrained), and the <a href=https://en.wikipedia.org/wiki/Other_(philosophy)>other</a> that is specific to the target domain. We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1507 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1507.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385446129 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1507/>Relating Simple Sentence Representations in <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Neural Networks</a> and the Brain</a></strong><br><a href=/people/s/sharmistha-jat/>Sharmistha Jat</a>
|
<a href=/people/h/hao-tang/>Hao Tang</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1507><div class="card-body p-3 small">What is the relationship between <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> learned by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep recurrent models</a> against those encoded by the <a href=https://en.wikipedia.org/wiki/Brain>brain</a>? Is there any correspondence between hidden layers of these <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent models</a> and <a href=https://en.wikipedia.org/wiki/List_of_regions_in_the_human_brain>brain regions</a> when processing sentences? Can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? We investigate these questions using sentences with simple syntax and semantics (e.g., The bone was eaten by the dog.). We consider multiple neural network architectures, including recently proposed ELMo and BERT. We use magnetoencephalography (MEG) brain recording data collected from human subjects when they were reading these simple sentences. Overall, we find that BERT&#8217;s activations correlate the best with MEG brain data. We also find that the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network representation</a> can be used to generate <a href=https://en.wikipedia.org/wiki/Brain>brain data</a> from new sentences to augment existing <a href=https://en.wikipedia.org/wiki/Brain>brain data</a>. To the best of our knowledge, this is the first work showing that the <a href=https://en.wikipedia.org/wiki/Magnetoencephalography>MEG brain recording</a> when reading a word in a sentence can be used to distinguish earlier words in the sentence. Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1510 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1510/>NNE : A Dataset for Nested Named Entity Recognition in English Newswire<span class=acl-fixed-case>NNE</span>: A Dataset for Nested Named Entity Recognition in <span class=acl-fixed-case>E</span>nglish Newswire</a></strong><br><a href=/people/n/nicky-ringland/>Nicky Ringland</a>
|
<a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a>
|
<a href=/people/j/james-r-curran/>James R. Curran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1510><div class="card-body p-3 small">Named entity recognition (NER) is widely used in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language processing applications</a> and downstream tasks. However, most NER tools target flat annotation from popular <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, eschewing the semantic information available in nested entity mentions. We describe NNEa fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> comprises 279,795 mentions of 114 entity types with up to 6 layers of <a href=https://en.wikipedia.org/wiki/Nesting_(computing)>nesting</a>. We hope the public release of this large <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for English newswire will encourage development of new techniques for nested NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1511 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1511" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1511/>Sequence-to-Nuggets : Nested Entity Mention Detection via Anchor-Region Networks</a></strong><br><a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1511><div class="card-body p-3 small">Sequential labeling-based NER approaches restrict each word belonging to at most one entity mention, which will face a serious problem when recognizing nested entity mentions. In this paper, we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity mentions</a>, i.e., although a mention can nest other mentions, they will not share the same <a href=https://en.wikipedia.org/wiki/Headword>head word</a>. Specifically, we propose Anchor-Region Networks (ARNs), a sequence-to-nuggets architecture for nested mention detection. ARNs first identify anchor words (i.e., possible head words) of all mentions, and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures. Furthermore, we also design Bag Loss, an <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> which can train ARNs in an end-to-end manner without using any anchor word annotation. Experiments show that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1515 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1515/>Incorporating <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>Linguistic Constraints</a> into Keyphrase Generation</a></strong><br><a href=/people/j/jing-zhao/>Jing Zhao</a>
|
<a href=/people/y/yuxiang-zhang/>Yuxiang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1515><div class="card-body p-3 small">Keyphrases, that concisely describe the high-level topics discussed in a document, are very useful for a wide range of natural language processing tasks. Though existing keyphrase generation methods have achieved remarkable performance on this task, they generate many overlapping phrases (including sub-phrases or super-phrases) of keyphrases. In this paper, we propose the parallel Seq2Seq network with the coverage attention to alleviate the overlapping phrase problem. Specifically, we integrate the linguistic constraints of keyphrase into the basic Seq2Seq network on the source side, and employ the multi-task learning framework on the target side. In addition, in order to prevent from generating overlapping phrases of keyphrases with correct <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, we introduce the coverage vector to keep track of the attention history and to decide whether the parts of source text have been covered by existing generated keyphrases. Experimental results show that our method can outperform the state-of-the-art CopyRNN on scientific datasets, and is also more effective in news domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1518 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1518" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1518/>A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1518><div class="card-body p-3 small">Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an <a href=https://en.wikipedia.org/wiki/Unordered_set>unordered set</a> rather than an <a href=https://en.wikipedia.org/wiki/List_of_order_structures_in_mathematics>ordered sequence</a>. This inconsistency tends to result in some <a href=https://en.wikipedia.org/wiki/Intrinsic_and_extrinsic_properties_(philosophy)>intractable problems</a>, e.g., sensitivity to the label order. To remedy this, we propose a simple but effective sequence-to-set model. The proposed model is trained via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, where <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward feedback</a> is designed to be independent of the label order. In this way, we can reduce the dependence of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the label order, as well as capture high-order correlations between labels. Extensive experiments show that our approach can substantially outperform competitive baselines, as well as effectively reduce the sensitivity to the label order.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1521 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1521" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1521/>Cost-sensitive Regularization for Label Confusion-aware Event Detection</a></strong><br><a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1521><div class="card-body p-3 small">In supervised event detection, most of the mislabeling occurs between a small number of confusing type pairs, including trigger-NIL pairs and sibling sub-types of the same coarse type. To address this label confusion problem, this paper proposes cost-sensitive regularization, which can force the training procedure to concentrate more on optimizing confusing type pairs. Specifically, we introduce a cost-weighted term into the training loss, which penalizes more on mislabeling between confusing label pairs. Furthermore, we also propose two <a href=https://en.wikipedia.org/wiki/Estimator>estimators</a> which can effectively measure such label confusion based on instance-level or population-level statistics. Experiments on TAC-KBP 2017 datasets demonstrate that the proposed method can significantly improve the performances of different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in both English and Chinese event detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1523 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1523" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1523/>Improving <a href=https://en.wikipedia.org/wiki/Open_information_extraction>Open Information Extraction</a> via Iterative Rank-Aware Learning</a></strong><br><a href=/people/z/zhengbao-jiang/>Zhengbao Jiang</a>
|
<a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1523><div class="card-body p-3 small">Open information extraction (IE) is the task of extracting open-domain assertions from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language sentences</a>. A key step in open IE is confidence modeling, ranking the extractions based on their estimated quality to adjust <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and recall of extracted assertions. We found that the extraction likelihood, a confidence measure used by current supervised open IE systems, is not well calibrated when comparing the quality of assertions extracted from different sentences. We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Code and data are available at https://github.com/jzbjyb/oie_rank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1524 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1524.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1524" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1524/>Towards Improving Neural Named Entity Recognition with Gazetteers</a></strong><br><a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1524><div class="card-body p-3 small">Most of the recently proposed neural models for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. This could increase the chance of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> since the models can not access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1529 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1529" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1529/>How to Best Use <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>Syntax</a> in Semantic Role Labelling</a></strong><br><a href=/people/y/yufei-wang/>Yufei Wang</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a>
|
<a href=/people/s/stephen-wan/>Stephen Wan</a>
|
<a href=/people/y/yifang-sun/>Yifang Sun</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1529><div class="card-body p-3 small">There are many different ways in which external information might be used in a NLP task. This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL&#8217;05 and CoNLL&#8217;12 benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1530 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1530" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1530/>PTB Graph Parsing with Tree Approximation<span class=acl-fixed-case>PTB</span> Graph Parsing with Tree Approximation</a></strong><br><a href=/people/y/yoshihide-kato/>Yoshihide Kato</a>
|
<a href=/people/s/shigeki-matsubara/>Shigeki Matsubara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1530><div class="card-body p-3 small">The Penn Treebank (PTB) represents syntactic structures as <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> due to nonlocal dependencies. This paper proposes a method that approximates PTB graph-structured representations by <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a>. By our approximation method, we can reduce nonlocal dependency identification and constituency parsing into single tree-based parsing. An experimental result demonstrates that our approximation method with an off-the-shelf tree-based constituency parser significantly outperforms the previous methods in nonlocal dependency identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1532 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1532/>A Prism Module for Semantic Disentanglement in Name Entity Recognition</a></strong><br><a href=/people/k/kun-liu/>Kun Liu</a>
|
<a href=/people/s/shen-li/>Shen Li</a>
|
<a href=/people/d/daqi-zheng/>Daqi Zheng</a>
|
<a href=/people/z/zhengdong-lu/>Zhengdong Lu</a>
|
<a href=/people/s/sheng-gao/>Sheng Gao</a>
|
<a href=/people/s/si-li/>Si Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1532><div class="card-body p-3 small">Natural Language Processing has been perplexed for many years by the problem that multiple <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> are mixed inside a word, even with the help of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. To solve this problem, we propose a prism module to disentangle the semantic aspects of words and reduce noise at the input layer of a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In the prism module, some words are selectively replaced with task-related semantic aspects, then these denoised word representations can be fed into downstream tasks to make them easier. Besides, we also introduce a structure to train this <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> jointly with the downstream model without additional data. This module can be easily integrated into the downstream model and significantly improve the performance of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on named entity recognition (NER) task. The ablation analysis demonstrates the rationality of the <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. As a side effect, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also provides a way to visualize the contribution of each word.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1534 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1534.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1534" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1534/>Towards Empathetic Open-domain Conversation Models : A New Benchmark and Dataset</a></strong><br><a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/e/eric-michael-smith/>Eric Michael Smith</a>
|
<a href=/people/m/margaret-li/>Margaret Li</a>
|
<a href=/people/y/y-lan-boureau/>Y-Lan Boureau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1534><div class="card-body p-3 small">One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others&#8217; feelings in a conversation, this is a significant challenge for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a> due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and EmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> or <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> without requiring lengthy re-training of the full model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1536 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1536/>Training Neural Response Selection for Task-Oriented Dialogue Systems</a></strong><br><a href=/people/m/matthew-henderson/>Matthew Henderson</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1536><div class="card-body p-3 small">Despite their popularity in the chatbot literature, retrieval-based models have had modest impact on task-oriented dialogue systems, with the main obstacle to their application being the low-data regime of most task-oriented dialogue tasks. Inspired by the recent success of pretraining in language modelling, we propose an effective method for deploying response selection in task-oriented dialogue. To train response selection models for task-oriented dialogue tasks, we propose a novel method which : 1) pretrains the response selection model on large general-domain conversational corpora ; and then 2) fine-tunes the pretrained model for the target dialogue domain, relying only on the small in-domain dataset to capture the nuances of the given dialogue domain. Our evaluation on five diverse application domains, ranging from <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a> to <a href=https://en.wikipedia.org/wiki/Bank>banking</a>, demonstrates the effectiveness of the proposed training method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1537 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1537.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1537/>Collaborative Dialogue in Minecraft<span class=acl-fixed-case>M</span>inecraft</a></strong><br><a href=/people/a/anjali-narayan-chen/>Anjali Narayan-Chen</a>
|
<a href=/people/p/prashant-jayannavar/>Prashant Jayannavar</a>
|
<a href=/people/j/julia-hockenmaier/>Julia Hockenmaier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1537><div class="card-body p-3 small">We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since <a href=https://en.wikipedia.org/wiki/PC_game>computer games</a> allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this <a href=https://en.wikipedia.org/wiki/Structure>structure</a>. Both players interact via a <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>chat interface</a>. A can observe B but can not place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1538 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1538/>Neural Response Generation with Meta-words</a></strong><br><a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/h/huang-hu/>Huang Hu</a>
|
<a href=/people/m/matt-schuerman/>Matt Schuerman</a>
|
<a href=/people/y/ying-wang/>Ying Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1538><div class="card-body p-3 small">We present open domain dialogue generation with meta-words. A meta-word is a structured record that describes attributes of a response, and thus allows us to explicitly model the one-to-many relationship within open domain dialogues and perform response generation in an explainable and controllable manner. To incorporate meta-words into generation, we propose a novel goal-tracking memory network that formalizes meta-word expression as a goal in response generation and manages the generation process to achieve the goal with a state memory panel and a state controller. Experimental results from both automatic evaluation and human judgment on two large-scale data sets indicate that our model can significantly outperform state-of-the-art generation models in terms of response relevance, response diversity, and accuracy of meta-word expression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1540 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1540/>Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System</a></strong><br><a href=/people/h/hardik-chauhan/>Hardik Chauhan</a>
|
<a href=/people/m/mauajama-firdaus/>Mauajama Firdaus</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1540><div class="card-body p-3 small">Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. The state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence can not capture the information present in the other sources such as <a href=https://en.wikipedia.org/wiki/Video>videos</a>, <a href=https://en.wikipedia.org/wiki/Videotape>audios</a>, <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> etc. With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. Without capturing the information from both the text and image, the <a href=https://en.wikipedia.org/wiki/System>system</a> will be incapable of generating correct and desirable responses. In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. Our evaluation shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate appropriate responses while preserving the position and attribute information. Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1543 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1543/>Reading Turn by Turn : Hierarchical Attention Architecture for Spoken Dialogue Comprehension</a></strong><br><a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1543><div class="card-body p-3 small">Comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> of passages due to the interactive nature of information exchange from at least two speakers. Unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. Therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. Experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. We empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1544 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1544.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1544" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1544/>A Novel Bi-directional Interrelated Model for Joint Intent Detection and Slot Filling</a></strong><br><a href=/people/h/haihong-e/>Haihong E</a>
|
<a href=/people/p/peiqing-niu/>Peiqing Niu</a>
|
<a href=/people/z/zhongfu-chen/>Zhongfu Chen</a>
|
<a href=/people/m/meina-song/>Meina Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1544><div class="card-body p-3 small">A spoken language understanding (SLU) system includes two main tasks, slot filling (SF) and intent detection (ID). The joint model for the two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> is becoming a tendency in SLU. But the bi-directional interrelated connections between the intent and slots are not established in the existing joint models. In this paper, we propose a novel bi-directional interrelated model for joint intent detection and slot filling. We introduce an SF-ID network to establish direct connections for the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> to help them promote each other mutually. Besides, we design an entirely new iteration mechanism inside the SF-ID network to enhance the bi-directional interrelated connections. The experimental results show that the relative improvement in the sentence-level semantic frame accuracy of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is 3.79 % and 5.42 % on ATIS and Snips datasets, respectively, compared to the state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1545 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1545" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1545/>Dual Supervised Learning for Natural Language Understanding and Generation</a></strong><br><a href=/people/s/shang-yu-su/>Shang-Yu Su</a>
|
<a href=/people/c/chao-wei-huang/>Chao-Wei Huang</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1545><div class="card-body p-3 small">Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP and dialogue fields. Natural language understanding is to extract the core semantic meaning from the given utterances, while <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> is opposite, of which the goal is to construct corresponding sentences based on the given <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. However, such <a href=https://en.wikipedia.org/wiki/Dual_(grammatical_number)>dual relationship</a> has not been investigated in literature. This paper proposes a novel learning framework for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and generation on top of dual supervised learning, providing a way to exploit the <a href=https://en.wikipedia.org/wiki/Duality_(mathematics)>duality</a>. The preliminary experiments show that the proposed approach boosts the performance for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, demonstrating the effectiveness of the <a href=https://en.wikipedia.org/wiki/Dual_relationship>dual relationship</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1549 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1549/>Modeling Semantic Relationship in Multi-turn Conversations with Hierarchical Latent Variables</a></strong><br><a href=/people/l/lei-shen/>Lei Shen</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/h/haolan-zhan/>Haolan Zhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1549><div class="card-body p-3 small">Multi-turn conversations consist of complex semantic structures, and it is still a challenge to generate coherent and diverse responses given previous utterances. It&#8217;s practical that a conversation takes place under a background, meanwhile, the query and response are usually most related and they are consistent in topic but also different in content. However, little work focuses on such hierarchical relationship among utterances. To address this problem, we propose a Conversational Semantic Relationship RNN (CSRR) model to construct the dependency explicitly. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> contains <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> in three hierarchies. The discourse-level one captures the global background, the pair-level one stands for the common topic information between query and response, and the utterance-level ones try to represent differences in content. Experimental results show that our model significantly improves the quality of responses in terms of <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>, and diversity compared to baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1550 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1550/>Rationally Reappraising ATIS-based Dialogue Systems<span class=acl-fixed-case>ATIS</span>-based Dialogue Systems</a></strong><br><a href=/people/j/jingcheng-niu/>Jingcheng Niu</a>
|
<a href=/people/g/gerald-penn/>Gerald Penn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1550><div class="card-body p-3 small">The Air Travel Information Service (ATIS) corpus has been the most common benchmark for evaluating Spoken Language Understanding (SLU) tasks for more than three decades since it was released. Recent state-of-the-art neural models have obtained <a href=https://en.wikipedia.org/wiki/F1_score>F1-scores</a> near 98 % on the task of slot filling. We developed a rule-based grammar for the ATIS domain that achieves a 95.82 % F1-score on our evaluation set. In the process, we furthermore discovered numerous shortcomings in the ATIS corpus annotation, which we have fixed. This paper presents a detailed account of these shortcomings, our proposed repairs, our rule-based grammar and the neural slot-filling architectures associated with ATIS. We also rationally reappraise the motivations for choosing a neural architecture in view of this account. Fixing the annotation errors results in a relative error reduction of between 19.4 and 52 % across all <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>. We nevertheless argue that neural models must play a different role in ATIS dialogues because of the latter&#8217;s lack of variety.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1551 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1551" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1551/>Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming</a></strong><br><a href=/people/c/caio-corro/>Caio Corro</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1551><div class="card-body p-3 small">We treat projective dependency trees as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision. Our approach relies on Gumbel perturbations and differentiable dynamic programming. Unlike previous approaches to latent tree learning, we stochastically sample global structures and our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is fully differentiable. We illustrate its effectiveness on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and natural language inference tasks. We also study its properties on a synthetic structure induction task. Ablation studies emphasize the importance of both <a href=https://en.wikipedia.org/wiki/Stochastic>stochasticity</a> and constraining <a href=https://en.wikipedia.org/wiki/Latent_variable>latent structures</a> to be projective trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1555.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1555 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1555 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1555" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1555/>Soft Contextual Data Augmentation for Neural Machine Translation</a></strong><br><a href=/people/f/fei-gao/>Fei Gao</a>
|
<a href=/people/j/jinhua-zhu/>Jinhua Zhu</a>
|
<a href=/people/l/lijun-wu/>Lijun Wu</a>
|
<a href=/people/y/yingce-xia/>Yingce Xia</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a>
|
<a href=/people/w/wengang-zhou/>Wengang Zhou</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1555><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> is an important trick to boost the accuracy of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision tasks</a>, its study in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a> is still very limited. In this paper, we present a novel data augmentation method for neural machine translation. Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distribution</a> (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced, the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1556 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1556/>Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks</a></strong><br><a href=/people/a/anush-kamath/>Anush Kamath</a>
|
<a href=/people/s/sparsh-gupta/>Sparsh Gupta</a>
|
<a href=/people/v/vitor-carvalho/>Vitor Carvalho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1556><div class="card-body p-3 small">Adversarial domain adaptation has been recently proposed as an effective technique for textual matching tasks, such as question deduplication. Here we investigate the use of gradient reversal on adversarial domain adaptation to explicitly learn both shared and unshared (domain specific) representations between two textual domains. In doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy. We evaluate reversing gradients for adversarial adaptation on multiple domains, and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment (RTE) tasks, achieving up to 7 % absolute boost in base model accuracy on some datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1559.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1559 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1559 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1559.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1559/>Generating Fluent Adversarial Examples for Natural Languages</a></strong><br><a href=/people/h/huangzhao-zhang/>Huangzhao Zhang</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/n/ning-miao/>Ning Miao</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1559><div class="card-body p-3 small">Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make <a href=https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)>small perturbations</a> along the direction of <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a>. Secondly, the fluency of the generated examples can not be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a>. Experiments on <a href=https://en.wikipedia.org/wiki/IMDb>IMDB</a> and SNLI show that our proposed MHAoutperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1560.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1560 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1560 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1560/>Towards Explainable NLP : A Generative Explanation Framework for Text Classification<span class=acl-fixed-case>NLP</span>: A Generative Explanation Framework for Text Classification</a></strong><br><a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1560><div class="card-body p-3 small">Building explainable systems is a critical problem in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, since most <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and is able to generate concise explanations at the same time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1561.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1561 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1561 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1561" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1561/>Combating Adversarial Misspellings with Robust Word Recognition</a></strong><br><a href=/people/d/danish-pruthi/>Danish Pruthi</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary C. Lipton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1561><div class="card-body p-3 small">To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves 32 % relative (and 3.3 % absolute) <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> over the vanilla semi-character model. Notably, our pipeline confers <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> on the downstream classifier, outperforming both adversarial training and off-the-shelf <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checkers</a>. Against a <a href=https://en.wikipedia.org/wiki/Boolean_satisfiability_problem>BERT model</a> fine-tuned for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, a single adversarially-chosen character attack lowers <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from 90.3 % to 45.8 %. Our defense restores <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> to 75 %. Surprisingly, better <a href=https://en.wikipedia.org/wiki/Word_recognition>word recognition</a> does not always entail greater <a href=https://en.wikipedia.org/wiki/Robustness_(morphology)>robustness</a>. Our analysis reveals that robustness also depends upon a quantity that we denote the <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>sensitivity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1562 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1562.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1562" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1562/>An Empirical Investigation of Structured Output Modeling for Graph-based Neural Dependency Parsing</a></strong><br><a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1562><div class="card-body p-3 small">In this paper, we investigate the aspect of structured output modeling for the state-of-the-art graph-based neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1563 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1563.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385223469 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1563" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1563/>Observing Dialogue in Therapy : Categorizing and Forecasting Behavioral Codes</a></strong><br><a href=/people/j/jie-cao/>Jie Cao</a>
|
<a href=/people/m/michael-tanana/>Michael Tanana</a>
|
<a href=/people/z/zac-imel/>Zac Imel</a>
|
<a href=/people/e/eric-poitras/>Eric Poitras</a>
|
<a href=/people/d/david-atkins/>David Atkins</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1563><div class="card-body p-3 small">Automatically analyzing dialogue can help understand and guide behavior in domains such as <a href=https://en.wikipedia.org/wiki/List_of_counseling_topics>counseling</a>, where interactions are largely mediated by <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a>. In this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing (MI), which is effective for addressing substance abuse and related problems. Specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client MI behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. For both tasks, we define <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> that build upon recent successes in dialogue modeling. Our experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can outperform several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1565.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1565 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1565 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1565.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385223824 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1565" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1565/>Target-Guided Open-Domain Conversation</a></strong><br><a href=/people/j/jianheng-tang/>Jianheng Tang</a>
|
<a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1565><div class="card-body p-3 small">Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a>, <a href=https://en.wikipedia.org/wiki/Psychotherapy>psychotherapy</a>, <a href=https://en.wikipedia.org/wiki/Education>education</a>, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no <a href=https://en.wikipedia.org/wiki/Public_data>public data</a> is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse-level constraints</a>. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our <a href=https://en.wikipedia.org/wiki/System>system</a> can produce meaningful and effective conversations, significantly improving over other approaches</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1566.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1566 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1566 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385225678 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1566" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1566/>Persuasion for Good : Towards a Personalized Persuasive Dialogue System for Social Good</a></strong><br><a href=/people/x/xuewei-wang/>Xuewei Wang</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/r/richard-kim/>Richard Kim</a>
|
<a href=/people/y/yoojung-oh/>Yoojung Oh</a>
|
<a href=/people/s/sijia-yang/>Sijia Yang</a>
|
<a href=/people/j/jingwen-zhang/>Jingwen Zhang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1566><div class="card-body p-3 small">Developing intelligent persuasive conversational agents to change people&#8217;s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> from a subset. Based on the annotation, we built a baseline classifier with <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> and sentence-level features to predict the 10 <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> used in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals&#8217; demographic and psychological backgrounds including <a href=https://en.wikipedia.org/wiki/Personality>personality</a>, <a href=https://en.wikipedia.org/wiki/Morality>morality</a>, <a href=https://en.wikipedia.org/wiki/Value_(ethics)>value systems</a>, and their willingness for donation. Then, we analyzed which types of <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> led to a greater amount of donation depending on the individuals&#8217; personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1569.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1569 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1569 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385428418 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1569" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1569/>Language Modelling Makes Sense : Propagating Representations through <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> for Full-Coverage Word Sense Disambiguation<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et for Full-Coverage Word Sense Disambiguation</a></strong><br><a href=/people/d/daniel-loureiro/>Daniel Loureiro</a>
|
<a href=/people/a/alipio-jorge/>Alípio Jorge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1569><div class="card-body p-3 small">Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1570.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1570 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1570 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385428467 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1570/>Word2Sense : Sparse Interpretable Word Embeddings<span class=acl-fixed-case>W</span>ord2<span class=acl-fixed-case>S</span>ense: Sparse Interpretable Word Embeddings</a></strong><br><a href=/people/a/abhishek-panigrahi/>Abhishek Panigrahi</a>
|
<a href=/people/h/harsha-vardhan-simhadri/>Harsha Vardhan Simhadri</a>
|
<a href=/people/c/chiranjib-bhattacharyya/>Chiranjib Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1570><div class="card-body p-3 small">We present an unsupervised method to generate Word2Sense word embeddings that are interpretable each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. On computational NLP tasks, Word2Sense embeddings compare well with other <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> generated by <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>. Across tasks such as word similarity, <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>, sense induction, and contextual interpretation, Word2Sense is competitive with the state-of-the-art method for that task. Word2Sense embeddings are at least as sparse and fast to compute as <a href=https://en.wikipedia.org/wiki/Prior_art>prior art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1571.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1571 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1571 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385428643 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1571" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1571/>Modeling Semantic Compositionality with Sememe Knowledge</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/j/junjie-huang/>Junjie Huang</a>
|
<a href=/people/c/chenghao-yang/>Chenghao Yang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1571><div class="card-body p-3 small">Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we verify the effectiveness of <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a>, the minimum semantic units of human languages, in modeling <a href=https://en.wikipedia.org/wiki/Semantic_space>SC</a> by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying <a href=https://en.wikipedia.org/wiki/Sememe>sememe knowledge</a> in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1573.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1573 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1573 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385428708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1573/>Empirical Linguistic Study of Sentence Embeddings</a></strong><br><a href=/people/k/katarzyna-krasnowska-kieras/>Katarzyna Krasnowska-Kieraś</a>
|
<a href=/people/a/alina-wroblewska/>Alina Wróblewska</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1573><div class="card-body p-3 small">The purpose of the research is to answer the question whether <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> is retained in <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>vector representations of sentences</a>. We introduce a method of analysing the content of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> based on universal probing tasks, along with the classification datasets for two contrasting languages. We perform a series of probing and downstream experiments with different types of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>, followed by a thorough analysis of the experimental results. Aside from dependency parser-based embeddings, linguistic information is retained best in the recently proposed LASER sentence embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1574.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1574 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1574 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385429181 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1574" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1574/>Probing for Semantic Classes : Diagnosing the Meaning Content of Word Embeddings</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/t/timothy-j-hazen/>T. J. Hazen</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1574><div class="card-body p-3 small">Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>, where <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> from different words are related by <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a>. This is the basis for novel diagnostic tests for an embedding&#8217;s content : we probe word embeddings for <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a> and analyze the embedding space by classifying embeddings into <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a>. Our main findings are : (i) Information about a sense is generally represented well in a single-vector embedding if the sense is frequent. (ii) A <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1575.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1575 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1575 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385434363 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1575/>Deep Neural Model Inspection and Comparison via Functional Neuron Pathways</a></strong><br><a href=/people/j/james-fiacco/>James Fiacco</a>
|
<a href=/people/s/samridhi-choudhary/>Samridhi Choudhary</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1575><div class="card-body p-3 small">We introduce a general <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for the interpretation and comparison of neural models. The method is used to factor a complex neural model into its functional components, which are comprised of sets of co-firing neurons that cut across layers of the <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a>, and which we call <a href=https://en.wikipedia.org/wiki/Neural_pathway>neural pathways</a>. The function of these pathways can be understood by identifying correlated task level and linguistic heuristics in such a way that this knowledge acts as a lens for approximating what the network has learned to apply to its intended <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. As a case study for investigating the utility of these <a href=https://en.wikipedia.org/wiki/Neural_pathway>pathways</a>, we present an examination of <a href=https://en.wikipedia.org/wiki/Neural_pathway>pathways</a> identified in models trained for two standard tasks, namely <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> and Recognizing Textual Entailment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1579.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1579 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1579 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385226257 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1579/>Generalized Data Augmentation for Low-Resource Translation</a></strong><br><a href=/people/m/mengzhou-xia/>Mengzhou Xia</a>
|
<a href=/people/x/xiang-kong/>Xiang Kong</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1579><div class="card-body p-3 small">Low-resource language pairs with a paucity of parallel data pose challenges for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> in terms of both adequacy and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1581.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1581 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1581 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385434714 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1581/>Better OOV Translation with Bilingual Terminology Mining<span class=acl-fixed-case>OOV</span> Translation with Bilingual Terminology Mining</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1581><div class="card-body p-3 small">Unseen words, also called out-of-vocabulary words (OOVs), are difficult for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, byte-pair encoding can be used to represent OOVs, but they are still often incorrectly translated. We improve the translation of OOVs in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> using easy-to-obtain monolingual data. We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (BWEs). In our MT experiments we take the 5-best candidates, which is motivated by intrinsic mining experiments. Using all five of the proposed target language words as queries we mine target-language sentences. We then back-translate, forcing the <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV. We show that by using this synthetic data to fine-tune our <a href=https://en.wikipedia.org/wiki/System>system</a> the translation of OOVs can be dramatically improved. In our experiments we use a system trained on <a href=https://en.wikipedia.org/wiki/Europarl>Europarl</a> and mine sentences containing <a href=https://en.wikipedia.org/wiki/Medical_terminology>medical terms</a> from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1583.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1583 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1583 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385434805 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1583/>Target Conditioned Sampling : Optimizing Data Selection for Multilingual Neural Machine Translation</a></strong><br><a href=/people/x/xinyi-wang/>Xinyi Wang</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1583><div class="card-body p-3 small">To improve low-resource Neural Machine Translation (NMT) with multilingual corpus, training on the most related high-resource language only is generally more effective than us- ing all data available (Neubig and Hu, 2018). However, it remains a question whether a smart data selection strategy can further improve low-resource NMT with data from other auxiliary languages. In this paper, we seek to construct a <a href=https://en.wikipedia.org/wiki/Sampling_distribution>sampling distribution</a> over all multilingual data, so that it minimizes the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training loss</a> of the low-resource language. Based on this formulation, we propose and efficient <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show TCS brings significant gains of up to 2 BLEU improvements on three of four languages we test, with minimal training overhead.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1585.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1585 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1585 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385226453 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1585" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1585/>Merge and Label : A Novel Neural Network Architecture for Nested NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/j/joseph-fisher/>Joseph Fisher</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1585><div class="card-body p-3 small">Named entity recognition (NER) is one of the best studied tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. However, most approaches are not capable of handling nested structures which are common in many applications. In this paper we introduce a novel neural network architecture that first merges tokens and/or entities into entities forming nested structures, and then labels each of them independently. Unlike previous work, our merge and label approach predicts real-valued instead of discrete segmentation structures, which allow it to combine word and nested entity embeddings while maintaining differentiability. We evaluate our approach using the ACE 2005 Corpus, where it achieves state-of-the-art F1 of 74.6, further improved with contextual embeddings (BERT) to 82.4, an overall improvement of close to 8 F1 points over previous approaches trained on the same data. Additionally we compare it against BiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1586.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1586 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1586 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385226574 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1586/>Low-resource Deep Entity Resolution with Transfer and Active Learning</a></strong><br><a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/s/sairam-gurajada/>Sairam Gurajada</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/lucian-popa/>Lucian Popa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1586><div class="card-body p-3 small">Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base creation</a> and <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a>. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>. We design an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1587.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1587 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1587 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1587.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385242676 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1587/>A Semi-Markov Structured Support Vector Machine Model for High-Precision Named Entity Recognition<span class=acl-fixed-case>M</span>arkov Structured Support Vector Machine Model for High-Precision Named Entity Recognition</a></strong><br><a href=/people/r/ravneet-arora/>Ravneet Arora</a>
|
<a href=/people/c/chen-tse-tsai/>Chen-Tse Tsai</a>
|
<a href=/people/k/ketevan-tsereteli/>Ketevan Tsereteli</a>
|
<a href=/people/p/prabhanjan-kambadur/>Prabhanjan Kambadur</a>
|
<a href=/people/y/yi-yang/>Yi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1587><div class="card-body p-3 small">Named entity recognition (NER) is the backbone of many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP solutions</a>. F1 score, the harmonic mean of precision and recall, is often used to select / evaluate the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. However, when <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> needs to be prioritized over <a href=https://en.wikipedia.org/wiki/Precision_recall>recall</a>, a state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> might not be the best choice. There is little in literature that directly addresses training-time modifications to achieve higher precision information extraction. In this paper, we propose a neural semi-Markov structured support vector machine model that controls the precision-recall trade-off by assigning weights to different types of errors in the loss-augmented inference during training. The semi-Markov property provides more accurate phrase-level predictions, thereby improving performance. We empirically demonstrate the advantage of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> when high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> is required by comparing against strong baselines based on CRF. In our experiments with the CoNLL 2003 dataset, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a better precision-recall trade-off at various precision levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1589.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1589 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1589 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/385243188 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1589/>Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision</a></strong><br><a href=/people/a/abiola-obamuyide/>Abiola Obamuyide</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1589><div class="card-body p-3 small">In this paper we frame the task of supervised relation classification as an instance of <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>. We propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, we aim to not only learn good parameters for classifying relations with sufficient supervision, but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision. In experiments conducted on two relation classification datasets, we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1601 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1601" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1601/>Style Transformer : Unpaired Text Style Transfer without Disentangled Latent Representation</a></strong><br><a href=/people/n/ning-dai/>Ning Dai</a>
|
<a href=/people/j/jianze-liang/>Jianze Liang</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1601><div class="card-body p-3 small">Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, can not well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1603 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1603/>Learning to Control the Fine-grained Sentiment for Story Ending Generation</a></strong><br><a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/d/damai-dai/>Damai Dai</a>
|
<a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1603><div class="card-body p-3 small">Automatic story ending generation is an interesting and challenging task in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating <a href=https://en.wikipedia.org/wiki/Ending_(linguistics)>endings</a>. Therefore, we propose a generic and novel framework which consists of a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyzer</a> and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment intensities</a> of the <a href=https://en.wikipedia.org/wiki/Story_arc>story dataset</a>. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1606 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1606/>Storyboarding of Recipes : Grounded Contextual Generation</a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1606><div class="card-body p-3 small">Information need of humans is essentially multimodal in nature, enabling maximum exploitation of situated context. We introduce a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for sequential procedural (how-to) text generation from images in cooking domain. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of 16,441 cooking recipes with 160,479 photos associated with different steps. We setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling (ViST) task. In addition, we introduce two models to incorporate high level structure learnt by a Finite State Machine (FSM) in neural sequential generation process by : (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL). Our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (SSiL) achieves a <a href=https://en.wikipedia.org/wiki/METEOR>METEOR score</a> of 0.31, which is an improvement of 0.6 over the baseline model. We also conducted human evaluation of the generated grounded recipes, which reveal that 61 % found that our proposed (SSiL) model is better than the baseline model in terms of overall recipes. We also discuss analysis of the output highlighting key important NLP issues for prospective directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1607 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1607/>Negative Lexically Constrained Decoding for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>Paraphrase Generation</a></a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1607><div class="card-body p-3 small">Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> rewrites only a limited portion of an input sentence. Hence, previous methods based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> by making necessary rewrites to an input sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1610 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1610" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1610/>Improving the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>Robustness</a> of <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering Systems</a> to Question Paraphrasing</a></strong><br><a href=/people/w/wee-chung-gan/>Wee Chung Gan</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1610><div class="card-body p-3 small">Despite the advancement of question answering (QA) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions. Paraphrased questions from the first test set are very similar to the original questions designed to test QA models&#8217; over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1611 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1611" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1611/>RankQA : Neural Question Answering with Answer Re-Ranking<span class=acl-fixed-case>R</span>ank<span class=acl-fixed-case>QA</span>: Neural Question Answering with Answer Re-Ranking</a></strong><br><a href=/people/b/bernhard-kratzwald/>Bernhard Kratzwald</a>
|
<a href=/people/a/anna-eigenmann/>Anna Eigenmann</a>
|
<a href=/people/s/stefan-feuerriegel/>Stefan Feuerriegel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1611><div class="card-body p-3 small">The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process : first, relevant text passages are retrieved and, subsequently, a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> for machine comprehension extracts the likeliest answer. However, both stages are largely isolated in the status quo and, hence, information from the two phases is never properly fused. In contrast, this work proposes RankQA : RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking. The re-ranking leverages different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are directly extracted from the <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA pipeline</a>, i.e., a combination of retrieval and comprehension features. While our intentionally simple design allows for an efficient, data-sparse estimation, it nevertheless outperforms more complex QA systems by a significant margin : in fact, RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets. Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic. Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size. As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1613 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1613" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1613/>Multi-hop Reading Comprehension through Question Decomposition and Rescoring</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/v/victor-zhong/>Victor Zhong</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1613><div class="card-body p-3 small">Multi-hop Reading Comprehension (RC) requires <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as human-authored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HotpotQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1614.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1614 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1614/>Combining Knowledge Hunting and Neural Language Models to Solve the Winograd Schema Challenge<span class=acl-fixed-case>W</span>inograd Schema Challenge</a></strong><br><a href=/people/a/ashok-prakash/>Ashok Prakash</a>
|
<a href=/people/a/arpit-sharma/>Arpit Sharma</a>
|
<a href=/people/a/arindam-mitra/>Arindam Mitra</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1614><div class="card-body p-3 small">Winograd Schema Challenge (WSC) is a pronoun resolution task which seems to require reasoning with commonsense knowledge. The needed knowledge is not present in the given text. Automatic extraction of the needed knowledge is a bottleneck in solving the challenge. The existing state-of-the-art approach uses the knowledge embedded in their pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. However, the <a href=https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)>language models</a> only embed part of the knowledge, the ones related to frequently co-existing concepts. This limits the performance of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the WSC problems. In this work, we build-up on the language model based methods and augment them with a commonsense knowledge hunting (using automatic extraction from text) module and an explicit reasoning module. Our <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end system</a> built in such a manner improves on the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of two of the available language model based approaches by 5.53 % and 7.7 % respectively. Overall our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the state-of-the-art accuracy of 71.06 % on the WSC dataset, an improvement of 7.36 % over the previous best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1615 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1615/>Careful Selection of Knowledge to Solve Open Book Question Answering</a></strong><br><a href=/people/p/pratyay-banerjee/>Pratyay Banerjee</a>
|
<a href=/people/k/kuntal-kumar-pal/>Kuntal Kumar Pal</a>
|
<a href=/people/a/arindam-mitra/>Arindam Mitra</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1615><div class="card-body p-3 small">Open book question answering is a type of natural language based QA (NLQA) where questions are expected to be answered with respect to a given set of open book facts, and <a href=https://en.wikipedia.org/wiki/Common_knowledge>common knowledge</a> about a topic. Recently a challenge involving such <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>, OpenBookQA, has been proposed. Unlike most other NLQA that focus on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>linguistic understanding</a>, OpenBookQA requires deeper reasoning involving <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>linguistic understanding</a> as well as reasoning with <a href=https://en.wikipedia.org/wiki/Common_knowledge_(logic)>common knowledge</a>. In this paper we address QA with respect to the OpenBookQA dataset and combine state of the art language models with abductive information retrieval (IR), information gain based re-ranking, passage selection and <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted scoring</a> to achieve 72.0 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, an 11.6 % improvement over the current state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1616 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1616" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1616/>Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering</a></strong><br><a href=/people/p/peng-wu/>Peng Wu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/r/rongxiang-weng/>Rongxiang Weng</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/j/jianbing-zhang/>Jianbing Zhang</a>
|
<a href=/people/x/xiaohui-yan/>Xiaohui Yan</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1616><div class="card-body p-3 small">Relation detection is a core step in many <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language process applications</a> including <a href=https://en.wikipedia.org/wiki/Question_answering>knowledge base question answering</a>. Previous efforts show that single-fact questions could be answered with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. However, one critical problem is that current approaches only get high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for questions whose relations have been seen in the training data. But for unseen relations, the performance will drop rapidly. The main reason for this problem is that the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for unseen relations are missing. In this paper, we propose a simple mapping method, named representation adapter, to learn the <a href=https://en.wikipedia.org/wiki/Representation_theory>representation mapping</a> for both seen and unseen relations based on previously learned relation embedding. We employ the adversarial objective and the reconstruction objective to improve the <a href=https://en.wikipedia.org/wiki/Cartography>mapping</a> performance. We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations. Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1621 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1621" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1621/>Are Red Roses Red? Evaluating Consistency of Question-Answering Models</a></strong><br><a href=/people/m/marco-tulio-ribeiro/>Marco Tulio Ribeiro</a>
|
<a href=/people/c/carlos-guestrin/>Carlos Guestrin</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1621><div class="card-body p-3 small">Although current evaluation of <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering systems</a> treats predictions in isolation, we need to consider the relationship between predictions to measure true understanding. A <a href=https://en.wikipedia.org/wiki/Model_(person)>model</a> should be penalized for answering no to Is the rose red? if it answers red to What color is the rose?. We propose a method to automatically extract such implications for instances from two QA datasets, VQA and SQuAD, which we then use to evaluate the consistency of models. Human evaluation shows these generated implications are well formed and valid. Consistency evaluation provides crucial insights into gaps in existing models, while retraining with implication-augmented data improves <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a> on both synthetic and human-generated implications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1623 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1623/>Reducing Word Omission Errors in Neural Machine Translation : A Contrastive Learning Approach</a></strong><br><a href=/people/z/zonghan-yang/>Zonghan Yang</a>
|
<a href=/people/y/yong-cheng/>Yong Cheng</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1623><div class="card-body p-3 small">While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a>, and <a href=https://en.wikipedia.org/wiki/Part_of_speech>part of speech</a>. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance than three baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1627 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1627" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1627/>An Automated Framework for Fast Cognate Detection and Bayesian Phylogenetic Inference in Computational Historical Linguistics<span class=acl-fixed-case>B</span>ayesian Phylogenetic Inference in Computational Historical Linguistics</a></strong><br><a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/j/johann-mattis-list/>Johann-Mattis List</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1627><div class="card-body p-3 small">We present a fully automated workflow for phylogenetic reconstruction on large datasets, consisting of two novel methods, one for fast detection of cognates and one for fast Bayesian phylogenetic inference. Our results show that the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> take less than a few minutes to process <a href=https://en.wikipedia.org/wiki/Language_family>language families</a> that have so far required large amounts of time and computational power. Moreover, the <a href=https://en.wikipedia.org/wiki/Cognate>cognates</a> and the trees inferred from the method are quite close, both to gold standard cognate judgments and to expert language family trees. Given its speed and ease of application, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is specifically useful for the exploration of very large datasets in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1628 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1628" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1628/>Sentence Centrality Revisited for Unsupervised Summarization</a></strong><br><a href=/people/h/hao-zheng/>Hao Zheng</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1628><div class="card-body p-3 small">Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network models</a> and the availability of <a href=https://en.wikipedia.org/wiki/Data_set>large-scale datasets</a>. In this paper we develop an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways : (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective <a href=https://en.wikipedia.org/wiki/Centrality>centrality</a> is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1629 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1629.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1629/>Discourse Representation Parsing for Sentences and Documents</a></strong><br><a href=/people/j/jiangming-liu/>Jiangming Liu</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1629><div class="card-body p-3 small">We introduce a novel semantic parsing task based on Discourse Representation Theory (DRT ; Kamp and Reyle 1993). Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> operates over Discourse Representation Tree Structures which we formally define for sentences and documents. We present a general <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for parsing discourse structures of arbitrary length and granularity. We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy. Experimental results on sentence- and document-level benchmarks show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms competitive baselines by a wide margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1631.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1631 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1631.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1631/>Incorporating Priors with Feature Attribution on Text Classification</a></strong><br><a href=/people/f/frederick-liu/>Frederick Liu</a>
|
<a href=/people/b/besim-avci/>Besim Avci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1631><div class="card-body p-3 small">Feature attribution methods, proposed recently, help users interpret the predictions of <a href=https://en.wikipedia.org/wiki/Complex_system>complex models</a>. Our approach integrates feature attributions into the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> to allow machine learning practitioners to incorporate <a href=https://en.wikipedia.org/wiki/Prior_probability>priors</a> in model building. To demonstrate the effectiveness our technique, we apply it to two tasks : (1) mitigating unintended bias in text classifiers by neutralizing identity terms ; (2) improving <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performance in scarce data setting by forcing model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the <a href=https://en.wikipedia.org/wiki/Object_(philosophy)>objective</a>. Our experiments show that i) a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained with our technique reduces undesired model biases without a tradeoff on the original task ; ii) incorporating prior helps model performance in scarce data settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1632 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1632" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1632/>Matching Article Pairs with Graphical Decomposition and Convolutions</a></strong><br><a href=/people/b/bang-liu/>Bang Liu</a>
|
<a href=/people/d/di-niu/>Di Niu</a>
|
<a href=/people/h/haojie-wei/>Haojie Wei</a>
|
<a href=/people/j/jinghong-lin/>Jinghong Lin</a>
|
<a href=/people/y/yancheng-he/>Yancheng He</a>
|
<a href=/people/k/kunfeng-lai/>Kunfeng Lai</a>
|
<a href=/people/y/yu-xu/>Yu Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1632><div class="card-body p-3 small">Identifying the relationship between two articles, e.g., whether two articles published from different sources describe the same <a href=https://en.wikipedia.org/wiki/Breaking_news>breaking news</a>, is critical to many document understanding tasks. Existing approaches for modeling and matching sentence pairs do not perform well in matching longer documents, which embody more complex interactions between the enclosed entities than a sentence does. To model article pairs, we propose the Concept Interaction Graph to represent an article as a graph of concepts. We then match a pair of articles by comparing the sentences that enclose the same concept vertex through a series of encoding techniques, and aggregate the matching signals through a graph convolutional network. To facilitate the evaluation of long article matching, we have created two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, each consisting of about 30 K pairs of <a href=https://en.wikipedia.org/wiki/Breaking_news>breaking news articles</a> covering diverse topics in the <a href=https://en.wikipedia.org/wiki/Open_domain>open domain</a>. Extensive evaluations of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on the two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> demonstrate significant improvements over a wide range of state-of-the-art methods for natural language matching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1636 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1636" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1636/>Large-Scale Multi-Label Text Classification on EU Legislation<span class=acl-fixed-case>EU</span> Legislation</a></strong><br><a href=/people/i/ilias-chalkidis/>Ilias Chalkidis</a>
|
<a href=/people/e/emmanouil-fergadiotis/>Emmanouil Fergadiotis</a>
|
<a href=/people/p/prodromos-malakasiotis/>Prodromos Malakasiotis</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1636><div class="card-body p-3 small">We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with 4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT&#8217;s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1641.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1641 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1641/>Dense Procedure Captioning in Narrated Instructional Videos</a></strong><br><a href=/people/b/botian-shi/>Botian Shi</a>
|
<a href=/people/l/lei-ji/>Lei Ji</a>
|
<a href=/people/y/yaobo-liang/>Yaobo Liang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/z/zhendong-niu/>Zhendong Niu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1641><div class="card-body p-3 small">Understanding narrated instructional videos is important for both research and real-world web applications. Motivated by video dense captioning, we propose a model to generate procedure captions from narrated instructional videos which are a sequence of step-wise clips with description. Previous works on video dense captioning learn video segments and generate <a href=https://en.wikipedia.org/wiki/Closed_captioning>captions</a> without considering <a href=https://en.wikipedia.org/wiki/Transcript_(law)>transcripts</a>. We argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information. In this paper, we introduce a framework to (1) extract procedures by a cross-modality module, which fuses video content with the entire transcript ; and (2) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure. Experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning, and the ablation studies demonstrate that both the video frames and the transcripts are important for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1642.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1642 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1642 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1642" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1642/>Latent Variable Model for Multi-modal Translation</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/m/miguel-rios/>Miguel Rios</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1642><div class="card-body p-3 small">In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable model</a>. This <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> can be seen as a multi-modal stochastic embedding of an image and its description in a <a href=https://en.wikipedia.org/wiki/Foreign_language>foreign language</a>. It is used in a target-language decoder and also to predict <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>image features</a>. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kadar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> on the KL term to promote models with non-negligible mutual information between inputs and <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1643.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1643 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1643" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1643/>Identifying Visible Actions in Lifestyle Vlogs</a></strong><br><a href=/people/o/oana-ignat/>Oana Ignat</a>
|
<a href=/people/l/laura-burdick/>Laura Burdick</a>
|
<a href=/people/j/jia-deng/>Jia Deng</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1643><div class="card-body p-3 small">We consider the task of identifying <a href=https://en.wikipedia.org/wiki/Human_behavior>human actions</a> visible in <a href=https://en.wikipedia.org/wiki/Online_video_platform>online videos</a>. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1644.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1644 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1644.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1644" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1644/>A Corpus for Reasoning about Natural Language Grounded in <a href=https://en.wikipedia.org/wiki/Photograph>Photographs</a></a></strong><br><a href=/people/a/alane-suhr/>Alane Suhr</a>
|
<a href=/people/s/stephanie-zhou/>Stephanie Zhou</a>
|
<a href=/people/a/ally-zhang/>Ally Zhang</a>
|
<a href=/people/i/iris-zhang/>Iris Zhang</a>
|
<a href=/people/h/huajun-bai/>Huajun Bai</a>
|
<a href=/people/y/yoav-artzi/>Yoav Artzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1644><div class="card-body p-3 small">We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a>, and visual reasoning challenges. The <a href=https://en.wikipedia.org/wiki/Data>data</a> contains 107,292 examples of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>English sentences</a> paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the <a href=https://en.wikipedia.org/wiki/Data>data</a> using sets of visually rich images and a compare-and-contrast task to elicit <a href=https://en.wikipedia.org/wiki/Linguistic_diversity>linguistically diverse language</a>. Qualitative analysis shows the <a href=https://en.wikipedia.org/wiki/Data>data</a> requires compositional joint reasoning, including about <a href=https://en.wikipedia.org/wiki/Physical_quantity>quantities</a>, comparisons, and <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. Evaluation using state-of-the-art visual reasoning methods shows the <a href=https://en.wikipedia.org/wiki/Data>data</a> presents a strong challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1645.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1645 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1645 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1645/>Learning to Discover, Ground and Use Words with Segmental Neural Language Models</a></strong><br><a href=/people/k/kazuya-kawakami/>Kazuya Kawakami</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1645><div class="card-body p-3 small">We propose a segmental neural language model that combines the generalization power of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with the ability to discover word-like units that are latent in unsegmented character sequences. In contrast to previous segmentation models that treat <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> as an isolated task, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> unifies word discovery, learning how words fit together to form sentences, and, by conditioning the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on visual context, how words&#8217; meanings ground in representations of nonlinguistic modalities. Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonparametric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1647.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1647 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1647" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1647/>Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language</a></strong><br><a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1647><div class="card-body p-3 small">A widespread approach to processing <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> is to first automatically transcribe it into text. An alternative is to use an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approach</a> : recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. We propose to use <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> to exploit existing transcribed speech within the end-to-end setting. We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding <a href=https://en.wikipedia.org/wiki/Image>images</a>, speech with text, and text with images. We show that the addition of the speech / text task leads to substantial performance improvements on <a href=https://en.wikipedia.org/wiki/Image_retrieval>image retrieval</a> when compared to training the speech / image task in isolation. We conjecture that this is due to a strong inductive bias transcribed speech provides to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, and offer supporting evidence for this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1648.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1648 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1648/>Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog</a></strong><br><a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a>
|
<a href=/people/a/ahmed-kholy/>Ahmed Kholy</a>
|
<a href=/people/l/linjie-li/>Linjie Li</a>
|
<a href=/people/j/jingjing-liu/>Jingjing Liu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1648><div class="card-body p-3 small">This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47 % NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1650.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1650 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1650/>Informative Image Captioning with External Sources of Information</a></strong><br><a href=/people/s/sanqiang-zhao/>Sanqiang Zhao</a>
|
<a href=/people/p/piyush-sharma/>Piyush Sharma</a>
|
<a href=/people/t/tomer-levinboim/>Tomer Levinboim</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1650><div class="card-body p-3 small">An image caption should fluently present the essential information in a given <a href=https://en.wikipedia.org/wiki/Image>image</a>, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important informativeness dimension. We present a mechanism for integrating <a href=https://en.wikipedia.org/wiki/Image>image information</a> together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the <a href=https://en.wikipedia.org/wiki/Image>image</a> in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>image features</a> and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1653.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1653 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1653" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1653/>Distilling Translations with Visual Awareness</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1653><div class="card-body p-3 small">Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> tend to learn to ignore this information. We propose a translate-and-refine approach to this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> where <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this <a href=https://en.wikipedia.org/wiki/Draft_document>draft</a> by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> leads to the state of the art results. Additionally, we show that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has the ability to recover from erroneous or missing words in the source language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1654.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1654 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1654/>VIFIDEL : Evaluating the Visual Fidelity of Image Descriptions<span class=acl-fixed-case>VIFIDEL</span>: Evaluating the Visual Fidelity of Image Descriptions</a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/j/josiah-wang/>Josiah Wang</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1654><div class="card-body p-3 small">We address the task of evaluating image description generation systems. We propose a novel image-aware metric for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> : VIFIDEL. It estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between labels of objects depicted in images and words in the description. The <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is also able to take into account the relative importance of objects mentioned in human reference descriptions during <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>. Even if these human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> achieves high correlation with human judgments on two well-known <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and is competitive with <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that depend on and rely exclusively on human references.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1655.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1655 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1655 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1655.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1655/>Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation</a></strong><br><a href=/people/r/ronghang-hu/>Ronghang Hu</a>
|
<a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/a/anna-rohrbach/>Anna Rohrbach</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a>
|
<a href=/people/t/trevor-darrell/>Trevor Darrell</a>
|
<a href=/people/k/kate-saenko/>Kate Saenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1655><div class="card-body p-3 small">Vision-and-Language Navigation (VLN) requires grounding instructions, such as turn right and stop at the door, to routes in a visual environment. The actual grounding can connect language to the environment through multiple <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modalities</a>, e.g. stop at the door might ground into visual objects, while turn right might rely only on the geometric structure of a route. We investigate where the <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> : <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1656 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1656" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1656/>Multimodal Transformer for Unaligned Multimodal Language Sequences</a></strong><br><a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/s/shaojie-bai/>Shaojie Bai</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/j/j-zico-kolter/>J. Zico Kolter</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1656><div class="card-body p-3 small">Human language is often multimodal, which comprehends a mixture of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist : 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality ; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1657.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1657 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1657 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-1657/>Show, Describe and Conclude : On Exploiting the Structure Information of Chest X-ray Reports<span class=acl-fixed-case>X</span>-ray Reports</a></strong><br><a href=/people/b/baoyu-jing/>Baoyu Jing</a>
|
<a href=/people/z/zeya-wang/>Zeya Wang</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1657><div class="card-body p-3 small">Chest X-Ray (CXR) images are commonly used for <a href=https://en.wikipedia.org/wiki/Screening_(medicine)>clinical screening and diagnosis</a>. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings ; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a <a href=https://en.wikipedia.org/wiki/Two-stage_theory>two-stage strategy</a> that explicitly models the relationship between Findings and Impression. Second, we design a novel co-operative multi-agent system that implicitly captures the imbalanced distribution between abnormality and <a href=https://en.wikipedia.org/wiki/Normal_distribution>normality</a>. Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high-quality <a href=https://en.wikipedia.org/wiki/Medical_record>medical reports</a> through integrating the structure information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1658 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-1658" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-1658/>Visual Story Post-Editing</a></strong><br><a href=/people/t/ting-yao-hsu/>Ting-Yao Hsu</a>
|
<a href=/people/c/chieh-yang-huang/>Chieh-Yang Huang</a>
|
<a href=/people/y/yen-chia-hsu/>Yen-Chia Hsu</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1658><div class="card-body p-3 small">We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected <a href=https://en.wikipedia.org/wiki/Editing>edits</a> may be used for the visual story post-editing task. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The <a href=https://en.wikipedia.org/wiki/Narrative>stories</a> were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-1660.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-1660 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-1660 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-1660.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-1660/>Learning to Relate from Captions and Bounding Boxes</a></strong><br><a href=/people/s/sarthak-garg/>Sarthak Garg</a>
|
<a href=/people/j/joel-ruben-antony-moniz/>Joel Ruben Antony Moniz</a>
|
<a href=/people/a/anshu-aviral/>Anshu Aviral</a>
|
<a href=/people/p/priyatham-bollimpalli/>Priyatham Bollimpalli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-1660><div class="card-body p-3 small">In this work, we propose a novel approach that predicts the relationships between various entities in an <a href=https://en.wikipedia.org/wiki/Image>image</a> in a weakly supervised manner by relying on <a href=https://en.wikipedia.org/wiki/Image>image captions</a> and object bounding box annotations as the sole source of supervision. Our proposed approach uses a top-down attention mechanism to align entities in captions to objects in the image, and then leverage the syntactic structure of the captions to align the relations. We use these alignments to train a relation classification network, thereby obtaining both grounded captions and dense relationships. We demonstrate the effectiveness of our model on the Visual Genome dataset by achieving a recall@50 of 15 % and recall@100 of 25 % on the relationships present in the image. We also show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> successfully predicts relations that are not present in the corresponding captions.</div></div></div><hr><div id=p19-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P19-2/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2000/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2002/>Robust to Noise Models in Natural Language Processing Tasks</a></strong><br><a href=/people/v/valentin-malykh/>Valentin Malykh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2002><div class="card-body p-3 small">There are a lot of noise texts surrounding a person in modern life. The traditional approach is to use spelling correction, yet the existing solutions are far from perfect. We propose robust to noise word embeddings model, which outperforms existing commonly used models, like fasttext and <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> in different tasks. In addition, we investigate the noise robustness of current <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in different natural language processing tasks. We propose extensions for modern <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in three <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a>, i.e. text classification, named entity recognition and aspect extraction, which shows improvement in noise robustness over existing solutions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2004/>Measuring the Value of <a href=https://en.wikipedia.org/wiki/Linguistics>Linguistics</a> : A Case Study from St. Lawrence Island Yupik<span class=acl-fixed-case>S</span>t. <span class=acl-fixed-case>L</span>awrence <span class=acl-fixed-case>I</span>sland <span class=acl-fixed-case>Y</span>upik</a></strong><br><a href=/people/e/emily-chen/>Emily Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2004><div class="card-body p-3 small">The adaptation of neural approaches to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is a landmark achievement that has called into question the utility of <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> in the development of <a href=https://en.wikipedia.org/wiki/Computational_science>computational systems</a>. This research proposal consequently explores this question in the context of a neural morphological analyzer for a <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic language</a>, <a href=https://en.wikipedia.org/wiki/St._Lawrence_Island_Yupik>St. Lawrence Island Yupik</a>. It asks whether incorporating elements of <a href=https://en.wikipedia.org/wiki/Yupik_languages>Yupik linguistics</a> into the implementation of the analyzer can improve performance, both in low-resource settings and in high-resource settings, where rich quantities of data are readily available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2005/>Not All Reviews Are Equal : Towards Addressing Reviewer Biases for Opinion Summarization</a></strong><br><a href=/people/w/wenyi-tay/>Wenyi Tay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2005><div class="card-body p-3 small">Consumers read online reviews for insights which help them to make decisions. Given the large volumes of reviews, succinct review summaries are important for many applications. Existing research has focused on mining for opinions from only review texts and largely ignores the reviewers. However, reviewers have biases and may write lenient or harsh reviews ; they may also have preferences towards some topics over others. Therefore, not all reviews are equal. Ignoring the biases in reviews can generate misleading summaries. We aim for summarization of reviews to include balanced opinions from reviewers of different biases and preferences. We propose to model reviewer biases from their review texts and rating distributions, and learn a bias-aware opinion representation. We further devise an approach for balanced opinion summarization of reviews using our bias-aware opinion representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2006/>Towards Turkish Abstract Meaning Representation<span class=acl-fixed-case>T</span>urkish <span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation</a></strong><br><a href=/people/z/zahra-azin/>Zahra Azin</a>
|
<a href=/people/g/gulsen-eryigit/>Gülşen Eryiğit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2006><div class="card-body p-3 small">Using rooted, directed and labeled graphs, Abstract Meaning Representation (AMR) abstracts away from syntactic features such as word order and does not annotate every constituent in a sentence. AMR has been specified for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and was not supposed to be an <a href=https://en.wikipedia.org/wiki/Interlingua>Interlingua</a>. However, several studies strived to overcome divergences in the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> between English AMRs and those of their target languages by refining the annotation specification. Following this line of research, we have started to build the first Turkish AMR corpus by hand-annotating 100 sentences of the Turkish translation of the novel The Little Prince and comparing the results with the English AMRs available for the same <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. The next step is to prepare the Turkish AMR annotation specification for training future annotators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2009/>Knowledge Discovery and Hypothesis Generation from Online Patient Forums : A Research Proposal</a></strong><br><a href=/people/a/anne-dirkson/>Anne Dirkson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2009><div class="card-body p-3 small">The unprompted patient experiences shared on patient forums contain a wealth of unexploited knowledge. Mining this knowledge and cross-linking it with biomedical literature, could expose novel insights, which could subsequently provide hypotheses for further clinical research. As of yet, <a href=https://en.wikipedia.org/wiki/Automation>automated methods</a> for open knowledge discovery on patient forum text are lacking. Thus, in this research proposal, we outline future research into methods for mining, aggregating and cross-linking patient knowledge from <a href=https://en.wikipedia.org/wiki/Internet_forum>online forums</a>. Additionally, we aim to address how one could measure the <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a> of this extracted knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2011/>Natural Language Generation : Recently Learned Lessons, Directions for Semantic Representation-based Approaches, and the Case of Brazilian Portuguese Language<span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese Language</a></strong><br><a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/t/thiago-pardo/>Thiago Pardo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2011><div class="card-body p-3 small">This paper presents a more recent literature review on <a href=https://en.wikipedia.org/wiki/Natural_language_generation>Natural Language Generation</a>. In particular, we highlight the efforts for <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> in order to show the available resources and the existent approaches for this <a href=https://en.wikipedia.org/wiki/Language>language</a>. We also focus on the approaches for generation from semantic representations (emphasizing the Abstract Meaning Representation formalism) as well as their advantages and limitations, including possible future directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2014/>Active Reading Comprehension : A Dataset for Learning the Question-Answer Relationship Strategy</a></strong><br><a href=/people/d/diana-galvan-sosa/>Diana Galván-Sosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2014><div class="card-body p-3 small">Reading comprehension (RC) through <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is a useful method for evaluating if a reader understands a text. Standard <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy metrics</a> are used for evaluation, where high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> is taken as indicative of a good understanding. However, literature in quality learning suggests that task performance should also be evaluated on the undergone process to answer. The Question-Answer Relationship (QAR) is one of the strategies for evaluating a reader&#8217;s understanding based on their ability to select different sources of information depending on the question type. We propose the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to learn the QAR strategy with weak supervision. We expect to complement current work on <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> by introducing a new setup for evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2015/>Paraphrases as Foreign Languages in Multilingual Neural Machine Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2015><div class="card-body p-3 small">Paraphrases, rewordings of the same semantic meaning, are useful for improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Unlike previous works that only explore <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> at the word or phrase level, we use different translations of the whole training data that are consistent in structure as <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> at the corpus level. We treat <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> as foreign languages, tag source sentences with paraphrase labels, and train on parallel paraphrases in the style of multilingual Neural Machine Translation (NMT). Our multi-paraphrase NMT that trains only on two languages outperforms the multilingual baselines. Adding <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> improves the rare word translation and increases <a href=https://en.wikipedia.org/wiki/Entropy>entropy</a> and diversity in <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a>. Adding the source paraphrases boosts performance better than adding the target ones, while adding both lifts performance further. We achieve a BLEU score of 57.2 for <a href=https://en.wikipedia.org/wiki/Bible_translations_into_French>French-to-English translation</a> using 24 corpus-level paraphrases of the <a href=https://en.wikipedia.org/wiki/Bible>Bible</a>, which outperforms the multilingual baselines and is +34.7 above the single-source single-target NMT baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2017/>Unsupervised Pretraining for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Using Elastic Weight Consolidation</a></strong><br><a href=/people/d/dusan-varis/>Dušan Variš</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2017><div class="card-body p-3 small">This work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT). In our method, we initialize the weights of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language modeling task. We compare the <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by EWC with the previous work that focuses on <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by language modeling objectives. The positive result is that using EWC with the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> achieves BLEU scores similar to the previous work. However, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage. In contrast, the <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> for the whole bidirectional context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2019/>Ranking of Potential Questions</a></strong><br><a href=/people/l/luise-schricker/>Luise Schricker</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2019><div class="card-body p-3 small">Questions are an integral part of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. They provide structure and support the exchange of information. One <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic theory</a>, the Questions Under Discussion model, takes question structures as integral to the functioning of a coherent discourse. This <a href=https://en.wikipedia.org/wiki/Theory>theory</a> has not been tested on the count of its validity for predicting observations in real dialogue data, however. In this submission, a system for ranking explicit and implicit questions by their appropriateness in a dialogue is presented. This <a href=https://en.wikipedia.org/wiki/System>system</a> implements constraints and principles put forward in the linguistic literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2020/>Controlling Grammatical Error Correction Using Word Edit Rate</a></strong><br><a href=/people/k/kengo-hotate/>Kengo Hotate</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/s/satoru-katsumata/>Satoru Katsumata</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2020><div class="card-body p-3 small">When professional English teachers correct grammatically erroneous sentences written by English learners, they use various methods. The correction method depends on how much corrections a learner requires. In this paper, we propose a method for neural grammar error correction (GEC) that can control the degree of correction. We show that it is possible to actually control the degree of GEC by using new training data annotated with word edit rate. Thereby, diverse corrected sentences is obtained from a single erroneous sentence. Moreover, compared to a GEC model that does not use information on the degree of correction, the proposed method improves correction accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2021/>From Brain Space to Distributional Space : The Perilous Journeys of fMRI Decoding<span class=acl-fixed-case>MRI</span> Decoding</a></strong><br><a href=/people/g/gosse-minnema/>Gosse Minnema</a>
|
<a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2021><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a> has introduced <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> for predicting distributional word meaning representations from <a href=https://en.wikipedia.org/wiki/Neuroimaging>brain imaging data</a>. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have great potential, but the quality of their predictions has not yet been thoroughly evaluated from a computational linguistics point of view. Due to the limited size of available brain imaging datasets, standard <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality metrics</a> (e.g. similarity judgments and analogies) can not be used. Instead, we investigate the use of several alternative <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> for evaluating the predicted distributional space against a corpus-derived distributional space. We show that a state-of-the-art decoder, while performing impressively on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that are commonly used in <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a>, performs unexpectedly poorly on our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. To address this, we propose <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> for improving the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance. Despite returning promising results, our experiments also demonstrate that much work remains to be done before distributional representations can reliably be predicted from brain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2023/>A Strong and Robust Baseline for Text-Image Matching</a></strong><br><a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/r/rongtian-ye/>Rongtian Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2023><div class="card-body p-3 small">We review the current schemes of text-image matching models and propose improvements for both <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. First, we empirically show limitations of two popular loss (sum and max-margin loss) widely used in training text-image embeddings and propose a trade-off : a kNN-margin loss which 1) utilizes information from hard negatives and 2) is robust to noise as all K-most hardest samples are taken into account, tolerating pseudo negatives and outliers. Second, we advocate the use of Inverted Softmax (IS) and Cross-modal Local Scaling (CSLS) during inference to mitigate the so-called hubness problem in high-dimensional embedding space, enhancing scores of all metrics by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2025/>Corpus Creation and Analysis for Named Entity Recognition in Telugu-English Code-Mixed Social Media Data<span class=acl-fixed-case>T</span>elugu-<span class=acl-fixed-case>E</span>nglish Code-Mixed Social Media Data</a></strong><br><a href=/people/v/vamshi-krishna-srirangam/>Vamshi Krishna Srirangam</a>
|
<a href=/people/a/appidi-abhinav-reddy/>Appidi Abhinav Reddy</a>
|
<a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2025><div class="card-body p-3 small">Named Entity Recognition(NER) is one of the important tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing(NLP)</a> and also is a subtask of <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a>. In this paper we present our work on NER in Telugu-English code-mixed social media data. Code-Mixing, a progeny of <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a> is a way in which multilingual people express themselves on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> by using linguistics units from different languages within a sentence or speech context. Entity Extraction from social media data such as tweets(twitter) is in general difficult due to its informal nature, code-mixed data further complicates the problem due to its informal, unstructured and incomplete information. We present a Telugu-English code-mixed corpus with the corresponding named entity tags. The <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> used to tag data are Person(&#8216;Per&#8217;), Organization(&#8216;Org&#8217;) and Location(&#8216;Loc&#8217;). We experimented with the machine learning models Conditional Random Fields(CRFs), <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision Trees</a> and BiLSTMs on our <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus</a> which resulted in a <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.96, 0.94 and 0.95 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2026/>Joint Learning of Named Entity Recognition and Entity Linking</a></strong><br><a href=/people/p/pedro-henrique-martins/>Pedro Henrique Martins</a>
|
<a href=/people/z/zita-marinho/>Zita Marinho</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2026><div class="card-body p-3 small">Named entity recognition (NER) and entity linking (EL) are two fundamentally related tasks, since in order to perform EL, first the mentions to entities have to be detected. However, most entity linking approaches disregard the mention detection part, assuming that the correct mentions have been previously detected. In this paper, we perform joint learning of NER and EL to leverage their relatedness and obtain a more robust and generalisable system. For that, we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> inspired by the Stack-LSTM approach. We observe that, in fact, doing <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> of NER and EL improves the performance in both tasks when comparing with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with individual objectives. Furthermore, we achieve results competitive with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in both NER and EL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2027/>Dialogue-Act Prediction of Future Responses Based on Conversation History</a></strong><br><a href=/people/k/koji-tanaka/>Koji Tanaka</a>
|
<a href=/people/j/junya-takayama/>Junya Takayama</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2027><div class="card-body p-3 small">Sequence-to-sequence models are a common approach to develop a <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a>. They can train a <a href=https://en.wikipedia.org/wiki/Conversational_model>conversational model</a> in an end-to-end manner. One significant drawback of such a neural network based approach is that the response generation process is a black-box, and how a specific response is generated is unclear. To tackle this problem, an interpretable response generation mechanism is desired. As a step toward this direction, we focus on dialogue-acts (DAs) that may provide insight to understand the response generation process. In particular, we propose a method to predict a DA of the next response based on the history of previous utterances and their DAs. Experiments using a Switch Board Dialogue Act corpus show that compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> considering only a single utterance, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves 10.8 % higher <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> and 3.0 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on DA prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2029/>Multiple Character Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/j/jianing-zhou/>Jianing Zhou</a>
|
<a href=/people/j/jingkang-wang/>Jingkang Wang</a>
|
<a href=/people/g/gongshen-liu/>Gongshen Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2029><div class="card-body p-3 small">Chinese word segmentation (CWS) is often regarded as a character-based sequence labeling task in most current works which have achieved great success with the help of powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. However, these works neglect an important clue : <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> incorporate both semantic and phonetic meanings. In this paper, we introduce multiple character embeddings including <a href=https://en.wikipedia.org/wiki/Pinyin_Romanization>Pinyin Romanization</a> and Wubi Input, both of which are easily accessible and effective in depicting semantics of characters. We propose a novel shared Bi-LSTM-CRF model to fuse linguistic features efficiently by sharing the LSTM network during the training procedure. Extensive experiments on five corpora show that extra embeddings help obtain a significant improvement in labeling accuracy. Specifically, we achieve the state-of-the-art performance in AS and CityU corpora with F1 scores of 96.9 and 97.3, respectively without leveraging any external lexical resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2033/>From Bilingual to Multilingual Neural Machine Translation by Incremental Training</a></strong><br><a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2033><div class="card-body p-3 small">Multilingual Neural Machine Translation approaches are based on the use of task specific models and the addition of one more language can only be done by retraining the whole system. In this work, we propose a new training schedule that allows the system to scale to more languages without modification of the previous components based on joint training and language-independent encoder / decoder modules allowing for zero-shot translation. This work in progress shows close results to state-of-the-art in the WMT task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2034 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2034" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2034/>STRASS : A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings<span class=acl-fixed-case>STRASS</span>: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings</a></strong><br><a href=/people/l/leo-bouscarrat/>Léo Bouscarrat</a>
|
<a href=/people/a/antoine-bonnefoy/>Antoine Bonnefoy</a>
|
<a href=/people/t/thomas-peel/>Thomas Peel</a>
|
<a href=/people/c/cecile-pereira/>Cécile Pereira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2034><div class="card-body p-3 small">This paper introduces STRASS : Summarization by TRAnsformation Selection and Scoring. It is an extractive text summarization method which leverages the semantic information in existing sentence embedding spaces. Our method creates an extractive summary by selecting the sentences with the closest embeddings to the document embedding. The model earns a transformation of the document embedding to minimize the similarity between the extractive summary and the ground truth summary. As the <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformation</a> is only composed of a dense layer, the training can be done on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a>, therefore, inexpensive. Moreover, <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a> is short and linear according to the number of sentences. As a second contribution, we introduce the French CASS dataset, composed of judgments from the <a href=https://en.wikipedia.org/wiki/Court_of_Cassation_(France)>French Court of cassation</a> and their corresponding summaries. On this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, our results show that our method performs similarly to the state of the art extractive methods with effective training and inferring time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2035/>Attention and Lexicon Regularized LSTM for Aspect-based Sentiment Analysis<span class=acl-fixed-case>LSTM</span> for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/l/lingxian-bao/>Lingxian Bao</a>
|
<a href=/people/p/patrik-lambert/>Patrik Lambert</a>
|
<a href=/people/t/toni-badia/>Toni Badia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2035><div class="card-body p-3 small">Abstract Attention based deep learning systems have been demonstrated to be the state of the art approach for aspect-level sentiment analysis, however, end-to-end deep neural networks lack flexibility as one can not easily adjust the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> to fix an obvious problem, especially when more training data is not available : e.g. when it always predicts positive when seeing the word disappointed. Meanwhile, it is less stressed that attention mechanism is likely to over-focus on particular parts of a sentence, while ignoring positions which provide key information for judging the polarity. In this paper, we describe a simple yet effective approach to leverage <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon information</a> so that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> becomes more flexible and robust. We also explore the effect of regularizing attention vectors to allow the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a> to have a broader focus on different parts of the sentence. The experimental results demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.<i>positive</i> when seeing the word <i>disappointed</i>. Meanwhile, it is less stressed that attention mechanism is likely to &#8220;over-focus&#8221; on particular parts of a sentence, while ignoring positions which provide key information for judging the polarity. In this paper, we describe a simple yet effective approach to leverage lexicon information so that the model becomes more flexible and robust. We also explore the effect of regularizing attention vectors to allow the network to have a broader &#8220;focus&#8221; on different parts of the sentence. The experimental results demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2037/>Normalizing Non-canonical Turkish Texts Using Machine Translation Approaches<span class=acl-fixed-case>T</span>urkish Texts Using Machine Translation Approaches</a></strong><br><a href=/people/t/talha-colakoglu/>Talha Çolakoğlu</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/a/ahmet-cuneyd-tantug/>Ahmet Cüneyd Tantuğ</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2037><div class="card-body p-3 small">With the growth of the <a href=https://en.wikipedia.org/wiki/Social_web>social web</a>, user-generated text data has reached unprecedented sizes. Non-canonical text normalization provides a way to exploit this as a practical source of training data for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing systems</a>. The state of the art in Turkish text normalization is composed of a token level pipeline of modules, heavily dependent on external linguistic resources and manually defined rules. Instead, we propose a fully automated, context-aware machine translation approach with fewer stages of processing. Experiments with various implementations of our approach show that we are able to surpass the current best-performing <a href=https://en.wikipedia.org/wiki/System>system</a> by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2039/>Investigating Political Herd Mentality : A Community Sentiment Based Approach</a></strong><br><a href=/people/a/anjali-bhavan/>Anjali Bhavan</a>
|
<a href=/people/r/rohan-mishra/>Rohan Mishra</a>
|
<a href=/people/p/pradyumna-prakhar-sinha/>Pradyumna Prakhar Sinha</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2039><div class="card-body p-3 small">Analyzing polarities and sentiments inherent in <a href=https://en.wikipedia.org/wiki/Public_speaking>political speeches</a> and <a href=https://en.wikipedia.org/wiki/Debate>debates</a> poses an important problem today. This experiment aims to address this issue by analyzing publicly-available Hansard transcripts of the debates conducted in the UK Parliament. Our proposed approach, which uses community-based graph information to augment hand-crafted features based on <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> and <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a> on debate transcripts, currently surpasses the benchmark results on the same dataset. Such sentiment classification systems could prove to be of great use in today&#8217;s politically turbulent times, for public knowledge of politicians&#8217; stands on various relevant issues proves vital for <a href=https://en.wikipedia.org/wiki/Good_governance>good governance</a> and citizenship. The experiments also demonstrate that continuous feature representations learned from <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> can improve performance on sentiment classification tasks significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2041/>Embedding Strategies for Specialized Domains : Application to Clinical Entity Recognition</a></strong><br><a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2041><div class="card-body p-3 small">Using pre-trained word embeddings in conjunction with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning models</a> has become the de facto approach in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. While this usually yields satisfactory results, off-the-shelf <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2044/>Improving Neural Entity Disambiguation with Graph Embeddings</a></strong><br><a href=/people/o/ozge-sevgili/>Özge Sevgili</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2044><div class="card-body p-3 small">Entity Disambiguation (ED) is the task of linking an ambiguous entity mention to a corresponding entry in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Current methods have mostly focused on unstructured text data to learn representations of entities, however, there is structured information in the knowledge base itself that should be useful to disambiguate entities. In this work, we propose a method that uses <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a> for integrating structured information from the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured information</a> from text-based representations. Our experiments confirm that graph embeddings trained on a graph of hyperlinks between Wikipedia articles improve the performances of simple feed-forward neural ED model and a state-of-the-art neural ED system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2046/>Convolutional Neural Networks for Financial Text Regression</a></strong><br><a href=/people/n/nesat-dereli/>Neşat Dereli</a>
|
<a href=/people/m/murat-saraclar/>Murat Saraclar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2046><div class="card-body p-3 small">Forecasting financial volatility of a publicly-traded company from its <a href=https://en.wikipedia.org/wiki/Annual_report>annual reports</a> has been previously defined as a text regression problem. Recent studies use a manually labeled lexicon to filter the annual reports by keeping sentiment words only. In order to remove the lexicon dependency without decreasing the performance, we replace bag-of-words model word features by word embedding vectors. Using word vectors increases the number of parameters. Considering the increase in number of parameters and excessive lengths of annual reports, a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network model</a> is proposed and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is applied. Experimental results show that the convolutional neural network model provides more accurate volatility predictions than lexicon based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-2049" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-2049/>Scheduled Sampling for Transformers</a></strong><br><a href=/people/t/tsvetomila-mihaylova/>Tsvetomila Mihaylova</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2049><div class="card-body p-3 small">Scheduled sampling is a technique for avoiding one of the known problems in sequence-to-sequence generation : <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>. It consists of feeding the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> a mix of the teacher forced embeddings and the model predictions from the previous step in training time. The <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> has been used for improving <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a>. In the Transformer model, unlike the RNN, the generation of a new word attends to the full sentence generated so far, not only to the last word, and it is not straightforward to apply the scheduled sampling technique. We propose some structural changes to allow scheduled sampling to be applied to Transformer architectures, via a two-pass decoding strategy. Experiments on two language pairs achieve performance close to a teacher-forcing baseline and show that this technique is promising for further exploration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2051 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2051/>Cross-domain and Cross-lingual Abusive Language Detection : A Hybrid Approach with Deep Learning and a Multilingual Lexicon</a></strong><br><a href=/people/e/endang-wahyu-pamungkas/>Endang Wahyu Pamungkas</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2051><div class="card-body p-3 small">The development of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational methods</a> to detect abusive language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> within variable and multilingual contexts has recently gained significant traction. The growing interest is confirmed by the large number of benchmark corpora for different languages developed in the latest years. However, abusive language behaviour is multifaceted and available datasets are featured by different topical focuses. This makes abusive language detection a domain-dependent task, and building a robust system to detect general abusive content a first challenge. Moreover, most resources are available for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, which makes detecting abusive language in low-resource languages a further challenge. We address both challenges by considering ten publicly available datasets across different domains and languages. A hybrid approach with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and a multilingual lexicon to cross-domain and cross-lingual detection of abusive content is proposed and compared with other simpler models. We show that training a <a href=https://en.wikipedia.org/wiki/System>system</a> on general abusive language datasets will produce a cross-domain robust system, which can be used to detect other more specific types of abusive content. We also found that using the domain-independent lexicon HurtLex is useful to transfer knowledge between domains and languages. In the cross-lingual experiment, we demonstrate the effectiveness of our jointlearning model also in out-domain scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2052/>De-Mixing Sentiment from Code-Mixed Text</a></strong><br><a href=/people/y/yash-kumar-lal/>Yash Kumar Lal</a>
|
<a href=/people/v/vaibhav-kumar/>Vaibhav Kumar</a>
|
<a href=/people/m/mrinal-dhar/>Mrinal Dhar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2052><div class="card-body p-3 small">Code-mixing is the phenomenon of mixing the vocabulary and syntax of multiple languages in the same sentence. It is an increasingly common occurrence in today&#8217;s multilingual society and poses a big challenge when encountered in different downstream tasks. In this paper, we present a hybrid architecture for the task of Sentiment Analysis of English-Hindi code-mixed data. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> consists of three components, each seeking to alleviate different issues. We first generate subword level representations for the sentences using a CNN architecture. The generated representations are used as inputs to a Dual Encoder Network which consists of two different BiLSTMs-the Collective and Specific Encoder. The Collective Encoder captures the overall sentiment of the sentence, while the Specific <a href=https://en.wikipedia.org/wiki/Encoder>Encoder</a> utilizes an attention mechanism in order to focus on individual sentiment-bearing sub-words. This, combined with a Feature Network consisting of orthographic features and specially trained <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, achieves state-of-the-art results-83.54 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 0.827 F1 score-on a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2055/>Deep Neural Models for Medical Concept Normalization in <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Texts</a></a></strong><br><a href=/people/z/zulfat-miftahutdinov/>Zulfat Miftahutdinov</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2055><div class="card-body p-3 small">In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a <a href=https://en.wikipedia.org/wiki/Formal_language>free-form text</a> to a concept in a <a href=https://en.wikipedia.org/wiki/Controlled_vocabulary>controlled vocabulary</a>, usually to the standard thesaurus in the <a href=https://en.wikipedia.org/wiki/Unified_Medical_Language_System>Unified Medical Language System (UMLS)</a>. This is a challenging task since <a href=https://en.wikipedia.org/wiki/Medical_terminology>medical terminology</a> is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> such as <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform existing state of the art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2056/>Using <a href=https://en.wikipedia.org/wiki/Semantic_similarity>Semantic Similarity</a> as Reward for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> in Sentence Generation</a></strong><br><a href=/people/g/go-yasui/>Go Yasui</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2056><div class="card-body p-3 small">Traditional model training for sentence generation employs cross-entropy loss as the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>. While cross-entropy loss has convenient properties for <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, it is unable to evaluate sentences as a whole, and lacks flexibility. We present the approach of training the generation model using the estimated semantic similarity between the output and reference sentences to alleviate the problems faced by the training with cross-entropy loss. We use the BERT-based scorer fine-tuned to the Semantic Textual Similarity (STS) task for semantic similarity estimation, and train the model with the estimated scores through reinforcement learning (RL). Our experiments show that <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> with semantic similarity reward improves the BLEU scores from the baseline LSTM NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2058 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2058/>Detecting Adverse Drug Reactions from Biomedical Texts with Neural Networks</a></strong><br><a href=/people/i/ilseyar-alimova/>Ilseyar Alimova</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2058><div class="card-body p-3 small">Detection of adverse drug reactions in postapproval periods is a crucial challenge for <a href=https://en.wikipedia.org/wiki/Pharmacology>pharmacology</a>. Social media and electronic clinical reports are becoming increasingly popular as a source for obtaining health related information. In this work, we focus on extraction information of adverse drug reactions from various sources of biomedical textbased information, including <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We formulate the problem as a binary classification task and compare the performance of four state-of-the-art attention-based neural networks in terms of the <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>. We show the effectiveness of these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on four different <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2059/>Annotating and Analyzing Semantic Role of Elementary Units and Relations in Online Persuasive Arguments</a></strong><br><a href=/people/r/ryo-egawa/>Ryo Egawa</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2059><div class="card-body p-3 small">For analyzing online persuasions, one of the important goals is to semantically understand how people construct comments to persuade others. However, analyzing the semantic role of arguments for online persuasion has been less emphasized. Therefore, in this study, we propose a novel annotation scheme that captures the semantic role of arguments in a popular online persuasion forum, so-called ChangeMyView. Through this study, we have made the following contributions : (i) proposing a <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> that includes five types of elementary units (EUs) and two types of <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. (ii) annotating ChangeMyView which results in 4612 EUs and 2713 relations in 345 posts. (iii) analyzing the semantic role of persuasive arguments. Our analyses captured certain characteristic phenomena for <a href=https://en.wikipedia.org/wiki/Persuasion>online persuasion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-2060/>A Japanese Word Segmentation Proposal<span class=acl-fixed-case>J</span>apanese Word Segmentation Proposal</a></strong><br><a href=/people/s/stalin-aguirre/>Stalin Aguirre</a>
|
<a href=/people/j/josafa-aguiar/>Josafá Aguiar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-2060><div class="card-body p-3 small">Current Japanese word segmentation methods, that use a morpheme-based approach, may produce different segmentations for the same strings. This occurs when these strings appear in different sentences. The cause is the influence of different contexts around these strings affecting the <a href=https://en.wikipedia.org/wiki/Statistical_model>probabilistic models</a> used in segmentation algorithms. This paper presents an alternative to the current morpheme-based scheme for Japanese word segmentation. The proposed scheme focuses on segmenting inflections as single words instead of separating the <a href=https://en.wikipedia.org/wiki/Auxiliary_verb>auxiliary verbs</a> and other morphemes from the stems. Some morphological segmentation rules are presented for each type of word and these <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> are implemented in a <a href=https://en.wikipedia.org/wiki/Computer_program>program</a> which is properly described. The program is used to generate a segmentation of a sentence corpus, whose consistency is calculated and compared with the current morpheme-based segmentation of the same corpus. The experiments show that this <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> produces a much more consistent segmentation than the morpheme-based one.</div></div></div><hr><div id=p19-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P19-3/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3000/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/e/enrique-alfonseca/>Enrique Alfonseca</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-3002.Note.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3002/>SLATE : A Super-Lightweight Annotation Tool for Experts<span class=acl-fixed-case>SLATE</span>: A Super-Lightweight Annotation Tool for Experts</a></strong><br><a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3002><div class="card-body p-3 small">Many annotation tools have been developed, covering a wide variety of tasks and providing features like user management, pre-processing, and automatic labeling. However, all of these tools use <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>Graphical User Interfaces</a>, and often require substantial effort to install and configure. This paper presents a new annotation tool that is designed to fill the niche of a lightweight interface for users with a terminal-based workflow. SLATE supports annotation at different scales (spans of characters, tokens, and lines, or a document) and of different types (free text, labels, and links), with easily customisable keybindings, and unicode support. In a user study comparing with other tools it was consistently the easiest to install and use. SLATE fills a need not met by existing systems, and has already been used to annotate two corpora, one of which involved over 250 hours of annotation effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3003/>lingvis.io-A Linguistic Visual Analytics Framework</a></strong><br><a href=/people/m/mennatallah-el-assady/>Mennatallah El-Assady</a>
|
<a href=/people/w/wolfgang-jentner/>Wolfgang Jentner</a>
|
<a href=/people/f/fabian-sperrle/>Fabian Sperrle</a>
|
<a href=/people/r/rita-sevastjanova/>Rita Sevastjanova</a>
|
<a href=/people/a/annette-hautli/>Annette Hautli-Janisz</a>
|
<a href=/people/m/miriam-butt/>Miriam Butt</a>
|
<a href=/people/d/daniel-keim/>Daniel Keim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3003><div class="card-body p-3 small">We present a modular framework for the rapid-prototyping of linguistic, web-based, visual analytics applications. Our framework gives developers access to a rich set of machine learning and natural language processing steps, through encapsulating them into <a href=https://en.wikipedia.org/wiki/Microservices>micro-services</a> and combining them into a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>computational pipeline</a>. This processing pipeline is auto-configured based on the requirements of the visualization front-end, making the linguistic processing and visualization design, detached independent development tasks. This paper describes the constellation and modality of our framework, which continues to support the efficient development of various human-in-the-loop, linguistic visual analytics research techniques and applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3004/>SARAL : A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage<span class=acl-fixed-case>SARAL</span>: A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage</a></strong><br><a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a>
|
<a href=/people/j/joel-barry/>Joel Barry</a>
|
<a href=/people/j/jayadev-billa/>Jayadev Billa</a>
|
<a href=/people/m/marjorie-freedman/>Marjorie Freedman</a>
|
<a href=/people/t/thamme-gowda/>Thamme Gowda</a>
|
<a href=/people/c/constantine-lignos/>Constantine Lignos</a>
|
<a href=/people/c/chester-palen-michel/>Chester Palen-Michel</a>
|
<a href=/people/m/michael-pust/>Michael Pust</a>
|
<a href=/people/b/banriskhem-kayang-khonglah/>Banriskhem Kayang Khonglah</a>
|
<a href=/people/s/srikanth-madikeri/>Srikanth Madikeri</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/s/scott-miller/>Scott Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3004><div class="card-body p-3 small">With the increasing democratization of <a href=https://en.wikipedia.org/wiki/Electronic_media>electronic media</a>, vast information resources are available in less-frequently-taught languages such as <a href=https://en.wikipedia.org/wiki/Swahili_language>Swahili</a> or <a href=https://en.wikipedia.org/wiki/Somali_language>Somali</a>. That information, which may be crucially important and not available elsewhere, can be difficult for monolingual English speakers to effectively access. In this paper we present an end-to-end cross-lingual information retrieval (CLIR) and summarization system for low-resource languages that 1) enables English speakers to search foreign language repositories of text and audio using English queries, 2) summarizes the retrieved documents in English with respect to a particular information need, and 3) provides complete transcriptions and translations as needed. The SARAL system achieved the top end-to-end performance in the most recent IARPA MATERIAL CLIR+summarization evaluations. Our demonstration system provides end-to-end open query retrieval and summarization capability, and presents the original source text or audio, <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>speech transcription</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, for two low resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3005/>Jiuge : A Human-Machine Collaborative Chinese Classical Poetry Generation System<span class=acl-fixed-case>J</span>iuge: A Human-Machine Collaborative <span class=acl-fixed-case>C</span>hinese Classical Poetry Generation System</a></strong><br><a href=/people/g/guo-zhipeng/>Guo Zhipeng</a>
|
<a href=/people/x/xiaoyuan-yi/>Xiaoyuan Yi</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/w/wenhao-li/>Wenhao Li</a>
|
<a href=/people/c/cheng-yang/>Cheng Yang</a>
|
<a href=/people/j/jiannan-liang/>Jiannan Liang</a>
|
<a href=/people/h/huimin-chen/>Huimin Chen</a>
|
<a href=/people/y/yuhui-zhang/>Yuhui Zhang</a>
|
<a href=/people/r/ruoyu-li/>Ruoyu Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3005><div class="card-body p-3 small">Research on the automatic generation of poetry, the treasure of human culture, has lasted for decades. Most existing systems, however, are merely model-oriented, which input some user-specified keywords and directly complete the generation process in one pass, with little user participation. We believe that the machine, being a collaborator or an assistant, should not replace human beings in poetic creation. Therefore, we proposed <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a>, a human-machine collaborative Chinese classical poetry generation system. Unlike previous systems, <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a> allows users to revise the unsatisfied parts of a generated poem draft repeatedly. According to the revision, the <a href=https://en.wikipedia.org/wiki/Poetry>poem</a> will be dynamically updated and regenerated. After the revision and modification procedure, the user can write a satisfying poem together with Jiuge system collaboratively. Besides, <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a> can accept multi-modal inputs, such as <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>, <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a> or <a href=https://en.wikipedia.org/wiki/Image>images</a>. By exposing the options of poetry genres, styles and revision modes, <a href=https://en.wikipedia.org/wiki/Jiuge>Jiuge</a>, acting as a professional assistant, allows constant and active participation of users in poetic creation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P19-3007.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3007/>A Multiscale Visualization of Attention in the Transformer Model</a></strong><br><a href=/people/j/jesse-vig/>Jesse Vig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3007><div class="card-body p-3 small">The Transformer is a sequence model that forgoes traditional <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent architectures</a> in favor of a fully attention-based approach. Besides improving performance, an advantage of using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is that it can also help to interpret a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by showing how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source tool</a> that visualizes <a href=https://en.wikipedia.org/wiki/Attention>attention</a> at multiple scales, each of which provides a unique perspective on the <a href=https://en.wikipedia.org/wiki/Attention>attention mechanism</a>. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases : detecting model bias, locating relevant attention heads, and linking neurons to model behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3008/>PostAc : A Visual Interactive Search, Exploration, and Analysis Platform for PhD Intensive Job Postings<span class=acl-fixed-case>P</span>ost<span class=acl-fixed-case>A</span>c : A Visual Interactive Search, Exploration, and Analysis Platform for <span class=acl-fixed-case>P</span>h<span class=acl-fixed-case>D</span> Intensive Job Postings</a></strong><br><a href=/people/c/chenchen-xu/>Chenchen Xu</a>
|
<a href=/people/i/inger-mewburn/>Inger Mewburn</a>
|
<a href=/people/w/will-j-grant/>Will J Grant</a>
|
<a href=/people/h/hanna-suominen/>Hanna Suominen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3008><div class="card-body p-3 small">Over 60 % of Australian PhD graduates land their first job after graduation outside academia, but this <a href=https://en.wikipedia.org/wiki/Job_market>job market</a> remains largely hidden to these <a href=https://en.wikipedia.org/wiki/Job_hunting>job seekers</a>. Employers&#8217; low awareness and interest in attracting PhD graduates means that the term PhD is rarely used as a keyword in job advertisements ; 80 % of companies looking to employ similar researchers do not specifically ask for a PhD qualification. As a result, typing in <a href=https://en.wikipedia.org/wiki/Doctor_of_Philosophy>PhD</a> to a <a href=https://en.wikipedia.org/wiki/Employment_website>job search engine</a> tends to return mostly academic jobs. We set out to make the market for advanced research skills more visible to <a href=https://en.wikipedia.org/wiki/Job_hunting>job seekers</a>. In this paper, we present PostAc, an online platform of authentic job postings that helps PhD graduates sharpen their career thinking. The platform is underpinned by research on the key factors that identify what an employer is looking for when they want to hire a highly skilled researcher. Its ranking model leverages the free-form text embedded in the <a href=https://en.wikipedia.org/wiki/Job_description>job description</a> to quantify the most sought-after PhD skills and educate information seekers about the Australian job-market appetite for <a href=https://en.wikipedia.org/wiki/Doctor_of_Philosophy>PhD skills</a>. The platform makes visible the geographic location, industry sector, <a href=https://en.wikipedia.org/wiki/International_Standard_Classification_of_Occupations>job title</a>, <a href=https://en.wikipedia.org/wiki/Working_time>working hours</a>, continuity, and wage of the research intensive jobs. This is the first data-driven exploration in this field. Both empirical results and <a href=https://en.wikipedia.org/wiki/Online_and_offline>online platform</a> will be presented in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3009/>An adaptable task-oriented dialog system for stand-alone embedded devices</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/v/vu-cong-duy-hoang/>Vu Cong Duy Hoang</a>
|
<a href=/people/t/tuyen-quang-pham/>Tuyen Quang Pham</a>
|
<a href=/people/y/yu-heng-hong/>Yu-Heng Hong</a>
|
<a href=/people/v/vladislavs-dovgalecs/>Vladislavs Dovgalecs</a>
|
<a href=/people/g/guy-bashkansky/>Guy Bashkansky</a>
|
<a href=/people/j/jason-black/>Jason Black</a>
|
<a href=/people/a/andrew-bleeker/>Andrew Bleeker</a>
|
<a href=/people/s/serge-le-huitouze/>Serge Le Huitouze</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3009><div class="card-body p-3 small">This paper describes a spoken-language end-to-end task-oriented dialogue system for small embedded devices such as <a href=https://en.wikipedia.org/wiki/Home_appliance>home appliances</a>. While the current <a href=https://en.wikipedia.org/wiki/System>system</a> implements a smart alarm clock with advanced calendar scheduling functionality, the <a href=https://en.wikipedia.org/wiki/System>system</a> is designed to make it easy to port to other application domains (e.g., the dialogue component factors out domain-specific execution from domain-general actions such as requesting and updating slot values). The system does not require internet connectivity because all components, including <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, dialogue management, execution and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech</a>, run locally on the <a href=https://en.wikipedia.org/wiki/Embedded_system>embedded device</a> (our demo uses a Raspberry Pi). This simplifies deployment, minimizes server costs and most importantly, eliminates user privacy risks. The demo video in alarm domain is here youtu.be/N3IBMGocvHU</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3013/>FASTDial : Abstracting Dialogue Policies for Fast Development of Task Oriented Agents<span class=acl-fixed-case>FASTD</span>ial: Abstracting Dialogue Policies for Fast Development of Task Oriented Agents</a></strong><br><a href=/people/s/serra-sinem-tekiroglu/>Serra Sinem Tekiroglu</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3013><div class="card-body p-3 small">We present a novel abstraction framework called FASTDial for designing task oriented dialogue agents, built on top of the OpenDial toolkit. This framework is meant to facilitate prototyping and development of dialogue systems from scratch also by non tech savvy especially when limited training data is available. To this end, we use a generic and simple frame-slots data-structure with pre-defined dialogue policies that allows for fast design and implementation at the price of some flexibility reduction. Moreover, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> allows for minimizing programming effort and domain expert training time, by hiding away many implementation details. We provide a system demonstration screencast video in the following link : https://vimeo.com/329840716</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3014/>A Neural, Interactive-predictive System for Multimodal Sequence to Sequence Tasks</a></strong><br><a href=/people/a/alvaro-peris/>Álvaro Peris</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3014><div class="card-body p-3 small">We present a demonstration of a neural interactive-predictive system for tackling multimodal sequence to sequence tasks. The system generates text predictions to different sequence to sequence tasks : <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, image and video captioning. These predictions are revised by a human agent, who introduces corrections in the form of characters. The <a href=https://en.wikipedia.org/wiki/System>system</a> reacts to each correction, providing alternative hypotheses, compelling with the feedback provided by the user. The final objective is to reduce the human effort required during this correction process. This <a href=https://en.wikipedia.org/wiki/System>system</a> is implemented following a <a href=https://en.wikipedia.org/wiki/Client&#8211;server_model>client-server architecture</a>. For accessing the <a href=https://en.wikipedia.org/wiki/System>system</a>, we developed a website, which communicates with the neural model, hosted in a local server. From this <a href=https://en.wikipedia.org/wiki/Website>website</a>, the different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> can be tackled following the interactivepredictive framework. We open-source all the code developed for building this <a href=https://en.wikipedia.org/wiki/System>system</a>. The demonstration in hosted in http://casmacat.prhlt.upv.es/interactive-seq2seq.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3017/>KCAT : A Knowledge-Constraint Typing Annotation Tool<span class=acl-fixed-case>KCAT</span>: A Knowledge-Constraint Typing Annotation Tool</a></strong><br><a href=/people/s/sheng-lin/>Sheng Lin</a>
|
<a href=/people/l/luye-zheng/>Luye Zheng</a>
|
<a href=/people/b/bo-chen/>Bo Chen</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3017><div class="card-body p-3 small">In this paper, we propose an efficient Knowledge Constraint Fine-grained Entity Typing Annotation Tool, which further improves the entity typing process through entity linking together with some practical functions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Demo Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P19-3020/>OpenKiwi : An Open Source Framework for <a href=https://en.wikipedia.org/wiki/Quality_assurance>Quality Estimation</a><span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>K</span>iwi: An Open Source Framework for Quality Estimation</a></strong><br><a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/j/jonay-trenous/>Jonay Trénous</a>
|
<a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3020><div class="card-body p-3 small">We introduce OpenKiwi, a Pytorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 201518 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentence-level tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3022/>PerspectroScope : A Window to the World of Diverse Perspectives<span class=acl-fixed-case>P</span>erspectro<span class=acl-fixed-case>S</span>cope: A Window to the World of Diverse Perspectives</a></strong><br><a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3022><div class="card-body p-3 small">This work presents PerspectroScope, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The <a href=https://en.wikipedia.org/wiki/System>system</a> thus lets users explore various perspectives that could touch upon aspects of the issue at hand. The <a href=https://en.wikipedia.org/wiki/System>system</a> is built as a combination of retrieval engines and learned textual-entailment-like classifiers built using a few recent developments in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. To make the <a href=https://en.wikipedia.org/wiki/System>system</a> more adaptive, expand its coverage, and improve its decisions over time, our <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> employs various mechanisms to get corrections from the users. PerspectroScope is available at github.com/CogComp/perspectroscope Web demo link : http://orwell.seas.upenn.edu:4002/ Link to demo video : https://www.youtube.com/watch?v=MXBTR1Sp3Bs</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3023/>HEIDL : Learning <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Expressions</a> with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Human-in-the-Loop<span class=acl-fixed-case>HEIDL</span>: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop</a></strong><br><a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/e/eser-kandogan/>Eser Kandogan</a>
|
<a href=/people/y/yiwei-yang/>Yiwei Yang</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3023><div class="card-body p-3 small">While the role of humans is increasingly recognized in machine learning community, representation of and interaction with <a href=https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)>models</a> in current human-in-the-loop machine learning (HITL-ML) approaches are too low-level and far-removed from human&#8217;s conceptual models. We demonstrate HEIDL, a prototype HITL-ML system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In HEIDL, human&#8217;s role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3026/>ClaimPortal : Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter<span class=acl-fixed-case>C</span>laim<span class=acl-fixed-case>P</span>ortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/sarthak-majithia/>Sarthak Majithia</a>
|
<a href=/people/f/fatma-arslan/>Fatma Arslan</a>
|
<a href=/people/s/sumeet-lubal/>Sumeet Lubal</a>
|
<a href=/people/d/damian-jimenez/>Damian Jimenez</a>
|
<a href=/people/p/priyank-arora/>Priyank Arora</a>
|
<a href=/people/j/josue-caraballo/>Josue Caraballo</a>
|
<a href=/people/c/chengkai-li/>Chengkai Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3026><div class="card-body p-3 small">We present ClaimPortal, a <a href=https://en.wikipedia.org/wiki/Web_application>web-based platform</a> for monitoring, searching, checking, and analyzing English factual claims on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> from the <a href=https://en.wikipedia.org/wiki/Politics_of_the_United_States>American political domain</a>. We explain the <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> of ClaimPortal, its components and functions, and the <a href=https://en.wikipedia.org/wiki/User_interface>user interface</a>. While the last several years have witnessed a substantial growth in interests and efforts in the area of computational fact-checking, ClaimPortal is a novel infrastructure in that <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checkers</a> have largely skipped factual claims in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. It can be a highly powerful tool to both general web users and <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checkers</a>. It will also be an educational resource in helping cultivate a society that is less susceptible to <a href=https://en.wikipedia.org/wiki/Falsity>falsehoods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3028" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3028/>Parallax : Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae<span class=acl-fixed-case>P</span>arallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae</a></strong><br><a href=/people/p/piero-molino/>Piero Molino</a>
|
<a href=/people/y/yang-wang/>Yang Wang</a>
|
<a href=/people/j/jiawei-zhang/>Jiawei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3028><div class="card-body p-3 small">Embeddings are a fundamental component of many modern machine learning and natural language processing models. Understanding <a href=https://en.wikipedia.org/wiki/Mathematical_model>them</a> and visualizing <a href=https://en.wikipedia.org/wiki/Mathematical_model>them</a> is essential for gathering insights about the information they capture and the behavior of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we introduce <a href=https://en.wikipedia.org/wiki/Parallax>Parallax</a>, a <a href=https://en.wikipedia.org/wiki/Tool>tool</a> explicitly designed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Parallax allows the user to use both state-of-the-art embedding analysis methods (PCA and t-SNE) and a simple yet effective task-oriented approach where users can explicitly define the axes of the projection through algebraic formulae. % consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a>, which makes detailed analyses and comparison among multiple sets of embeddings challenging. In this approach, <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are projected into a semantically meaningful subspace, which enhances interpretability and allows for more fine-grained analysis. We demonstrate the power of the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> and the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> through a series of <a href=https://en.wikipedia.org/wiki/Case_study>case studies</a> and a <a href=https://en.wikipedia.org/wiki/User_study>user study</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3031" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3031/>TARGER : Neural Argument Mining at Your Fingertips<span class=acl-fixed-case>TARGER</span>: Neural Argument Mining at Your Fingertips</a></strong><br><a href=/people/a/artem-chernodub/>Artem Chernodub</a>
|
<a href=/people/o/oleksiy-oliynyk/>Oleksiy Oliynyk</a>
|
<a href=/people/p/philipp-heidenreich/>Philipp Heidenreich</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3031><div class="card-body p-3 small">We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user&#8217;s side. The open source code ensures portability to other domains and use cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3032 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-3032" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-3032/>MoNoise : A Multi-lingual and Easy-to-use Lexical Normalization Tool<span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>N</span>oise: A Multi-lingual and Easy-to-use Lexical Normalization Tool</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3032><div class="card-body p-3 small">In this paper, we introduce and demonstrate the online demo as well as the command line interface of a lexical normalization system (MoNoise) for a variety of languages. We further improve this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> by using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from the original word for every <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization candidate</a>. For comparison with future work, we propose the bundling of seven datasets in six languages to form a new <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, together with a novel evaluation metric which is particularly suitable for cross-dataset comparisons. MoNoise reaches a new state-of-art performance for six out of seven of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Furthermore, we allow the user to tune the &#8216;aggressiveness&#8217; of the normalization, and show how the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be made more efficient with only a small loss in performance. The online demo can be found on : http://www.robvandergoot.com/monoise and the corresponding code on : https://bitbucket.org/robvanderg/monoise/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-3033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-3033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-3033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-3033/>Level-Up : Learning to Improve Proficiency Level of Essays</a></strong><br><a href=/people/w/wen-bin-han/>Wen-Bin Han</a>
|
<a href=/people/j/jhih-jie-chen/>Jhih-Jie Chen</a>
|
<a href=/people/c/chingyu-yang/>Chingyu Yang</a>
|
<a href=/people/j/jason-s-chang/>Jason Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-3033><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for generating suggestions on a given sentence for improving the proficiency level. In our approach, the sentence is transformed into a sequence of grammatical elements aimed at providing suggestions of more advanced grammar elements based on originals. The method involves parsing the sentence, identifying <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical elements</a>, and ranking related elements to recommend a higher level of <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical element</a>. We present a prototype tutoring system, Level-Up, that applies the method to English learners&#8217; essays in order to assist them in writing and reading. Evaluation on a set of <a href=https://en.wikipedia.org/wiki/Essay>essays</a> shows that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> does assist user in writing.</div></div></div><hr><div id=p19-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P19-4/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-4000/>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P19-4002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P19-4002/>Graph-Based Meaning Representations : Design and Processing</a></strong><br><a href=/people/a/alexander-koller/>Alexander Koller</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-4002><div class="card-body p-3 small">This tutorial is on representing and processing <a href=https://en.wikipedia.org/wiki/Sentence_(mathematical_logic)>sentence meaning</a> in the form of labeled directed graphs. The tutorial will (a) briefly review relevant background in formal and linguistic semantics ; (b) semi-formally define a unified abstract view on different flavors of semantic graphs and associated terminology ; (c) survey common frameworks for graph-based meaning representation and available graph banks ; and (d) offer a technical overview of a representative selection of different parsing approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-4004/>Computational Analysis of Political Texts : Bridging Research Efforts Across Communities</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-4004><div class="card-body p-3 small">In the last twenty years, political scientists started adopting and developing natural language processing (NLP) methods more actively in order to exploit <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> as an additional source of data in their analyses. Over the last decade the usage of computational methods for analysis of political texts has drastically expanded in scope, allowing for a sustained growth of the text-as-data community in <a href=https://en.wikipedia.org/wiki/Political_science>political science</a>. In <a href=https://en.wikipedia.org/wiki/Political_science>political science</a>, NLP methods have been extensively used for a number of analyses types and tasks, including inferring policy position of actors from textual evidence, detecting topics in <a href=https://en.wikipedia.org/wiki/Political_philosophy>political texts</a>, and analyzing stylistic aspects of <a href=https://en.wikipedia.org/wiki/Political_philosophy>political texts</a> (e.g., assessing the role of <a href=https://en.wikipedia.org/wiki/Ambiguity>language ambiguity</a> in framing the political agenda). Just like in numerous other domains, much of the work on computational analysis of political texts has been enabled and facilitated by the development of resources such as, the topically coded electoral programmes (e.g., the Manifesto Corpus) or topically coded legislative texts (e.g., the Comparative Agenda Project). Political scientists created resources and used available NLP methods to process textual data largely in isolation from the NLP community. At the same time, NLP researchers addressed closely related tasks such as election prediction, ideology classification, and stance detection. In other words, these two communities have been largely agnostic of one another, with NLP researchers mostly unaware of interesting applications in <a href=https://en.wikipedia.org/wiki/Political_science>political science</a> and political scientists not applying cutting-edge NLP methodology to their problems. The main goal of this tutorial is to systematize and analyze the body of research work on political texts from both communities. We aim to provide a gentle, all-round introduction to methods and tasks related to computational analysis of political texts. Our vision is to bring the two research communities closer to each other and contribute to faster and more significant developments in this interdisciplinary research area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-4005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-4005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-4005/>Wikipedia as a Resource for Text Analysis and Retrieval<span class=acl-fixed-case>W</span>ikipedia as a Resource for Text Analysis and Retrieval</a></strong><br><a href=/people/m/marius-pasca/>Marius Pasca</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-4005><div class="card-body p-3 small">This tutorial examines the role of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> in tasks related to text analysis and retrieval. Text analysis tasks, which take advantage of Wikipedia, include <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity disambiguation</a> and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. In <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, a better understanding of the structure and meaning of queries helps in matching queries against documents, clustering search results, answer and entity retrieval and retrieving knowledge panels for queries asking about popular entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-4006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-4006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-4006/>Deep Bayesian Natural Language Processing<span class=acl-fixed-case>B</span>ayesian Natural Language Processing</a></strong><br><a href=/people/j/jen-tzung-chien/>Jen-Tzung Chien</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-4006><div class="card-body p-3 small">This introductory tutorial addresses the advances in deep Bayesian learning for natural language with ubiquitous applications ranging from <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> to <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a>, text classification, <a href=https://en.wikipedia.org/wiki/Text_segmentation>text segmentation</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, image caption generation, sentence generation, dialogue control, sentiment classification, <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation system</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, to name a few. Traditionally, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> is taken to be a <a href=https://en.wikipedia.org/wiki/Machine_learning>learning process</a> where the inference or optimization is based on the real-valued deterministic model. The semantic structure in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in <a href=https://en.wikipedia.org/wiki/Mathematical_logic>mathematical logic</a> or <a href=https://en.wikipedia.org/wiki/Computer_program>computer programs</a>. The <a href=https://en.wikipedia.org/wiki/Cumulative_distribution_function>distribution function</a> in discrete or continuous latent variable model for <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> may not be properly decomposed or estimated. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including <a href=https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process>hierarchical Dirichlet process</a>, <a href=https://en.wikipedia.org/wiki/Chinese_restaurant_process>Chinese restaurant process</a>, hierarchical Pitman-Yor process, <a href=https://en.wikipedia.org/wiki/Indian_buffet_process>Indian buffet process</a>, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>, <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>long short-term memory</a>, sequence-to-sequence model, variational auto-encoder, <a href=https://en.wikipedia.org/wiki/Generative_adversarial_network>generative adversarial network</a>, attention mechanism, memory-augmented neural network, skip neural network, <a href=https://en.wikipedia.org/wiki/Stochastic_neural_network>stochastic neural network</a>, predictive state neural network and policy neural network. We present how these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> for complicated models. The <a href=https://en.wikipedia.org/wiki/Word_embedding>word and sentence embeddings</a>, clustering and co-clustering are merged with linguistic and semantic constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P19-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P19-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P19-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P19-4007/>Unsupervised Cross-Lingual Representation Learning</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P19-4007><div class="card-body p-3 small">In this tutorial, we provide a comprehensive survey of the exciting recent work on cutting-edge weakly-supervised and unsupervised cross-lingual word representations. After providing a brief history of supervised cross-lingual word representations, we focus on : 1) how to induce weakly-supervised and unsupervised cross-lingual word representations in truly resource-poor settings where bilingual supervision can not be guaranteed ; 2) critical examinations of different training conditions and requirements under which unsupervised algorithms can and can not work effectively ; 3) more robust methods for distant language pairs that can mitigate instability issues and low performance for distant language pairs ; 4) how to comprehensively evaluate such representations ; and 5) diverse applications that benefit from cross-lingual word representations (e.g., MT, dialogue, cross-lingual sequence labeling and structured prediction applications, cross-lingual IR).</div></div></div><hr><div id=w19-32><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-32.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-32/>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3200/>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task</a></strong><br><a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3201/>Extracting Kinship from Obituary to Enhance Electronic Health Records for Genetic Research</a></strong><br><a href=/people/k/kai-he/>Kai He</a>
|
<a href=/people/j/jialun-wu/>Jialun Wu</a>
|
<a href=/people/x/xiaoyong-ma/>Xiaoyong Ma</a>
|
<a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/m/ming-huang/>Ming Huang</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/l/lixia-yao/>Lixia Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3201><div class="card-body p-3 small">Claims database and electronic health records database do not usually capture kinship or family relationship information, which is imperative for <a href=https://en.wikipedia.org/wiki/Genetic_research>genetic research</a>. We identify online obituaries as a new data source and propose a special named entity recognition and relation extraction solution to extract names and kinships from online obituaries. Built on 1,809 annotated obituaries and a novel tagging scheme, our joint neural model achieved macro-averaged precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/F-number>F measure</a> of 72.69 %, 78.54 % and 74.93 %, and micro-averaged precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/F-number>F measure</a> of 95.74 %, 98.25 % and 96.98 % using 57 <a href=https://en.wikipedia.org/wiki/Kinship>kinships</a> with 10 or more examples in a 10-fold cross-validation experiment. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> performance improved dramatically when trained with 34 <a href=https://en.wikipedia.org/wiki/Kinship>kinships</a> with 50 or more examples. Leveraging additional information such as age, death date, birth date and residence mentioned by obituaries, we foresee a promising future of supplementing EHR databases with comprehensive and accurate kinship information for genetic research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3202/>Lexical Normalization of User-Generated Medical Text</a></strong><br><a href=/people/a/anne-dirkson/>Anne Dirkson</a>
|
<a href=/people/s/suzan-verberne/>Suzan Verberne</a>
|
<a href=/people/w/wessel-kraaij/>Wessel Kraaij</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3202><div class="card-body p-3 small">In the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>, <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated social media text</a> is increasingly used as a valuable complementary knowledge source to <a href=https://en.wikipedia.org/wiki/Medical_literature>scientific medical literature</a>. The extraction of this knowledge is complicated by <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial language use</a> and misspellings. Yet, lexical normalization of such <a href=https://en.wikipedia.org/wiki/Data>data</a> has not been addressed properly. This paper presents an unsupervised, data-driven spelling correction module for medical social media. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms state-of-the-art spelling correction and can detect mistakes with an <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>F0.5</a> of 0.888. Additionally, we present a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for spelling mistake detection and correction on a medical patient forum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3206/>HITSZ-ICRC : A Report for SMM4H Shared Task 2019-Automatic Classification and Extraction of Adverse Effect Mentions in Tweets<span class=acl-fixed-case>HITSZ</span>-<span class=acl-fixed-case>ICRC</span>: A Report for <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> Shared Task 2019-Automatic Classification and Extraction of Adverse Effect Mentions in Tweets</a></strong><br><a href=/people/s/shuai-chen/>Shuai Chen</a>
|
<a href=/people/y/yuanhang-huang/>Yuanhang Huang</a>
|
<a href=/people/x/xiaowei-huang/>Xiaowei Huang</a>
|
<a href=/people/h/haoming-qin/>Haoming Qin</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/b/buzhou-tang/>Buzhou Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3206><div class="card-body p-3 small">This is the system description of the Harbin Institute of Technology Shenzhen (HITSZ) team for the first and second subtasks of the fourth Social Media Mining for Health Applications (SMM4H) shared task in 2019. The two subtasks are automatic classification and extraction of adverse effect mentions in tweets. The systems for the two subtasks are based on bidirectional encoder representations from transformers (BERT), and achieves promising results. Among the systems we developed for subtask1, the best F1-score was 0.6457, for subtask2, the best relaxed F1-score and the best strict F1-score were 0.614 and 0.407 respectively. Our <a href=https://en.wikipedia.org/wiki/System>system</a> ranks first among all <a href=https://en.wikipedia.org/wiki/System>systems</a> on subtask1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3208/>Approaching SMM4H with Merged Models and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task Learning</a><span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> with Merged Models and Multi-task Learning</a></strong><br><a href=/people/t/tilia-ellendorff/>Tilia Ellendorff</a>
|
<a href=/people/l/lenz-furrer/>Lenz Furrer</a>
|
<a href=/people/n/nicola-colic/>Nicola Colic</a>
|
<a href=/people/n/noemi-aepli/>Noëmi Aepli</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3208><div class="card-body p-3 small">We describe our submissions to the 4th edition of the Social Media Mining for Health Applications (SMM4H) shared task. Our team (UZH) participated in two sub-tasks : Automatic classifications of adverse effects mentions in tweets (Task 1) and Generalizable identification of personal health experience mentions (Task 4). For our submissions, we exploited ensembles based on a pre-trained language representation with a neural transformer architecture (BERT) (Tasks 1 and 4) and a CNN-BiLSTM(-CRF) network within a multi-task learning scenario (Task 1). These <a href=https://en.wikipedia.org/wiki/System>systems</a> are placed on top of a carefully crafted pipeline of domain-specific preprocessing steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3209/>Identifying Adverse Drug Events Mentions in Tweets Using Attentive, Collocated, and Aggregated Medical Representation</a></strong><br><a href=/people/x/xinyan-zhao/>Xinyan Zhao</a>
|
<a href=/people/d/deahan-yu/>Deahan Yu</a>
|
<a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3209><div class="card-body p-3 small">Identifying mentions of medical concepts in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is challenging because of high variability in free text. In this paper, we propose a novel neural network architecture, the Collocated LSTM with Attentive Pooling and Aggregated representation (CLAPA), that integrates a bidirectional LSTM model with attention and pooling strategy and utilizes the collocation information from training data to improve the representation of medical concepts. The collocation and aggregation layers improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on the task of identifying mentions of adverse drug events (ADE) in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. Using the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> made available as part of the workshop shared task, we show that careful selection of neighborhood contexts can help uncover useful local information and improve the overall medical concept representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3210/>Correlating Twitter Language with Community-Level Health Outcomes<span class=acl-fixed-case>T</span>witter Language with Community-Level Health Outcomes</a></strong><br><a href=/people/a/arno-schneuwly/>Arno Schneuwly</a>
|
<a href=/people/r/ralf-grubenmann/>Ralf Grubenmann</a>
|
<a href=/people/s/severine-rion-logean/>Séverine Rion Logean</a>
|
<a href=/people/m/mark-cieliebak/>Mark Cieliebak</a>
|
<a href=/people/m/martin-jaggi/>Martin Jaggi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3210><div class="card-body p-3 small">We study how language on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is linked to mortal diseases such as atherosclerotic heart disease (AHD), <a href=https://en.wikipedia.org/wiki/Diabetes>diabetes</a> and various types of <a href=https://en.wikipedia.org/wiki/Cancer>cancer</a>. Our proposed model leverages state-of-the-art <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>, followed by a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> and <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>, without the need of additional labelled data. It allows to predict community-level medical outcomes from <a href=https://en.wikipedia.org/wiki/Language>language</a>, and thereby potentially translate these to the individual level. The method is applicable to a wide range of target variables and allows us to discover known and potentially novel correlations of <a href=https://en.wikipedia.org/wiki/Outcome_(probability)>medical outcomes</a> with <a href=https://en.wikipedia.org/wiki/Quality_of_life>life-style aspects</a> and other socioeconomic risk factors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3211/>Affective Behaviour Analysis of On-line User Interactions : Are On-line Support Groups More Therapeutic than <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>?<span class=acl-fixed-case>T</span>witter?</a></strong><br><a href=/people/g/giuliano-tortoreto/>Giuliano Tortoreto</a>
|
<a href=/people/e/evgeny-stepanov/>Evgeny Stepanov</a>
|
<a href=/people/a/alessandra-cervone/>Alessandra Cervone</a>
|
<a href=/people/m/mateusz-dubiel/>Mateusz Dubiel</a>
|
<a href=/people/g/giuseppe-riccardi/>Giuseppe Riccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3211><div class="card-body p-3 small">The increase in the prevalence of mental health problems has coincided with a growing popularity of health related social networking sites. Regardless of their therapeutic potential, on-line support groups (OSGs) can also have negative effects on patients. In this work we propose a novel <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to automatically verify the presence of therapeutic factors in <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking websites</a> by using Natural Language Processing (NLP) techniques. The <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is evaluated on on-line asynchronous multi-party conversations collected from an <a href=https://en.wikipedia.org/wiki/Operations_research>OSG</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. The results of the analysis indicate that therapeutic factors occur more frequently in OSG conversations than in Twitter conversations. Moreover, the analysis of OSG conversations reveals that the users of that <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> are supportive, and interactions are likely to lead to the improvement of their <a href=https://en.wikipedia.org/wiki/Emotion>emotional state</a>. We believe that our method provides a stepping stone towards automatic analysis of emotional states of users of online platforms. Possible applications of the method include provision of guidelines that highlight potential implications of using such platforms on users&#8217; mental health, and/or support in the analysis of their impact on specific individuals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3213/>NLP@UNED at SMM4H 2019 : Neural Networks Applied to Automatic Classifications of Adverse Effects Mentions in Tweets<span class=acl-fixed-case>NLP</span>@<span class=acl-fixed-case>UNED</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2019: Neural Networks Applied to Automatic Classifications of Adverse Effects Mentions in Tweets</a></strong><br><a href=/people/j/javier-cortes-tejada/>Javier Cortes-Tejada</a>
|
<a href=/people/j/juan-martinez-romo/>Juan Martinez-Romo</a>
|
<a href=/people/l/lourdes-araujo/>Lourdes Araujo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3213><div class="card-body p-3 small">This paper describes a system for automatically classifying adverse effects mentions in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> developed for the task 1 at Social Media Mining for Health Applications (SMM4H) Shared Task 2019. We have developed a system based on LSTM neural networks inspired by the excellent results obtained by deep learning classifiers in the last edition of this task. The <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is trained along with Twitter GloVe pre-trained word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3214/>Detecting and Extracting of Adverse Drug Reaction Mentioning Tweets with Multi-Head Self Attention</a></strong><br><a href=/people/s/suyu-ge/>Suyu Ge</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3214><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> for the first and second shared tasks of the fourth Social Media Mining for Health Applications (SMM4H) workshop. We enhance tweet representation with a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and distinguish the importance of different words with Multi-Head Self-Attention. In addition, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> is exploited to make up for the data shortage. Our system achieved competitive results on both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> with an F1-score of 0.5718 for task 1 and 0.653 (overlap) / 0.357 (strict) for task 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3215/>Deep Learning for Identification of Adverse Effect Mentions In Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/p/paul-barry/>Paul Barry</a>
|
<a href=/people/o/ozlem-uzuner/>Ozlem Uzuner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3215><div class="card-body p-3 small">Social Media Mining for Health Applications (SMM4H) Adverse Effect Mentions Shared Task challenges participants to accurately identify spans of text within a tweet that correspond to Adverse Effects (AEs) resulting from medication usage (Weissenbacher et al., 2019). This task features a training data set of 2,367 tweets, in addition to a 1,000 tweet evaluation data set. The solution presented here features a bidirectional Long Short-term Memory Network (bi-LSTM) for the generation of character-level embeddings. It uses a second bi-LSTM trained on both character and token level embeddings to feed a Conditional Random Field (CRF) which provides the final <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. This paper further discusses the deep learning algorithms used in our <a href=https://en.wikipedia.org/wiki/Solution>solution</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3216/>Using Machine Learning and Deep Learning Methods to Find Mentions of Adverse Drug Reactions in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/p/pilar-lopez-ubeda/>Pilar López Úbeda</a>
|
<a href=/people/m/manuel-carlos-diaz-galiano/>Manuel Carlos Díaz Galiano</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Maite Martin</a>
|
<a href=/people/l/l-alfonso-urena-lopez/>L. Alfonso Urena Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3216><div class="card-body p-3 small">Over time the use of <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> is becoming very popular platforms for <a href=https://en.wikipedia.org/wiki/Health_informatics>sharing health related information</a>. Social Media Mining for Health Applications (SMM4H) provides tasks such as those described in this document to help manage information in the health domain. This document shows the first participation of the SINAI group. We study approaches based on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to extract adverse drug reaction mentions from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. The results obtained in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are encouraging, we are close to the average of all participants and even above in some cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3217/>Towards Text Processing Pipelines to Identify Adverse Drug Events-related Tweets : University of Michigan @ SMM4H 2019 Task 1<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>ichigan @ <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2019 Task 1</a></strong><br><a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a>
|
<a href=/people/g/grace-ganzel/>Grace Ganzel</a>
|
<a href=/people/b/bryan-romas/>Bryan Romas</a>
|
<a href=/people/d/deahan-yu/>Deahan Yu</a>
|
<a href=/people/a/amy-austin/>Amy Austin</a>
|
<a href=/people/n/neha-bhomia/>Neha Bhomia</a>
|
<a href=/people/s/socheatha-chan/>Socheatha Chan</a>
|
<a href=/people/s/stephanie-hall/>Stephanie Hall</a>
|
<a href=/people/v/van-le/>Van Le</a>
|
<a href=/people/a/aaron-miller/>Aaron Miller</a>
|
<a href=/people/o/olawunmi-oduyebo/>Olawunmi Oduyebo</a>
|
<a href=/people/a/aulia-song/>Aulia Song</a>
|
<a href=/people/r/radhika-sondhi/>Radhika Sondhi</a>
|
<a href=/people/d/danny-teng/>Danny Teng</a>
|
<a href=/people/h/hao-tseng/>Hao Tseng</a>
|
<a href=/people/k/kim-vuong/>Kim Vuong</a>
|
<a href=/people/s/stephanie-zimmerman/>Stephanie Zimmerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3217><div class="card-body p-3 small">We participated in Task 1 of the Social Media Mining for Health Applications (SMM4H) 2019 Shared Tasks on detecting mentions of adverse drug events (ADEs) in tweets. Our approach relied on a text processing pipeline for <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, and training traditional <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning and deep learning models</a>. Our submitted runs performed above average for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3219/>Give It a Shot : Few-shot Learning to Normalize ADR Mentions in Social Media Posts<span class=acl-fixed-case>ADR</span> Mentions in Social Media Posts</a></strong><br><a href=/people/e/emmanouil-manousogiannis/>Emmanouil Manousogiannis</a>
|
<a href=/people/s/sepideh-mesbah/>Sepideh Mesbah</a>
|
<a href=/people/a/alessandro-bozzon/>Alessandro Bozzon</a>
|
<a href=/people/s/selene-baez/>Selene Baez</a>
|
<a href=/people/r/robert-jan-sips/>Robert Jan Sips</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3219><div class="card-body p-3 small">This paper describes the system that team MYTOMORROWS-TU DELFT developed for the 2019 Social Media Mining for Health Applications (SMM4H) Shared Task 3, for the end-to-end normalization of ADR tweet mentions to their corresponding MEDDRA codes. For the first two steps, we reuse a state-of-the art approach, focusing our contribution on the final entity-linking step. For that we propose a simple Few-Shot learning approach, based on pre-trained word embeddings and data from the <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a>, combined with the provided training data. Our system (relaxed F1 : 0.337-0.345) outperforms the average (relaxed F1 0.2972) of the participants in this task, demonstrating the potential feasibility of few-shot learning in the context of medical text normalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3221/>Detection of Adverse Drug Reaction Mentions in Tweets Using ELMo<span class=acl-fixed-case>ELM</span>o</a></strong><br><a href=/people/s/sarah-sarabadani/>Sarah Sarabadani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3221><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> used by our team in SMM4H 2019 shared task. We submitted results for subtasks 1 and 2. For task 1 which aims to detect tweets with Adverse Drug Reaction (ADR) mentions we used ELMo embeddings which is a deep contextualized word representation able to capture both syntactic and semantic characteristics. For task 2, which focuses on extraction of ADR mentions, first the same architecture as task 1 was used to identify whether or not a tweet contains ADR. Then, for tweets positively classified as mentioning ADR, the relevant text span was identified by similarity matching with 3 different lexicon sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3222/>Adverse Drug Effect and Personalized Health Mentions, CLaC at SMM4H 2019, Tasks 1 and 4<span class=acl-fixed-case>CL</span>a<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2019, Tasks 1 and 4</a></strong><br><a href=/people/p/parsa-bagherzadeh/>Parsa Bagherzadeh</a>
|
<a href=/people/n/nadia-sheikh/>Nadia Sheikh</a>
|
<a href=/people/s/sabine-bergler/>Sabine Bergler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3222><div class="card-body p-3 small">CLaC labs participated in Task 1 and 4 of SMM4H 2019. We pursed two main objectives in our submission. First we tried to use some textual features in a deep net framework, and second, the potential use of more than one <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> was tested. The results seem positively affected by the proposed <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3223 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3223/>MIDAS@SMM4H-2019 : Identifying Adverse Drug Reactions and Personal Health Experience Mentions from Twitter<span class=acl-fixed-case>MIDAS</span>@<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>-2019: Identifying Adverse Drug Reactions and Personal Health Experience Mentions from <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/d/debanjan-mahata/>Debanjan Mahata</a>
|
<a href=/people/s/sarthak-anand/>Sarthak Anand</a>
|
<a href=/people/h/haimin-zhang/>Haimin Zhang</a>
|
<a href=/people/s/simra-shahid/>Simra Shahid</a>
|
<a href=/people/l/laiba-mehnaz/>Laiba Mehnaz</a>
|
<a href=/people/y/yaman-kumar/>Yaman Kumar</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Ratn Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3223><div class="card-body p-3 small">In this paper, we present our approach and the system description for the Social Media Mining for Health Applications (SMM4H) Shared Task 1,2 and 4 (2019). Our main contribution is to show the effectiveness of Transfer Learning approaches like BERT and ULMFiT, and how they generalize for the classification tasks like identification of adverse drug reaction mentions and reporting of personal health problems in tweets. We show the use of stacked embeddings combined with BLSTM+CRF tagger for identifying spans mentioning adverse drug reactions in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. We also show that these approaches perform well even with imbalanced dataset in comparison to undersampling and oversampling.</div></div></div><hr><div id=w19-33><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-33.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-33/>Proceedings of the First International Workshop on Designing Meaning Representations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3300/>Proceedings of the First International Workshop on Designing Meaning Representations</a></strong><br><a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/j/james-pustejovksy/>James Pustejovksy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3303/>Modeling Quantification and Scope in Abstract Meaning Representations<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentations</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/k/ken-lai/>Ken Lai</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3303><div class="card-body p-3 small">In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification : <a href=https://en.wikipedia.org/wiki/Quantification_(science)>quantification</a>, <a href=https://en.wikipedia.org/wiki/Negation>negation</a> (generally), and <a href=https://en.wikipedia.org/wiki/Modal_logic>modality</a>. The resulting representation, which we call Uniform Meaning Representation (UMR), adopts the predicative core of AMR and embeds it under a scope graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways : (a) they are more transparent ; and (b) they specify default scope when possible. &#8216;</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3306/>Generating Discourse Inferences from Unscoped Episodic Logical Formulas</a></strong><br><a href=/people/g/gene-kim/>Gene Kim</a>
|
<a href=/people/b/benjamin-kane/>Benjamin Kane</a>
|
<a href=/people/v/viet-duong/>Viet Duong</a>
|
<a href=/people/m/muskaan-mendiratta/>Muskaan Mendiratta</a>
|
<a href=/people/g/graeme-mcguire/>Graeme McGuire</a>
|
<a href=/people/s/sophie-sackstein/>Sophie Sackstein</a>
|
<a href=/people/g/georgiy-platonov/>Georgiy Platonov</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3306><div class="card-body p-3 small">Abstract Unscoped episodic logical form (ULF) is a semantic representation capturing the predicate-argument structure of English within the episodic logic formalism in relation to the syntactic structure, while leaving scope, <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a>, and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> unresolved. We describe how ULF can be used to generate natural language inferences that are grounded in the semantic and syntactic structure through a small set of rules defined over interpretable predicates and transformations on ULFs. The semantic restrictions placed by ULF semantic types enables us to ensure that the inferred structures are semantically coherent while the nearness to <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> enables accurate mapping to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We demonstrate these <a href=https://en.wikipedia.org/wiki/Statistical_inference>inferences</a> on four classes of conversationally-oriented inferences in a <a href=https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance>mixed genre dataset</a> with 68.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> from human judgments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3307/>A Plea for Information Structure as a Part of Meaning Representation</a></strong><br><a href=/people/e/eva-hajicova/>Eva Hajicova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3307><div class="card-body p-3 small">The view that the representation of information structure (IS) should be a part of (any type of) representation of meaning is based on the fact that IS is a semantically relevant phenomenon. In the contribution, three arguments supporting this view are briefly summarized, namely, the relation of IS to the interpretation of negation and presupposition, the relevance of IS to the understanding of discourse connectivity and for the establishment and interpretation of coreference relations. Afterwards, possible integration of the description of the main ingredient of <a href=https://en.wikipedia.org/wiki/Isoamyl_acetate>IS</a> into a meaning representation is illustrated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3308/>TCL-a Lexicon of Turkish Discourse Connectives<span class=acl-fixed-case>TCL</span> - a Lexicon of <span class=acl-fixed-case>T</span>urkish Discourse Connectives</a></strong><br><a href=/people/d/deniz-zeyrek/>Deniz Zeyrek</a>
|
<a href=/people/k/kezban-basibuyuk/>Kezban Başıbüyük</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3308><div class="card-body p-3 small">It is known that discourse connectives are the most salient indicators of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>. State-of-the-art parsers being developed to predict explicit discourse connectives exploit annotated discourse corpora but a lexicon of discourse connectives is also needed to enable further research in discourse structure and support the development of language technologies that use these structures for text understanding. This paper presents a lexicon of Turkish discourse connectives built by automatic means. The <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> has the format of the German connective lexicon, DiMLex, where for each discourse connective, information about the connective&#8216;s orthographic variants, syntactic category and senses are provided along with sample relations. In this paper, we describe the data sources we used and the development steps of the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3310/>Ellipsis in Chinese AMR Corpus<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>AMR</span> Corpus</a></strong><br><a href=/people/y/yihuan-liu/>Yihuan Liu</a>
|
<a href=/people/b/bin-li/>Bin Li</a>
|
<a href=/people/p/peiyi-yan/>Peiyi Yan</a>
|
<a href=/people/l/li-song/>Li Song</a>
|
<a href=/people/w/weiguang-qu/>Weiguang Qu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3310><div class="card-body p-3 small">Ellipsis is very common in language. It&#8217;s necessary for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> to restore the <a href=https://en.wikipedia.org/wiki/Elision>elided elements</a> in a sentence. However, there&#8217;s only a few corpora annotating the <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipsis</a>, which draws back the automatic detection and recovery of the ellipsis. This paper introduces the annotation of ellipsis in Chinese sentences, using a novel graph-based representation Abstract Meaning Representation (AMR), which has a good mechanism to restore the elided elements manually. We annotate 5,000 sentences selected from Chinese TreeBank (CTB). We find that 54.98 % of sentences have <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipses</a>. 92 % of the <a href=https://en.wikipedia.org/wiki/Ellipse>ellipses</a> are restored by copying the antecedents&#8217; concepts. and 12.9 % of them are the new added concepts. In addition, we find that the elided element is a word or phrase in most cases, but sometimes only the head of a phrase or parts of a phrase, which is rather hard for the automatic recovery of ellipsis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3313/>Meaning Representation of Null Instantiated Semantic Roles in FrameNet<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/m/miriam-r-l-petruck/>Miriam R L Petruck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3313><div class="card-body p-3 small">Humans have the unique ability to infer information about participants in a scene, even if they are not mentioned in a text about that scene. Computer systems can not do so without explicit information about those participants. This paper addresses the linguistic phenomenon of null-instantiated frame elements, i.e., implicit semantic roles, and their representation in FrameNet (FN). It motivates FN&#8217;s annotation practice, and illustrates three types of null-instantiated arguments that <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> tracks, noting that other lexical resources do not record such semantic-pragmatic information, despite its need in natural language understanding (NLU), and the elaborate efforts to create new datasets. It challenges the community to appeal to FN data to develop more sophisticated techniques for recognizing implicit semantic roles, and creating needed datasets. Although the annotation of null-instantiated roles was lexicographically motivated, FN provides useful information for <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>, and therefore must be considered in the design of any meaning representation for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3314/>Copula and Case-Stacking Annotations for Korean AMR<span class=acl-fixed-case>K</span>orean <span class=acl-fixed-case>AMR</span></a></strong><br><a href=/people/h/hyonsu-choe/>Hyonsu Choe</a>
|
<a href=/people/j/jiyoon-han/>Jiyoon Han</a>
|
<a href=/people/h/hyejin-park/>Hyejin Park</a>
|
<a href=/people/h/hansaem-kim/>Hansaem Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3314><div class="card-body p-3 small">This paper concerns the application of Abstract Meaning Representation (AMR) to <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. In this regard, it focuses on the <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>copula construction</a> and its negation and the case-stacking phenomenon thereof. To illustrate this clearly, we reviewed the : domain annotation scheme from various perspectives. In this process, the existing <a href=https://en.wikipedia.org/wiki/Annotation>annotation guidelines</a> were improved to devise <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a> for each issue under the principle of pursuing consistency and efficiency of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> without distorting the characteristics of <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3316 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-3316.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3316/>Preparing SNACS for Subjects and Objects<span class=acl-fixed-case>SNACS</span> for Subjects and Objects</a></strong><br><a href=/people/a/adi-shalev/>Adi Shalev</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3316><div class="card-body p-3 small">Research on adpositions and possessives in multiple languages has led to a small inventory of general-purpose meaning classes that disambiguate tokens. Importantly, that work has argued for a principled separation of the semantic role in a scene from the function coded by <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntax</a>. Here, we ask whether this approach can be generalized beyond adpositions and <a href=https://en.wikipedia.org/wiki/Possessive>possessives</a> to cover all scene participantsincluding subjects and objectsdirectly, without reference to a <a href=https://en.wikipedia.org/wiki/Frame_language>frame lexicon</a>. We present new guidelines for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the results of an interannotator agreement study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3317/>A Case Study on <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>Meaning Representation</a> for Vietnamese<span class=acl-fixed-case>V</span>ietnamese</a></strong><br><a href=/people/h/ha-linh/>Ha Linh</a>
|
<a href=/people/h/huyen-nguyen/>Huyen Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3317><div class="card-body p-3 small">This paper presents a case study on meaning representation for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>. Having introduced several existing semantic representation schemes for different languages, we select as basis for our work on Vietnamese AMR (Abstract Meaning Representation). From it, we define a meaning representation label set by adapting the English schema and taking into account the specific characteristics of <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3318/>VerbNet Representations : Subevent Semantics for Transfer Verbs<span class=acl-fixed-case>V</span>erb<span class=acl-fixed-case>N</span>et Representations: Subevent Semantics for Transfer Verbs</a></strong><br><a href=/people/s/susan-windisch-brown/>Susan Windisch Brown</a>
|
<a href=/people/j/julia-bonn/>Julia Bonn</a>
|
<a href=/people/j/james-gung/>James Gung</a>
|
<a href=/people/a/annie-zaenen/>Annie Zaenen</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3318><div class="card-body p-3 small">This paper announces the release of a new version of the English lexical resource VerbNet with substantially revised semantic representations designed to facilitate computer planning and reasoning based on human language. We use the transfer of possession and transfer of information event representations to illustrate both the general framework of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> and the types of nuances the new <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> can capture. These representations use a Generative Lexicon-inspired subevent structure to track attributes of event participants across time, highlighting oppositions and temporal and causal relations among the subevents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3319/>Semantically Constrained Multilayer Annotation : The Case of Coreference</a></strong><br><a href=/people/j/jakob-prange/>Jakob Prange</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3319><div class="card-body p-3 small">We propose a coreference annotation scheme as a layer on top of the Universal Conceptual Cognitive Annotation foundational layer, treating units in predicate-argument structure as a basis for entity and event mentions. We argue that this allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from other schemes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3320/>Towards Universal Semantic Representation</a></strong><br><a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/laura-chiticariu/>Laura Chiticariu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3320><div class="card-body p-3 small">Natural language understanding at the <a href=https://en.wikipedia.org/wiki/Semantics>semantic level</a> and independent of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variations</a> is of great practical value. Existing approaches such as semantic role labeling (SRL) and abstract meaning representation (AMR) still have features related to the peculiarities of the particular language. In this work we describe various challenges and possible solutions in designing a semantic representation that is universal across a variety of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3322/>Augmenting Abstract Meaning Representation for <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>Human-Robot Dialogue</a><span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation for Human-Robot Dialogue</a></strong><br><a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a>
|
<a href=/people/s/stephen-tratz/>Stephen Tratz</a>
|
<a href=/people/r/ron-artstein/>Ron Artstein</a>
|
<a href=/people/d/david-traum/>David Traum</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3322><div class="card-body p-3 small">We detail refinements made to Abstract Meaning Representation (AMR) that make the representation more suitable for supporting a situated dialogue system, where a human remotely controls a robot for purposes of <a href=https://en.wikipedia.org/wiki/Search_and_rescue>search and rescue</a> and reconnaissance. We propose 36 augmented AMRs that capture speech acts, <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and aspect, and <a href=https://en.wikipedia.org/wiki/Spatial_analysis>spatial information</a>. This linguistic information is vital for representing important distinctions, for example whether the robot has moved, is moving, or will move. We evaluate two existing AMR parsers for their performance on dialogue data. We also outline a model for graph-to-graph conversion, in which output from AMR parsers is converted into our refined AMRs. The design scheme presented here, though task-specific, is extendable for broad coverage of <a href=https://en.wikipedia.org/wiki/Speech_act>speech acts</a> using <a href=https://en.wikipedia.org/wiki/Adaptive_Multi-Rate_audio_codec>AMR</a> in future task-independent work.</div></div></div><hr><div id=w19-34><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-34.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-34/>Proceedings of the Second Workshop on Storytelling</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3400/>Proceedings of the Second Workshop on Storytelling</a></strong><br><a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao ‘Kenneth’ Huang</a>
|
<a href=/people/s/stephanie-lukin/>Stephanie M. Lukin</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3404/>A Hybrid Model for Globally Coherent Story Generation</a></strong><br><a href=/people/f/fangzhou-zhai/>Fangzhou Zhai</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/p/pavel-shkadzko/>Pavel Shkadzko</a>
|
<a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3404><div class="card-body p-3 small">Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data ; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3405/>Guided Neural Language Generation for Automated Storytelling</a></strong><br><a href=/people/p/prithviraj-ammanabrolu/>Prithviraj Ammanabrolu</a>
|
<a href=/people/e/ethan-tien/>Ethan Tien</a>
|
<a href=/people/w/wesley-cheung/>Wesley Cheung</a>
|
<a href=/people/z/zhaochen-luo/>Zhaochen Luo</a>
|
<a href=/people/w/william-ma/>William Ma</a>
|
<a href=/people/l/lara-martin/>Lara Martin</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3405><div class="card-body p-3 small">Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries. Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into : (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence). However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences. We present an ensemble-based model that generates <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> guided by <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>. Our <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> outperforms the baseline sequence-to-sequence model. Additionally, we provide results for a full end-to-end automated story generation system, demonstrating how our model works with existing systems designed for the event-to-event problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3407" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3407/>Narrative Generation in the Wild : Methods from NaNoGenMo<span class=acl-fixed-case>N</span>arrative <span class=acl-fixed-case>G</span>eneration in the <span class=acl-fixed-case>W</span>ild: <span class=acl-fixed-case>M</span>ethods from <span class=acl-fixed-case>N</span>a<span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>G</span>en<span class=acl-fixed-case>M</span>o</a></strong><br><a href=/people/j/judith-van-stegeren/>Judith van Stegeren</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3407><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text generation</a>, generating long stories is still a challenge. Coherence tends to decrease rapidly as the output length increases. Especially for generated stories, coherence of the narrative is an important quality aspect of the output text. In this paper we examine how narrative coherence is attained in the submissions of NaNoGenMo 2018, an online text generation event where participants are challenged to generate a 50,000 word novel. We list the main approaches that were used to generate coherent narratives and link them to <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>. Finally, we give recommendations on when to use which <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3408/>Lexical concreteness in narrative</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3408><div class="card-body p-3 small">This study explores the relation between lexical concreteness and narrative text quality. We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to quantitatively measure lexical concreteness of a text. We apply <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to a corpus of student stories, scored according to writing evaluation rubrics. Lexical concreteness is weakly-to-moderately related to story quality, depending on story-type. The relation is mostly borne by <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>, but also found for <a href=https://en.wikipedia.org/wiki/Adverb>adverbs</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3411/>Personality Traits Recognition in <a href=https://en.wikipedia.org/wiki/Literary_language>Literary Texts</a></a></strong><br><a href=/people/d/daniele-pizzolli/>Daniele Pizzolli</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3411><div class="card-body p-3 small">Interesting stories often are built around interesting characters. Finding and detailing what makes an interesting character is a real challenge, but certainly a significant cue is the character personality traits. Our exploratory work tests the adaptability of the current <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits theories</a> to literal characters, focusing on the analysis of utterances in <a href=https://en.wikipedia.org/wiki/Play_(theatre)>theatre scripts</a>. And, at the opposite, we try to find significant <a href=https://en.wikipedia.org/wiki/Trait_theory>traits</a> for interesting characters. The preliminary results demonstrate that our approach is reasonable. Using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> for gaining insight into the <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits</a> of <a href=https://en.wikipedia.org/wiki/Character_(arts)>fictional characters</a> can make sense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3413" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3413/>WriterForcing : Generating more interesting story endings<span class=acl-fixed-case>W</span>riter<span class=acl-fixed-case>F</span>orcing: Generating more interesting story endings</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/v/vinayshekhar-bannihatti-kumar/>Vinayshekhar Bannihatti Kumar</a>
|
<a href=/people/m/mukul-bhutani/>Mukul Bhutani</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3413><div class="card-body p-3 small">We study the problem of <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>generating interesting endings</a> for stories. Neural generative models have shown promising results for various text generation problems. Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2Seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same <a href=https://en.wikipedia.org/wiki/Context_(language_use)>story context</a>. In this paper, we propose <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which generate more diverse and interesting outputs by 1) training <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings.</div></div></div><hr><div id=w19-35><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-35.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-35/>Proceedings of the Third Workshop on Abusive Language Online</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3500/>Proceedings of the Third Workshop on Abusive Language Online</a></strong><br><a href=/people/s/sarah-t-roberts/>Sarah T. Roberts</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3501/>Subversive Toxicity Detection using Sentiment Information</a></strong><br><a href=/people/e/eloi-brassard-gourdeau/>Eloi Brassard-Gourdeau</a>
|
<a href=/people/r/richard-khoury/>Richard Khoury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3501><div class="card-body p-3 small">The presence of toxic content has become a major problem for many <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a>. Moderators try to limit this problem by implementing more and more refined comment filters, but toxic users are constantly finding new ways to circumvent them. Our hypothesis is that while modifying toxic content and <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> to fool filters can be easy, hiding sentiment is harder. In this paper, we explore various aspects of sentiment detection and their correlation to <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>, and use our results to implement a <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity detection tool</a>. We then test how adding the sentiment information helps detect <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in three different real-world datasets, and incorporate <a href=https://en.wikipedia.org/wiki/Subversion>subversion</a> to these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to simulate a user trying to circumvent the system. Our results show <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> has a positive impact on toxicity detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3504" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3504/>Racial Bias in <a href=https://en.wikipedia.org/wiki/Hate_speech>Hate Speech</a> and Abusive Language Detection Datasets</a></strong><br><a href=/people/t/thomas-davidson/>Thomas Davidson</a>
|
<a href=/people/d/debasmita-bhattacharya/>Debasmita Bhattacharya</a>
|
<a href=/people/i/ingmar-weber/>Ingmar Weber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3504><div class="card-body p-3 small">Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine <a href=https://en.wikipedia.org/wiki/Racism_in_the_United_States>racial bias</a> in five different sets of Twitter data annotated for hate speech and abusive language. We train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on these datasets and compare the predictions of these <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on tweets written in <a href=https://en.wikipedia.org/wiki/African-American_English>African-American English</a> with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on them tend to predict that tweets written in <a href=https://en.wikipedia.org/wiki/African-American_English>African-American English</a> are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these <a href=https://en.wikipedia.org/wiki/System>systems</a> may discriminate against the groups who are often the targets of the abuse we are trying to detect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3505/>Automated Identification of Verbally Abusive Behaviors in Online Discussions</a></strong><br><a href=/people/s/srecko-joksimovic/>Srecko Joksimovic</a>
|
<a href=/people/r/ryan-s-baker/>Ryan S. Baker</a>
|
<a href=/people/j/jaclyn-ocumpaugh/>Jaclyn Ocumpaugh</a>
|
<a href=/people/j/juan-miguel-l-andres/>Juan Miguel L. Andres</a>
|
<a href=/people/i/ivan-tot/>Ivan Tot</a>
|
<a href=/people/e/elle-yuan-wang/>Elle Yuan Wang</a>
|
<a href=/people/s/shane-dawson/>Shane Dawson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3505><div class="card-body p-3 small">Discussion forum participation represents one of the crucial factors for <a href=https://en.wikipedia.org/wiki/Learning>learning</a> and often the only way of supporting <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a> in <a href=https://en.wikipedia.org/wiki/Online_and_offline>online settings</a>. However, as much as sharing new ideas or asking thoughtful questions contributes <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, verbally abusive behaviors, such as expressing negative emotions in online discussions, could have disproportionate detrimental effects. To provide means for mitigating the potential negative effects on course participation and <a href=https://en.wikipedia.org/wiki/Learning>learning</a>, we developed an automated classifier for identifying <a href=https://en.wikipedia.org/wiki/Communication>communication</a> that show linguistic patterns associated with <a href=https://en.wikipedia.org/wiki/Hostility>hostility</a> in online forums. In so doing, we employ several well-established automated text analysis tools and build on the common practices for handling highly imbalanced datasets and reducing the sensitivity to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. Although still in its infancy, our approach shows promising results (ROC AUC.73) towards establishing a robust detector of abusive behaviors. We further provide an overview of the classification (linguistic and contextual) features most indicative of online aggression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3508/>Pay Attention to your Context when Classifying Abusive Language</a></strong><br><a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a>
|
<a href=/people/k/kilol-gupta/>Kilol Gupta</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3508><div class="card-body p-3 small">The goal of any <a href=https://en.wikipedia.org/wiki/Social_media>social media platform</a> is to facilitate healthy and meaningful interactions among its users. But more often than not, it has been found that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> becomes an avenue for wanton attacks. We propose an experimental study that has three aims : 1) to provide us with a deeper understanding of current data sets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language, and personal attacks) ; 2) to investigate what type of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> (contextual vs. self-attention) is better for abusive language detection using deep learning architectures ; and 3) to investigate whether stacked architectures provide an advantage over simple architectures for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3509/>Challenges and frontiers in abusive content detection</a></strong><br><a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/a/alex-harris/>Alex Harris</a>
|
<a href=/people/d/dong-nguyen/>Dong Nguyen</a>
|
<a href=/people/r/rebekah-tromble/>Rebekah Tromble</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3509><div class="card-body p-3 small">Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which <a href=https://en.wikipedia.org/wiki/Social_science>social scientific insights</a> can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3511/>A System to Monitor Cyberbullying based on Message Classification and Social Network Analysis</a></strong><br><a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/g/giovanni-moretti/>Giovanni Moretti</a>
|
<a href=/people/m/michele-corazza/>Michele Corazza</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3511><div class="card-body p-3 small">Social media platforms like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Instagram>Instagram</a> face a surge in cyberbullying phenomena against young users and need to develop scalable computational methods to limit the negative consequences of this kind of abuse. Despite the number of approaches recently proposed in the Natural Language Processing (NLP) research area for detecting different forms of abusive language, the issue of identifying cyberbullying phenomena at scale is still an unsolved problem. This is because of the need to couple abusive language detection on textual message with network analysis, so that repeated attacks against the same person can be identified. In this paper, we present a system to monitor cyberbullying phenomena by combining message classification and <a href=https://en.wikipedia.org/wiki/Social_network_analysis>social network analysis</a>. We evaluate the classification module on a data set built on <a href=https://en.wikipedia.org/wiki/Instagram>Instagram messages</a>, and we describe the cyberbullying monitoring user interface.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3512/>L-HSAB : A Levantine Twitter Dataset for Hate Speech and Abusive Language<span class=acl-fixed-case>L</span>-<span class=acl-fixed-case>HSAB</span>: A <span class=acl-fixed-case>L</span>evantine <span class=acl-fixed-case>T</span>witter Dataset for Hate Speech and Abusive Language</a></strong><br><a href=/people/h/hala-mulki/>Hala Mulki</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/c/chedi-bechikh-ali/>Chedi Bechikh Ali</a>
|
<a href=/people/h/halima-alshabani/>Halima Alshabani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3512><div class="card-body p-3 small">Hate speech and abusive language have become a common phenomenon on Arabic social media. Automatic hate speech and abusive detection systems can facilitate the prohibition of toxic textual contents. The <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>, informality and ambiguity of the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> hindered the provision of the needed resources for Arabic abusive / hate speech detection research. In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents. We, further, provide a detailed review of the data collection steps and how we design the annotation guidelines such that a reliable dataset annotation is guaranteed. This has been later emphasized through the comprehensive evaluation of the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> as the annotation agreement metrics of Cohen&#8217;s Kappa (k) and Krippendorff&#8217;s alpha () indicated the consistency of the annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3514/>Preemptive Toxic Language Detection in Wikipedia Comments Using Thread-Level Context<span class=acl-fixed-case>W</span>ikipedia Comments Using Thread-Level Context</a></strong><br><a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3514><div class="card-body p-3 small">We address the task of automatically detecting toxic content in <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated texts</a>. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that jointly considers all comments in a <a href=https://en.wikipedia.org/wiki/Conversation_threading>conversation thread</a> outperforms a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that considers only individual comments. Using an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of conversations among <a href=https://en.wikipedia.org/wiki/Wikipedia_community>Wikipedia contributors</a> as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3516/>A Platform Agnostic Dual-Strand Hate Speech Detector</a></strong><br><a href=/people/j/johannes-skjeggestad-meyer/>Johannes Skjeggestad Meyer</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3516><div class="card-body p-3 small">Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text&#8217;s author may differ between services, and so using such data would reduce a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s general applicability. The paper thus focuses on using exclusively <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-based input</a> in the detection, in an optimised architecture combining <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and Long Short-Term Memory-networks. The hate speech detector merges two strands with <a href=https://en.wikipedia.org/wiki/N-gram>character n-grams</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to produce the final classification, and is shown to outperform comparable previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3520/>Online aggression from a sociological perspective : An integrative view on determinants and possible countermeasures</a></strong><br><a href=/people/s/sebastian-weingartner/>Sebastian Weingartner</a>
|
<a href=/people/l/lea-stahel/>Lea Stahel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3520><div class="card-body p-3 small">The present paper introduces a <a href=https://en.wikipedia.org/wiki/Mathematical_model>theoretical model</a> for explaining aggressive online comments from a sociological perspective. It is innovative as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> combines individual, situational, and social-structural determinants of online aggression and tries to theoretically derive their interplay. Moreover, the paper suggests an empirical strategy for testing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The main contribution will be to match online commenting data with survey data containing rich background data of non- /aggressive online commentators.</div></div></div><hr><div id=w19-36><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W19-36/>Proceedings of the 2019 Workshop on Widening NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3600/>Proceedings of the 2019 Workshop on Widening NLP</a></strong><br><a href=/people/a/amittai-axelrod/>Amittai Axelrod</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/r/rossana-cunha/>Rossana Cunha</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3601/>Development of a General Purpose Sentiment Lexicon for <span class=acl-fixed-case>I</span>gbo Language</a></strong><br><a href=/people/e/emeka-ogbuju/>Emeka Ogbuju</a>
|
<a href=/people/m/moses-onyesolu/>Moses Onyesolu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3601><div class="card-body p-3 small">There are publicly available general purpose sentiment lexicons in some high resource languages but very few exist in the low resource languages. This makes it difficult to directly perform sentiment analysis tasks in such languages. The objective of this work is to create a general purpose sentiment lexicon for Igbo language that can determine the sentiment of documents written in Igbo language without having to translate it to English language. The material used was an automatically translated Liu&#8217;s lexicon and manual addition of Igbo native words. The result of this work is a general purpose lexicon &#8211; IgboSentilex. The performance was tested on the BBC Igbo news channel. It returned an average polarity agreement of 95% with other general purpose sentiment lexicons.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3602/>Towards a Resource Grammar for <span class=acl-fixed-case>R</span>unyankore and Rukiga</a></strong><br><a href=/people/d/david-bamutura/>David Bamutura</a>
|
<a href=/people/p/peter-ljunglof/>Peter Ljunglöf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3602><div class="card-body p-3 small">Currently, there is a lack of computational grammar resources for many under-resourced languages which limits the ability to develop Natural Language Processing (NLP) tools and applications such as Multilingual Document Authoring, Computer-Assisted Language Learning (CALL) and Low-Coverage Machine Translation (MT) for these languages. In this paper, we present our attempt to formalise the grammar of two such languages: Runyankore and Rukiga. For this formalisation we use the Grammatical Framework (GF) and its Resource Grammar Library (GF-RGL).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3603/>Speech Recognition for <span class=acl-fixed-case>T</span>igrinya language Using Deep Neural Network Approach</a></strong><br><a href=/people/h/hafte-abera/>Hafte Abera</a>
|
<a href=/people/s/sebsibe-h-mariam/>Sebsibe H/mariam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3603><div class="card-body p-3 small">This work presents a speech recognition model for Tigrinya language .The Deep Neural Network is used to make the recognition model. The Long Short-Term Memory Network (LSTM), which is a special kind of Recurrent Neural Network composed of Long Short-Term Memory blocks, is the primary layer of our neural network model. The 40-dimensional features are MFCC-LDA-MLLT-fMLLR with CMN were used. The acoustic models are trained on features that are obtained by projecting down to 40 dimensions using linear discriminant analysis (LDA). Moreover, speaker adaptive training (SAT) is done using a single feature-space maximum likelihood linear regression (FMLLR) transform estimated per speaker. We train and compare LSTM and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. Finally, the accuracy of the model is evaluated based on the recognition rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3604/>Knowledge-Based Word Sense Disambiguation with Distributional Semantic Expansion</a></strong><br><a href=/people/h/hossein-rouhizadeh/>Hossein Rouhizadeh</a>
|
<a href=/people/m/mehrnoush-shamsfard/>Mehrnoush Shamsfard</a>
|
<a href=/people/m/masoud-rouhizadeh/>Masoud Rouhizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3604><div class="card-body p-3 small">In this paper, we presented a WSD system that uses LDA topics for semantic expansion of document words. Our system also uses sense frequency information from SemCor to give higher priority to the senses which are more probable to happen.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3605/><span class=acl-fixed-case>A</span>spe<span class=acl-fixed-case>R</span>a: Aspect-Based Rating Prediction Based on User Reviews</a></strong><br><a href=/people/e/elena-tutubalina/>Elena Tutubalina</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a>
|
<a href=/people/s/sergey-nikolenko/>Sergey Nikolenko</a>
|
<a href=/people/a/anton-alekseev/>Anton Alekseev</a>
|
<a href=/people/i/ilya-shenbin/>Ilya Shenbin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3605><div class="card-body p-3 small">We propose a novel Aspect-based Rating Prediction model (AspeRa) that estimates user rating based on review texts for the items. It is based on aspect extraction with neural networks and combines the advantages of deep learning and topic modeling. It is mainly designed for recommendations, but an important secondary goal of AspeRa is to discover coherent aspects of reviews that can be used to explain predictions or for user profiling. We conduct a comprehensive empirical study of AspeRa, showing that it outperforms state-of-the-art models in terms of recommendation quality and produces interpretable aspects. This paper is an abridged version of our work (Nikolenko et al., 2019)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3606/>Recognizing Arrow Of Time In The Short Stories</a></strong><br><a href=/people/f/fahimeh-hosseini/>Fahimeh Hosseini</a>
|
<a href=/people/h/hosein-fooladi/>Hosein Fooladi</a>
|
<a href=/people/m/mohammad-reza-samsami/>Mohammad Reza Samsami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3606><div class="card-body p-3 small">Recognizing the arrow of time in the context of paragraphs in short stories is a challenging task. i.e., given only two paragraphs (excerpted from a random position in a short story), determining which comes first and which comes next is a difficult task even for humans. In this paper, we have collected and curated a novel dataset for tackling this challenging task. We have shown that a pre-trained BERT architecture achieves reasonable accuracy on the task, and outperforms RNN-based architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3607/><span class=acl-fixed-case>A</span>mharic Word Sequence Prediction</a></strong><br><a href=/people/n/nuniyat-kifle/>Nuniyat Kifle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3607><div class="card-body p-3 small">The significance of computers and handheld devices are not deniable in the modern world of today. Texts are entered to these devices using word processing programs as well as other techniques and word prediction is one of the techniques. Word Prediction is the action of guessing or forecasting what word comes after, based on some current information, and it is the main focus of this study. Even though Amharic is used by a large number of populations, no significant work is done on the topic of word sequence prediction. In this study, Amharic word sequence prediction model is developed with statistical methods using Hidden Markov Model by incorporating detailed Part of speech tag. Evaluation of the model is performed using developed prototype and keystroke savings (KSS) as a metrics. According to our experiment, prediction result using a bi-gram with detailed Part of Speech tag model has higher KSS and it is better compared to tri-gram model and better than those without Part of Speech tag. Therefore, statistical approach with Detailed POS has quite good potential on word sequence prediction for Amharic language. This research deals with designing word sequence prediction model in Amharic language. It is a language that is spoken in eastern Africa. One of the needs for Amharic word sequence prediction for mobile use and other digital devices is in order to facilitate data entry and communication in our language. Word sequence prediction is a challenging task for inflected languages. (Arora, 2007) These kinds of languages are morphologically rich and have enormous word forms. i.e. one word can have different forms. As Amharic language is highly inflected language and morphologically rich it shares this problem. (prediction, 2008) This problem makes word prediction system much more difficult and results poor performance. Due to this reason storing all forms in dictionary won&#8217;t solve the problem as in English and other less inflected languages. But considering other techniques that could help the predictor to suggest the next word like a POS based prediction should be used. Previous researches used dictionary approach with no consideration of context information. Hence storing all forms of words in dictionary for inflected languages such as Amharic language has been less effective. The main goal of this thesis is to implement Amharic word prediction model that works with better prediction speed and with narrowed search space as much as possible. We introduced two models; tags and words and linear interpolation that use part of speech tag information in addition to word n-grams in order to maximize the likelihood of syntactic appropriateness of the suggestions. We believe the results found reflect this. Amharic word sequence prediction using bi-gram model with higher POS weight and detailed Part of speech tag gave better keystroke savings in all scenarios of our experiment. The study followed Design Science Research Methodology (DSRM). Since DSRM includes approaches, techniques, tools, algorithms and evaluation mechanisms in the process, we followed statistical approach with statistical language modeling and built Amharic prediction model based on information from Part of Speech tagger. The statistics included in the systems varies from single word frequencies to part-of-speech tag n-grams. That means it included the statistics of Word frequencies, Word sequence frequencies, Part-of-speech sequence frequencies and other important information. Later on the system was evaluated using Keystroke Savings. (Lindh, 011). Linux mint was used as the main Operation System during the frame work design. We used corpus of 680,000 tagged words that has 31 tag sets, python programming language and its libraries for both the part of speech tagger and the predictor module. Other Tool that was used is the SRILIM (The SRI language modeling toolkit) in order to generate unigram bigram and trigram count as an input for the language model. SRILIM is toolkit that uses to build and apply statistical language modeling. This thesis presented Amharic word sequence prediction model using the statistical approach. We described a combined statistical and lexical word prediction system for handling inflected languages by making use of POS tags to build the language model. We developed Amharic language models of bigram and trigram for the training purpose. We obtained 29% of KSS using bigram model with detailed part ofspeech tag. Hence, Based on the experiments carried out for this study and the results obtained, the following conclusions were made. We concluded that employing syntactic information in the form of Part-of-Speech (POS) n-grams promises more effective predictions. We also can conclude data quantity, performance of POS tagger and data quality highly affects the keystroke savings. Here in our study the tests were done on a small collection of 100 phrases. According to our evaluation better Keystroke saving (KSS) is achieved when using bi-gram model than the tri-gram models. We believe the results obtained using the experiment of detailed Part of speech tags were effective Since speed and search space are the basic issues in word sequence prediction</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3608/>A Framework for Relation Extraction Across Multiple Datasets in Multiple Domains</a></strong><br><a href=/people/g/geeticka-chauhan/>Geeticka Chauhan</a>
|
<a href=/people/m/matthew-mcdermott/>Matthew McDermott</a>
|
<a href=/people/p/peter-szolovits/>Peter Szolovits</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3608><div class="card-body p-3 small">In this work, we aim to build a unifying framework for relation extraction (RE), applying this on 3 highly used datasets with the ability to be extendable to new datasets. At the moment, the domain suffers from lack of reproducibility as well as a lack of consensus on generalizable techniques. Our framework will be open-sourced and will aid in performing systematic exploration on the effect of different modeling techniques, pre-processing, training methodologies and evaluation metrics on the 3 datasets to help establish a consensus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3609/>Learning and Understanding Different Categories of Sexism Using Convolutional Neural Network’s Filters</a></strong><br><a href=/people/s/sima-sharifirad/>Sima Sharifirad</a>
|
<a href=/people/a/alon-jacovi/>Alon Jacovi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3609><div class="card-body p-3 small">Sexism is very common in social media and makes the boundaries of free speech tighter for female users. Automatically flagging and removing sexist content requires niche identification and description of the categories. In this study, inspired by social science work, we propose three categories of sexism toward women as follows: &#8220;Indirect sexism&#8221;, &#8220;Sexual sexism&#8221; and &#8220;Physical sexism&#8221;. We build classifiers such as Convolutional Neural Network (CNN) to automatically detect different types of sexism and address problems of annotation. Even though inherent non-interpretability of CNN is a challenge for users who detect sexism, as the reason classifying a given speech instance with regard to sexism is difficult to glance from a CNN. However, recent research developed interpretable CNN filters for text data. In a CNN, filters followed by different activation patterns along with global max-pooling can help us tease apart the most important ngrams from the rest. In this paper, we interpret a CNN model trained to classify sexism in order to understand different categories of sexism by detecting semantic categories of ngrams and clustering them. Then, these ngrams in each category are used to improve the performance of the classification task. It is a preliminary work using machine learning and natural language techniques to learn the concept of sexism and distinguishes itself by looking at more precise categories of sexism in social media along with an in-depth investigation of CNN&#8217;s filters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3610/>Modeling Five Sentence Quality Representations by Finding Latent Spaces Produced with Deep Long Short-Memory Models</a></strong><br><a href=/people/p/pablo-rivas/>Pablo Rivas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3610><div class="card-body p-3 small">We present a study in which we train neural models that approximate rules that assess the quality of English sentences. We modeled five rules using deep LSTMs trained over a dataset of sentences whose quality is evaluated under such rules. Preliminary results suggest the neural architecture can model such rules to high accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3611/><span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>E</span>thiopian Languages Statistical Machine Translation</a></strong><br><a href=/people/s/solomon-teferra-abate/>Solomon Teferra Abate</a>
|
<a href=/people/m/michael-melese/>Michael Melese</a>
|
<a href=/people/m/martha-yifiru-tachbelie/>Martha Yifiru Tachbelie</a>
|
<a href=/people/m/million-meshesha/>Million Meshesha</a>
|
<a href=/people/s/solomon-atinafu/>Solomon Atinafu</a>
|
<a href=/people/w/wondwossen-mulugeta/>Wondwossen Mulugeta</a>
|
<a href=/people/y/yaregal-assabie/>Yaregal Assabie</a>
|
<a href=/people/h/hafte-abera/>Hafte Abera</a>
|
<a href=/people/b/biniyam-ephrem/>Biniyam Ephrem</a>
|
<a href=/people/t/tewodros-gebreselassie/>Tewodros Gebreselassie</a>
|
<a href=/people/w/wondimagegnhue-tsegaye-tufa/>Wondimagegnhue Tsegaye Tufa</a>
|
<a href=/people/a/amanuel-lemma/>Amanuel Lemma</a>
|
<a href=/people/t/tsegaye-andargie/>Tsegaye Andargie</a>
|
<a href=/people/s/seifedin-shifaw/>Seifedin Shifaw</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3611><div class="card-body p-3 small">In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge&#8217;ez. The corpora are used for conducting bi-directional SMT experiments. The BLEU scores of the bi-directional SMT systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT especially when the targets are Ethiopian languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3612 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3612/>An automatic discourse relation alignment experiment on <span class=acl-fixed-case>TED</span>-<span class=acl-fixed-case>MDB</span></a></strong><br><a href=/people/s/sibel-ozer/>Sibel Ozer</a>
|
<a href=/people/d/deniz-zeyrek/>Deniz Zeyrek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3612><div class="card-body p-3 small">This paper describes an automatic discourse relation alignment experiment as an empirical justification of the planned annotation projection approach to enlarge the 3600-word multilingual corpus of TED Multilingual Discourse Bank (TED-MDB). The experiment is carried out on a single language pair (English-Turkish) included in TED-MDB. The paper first describes the creation of a large corpus of English-Turkish bi-sentences, then it presents a sense-based experiment that automatically aligns the relations in the English sentences of TED-MDB with the Turkish sentences. The results are very close to the results obtained from an earlier semi-automatic post-annotation alignment experiment validated by human annotators and are encouraging for future annotation projection tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3613/>The Design and Construction of the Corpus of <span class=acl-fixed-case>C</span>hina <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/l/lixin-xia/>Lixin Xia</a>
|
<a href=/people/y/yun-xia/>Yun Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3613><div class="card-body p-3 small">The paper describes the development a corpus of an English variety, i.e. China English, in or-der to provide a linguistic resource for researchers in the field of China English. The Corpus of China English (CCE) was built with due consideration given to its representativeness and authenticity. It was composed of more than 13,962,102 tokens in 15,333 texts evenly divided between the following four genres: newspapers, magazines, fiction and academic writings. The texts cover a wide range of domains, such as news, financial, politics, environment, social, culture, technology, sports, education, philosophy, literary, etc. It is a helpful resource for research on China English, computational linguistics, natural language processing, corpus linguistics and English language education.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3614 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3614/>Learning Trilingual Dictionaries for <span class=acl-fixed-case>U</span>rdu – <span class=acl-fixed-case>R</span>oman <span class=acl-fixed-case>U</span>rdu – <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/moiz-rauf/>Moiz Rauf</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3614><div class="card-body p-3 small">In this paper, we present an effort to generate a joint Urdu, Roman Urdu and English trilingual lexicon using automated methods. We make a case for using statistical machine translation approaches and parallel corpora for dictionary creation. To this purpose, we use word alignment tools on the corpus and evaluate translations using human evaluators. Despite different writing script and considerable noise in the corpus our results show promise with over 85% accuracy of Roman Urdu&#8211;Urdu and 45% English&#8211;Urdu pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3615/>Joint Inference on Bilingual Parse Trees for <span class=acl-fixed-case>PP</span>-attachment Disambiguation</a></strong><br><a href=/people/g/geetanjali-rakshit/>Geetanjali Rakshit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3615><div class="card-body p-3 small">Prepositional Phrase (PP) attachment is a classical problem in NLP for languages like English, which suffer from structural ambiguity. In this work, we solve this problem with the help of another language free from such ambiguities, using the parse tree of the parallel sentence in the other language, and word alignments. We formulate an optimization framework that encourages agreement between the parse trees for two languages, and solve it using a novel Dual Decomposition (DD) based algorithm. Experiments on the English-Hindi language pair show promising improvements over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3616/>Using Attention-based Bidirectional <span class=acl-fixed-case>LSTM</span> to Identify Different Categories of Offensive Language Directed Toward Female Celebrities</a></strong><br><a href=/people/s/sima-sharifirad/>Sima Sharifirad</a>
|
<a href=/people/s/stan-matwin/>Stan Matwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3616><div class="card-body p-3 small">Social media posts reflect the emotions, intentions and mental state of the users. Twitter users who harass famous female figures may do so with different intentions and intensities. Recent studies have published datasets focusing on different types of online harassment, vulgar language, and emotional intensities. We trained, validate and test our proposed model, attention-based bidirectional neural network, on the three datasets:&#8221;online harassment&#8221;, &#8220;vulgar language&#8221; and &#8220;valance&#8221; and achieved state of the art performance in two of the datasets. We report F1 score for each dataset separately along with the final precision, recall and macro-averaged F1 score. In addition, we identify ten female figures from different professions and racial backgrounds who have experienced harassment on Twitter. We tested the trained models on ten collected corpuses each related to one famous female figure to predict the type of harassing language, the type of vulgar language and the degree of intensity of language occurring on their social platforms. Interestingly, the achieved results show different patterns of linguistic use targeting different racial background and occupations. The contribution of this study is two-fold. From the technical perspective, our proposed methodology is shown to be effective with a good margin in comparison to the previous state-of-the-art results on one of the two available datasets. From the social perspective, we introduce a methodology which can unlock facts about the nature of offensive language targeting women on online social platforms. The collected dataset will be shared publicly for further investigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3617/>Sentiment Analysis Model for Opinionated <span class=acl-fixed-case>A</span>wngi Text: Case of Music Reviews</a></strong><br><a href=/people/m/melese-mihret/>Melese Mihret</a>
|
<a href=/people/m/muluneh-atinaf/>Muluneh Atinaf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3617><div class="card-body p-3 small">Abstract The analysis of sentiments is imperative to make a decision for individuals, organizations, and governments. Due to the rapid growth of Awngi (Agew) text on the web, there is no available corpus annotated for sentiment analysis. In this paper, we present a SA model for the Awngi language spoken in Ethiopia, by using a supervised machine learning approach. We developed our corpus by collecting around 1500 posts from online sources. This research is begun to build and evaluate the model for opinionated Awngi music reviews. Thus, pre-processing techniques have been employed to clean the data, to convert transliterations to the native Ethiopic script for accessibility and convenience to typing and to change the words to their base form by removing the inflectional morphemes. After pre-processing, the corpus is manually annotated by three the language professional for giving polarity, and rate, their level of confidence in their selection and sentiment intensity scale values. To improve the calculation method of feature selection and weighting and proposed a more suitable SA algorithm for feature extraction named CHI and weight calculation named TF IDF, increasing the proportion and weight of sentiment words in the feature words. We employed Support Vector Machines (SVM), Na&#239;ve Bayes (NB) and Maximum Entropy (MxEn) machine learning algorithms. Generally, the results are encouraging, despite the morphological challenge in Awngi, the data cleanness and small size of data. We are believed that the results could improve further with a larger corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3618/>A compositional view of questions</a></strong><br><a href=/people/m/maria-boritchev/>Maria Boritchev</a>
|
<a href=/people/m/maxime-amblard/>Maxime Amblard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3618><div class="card-body p-3 small">We present a research on compositional treatment of questions in neo-davidsonian event semantics style. Our work is based on (Champollion, 2011) where only declarative sentences were considered. Our research is based on complex formal examples, paving the way towards further research in this domain and further testing on real-life corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3619 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3619/>Controlling the Specificity of Clarification Question Generation</a></strong><br><a href=/people/y/yang-trista-cao/>Yang Trista Cao</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3619><div class="card-body p-3 small">Unlike comprehension-style questions, clarification questions look for some missing information in a given context. However, without guidance, neural models for question generation, similar to dialog generation models, lead to generic and bland questions that cannot elicit useful information. We argue that controlling the level of specificity of the generated questions can have useful applications and propose a neural clarification question generation model for the same. We first train a classifier that annotates a clarification question with its level of specificity (generic or specific) to the given context. Our results on the Amazon questions dataset demonstrate that training a clarification question generation model on specificity annotated data can generate questions with varied levels of specificity to the given context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3620 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3620/>Non-Monotonic Sequential Text Generation</a></strong><br><a href=/people/k/kiante-brantley/>Kiante Brantley</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3620><div class="card-body p-3 small">Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy&#8217;s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order while achieving competitive performance with conventional left-to-right generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3621/>Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3621><div class="card-body p-3 small">Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between &#8220;gender-neutralized&#8221; words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3622/>How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3622><div class="card-body p-3 small">Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun&#8217;s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While &#8220;embedding debiasing&#8221; methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words&#8217; context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3623 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3623/>Automatic Product Categorization for Official Statistics</a></strong><br><a href=/people/a/andrea-roberson/>Andrea Roberson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3623><div class="card-body p-3 small">The North American Product Classification System (NAPCS) is a comprehensive, hierarchical classification system for products (goods and services) that is consistent across the three North American countries. Beginning in 2017, the Economic Census will use NAPCS to produce economy-wide product tabulations. Respondents are asked to report data from a long, pre-specified list of potential products in a given industry, with some lists containing more than 50 potential products. Businesses have expressed the desire to alternatively supply Universal Product Codes (UPC) to the U. S. Census Bureau. Much work has been done around the categorization of products using product descriptions. No study has applied these efforts for the calculation of official statistics (statistics published by government agencies) using only the text of UPC product descriptions. The question we address in this paper is: Given UPC codes and their associated product descriptions, can we accurately predict NAPCS? We tested the feasibility of businesses submitting a spreadsheet with Universal Product Codes and their associated text descriptions. This novel strategy classified text with very high accuracy rates, all of our algorithms surpassed over 90 percent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3624/>An Online Topic Modeling Framework with Topics Automatically Labeled</a></strong><br><a href=/people/j/jin-fenglei/>Jin Fenglei</a>
|
<a href=/people/g/gao-cuiyun/>Gao Cuiyun</a>
|
<a href=/people/l/lyu-michael-r/>Lyu Michael R.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3624><div class="card-body p-3 small">In this paper, we propose a novel online topic tracking framework, named IEDL, for tracking the topic changes related to deep learning techniques on Stack Exchange and automatically interpreting each identified topic. The proposed framework combines the prior topic distributions in a time window during inferring the topics in current time slice, and introduces a new ranking scheme to select most representative phrases and sentences for the inferred topics. Experiments on 7,076 Stack Exchange posts show the effectiveness of IEDL in tracking topic changes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3625 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3625/>Construction and Alignment of Multilingual Entailment Graphs for Semantic Inference</a></strong><br><a href=/people/s/sabine-weber/>Sabine Weber</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3625><div class="card-body p-3 small">This paper presents ongoing work on the construction and alignment of predicate entailment graphs in English and German. We extract predicate-argument pairs from large corpora of monolingual English and German news text and construct monolingual paraphrase clusters and entailment graphs. We use an aligned subset of entities to derive the bilingual alignment of entities and relations, and achieve better than baseline results on a translated subset of a predicate entailment data set (Levy and Dagan, 2016) and the German portion of XNLI (Conneau et al., 2018).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3626 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3626 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3626/><span class=acl-fixed-case>KB</span>-<span class=acl-fixed-case>NLG</span>: From Knowledge Base to Natural Language Generation</a></strong><br><a href=/people/w/wen-cui/>Wen Cui</a>
|
<a href=/people/m/minghui-zhou/>Minghui Zhou</a>
|
<a href=/people/r/rongwen-zhao/>Rongwen Zhao</a>
|
<a href=/people/n/narges-norouzi/>Narges Norouzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3626><div class="card-body p-3 small">We perform the natural language generation (NLG) task by mapping sets of Resource Description Framework (RDF) triples into text. First we investigate the impact of increasing the number of entity types in delexicalisaiton on the generation quality. Second we conduct different experiments to evaluate two widely applied language generation systems, encoder-decoder with attention and the Transformer model on a large benchmark dataset. We evaluate different models on automatic metrics, as well as the training time. To our knowledge, we are the first to apply Transformer model to this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3627 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3627/>Acoustic Characterization of Singaporean Children’s <span class=acl-fixed-case>E</span>nglish: Comparisons to <span class=acl-fixed-case>A</span>merican and <span class=acl-fixed-case>B</span>ritish Counterparts</a></strong><br><a href=/people/y/yuling-gu/>Yuling Gu</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3627><div class="card-body p-3 small">We investigate English pronunciation patterns in Singaporean children in relation to their American and British counterparts by conducting archetypal analysis on selected vowel pairs. Given that Singapore adopts British English as the institutional standard, one might expect Singaporean children to follow British pronunciation patterns, but we observe that Singaporean children also present similar patterns to Americans for TRAP-BATH spilt vowels: (1) British and Singaporean children both produce these vowels with a relatively lowered tongue height. (2) These vowels are more fronted for American and Singaporean children (p &lt; 0.001). In addition, when comparing /&#230;/ and /&#949;/ productions, British speakers show the clearest distinction between the two vowels; Singaporean and American speakers exhibit a higher and more fronted tongue position for /&#230;/ (p &lt; 0.001), causing /&#230;/ to be acoustically more similar to /&#949;/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3628 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3628/>Rethinking Phonotactic Complexity</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3628><div class="card-body p-3 small">In this work, we propose the use of phone-level language models to estimate phonotactic complexity&#8212;measured in bits per phoneme&#8212;which makes cross-linguistic comparison straightforward. We compare the entropy across languages using this simple measure, gaining insight on how complex different language&#8217;s phonotactics are. Finally, we show a very strong negative correlation between phonotactic complexity and the average length of words&#8212;Spearman rho=-0.744&#8212;when analysing a collection of 106 languages with 1016 basic concepts each.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3629 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3629/>Implementing a Multi-lingual Chatbot for Positive Reinforcement in Young Learners</a></strong><br><a href=/people/f/francisca-oladipo/>Francisca Oladipo</a>
|
<a href=/people/a/abdulmalik-rufai/>Abdulmalik Rufai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3629><div class="card-body p-3 small">This is a humanitarian work &#8211;a counter-terrorism effort. The presentation describes the experiences of developing a multi-lingua, interactive chatbot trained on the corpus of two Nigerian Languages (Hausa and Fulfude), with simultaneous translation to a third (Kanuri), to stimulate conversations, deliver tailored contents to the users thereby aiding in the detection of the probability and degree of radicalization in young learners through data analysis of the games moves and vocabularies. As chatbots have the ability to simulate a human conversation based on rhetorical behavior, the system is able to learn the need of individual user through constant interaction and deliver tailored contents that promote good behavior in Hausa, Fulfulde and Kanuri languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3630 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3630/>A Deep Learning Approach to Language-independent Gender Prediction on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/reyhaneh-hashempour/>Reyhaneh Hashempour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3630><div class="card-body p-3 small">This work presents a set of experiments conducted to predict the gender of Twitter users based on language-independent features extracted from the text of the users&#8217; tweets. The experiments were performed on a version of TwiSty dataset including tweets written by the users of six different languages: Portuguese, French, Dutch, English, German, and Italian. Logistic regression (LR), and feed-forward neural networks (FFNN) with back-propagation were used to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual (CL). In the IL setting, the training and testing were performed on the same language whereas in the CL, Italian and German datasets were set aside and only used as test sets and the rest were combined to compose training and development sets. In the IL, the highest accuracy score belongs to LR whereas, in the CL, FFNN with three hidden layers yields the highest score. The results show that neural network based models underperform traditional models when the size of the training set is small; however, they beat traditional models by a non-trivial margin, when they are fed with large enough data. Finally, the feature analysis confirms that men and women have different writing styles independent of their language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3631 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3631/>Isolating the Effects of Modeling Recursive Structures: A Case Study in Pronunciation Prediction of <span class=acl-fixed-case>C</span>hinese Characters</a></strong><br><a href=/people/m/minh-nguyen/>Minh Nguyen</a>
|
<a href=/people/g/gia-h-ngo/>Gia H Ngo</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3631><div class="card-body p-3 small">Finding that explicitly modeling structures leads to better generalization, we consider the task of predicting Cantonese pronunciations of logographs (Chinese characters) using logographs&#8217; recursive structures. This task is a suitable case study for two reasons. First, logographs&#8217; pronunciations depend on structures (i.e. the hierarchies of sub-units in logographs) Second, the quality of logographic structures is consistent since the structures are constructed automatically using a set of rules. Thus, this task is less affected by confounds such as varying quality between annotators. Empirical results show that modeling structures explicitly using treeLSTM outperforms LSTM baseline, reducing prediction error by 6.0% relative.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3632 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3632/>Benchmarking Neural Machine Translation for <span class=acl-fixed-case>S</span>outhern <span class=acl-fixed-case>A</span>frican Languages</a></strong><br><a href=/people/j/jade-abbott/>Jade Abbott</a>
|
<a href=/people/l/laura-martinus/>Laura Martinus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3632><div class="card-body p-3 small">Unlike major Western languages, most African languages are very low-resourced. Furthermore, the resources that do exist are often scattered and difficult to obtain and discover. As a result, the data and code for existing research has rarely been shared, meaning researchers struggle to reproduce reported results, and almost no publicly available benchmarks or leaderboards for African machine translation models exist. To start to address these problems, we trained neural machine translation models for a subset of Southern African languages on publicly-available datasets. We provide the code for training the models and evaluate the models on a newly released evaluation set, with the aim of starting a leaderboard for Southern African languages and spur future research in the field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3633 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3633 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3633/><span class=acl-fixed-case>OCR</span> Quality and <span class=acl-fixed-case>NLP</span> Preprocessing</a></strong><br><a href=/people/m/margot-mieskes/>Margot Mieskes</a>
|
<a href=/people/s/stefan-schmunk/>Stefan Schmunk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3633><div class="card-body p-3 small">We present initial experiments to evaluate the performance of tasks such as Part of Speech Tagging on data corrupted by Optical Character Recognition (OCR). Our results, based on English and German data, using artificial experiments as well as initial real OCRed data indicate that already a small drop in OCR quality considerably increases the error rates, which would have a significant impact on subsequent processing steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3634 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3634 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3634/>Developing a Fine-grained Corpus for a Less-resourced Language: the case of <span class=acl-fixed-case>K</span>urdish</a></strong><br><a href=/people/r/roshna-abdulrahman/>Roshna Abdulrahman</a>
|
<a href=/people/h/hossein-hassani/>Hossein Hassani</a>
|
<a href=/people/s/sina-ahmadi/>Sina Ahmadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3634><div class="card-body p-3 small">Kurdish is a less-resourced language consisting of different dialects written in various scripts. Approximately 30 million people in different countries speak the language. The lack of corpora is one of the main obstacles in Kurdish language processing. In this paper, we present KTC-the Kurdish Textbooks Corpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is normalized and categorized into 12 educational subjects containing 693,800 tokens (110,297 types). Our resource is publicly available for non-commercial use under the CC BY-NC-SA 4.0 license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3635 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3635 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3635/><span class=acl-fixed-case>A</span>mharic Question Answering for Biography, Definition, and Description Questions</a></strong><br><a href=/people/t/tilahun-abedissa-taffa/>Tilahun Abedissa Taffa</a>
|
<a href=/people/m/mulugeta-libsie/>Mulugeta Libsie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3635><div class="card-body p-3 small">A broad range of information needs can often be stated as a question. Question Answering (QA) systems attempt to provide users concise answer(s) to natural language questions. The existing Amharic QA systems handle fact-based questions that usually take named entities as an answer. To deal with more complex information needs we developed an Amharic non-factoid QA for biography, definition, and description questions. A hybrid approach has been used for the question classification. For document filtering and answer extraction we have used lexical patterns. On the other hand to answer biography questions we have used a summarizer and the generated summary is validated using a text classifier. Our QA system is evaluated and has shown a promising result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3636 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3636/>Polysemous Language in Child Directed Speech</a></strong><br><a href=/people/s/sammy-floyd/>Sammy Floyd</a>
|
<a href=/people/l/libby-barak/>Libby Barak</a>
|
<a href=/people/a/adele-goldberg/>Adele Goldberg</a>
|
<a href=/people/c/casey-lew-williams/>Casey Lew-Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3636><div class="card-body p-3 small">Polysemous Language in Child Directed Speech Learning the meaning of words is one of the fundamental building blocks of verbal communication. Models of child language acquisition have generally made the simplifying assumption that each word appears in child-directed speech with a single meaning. To understand naturalistic word learning during childhood, it is essential to know whether children hear input that is in fact constrained to single meaning per word, or whether the environment naturally contains multiple senses.In this study, we use a topic modeling approach to automatically induce word senses from child-directed speech. Our results confirm the plausibility of our automated analysis approach and reveal an increasing rate of using multiple senses in child-directed speech, starting with corpora from children as early as the first year of life.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3637 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3637 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3637/>Principled Frameworks for Evaluating Ethics in <span class=acl-fixed-case>NLP</span> Systems</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3637><div class="card-body p-3 small">We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3638 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3638 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3638/>Understanding the Shades of Sexism in Popular <span class=acl-fixed-case>TV</span> Series</a></strong><br><a href=/people/n/nayeon-lee/>Nayeon Lee</a>
|
<a href=/people/y/yejin-bang/>Yejin Bang</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3638><div class="card-body p-3 small">[Multiple-submission] In the midst of a generation widely exposed to and influenced by media entertainment, the NLP research community has shown relatively little attention on the sexist comments in popular TV series. To understand sexism in TV series, we propose a way of collecting distant supervision dataset using Character Persona information with the psychological theories on sexism. We assume that sexist characters from TV shows are more prone to making sexist comments when talking about women, and show that this hypothesis is valid through experiment. Finally, we conduct an interesting analysis on popular TV show characters and successfully identify different shades of sexism that is often overlooked.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3639 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3639/>Evaluating Ways of Adapting Word Similarity</a></strong><br><a href=/people/l/libby-barak/>Libby Barak</a>
|
<a href=/people/a/adele-goldberg/>Adele Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3639><div class="card-body p-3 small">People judge pairwise similarity by deciding which aspects of the words&#8217; meanings are relevant for the comparison of the given pair. However, computational representations of meaning rely on dimensions of the vector representation for similarity comparisons, without considering the specific pairing at hand. Prior work has adapted computational similarity judgments by using the softmax function in order to address this limitation by capturing asymmetry in human judgments. We extend this analysis by showing that a simple modification of cosine similarity offers a better correlation with human judgments over a comprehensive dataset. The modification performs best when the similarity between two words is calculated with reference to other words that are most similar and dissimilar to the pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3640 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3640 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3640/>Exploring the Use of Lexicons to aid Deep Learning towards the Detection of Abusive Language</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/j/jason-scott/>Jason Scott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3640><div class="card-body p-3 small">Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focused on detecting personal attacks in online conversations. State-of-the-art research on this task has largely used deep learning with word embeddings. We explored the use of sentiment lexicons as well as semantic lexicons towards improving the accuracy of the baseline Convolutional Neural Network (CNN) using regular word embeddings. This is a work in progress, limited by time constraints and appropriate infrastructure. Our preliminary results showed promise for utilizing lexicons, especially semantic lexicons, for the task of detecting abusive language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3641 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3641 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3641/>Entity-level Classification of Adverse Drug Reactions: a Comparison of Neural Network Models</a></strong><br><a href=/people/i/ilseyar-alimova/>Ilseyar Alimova</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3641><div class="card-body p-3 small">This paper presents our experimental work on exploring the potential of neural network models developed for aspect-based sentiment analysis for entity-level adverse drug reaction (ADR) classification. Our goal is to explore how to represent local context around ADR mentions and learn an entity representation, interacting with its context. We conducted extensive experiments on various sources of text-based information, including social media, electronic health records, and abstracts of scientific articles from PubMed. The results show that Interactive Attention Neural Network (IAN) outperformed other models on four corpora in terms of macro F-measure. This work is an abridged version of our recent paper accepted to Programming and Computer Software journal in 2019.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3642 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3642 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3642/>Context Effects on Human Judgments of Similarity</a></strong><br><a href=/people/l/libby-barak/>Libby Barak</a>
|
<a href=/people/n/noe-kong-johnson/>Noe Kong-Johnson</a>
|
<a href=/people/a/adele-goldberg/>Adele Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3642><div class="card-body p-3 small">The semantic similarity of words forms the basis of many natural language processing methods. These computational similarity measures are often based on a mathematical comparison of vector representations of word meanings, while human judgments of similarity differ in lacking geometrical properties, e.g., symmetric similarity and triangular similarity. In this study, we propose a novel task design to further explore human behavior by asking whether a pair of words is deemed more similar depending on an immediately preceding judgment. Results from a crowdsourcing experiment show that people consistently judge words as more similar when primed by a judgment that evokes a relevant relationship. Our analysis further shows that word2vec similarity correlated significantly better with the out-of-context judgments, thus confirming the methodological differences in human-computer judgments, and offering a new testbed for probing the differences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3643 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3643 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3643/><span class=acl-fixed-case>NLP</span> Automation to Read Radiological Reports to Detect the Stage of Cancer Among Lung Cancer Patients</a></strong><br><a href=/people/k/khushbu-gupta/>Khushbu Gupta</a>
|
<a href=/people/r/ratchainant-thammasudjarit/>Ratchainant Thammasudjarit</a>
|
<a href=/people/a/ammarin-thakkinstian/>Ammarin Thakkinstian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3643><div class="card-body p-3 small">A common challenge in the healthcare industry today is physicians have access to massive amounts of healthcare data but have little time and no appropriate tools. For instance, the risk prediction model generated by logistic regression could predict the probability of diseases occurrence and thus prioritizing patients&#8217; waiting list for further investigations. However, many medical reports available in current clinical practice system are not yet ready for analysis using either statistics or machine learning as they are in unstructured text format. The complexity of medical information makes the annotation or validation of data very challenging and thus acts as a bottleneck to apply machine learning techniques in medical data. This study is therefore conducted to create such annotations automatically where the computer can read radiological reports for oncologists and mark the staging of lung cancer. This staging information is obtained using the rule-based method implemented using the standards of Tumor Node Metastasis (TNM) staging along with deep learning technology called Long Short Term Memory (LSTM) to extract clinical information from the Computed Tomography (CT) text report. The empirical experiment shows promising results being the accuracy of up to 85%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3644 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3644 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3644/>Augmenting Named Entity Recognition with Commonsense Knowledge</a></strong><br><a href=/people/g/gaith-dekhili/>Gaith Dekhili</a>
|
<a href=/people/t/tan-ngoc-le/>Tan Ngoc Le</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3644><div class="card-body p-3 small">Commonsense can be vital in some applications like Natural Language Understanding (NLU), where it is often required to resolve ambiguity arising from implicit knowledge and underspecification. In spite of the remarkable success of neural network approaches on a variety of Natural Language Processing tasks, many of them struggle to react effectively in cases that require commonsense knowledge. In the present research, we take advantage of the availability of the open multilingual knowledge graph ConceptNet, by using it as an additional external resource in Named Entity Recognition (NER). Our proposed architecture involves BiLSTM layers combined with a CRF layer that was augmented with some features such as pre-trained word embedding layers and dropout layers. Moreover, apart from using word representations, we used also character-based representation to capture the morphological and the orthographic information. Our experiments and evaluations showed an improvement in the overall performance with +2.86 in the F1-measure. Commonsense reasonnig has been employed in other studies and NLP tasks but to the best of our knowledge, there is no study relating the integration of a commonsense knowledge base in NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3645 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3645 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3645/>Pardon the Interruption: Automatic Analysis of Gender and Competitive Turn-Taking in <span class=acl-fixed-case>U</span>nited <span class=acl-fixed-case>S</span>tates <span class=acl-fixed-case>S</span>upreme <span class=acl-fixed-case>C</span>ourt Hearings</a></strong><br><a href=/people/h/haley-lepp/>Haley Lepp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3645><div class="card-body p-3 small">The United States Supreme Court plays a key role in defining the legal basis for gender discrimination throughout the country, yet there are few checks on gender bias within the court itself. In conversational turn-taking, interruptions have been documented as a marker of bias between speakers of different genders. The goal of this study is to automatically differentiate between respectful and disrespectful conversational turns taken during official hearings, which could help in detecting bias and finding remediation techniques for discourse in the courtroom. In this paper, I present a corpus of turns annotated by legal professionals, and describe the design of a semi-supervised classifier that will use acoustic and lexical features to analyze turn-taking at scale. On completion of annotations, this classifier will be trained to extract the likelihood that turns are respectful or disrespectful for use in studies of speech trends.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3646 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3646/>Evaluating Coherence in Dialogue Systems using Entailment</a></strong><br><a href=/people/n/nouha-dziri/>Nouha Dziri</a>
|
<a href=/people/e/ehsan-kamalloo/>Ehsan Kamalloo</a>
|
<a href=/people/k/kory-mathewson/>Kory Mathewson</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaiane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3646><div class="card-body p-3 small">Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses. This paper has been accepted in NAACL 2019.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3647 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3647 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3647/>Exploiting machine algorithms in vocalic quantification of <span class=acl-fixed-case>A</span>frican <span class=acl-fixed-case>E</span>nglish corpora</a></strong><br><a href=/people/l/lasisi-adeiza-isiaka/>Lasisi Adeiza Isiaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3647><div class="card-body p-3 small">Towards procedural fidelity in the processing of African English speech corpora, this work demonstrates how the adaptation of machine-assisted segmentation of phonemes and automatic extraction of acoustic values can significantly speed up the processing of naturalistic data and make the vocalic analysis of the varieties less impressionistic. Research in African English phonology has, till date, been least data-driven &#8211; much less the use of comparative corpora for cross-varietal assessments. Using over 30 hours of naturalistic data (from 28 speakers in 5 Nigerian cities), the procedures for segmenting audio files into phonemic units via the Munich Automatic Segmentation System (MAUS), and the extraction of their spectral values in Praat are explained. Evidence from the speech corpora supports a more complex vocalic inventory than attested in previous auditory/manual-based accounts &#8211; thus reinforcing the resourcefulness of the algorithms for the current data and cognate varieties. Keywords: machine algorithms; naturalistic data; African English phonology; vowel segmentation</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3648 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3648 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3648/>Assessing the Ability of Neural Machine Translation Models to Perform Syntactic Rewriting</a></strong><br><a href=/people/j/jahkel-robin/>Jahkel Robin</a>
|
<a href=/people/a/alvin-grissom-ii/>Alvin Grissom II</a>
|
<a href=/people/m/matthew-roselli/>Matthew Roselli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3648><div class="card-body p-3 small">We describe work in progress for evaluating performance of sequence-to-sequence neural networks on the task of syntax-based reordering for rules applicable to simultaneous machine translation. We train models that attempt to rewrite English sentences using rules that are commonly used by human interpreters. We examine the performance of these models to determine which forms of rewriting are more difficult for them to learn and which architectures are the best at learning them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3649 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3649 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3649/>Authorship Recognition with Short-Text using Graph-based Techniques</a></strong><br><a href=/people/l/laura-cruz/>Laura Cruz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3649><div class="card-body p-3 small">In recent years, studies of authorship recognition has aroused great interest in graph-based analysis. Modeling the writing style of each author using a network of co-occurrence words. However, short texts can generate some changes in the topology of network that cause impact on techniques of feature extraction based on graph topology. In this work, we evaluate the robustness of global-strategy and local-strategy based on complex network measurements comparing with graph2vec a graph embedding technique based on skip-gram model. The experiment consists of evaluating how each modification in the length of text affects the accuracy of authorship recognition on both techniques using cross-validation and machine learning techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3650 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3650/>A Parallel Corpus <span class=acl-fixed-case>M</span>ixtec-<span class=acl-fixed-case>S</span>panish</a></strong><br><a href=/people/c/cynthia-montano/>Cynthia Montaño</a>
|
<a href=/people/g/gerardo-sierra-martinez/>Gerardo Sierra Martínez</a>
|
<a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a>
|
<a href=/people/h/helena-gomez/>Helena Gomez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3650><div class="card-body p-3 small">This work is about the compilation process of parallel documents Spanish-Mixtec. There are not many Spanish-Mixec parallel texts and most of the sources are non-digital books. Due to this, we need to face the errors when digitizing the sources and difficulties in sentence alignment, as well as the fact that does not exist a standard orthography. Our parallel corpus consists of sixty texts coming from books and digital repositories. These documents belong to different domains: history, traditional stories, didactic material, recipes, ethnographical descriptions of each town and instruction manuals for disease prevention. We have classified this material in five major categories: didactic (6 texts), educative (6 texts), interpretative (7 texts), narrative (39 texts), and poetic (2 texts). The final total of tokens is 49,814 Spanish words and 47,774 Mixtec words. The texts belong to the states of Oaxaca (48 texts), Guerrero (9 texts) and Puebla (3 texts). According to this data, we see that the corpus is unbalanced in what refers to the representation of the different territories. While 55% of speakers are in Oaxaca, 80% of texts come from this region. Guerrero has the 30% of speakers and the 15% of texts and Puebla, with the 15% of the speakers has a representation of the 5% in the corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3651 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3651 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3651/>Emoji Usage Across Platforms: A Case Study for the Charlottesville Event</a></strong><br><a href=/people/k/khyati-mahajan/>Khyati Mahajan</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3651><div class="card-body p-3 small">We study emoji usage patterns across two social media platforms, one of them considered a fringe community called Gab, and the other Twitter. We find that Gab tends to comparatively use more emotionally charged emoji, but also seems more apathetic towards the violence during the event, while Twitter takes a more empathetic approach to the event.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3652 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3652 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3652/>Reading <span class=acl-fixed-case>KITTY</span>: Pitch Range as an Indicator of Reading Skill</a></strong><br><a href=/people/a/alfredo-gomez/>Alfredo Gomez</a>
|
<a href=/people/a/alicia-ngo/>Alicia Ngo</a>
|
<a href=/people/a/alessandra-otondo/>Alessandra Otondo</a>
|
<a href=/people/j/julie-medero/>Julie Medero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3652><div class="card-body p-3 small">While affective outcomes are generally positive for the use of eBooks and computer-based reading tutors in teaching children to read, learning outcomes are often poorer (Korat and Shamir, 2004). We describe the first iteration of Reading Kitty, an iOS application that uses NLP and speech processing to focus children&#8217;s time on close reading and prosody in oral reading, while maintaining an emphasis on creativity and artifact creation. We also share preliminary results demonstrating that pitch range can be used to automatically predict readers&#8217; skill level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3653 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3653/>Adversarial Attack on Sentiment Classification</a></strong><br><a href=/people/y/yi-ting-alicia-tsai/>Yi-Ting (Alicia) Tsai</a>
|
<a href=/people/m/min-chu-yang/>Min-Chu Yang</a>
|
<a href=/people/h/han-yu-chen/>Han-Yu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3653><div class="card-body p-3 small">In this paper, we propose a white-box attack algorithm called &#8220;Global Search&#8221; method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called &#8220;Greedy Search&#8221;. The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed &#8220;Global Search&#8221; method generates more powerful adversarial examples with less distortion or less modification to the source text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3654 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3654/><span class=acl-fixed-case>CSI</span> <span class=acl-fixed-case>P</span>eru News: finding the culprit, victim and location in news articles</a></strong><br><a href=/people/g/gina-bustamante/>Gina Bustamante</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3654><div class="card-body p-3 small">We introduce a shift on the DS method over the domain of crime-related news from Peru, attempting to find the culprit, victim and location of a crime description from a RE perspective. Obtained results are highly promising and show that proposed modifications are effective in non-traditional domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3655 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3655 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3655/>Exploring Social Bias in Chatbots using Stereotype Knowledge</a></strong><br><a href=/people/n/nayeon-lee/>Nayeon Lee</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3655><div class="card-body p-3 small">Exploring social bias in chatbot is an important, yet relatively unexplored problem. In this paper, we propose an approach to understand social bias in chatbots by leveraging stereotype knowledge. It allows interesting comparison of bias between chatbots and humans, and provides intuitive analysis of existing chatbots by borrowing the finer-grain concepts of sexism and racism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3656 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3656/>Cross-Sentence Transformations in Text Simplification</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3656><div class="card-body p-3 small">Current approaches to Text Simplification focus on simplifying sentences individually. However, certain simplification transformations span beyond single sentences (e.g. joining and re-ordering sentences). In this paper, we motivate the need for modelling the simplification task at the document level, and assess the performance of sequence-to-sequence neural models in this setup. We analyse parallel original-simplified documents created by professional editors and show that there are frequent rewriting transformations that are not restricted to sentence boundaries. We also propose strategies to automatically evaluate the performance of a simplification model on these cross-sentence transformations. Our experiments show the inability of standard sequence-to-sequence neural models to learn these transformations, and suggest directions towards document-level simplification.</div></div></div><hr><div id=w19-37><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-37.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-37/>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3700/>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</a></strong><br><a href=/people/t/tomaz-erjavec/>Tomaž Erjavec</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3702/>Multiple Admissibility : Judging Grammaticality using Unlabeled Data in Language Learning</a></strong><br><a href=/people/a/anisia-katinskaia/>Anisia Katinskaia</a>
|
<a href=/people/s/sardana-ivanova/>Sardana Ivanova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3702><div class="card-body p-3 small">We present our work on the problem of Multiple Admissibility (MA) in <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a>. Multiple Admissibility occurs in many languages when more than one <a href=https://en.wikipedia.org/wiki/Grammar>grammatical form</a> of a word fits syntactically and semantically in a given context. In second language (L2) education-in particular, in <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a> / computer-aided language learning (ITS / CALL) systems, which generate exercises automatically-this implies that multiple alternative answers are possible. We treat the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a grammaticality judgement task. We train a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> with an objective to label sentences as grammatical or ungrammatical, using a simulated learner corpus : a dataset with correct text, and with artificial errors generated automatically. While MA occurs commonly in many languages, this paper focuses on learning <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. We present a detailed classification of the types of constructions in <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, in which MA is possible, and evaluate the model using a test set built from answers provided by the users of a running <a href=https://en.wikipedia.org/wiki/Machine_learning>language learning system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3705/>AGRR 2019 : Corpus for Gapping Resolution in Russian<span class=acl-fixed-case>AGRR</span> 2019: Corpus for Gapping Resolution in <span class=acl-fixed-case>R</span>ussian</a></strong><br><a href=/people/m/maria-ponomareva/>Maria Ponomareva</a>
|
<a href=/people/k/kira-droganova/>Kira Droganova</a>
|
<a href=/people/i/ivan-smurov/>Ivan Smurov</a>
|
<a href=/people/t/tatiana-shavrina/>Tatiana Shavrina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3705><div class="card-body p-3 small">This paper provides a comprehensive overview of the <a href=https://en.wikipedia.org/wiki/Gapping>gapping dataset</a> for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> that consists of 7.5k sentences with <a href=https://en.wikipedia.org/wiki/Gapping>gapping</a> (as well as 15k relevant negative sentences) and comprises data from various genres : <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Fiction>fiction</a>, <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019)-a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. In this paper, we pay special attention to the gapping resolution methods that were introduced within the shared task as well as an alternative test set that illustrates that our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is a diverse and representative subset of Russian language gapping sufficient for effective utilization of machine learning techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3707/>Data Set for Stance and Sentiment Analysis from User Comments on Croatian News<span class=acl-fixed-case>C</span>roatian News</a></strong><br><a href=/people/m/mihaela-bosnjak/>Mihaela Bošnjak</a>
|
<a href=/people/m/mladen-karan/>Mladen Karan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3707><div class="card-body p-3 small">Nowadays it is becoming more important than ever to find new ways of extracting useful information from the evergrowing amount of user-generated data available online. In this paper, we describe the creation of a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> that contains <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> and corresponding comments from Croatian news outlet 24 sata. Our annotation scheme is specifically tailored for the task of detecting stances and sentiment from user comments as well as assessing if commentator claims are verifiable. Through this <a href=https://en.wikipedia.org/wiki/Data>data</a>, we hope to get a better understanding of the publics viewpoint on various events. In addition, we also explore the potential of applying supervised machine learning models toautomate annotation of more data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3708/>A Dataset for Noun Compositionality Detection for a <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Language</a><span class=acl-fixed-case>S</span>lavic Language</a></strong><br><a href=/people/d/dmitry-puzyrev/>Dmitry Puzyrev</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3708><div class="card-body p-3 small">This paper presents the first gold-standard resource for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> annotated with compositionality information of noun compounds. The compound phrases are collected from the Universal Dependency treebanks according to part of speech patterns, such as ADJ+NOUN or NOUN+NOUN, using the gold-standard annotations. Each <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound phrase</a> is annotated by two experts and a moderator according to the following schema : the phrase can be either compositional, non-compositional, or ambiguous (i.e., depending on the context it can be interpreted both as compositional or non-compositional). We conduct an experimental evaluation of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for predicting compositionality of noun compounds in unsupervised and supervised setups. We show that <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> from previous work evaluated on the proposed Russian-language resource achieve the performance comparable with results on <a href=https://en.wikipedia.org/wiki/English_language>English corpora</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3709/>The Second Cross-Lingual Challenge on Recognition, <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>Normalization</a>, <a href=https://en.wikipedia.org/wiki/Language_classification>Classification</a>, and Linking of Named Entities across <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Languages</a><span class=acl-fixed-case>S</span>lavic Languages</a></strong><br><a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/l/laska-laskova/>Laska Laskova</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3709><div class="card-body p-3 small">We describe the Second Multilingual Named Entity Challenge in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a>. The task is recognizing mentions of named entities in <a href=https://en.wikipedia.org/wiki/Web_page>Web documents</a>, their normalization, and cross-lingual linking. The Challenge was organized as part of the 7th Balto-Slavic Natural Language Processing Workshop, co-located with the ACL-2019 conference. Eight teams participated in the <a href=https://en.wikipedia.org/wiki/Competition>competition</a>, which covered four languages and five entity types. Performance for the named entity recognition task reached 90 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>, much higher than reported in the first edition of the Challenge. Seven teams covered all four languages, and five teams participated in the cross-lingual entity linking task. Detailed evaluation information is available on the shared task web page.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3711/>TLR at BSNLP2019 : A Multilingual Named Entity Recognition System<span class=acl-fixed-case>TLR</span> at <span class=acl-fixed-case>BSNLP</span>2019: A Multilingual Named Entity Recognition System</a></strong><br><a href=/people/j/jose-g-moreno/>Jose G. Moreno</a>
|
<a href=/people/e/elvys-linhares-pontes/>Elvys Linhares Pontes</a>
|
<a href=/people/m/mickael-coustaty/>Mickael Coustaty</a>
|
<a href=/people/a/antoine-doucet/>Antoine Doucet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3711><div class="card-body p-3 small">This paper presents our participation at the shared task on multilingual named entity recognition at BSNLP2019. Our strategy is based on a standard neural architecture for <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. In particular, we use a <a href=https://en.wikipedia.org/wiki/Mixed_model>mixed model</a> which combines multilingualcontextual and language-specific embeddings. Our only submitted run is based on a voting schema using multiple models, one for each of the four languages of the task (Bulgarian, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, and Russian) and another for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Results for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> are encouraging for all languages, varying from 60 % to 83 % in terms of Strict and Relaxed metrics, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3714/>JRC TMA-CC : Slavic Named Entity Recognition and Linking. Participation in the BSNLP-2019 shared task<span class=acl-fixed-case>JRC</span> <span class=acl-fixed-case>TMA</span>-<span class=acl-fixed-case>CC</span>: <span class=acl-fixed-case>S</span>lavic Named Entity Recognition and Linking. Participation in the <span class=acl-fixed-case>BSNLP</span>-2019 shared task</a></strong><br><a href=/people/g/guillaume-jacquet/>Guillaume Jacquet</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/h/hristo-tanev/>Hristo Tanev</a>
|
<a href=/people/r/ralf-steinberger/>Ralf Steinberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3714><div class="card-body p-3 small">We report on the participation of the JRC Text Mining and Analysis Competence Centre (TMA-CC) in the BSNLP-2019 Shared Task, which focuses on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Lemmatisation>lemmatisation</a> and cross-lingual linking. We propose a <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a> combining a <a href=https://en.wikipedia.org/wiki/Rule-based_system>rule-based approach</a> and light ML techniques. We use multilingual lexical resources such as JRC-NAMES and <a href=https://en.wikipedia.org/wiki/Babelnet>BABELNET</a> together with a named entity guesser to recognise names. In a second step, we combine known names with wild cards to increase <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recognition recall</a> by also capturing <a href=https://en.wikipedia.org/wiki/Inflection>inflection variants</a>. In a third step, we increase precision by filtering these name candidates with automatically learnt inflection patterns derived from name occurrences in large news article collections. Our major requirement is to achieve high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. We achieved an average of 65 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> with 93 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> on the four languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3716" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3716/>Improving Sentiment Classification in Slovak Language<span class=acl-fixed-case>S</span>lovak Language</a></strong><br><a href=/people/s/samuel-pecar/>Samuel Pecar</a>
|
<a href=/people/m/marian-simko/>Marian Simko</a>
|
<a href=/people/m/maria-bielikova/>Maria Bielikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3716><div class="card-body p-3 small">Using different neural network architectures is widely spread for many different NLP tasks. Unfortunately, most of the research is performed and evaluated only in <a href=https://en.wikipedia.org/wiki/English_language>English language</a> and minor languages are often omitted. We believe using similar <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> for other languages can show interesting results. In this paper, we present our study on <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for improving sentiment classification in <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak language</a>. We performed several experiments for two different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, one containing <a href=https://en.wikipedia.org/wiki/Customer_review>customer reviews</a>, the other one general <a href=https://en.wikipedia.org/wiki/Twitter>Twitter posts</a>. We show comparison of performance of different neural network architectures and also different word representations. We show that another improvement can be achieved by using a <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>model ensemble</a>. We performed experiments utilizing different methods of <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>model ensemble</a>. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieved better results than previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Our experiments showed also other potential research areas.</div></div></div><hr><div id=w19-38><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-38.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-38/>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3800/>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/w/will-radford/>Will Radford</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3803/>Relating Word Embedding Gender Biases to Gender Gaps : A Cross-Cultural Analysis</a></strong><br><a href=/people/s/scott-friedman/>Scott Friedman</a>
|
<a href=/people/s/sonja-schmer-galunder/>Sonja Schmer-Galunder</a>
|
<a href=/people/a/anthony-chen/>Anthony Chen</a>
|
<a href=/people/j/jeffrey-rye/>Jeffrey Rye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3803><div class="card-body p-3 small">Modern <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for common NLP tasks often employ <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a> and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them ; however, these <a href=https://en.wikipedia.org/wiki/Bias>biases</a> may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through <a href=https://en.wikipedia.org/wiki/Big_data>big data</a>. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in <a href=https://en.wikipedia.org/wiki/Education>education</a>, <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, <a href=https://en.wikipedia.org/wiki/Economics>economics</a>, and <a href=https://en.wikipedia.org/wiki/Health>health</a>. We validate these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3804" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3804/>Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories</a></strong><br><a href=/people/k/kaytlin-chaloner/>Kaytlin Chaloner</a>
|
<a href=/people/a/alfredo-maldonado/>Alfredo Maldonado</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3804><div class="card-body p-3 small">Prior work has shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> capture <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>human stereotypes</a>, including <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a>. However, there is a lack of studies testing the presence of specific gender bias categories in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains : <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking</a>, <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical</a> and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> than others, and that the categories of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> present also vary for each set of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We detect some <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT&#8217;s hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3805/>Evaluating the Underlying <a href=https://en.wikipedia.org/wiki/Gender_bias>Gender Bias</a> in Contextualized Word Embeddings</a></strong><br><a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/n/noe-casas/>Noe Casas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3805><div class="card-body p-3 small">Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a>. Our analysis includes different measures previously applied in the literature to standard <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3806/>Conceptor Debiasing of Word Representations Evaluated on WEAT<span class=acl-fixed-case>WEAT</span></a></strong><br><a href=/people/s/saket-karve/>Saket Karve</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3806><div class="card-body p-3 small">Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for <a href=https://en.wikipedia.org/wiki/Post-processing>post-processing</a> both traditional and contextualized word embeddings. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can simultaneously remove racial and gender biases from <a href=https://en.wikipedia.org/wiki/Word_formation>word representations</a>. Unlike standard <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing methods</a>, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3807/>Filling Gender & Number Gaps in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Black-box Context Injection</a></strong><br><a href=/people/a/amit-moryossef/>Amit Moryossef</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3807><div class="card-body p-3 small">When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must guess this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>translation accuracy</a> in up to 2.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3811/>BERT Masked Language Modeling for Co-reference Resolution<span class=acl-fixed-case>BERT</span> Masked Language Modeling for Co-reference Resolution</a></strong><br><a href=/people/f/felipe-alfaro/>Felipe Alfaro</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3811><div class="card-body p-3 small">This paper explains the TALP-UPC participation for the Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. We have implemented two models for mask language modeling using pre-trained BERT adjusted to work for a classification problem. The proposed solutions are based on the word probabilities of the original BERT model, but using common English names to replace the original test names.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3814" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3814/>Look Again at the Syntax : Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution</a></strong><br><a href=/people/y/yinchuan-xu/>Yinchuan Xu</a>
|
<a href=/people/j/junlin-yang/>Junlin Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3814><div class="card-body p-3 small">Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9 % F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN&#8217;s embeddings outperform the original BERT embeddings on the coreference task. Our <a href=https://en.wikipedia.org/wiki/Work_(thermodynamics)>work</a> significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9 % to 80.3 %. We participated in the Gender Bias for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> 2019 shared task, and our codes are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3818 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3818" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3818/>Anonymized BERT : An Augmentation Approach to the Gendered Pronoun Resolution Challenge<span class=acl-fixed-case>BERT</span>: An Augmentation Approach to the Gendered Pronoun Resolution Challenge</a></strong><br><a href=/people/b/bo-liu/>Bo Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3818><div class="card-body p-3 small">We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data size</a>, this approach diversifies idiosyncratic information embedded in <a href=https://en.wikipedia.org/wiki/Name>names</a>. Using same set of common first names can also help the model recognize <a href=https://en.wikipedia.org/wiki/Name>names</a> better, shorten token length, and remove gender and regional biases associated with <a href=https://en.wikipedia.org/wiki/Name>names</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the <a href=https://en.wikipedia.org/wiki/System>system</a> scores 0.1799 which would be third place.</div></div></div><hr><div id=w19-39><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-39.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-39/>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3900/>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</a></strong><br><a href=/people/j/jason-eisner/>Jason Eisner</a>
|
<a href=/people/m/matthias-galle/>Matthias Gallé</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a>
|
<a href=/people/a/ariadna-quattoni/>Ariadna Quattoni</a>
|
<a href=/people/g/guillaume-rabusseau/>Guillaume Rabusseau</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3903/>Relating RNN Layers with the Spectral WFA Ranks in Sequence Modelling<span class=acl-fixed-case>RNN</span> Layers with the Spectral <span class=acl-fixed-case>WFA</span> Ranks in Sequence Modelling</a></strong><br><a href=/people/f/farhana-ferdousi-liza/>Farhana Ferdousi Liza</a>
|
<a href=/people/m/marek-grzes/>Marek Grzes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3903><div class="card-body p-3 small">We analyse Recurrent Neural Networks (RNNs) to understand the significance of multiple LSTM layers. We argue that the Weighted Finite-state Automata (WFA) trained using a spectral learning algorithm are helpful to analyse RNNs. Our results suggest that multiple LSTM layers in RNNs help learning distributed hidden states, but have a smaller impact on the ability to learn long-term dependencies. The analysis is based on the empirical results, however relevant theory (whenever possible) was discussed to justify and support our conclusions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3904/>Multi-Element Long Distance Dependencies : Using SPk Languages to Explore the Characteristics of Long-Distance Dependencies<span class=acl-fixed-case>SP</span>k Languages to Explore the Characteristics of Long-Distance Dependencies</a></strong><br><a href=/people/a/abhijit-mahalunkar/>Abhijit Mahalunkar</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3904><div class="card-body p-3 small">In order to successfully model Long Distance Dependencies (LDDs) it is necessary to under-stand the full-range of the characteristics of the LDDs exhibited in a target dataset. In this paper, we use Strictly k-Piecewise languages to generate <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with various properties. We then compute the characteristics of the LDDs in these datasets using <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> and analyze the impact of factors such as (i) k, (ii) length of LDDs, (iii) vocabulary size, (iv) forbidden strings, and (v) dataset size. This analysis reveal that the number of interacting elements in a dependency is an important characteristic of LDDs. This leads us to the challenge of modelling multi-element long-distance dependencies. Our results suggest that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> may aide in modeling datasets with multi-element long-distance dependencies. However, we conclude that there is a need to develop more efficient attention mechanisms to address this issue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3905/>LSTM Networks Can Perform Dynamic Counting<span class=acl-fixed-case>LSTM</span> Networks Can Perform Dynamic Counting</a></strong><br><a href=/people/m/mirac-suzgun/>Mirac Suzgun</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/s/stuart-m-shieber/>Stuart Shieber</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3905><div class="card-body p-3 small">In this paper, we systematically assess the ability of standard <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent networks</a> to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for recognition.</div></div></div><hr><div id=w19-40><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-40.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-40/>Proceedings of the 13th Linguistic Annotation Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4000/>Proceedings of the 13th Linguistic Annotation Workshop</a></strong><br><a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/d/deniz-zeyrek/>Deniz Zeyrek</a>
|
<a href=/people/j/jet-hoek/>Jet Hoek</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4002/>WiRe57 : A Fine-Grained Benchmark for Open Information Extraction<span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>R</span>e57 : A Fine-Grained Benchmark for Open Information Extraction</a></strong><br><a href=/people/w/william-lechelle/>William Lechelle</a>
|
<a href=/people/f/fabrizio-gotti/>Fabrizio Gotti</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4002><div class="card-body p-3 small">We build a <a href=https://en.wikipedia.org/wiki/Reference_work>reference</a> for the task of <a href=https://en.wikipedia.org/wiki/Open_information_extraction>Open Information Extraction</a>, on five documents. We tentatively resolve a number of issues that arise, including <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, and we take steps toward addressing <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, a significant problem. We seek to better pinpoint the requirements for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We produce our annotation guidelines specifying what is correct to extract and what is not. In turn, we use this <a href=https://en.wikipedia.org/wiki/Reference_(computer_science)>reference</a> to score existing Open IE systems. We address the non-trivial problem of evaluating the extractions produced by <a href=https://en.wikipedia.org/wiki/System>systems</a> against the reference tuples, and share our evaluation script. Among seven compared extractors, we find the MinIE system to perform best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4003/>Crowdsourcing Discourse Relation Annotations by a Two-Step Connective Insertion Task</a></strong><br><a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/m/merel-scholman/>Merel Scholman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4003><div class="card-body p-3 small">The perspective of being able to crowd-source coherence relations bears the promise of acquiring <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for new texts quickly, which could then increase the size and variety of discourse-annotated corpora. It would also open the avenue to answering new research questions : Collecting annotations from a larger number of individuals per instance would allow to investigate the distribution of inferred relations, and to study individual differences in coherence relation interpretation. However, annotating <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence relations</a> with untrained workers is not trivial. We here propose a novel two-step annotation procedure, which extends an earlier method by Scholman and Demberg (2017a). In our approach, coherence relation labels are inferred from <a href=https://en.wikipedia.org/wiki/Logical_connective>connectives</a> that workers insert into the text. We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from PDTB and RSTDT on the same texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4004/>Annotating and analyzing the interactions between meaning relations</a></strong><br><a href=/people/d/darina-gold/>Darina Gold</a>
|
<a href=/people/v/venelin-kovatchev/>Venelin Kovatchev</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4004><div class="card-body p-3 small">Pairs of sentences, phrases, or other text pieces can hold semantic relations such as <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a>, textual entailment, <a href=https://en.wikipedia.org/wiki/Contradiction>contradiction</a>, specificity, and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. These <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> are usually studied in isolation and no dataset exists where they can be compared empirically. Here we present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> annotated with these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> and the analysis of these results. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains 520 sentence pairs, annotated with these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. We measure the annotation reliability of each individual relation and we examine their interactions and correlations. Among the unexpected results revealed by our analysis is that the traditionally considered direct relationship between <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a> and bi-directional entailment does not hold in our <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4008/>Tagging modality in Oceanic languages of Melanesia</a></strong><br><a href=/people/a/annika-tjuka/>Annika Tjuka</a>
|
<a href=/people/l/lena-weissmann/>Lena Weißmann</a>
|
<a href=/people/k/kilu-von-prince/>Kilu von Prince</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4008><div class="card-body p-3 small">Primary data from small, low-resource languages of Oceania have only recently become available through <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a>. In our study, we explore corpus data of five Oceanic languages of Melanesia which are known to be mood-prominent (in the sense of Bhat, 1999). In order to find out more about <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_modality>modality</a>, and <a href=https://en.wikipedia.org/wiki/Grammatical_polarity>polarity</a>, we tagged these categories in a subset of our corpora. For the category of modality, we developed a novel tag set (MelaTAMP, 2017), which categorizes clauses into factual, possible, and counterfactual. Based on an analysis of the inter-annotator consistency, we argue that our tag set for the modal domain is efficient for our subject languages and might be useful for other languages and purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4009 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4009/>Harmonizing Different Lemmatization Strategies for Building a Knowledge Base of Linguistic Resources for Latin<span class=acl-fixed-case>L</span>atin</a></strong><br><a href=/people/f/francesco-mambrini/>Francesco Mambrini</a>
|
<a href=/people/m/marco-passarotti/>Marco Passarotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4009><div class="card-body p-3 small">The interoperability between lemmatized corpora of Latin and other resources that use the lemma as indexing key is hampered by the multiple lemmatization strategies that different projects adopt. In this paper we discuss how we tackle the challenges raised by harmonizing different lemmatization criteria in the context of a project that aims to connect linguistic resources for <a href=https://en.wikipedia.org/wiki/Latin>Latin</a> using the Linked Data paradigm. The paper introduces the architecture supporting an open-ended, lemma-based Knowledge Base, built to make textual and lexical resources for Latin interoperable. Particularly, the paper describes the inclusion into the <a href=https://en.wikipedia.org/wiki/Knowledge_Base>Knowledge Base</a> of its lexical basis, of a word formation lexicon and of a lemmatized and syntactically annotated corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4012/>An Online Annotation Assistant for Argument Schemes</a></strong><br><a href=/people/j/john-lawrence/>John Lawrence</a>
|
<a href=/people/j/jacky-visser/>Jacky Visser</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4012><div class="card-body p-3 small">Understanding the inferential principles underpinning an <a href=https://en.wikipedia.org/wiki/Argument>argument</a> is essential to the proper interpretation and evaluation of persuasive discourse. Argument schemes capture the conventional patterns of reasoning appealed to in <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>. The empirical study of these patterns relies on the availability of data about the actual use of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> in communicative practice. Annotated corpora of argument schemes, however, are scarce, small, and unrepresentative. Aiming to address this issue, we present one step in the development of improved datasets by integrating the Argument Scheme Key a novel annotation method based on one of the most popular typologies of argument schemes into the widely used OVA software for argument analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4016 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4016/>Explaining Simple Natural Language Inference</a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/a/annebeth-buis/>Annebeth Buis</a>
|
<a href=/people/l/livy-real/>Livy Real</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4016><div class="card-body p-3 small">The vast amount of research introducing new <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> and techniques for semi-automatically annotating corpora shows the important role that <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> play in today&#8217;s research, especially in the <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning community</a>. This rapid development raises concerns about the quality of the datasets created and consequently of the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained, as recently discussed with respect to the Natural Language Inference (NLI) task. In this work we conduct an annotation experiment based on a small subset of the SICK corpus. The experiment reveals several problems in the annotation guidelines, and various challenges of the NLI task itself. Our quantitative evaluation of the experiment allows us to assign our empirical observations to specific linguistic phenomena and leads us to recommendations for future annotation tasks, for NLI and possibly for other tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4017/>On the role of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> in persuasive texts</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4017><div class="card-body p-3 small">This paper investigates the use of explicitly signalled discourse relations in persuasive texts. We present a corpus study where we control for speaker and topic and show that the distribution of different discourse connectives varies considerably across different discourse settings. While this variation can be explained by genre differences, we also observe variation regarding the distribution of discourse relations across different settings. This variation, however, can not be easily explained by <a href=https://en.wikipedia.org/wiki/Genre>genre differences</a>. We argue that the differences regarding the use of discourse relations reflects different strategies of persuasion and that these might be due to <a href=https://en.wikipedia.org/wiki/Audience_design>audience design</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4018/>One format to rule them all The emtsv pipeline for Hungarian<span class=acl-fixed-case>H</span>ungarian</a></strong><br><a href=/people/b/balazs-indig/>Balázs Indig</a>
|
<a href=/people/b/balint-sass/>Bálint Sass</a>
|
<a href=/people/e/eszter-simon/>Eszter Simon</a>
|
<a href=/people/i/ivan-mittelholcz/>Iván Mittelholcz</a>
|
<a href=/people/n/noemi-vadasz/>Noémi Vadász</a>
|
<a href=/people/m/marton-makrai/>Márton Makrai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4018><div class="card-body p-3 small">We present a more efficient version of the e-magyar NLP pipeline for <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a> called emtsv. It integrates Hungarian NLP tools in a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> whose individual modules can be developed or replaced independently and allows new ones to be added. The <a href=https://en.wikipedia.org/wiki/Design>design</a> also allows convenient investigation and manual correction of the data flow from one module to another. The improvements we publish include effective communication between the modules and support of the use of individual <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> both in the chain and standing alone. Our goals are accomplished using extended tsv (tab separated values) files, a simple, uniform, generic and self-documenting input / output format. Our vision is maintaining the <a href=https://en.wikipedia.org/wiki/System>system</a> for a long time and making it easier for external developers to fit their own modules into the <a href=https://en.wikipedia.org/wiki/System>system</a>, thus sharing existing competencies in the field of processing <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, a mid-resourced language. The source code is available under LGPL 3.0 license at https://github.com/dlt-rilmta/emtsv.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4019/>Turkish Treebanking : Unifying and Constructing Efforts<span class=acl-fixed-case>T</span>urkish Treebanking: Unifying and Constructing Efforts</a></strong><br><a href=/people/u/utku-turk/>Utku Türk</a>
|
<a href=/people/f/furkan-atmaca/>Furkan Atmaca</a>
|
<a href=/people/s/saziye-betul-ozates/>Şaziye Betül Özateş</a>
|
<a href=/people/a/abdullatif-koksal/>Abdullatif Köksal</a>
|
<a href=/people/b/balkiz-ozturk-basaran/>Balkiz Ozturk Basaran</a>
|
<a href=/people/t/tunga-gungor/>Tunga Gungor</a>
|
<a href=/people/a/arzucan-ozgur/>Arzucan Özgür</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4019><div class="card-body p-3 small">In this paper, we present the current version of two different treebanks, the re-annotation of the Turkish PUD Treebank and the first annotation of the Turkish National Corpus Universal Dependency (henceforth TNC-UD). The annotation of both treebanks, the Turkish PUD Treebank and TNC-UD, was carried out based on the decisions concerning linguistic adequacy of re-annotation of the Turkish IMST-UD Treebank (Trk et. al., forthcoming). Both of the <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> were annotated with the same <a href=https://en.wikipedia.org/wiki/Annotation>annotation process</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological and syntactic analyses</a>. The TNC-UD is planned to have 10,000 sentences. In this paper, we will present the first 500 sentences along with the annotation PUD Treebank. Moreover, this paper also offers the parsing results of a graph-based neural parser on the previous and re-annotated PUD, as well as the TNC-UD. In light of the comparisons, even though we observe a slight decrease in the attachment scores of the <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish PUD treebank</a>, we demonstrate that the annotation of the TNC-UD improves the parsing accuracy of <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>. In addition to the <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>, we have also constructed a custom <a href=https://en.wikipedia.org/wiki/Annotation>annotation software</a> with advanced filtering and morphological editing options. Both the treebanks, including a full edit-history and the annotation guidelines, and the custom software are publicly available under an open license online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4020/>A Dataset for Semantic Role Labelling of Hindi-English Code-Mixed Tweets<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Code-Mixed Tweets</a></strong><br><a href=/people/r/riya-pal/>Riya Pal</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4020><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 1460 Hindi-English code-mixed tweets consisting of 20,949 tokens labelled with Proposition Bank labels marking their semantic roles. We created verb frames for complex predicates present in the corpus and formulated mappings from Paninian dependency labels to Proposition Bank labels. With the help of these mappings and the dependency tree, we propose a baseline rule based system for <a href=https://en.wikipedia.org/wiki/Semantic_Role_Labelling>Semantic Role Labelling</a> of Hindi-English code-mixed data. We obtain an accuracy of 96.74 % for Argument Identification and are able to further classify 73.93 % of the labels correctly. While there is relevant ongoing research on Semantic Role Labelling and on building tools for code-mixed social media data, this is the first attempt at labelling semantic roles in code-mixed data, to the best of our knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4021/>A Multi-Platform Annotation Ecosystem for Domain Adaptation</a></strong><br><a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/n/nancy-ide/>Nancy Ide</a>
|
<a href=/people/j/jin-dong-kim/>Jin-Dong Kim</a>
|
<a href=/people/j/jan-christoph-klie/>Jan-Christoph Klie</a>
|
<a href=/people/k/keith-suderman/>Keith Suderman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4021><div class="card-body p-3 small">This paper describes an <a href=https://en.wikipedia.org/wiki/Ecosystem>ecosystem</a> consisting of three independent text annotation platforms. To demonstrate their ability to work in concert, we illustrate how to use them to address an interactive domain adaptation task in biomedical entity recognition. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> and the approach are in general domain-independent and can be readily applied to other areas of science.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4024/>Comparative judgments are more consistent than <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> for labelling word complexity</a></strong><br><a href=/people/s/sian-gooding/>Sian Gooding</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/a/advait-sarkar/>Advait Sarkar</a>
|
<a href=/people/a/alan-blackwell/>Alan Blackwell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4024><div class="card-body p-3 small">Lexical simplification systems replace complex words with simple ones based on a model of which words are complex in context. We explore how users can help train complex word identification models through <a href=https://en.wikipedia.org/wiki/Labelling>labelling</a> more efficiently and reliably. We show that using an interface where annotators make comparative rather than binary judgments leads to more reliable and consistent labels, and explore whether comparative judgments may provide a faster way for collecting labels.</div></div></div><hr><div id=w19-41><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-41.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-41/>Proceedings of the First Workshop on NLP for Conversational AI</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4100/>Proceedings of the First Workshop on NLP for Conversational AI</a></strong><br><a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/t/tania-bedrax-weiss/>Tania Bedrax-Weiss</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/t/thang-minh-luong/>Thang-Minh Luong</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4101/>A Repository of Conversational Datasets</a></strong><br><a href=/people/m/matthew-henderson/>Matthew Henderson</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/g/girish-kumar/>Girish Kumar</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4101><div class="card-body p-3 small">Progress in <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The <a href=https://en.wikipedia.org/wiki/Disciplinary_repository>repository</a> contains scripts that allow researchers to reproduce the standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4104/>Building a <a href=https://en.wikipedia.org/wiki/Production_model>Production Model</a> for Retrieval-Based Chatbots</a></strong><br><a href=/people/k/kyle-swanson/>Kyle Swanson</a>
|
<a href=/people/l/lili-yu/>Lili Yu</a>
|
<a href=/people/c/christopher-fox/>Christopher Fox</a>
|
<a href=/people/j/jeremy-wohlwend/>Jeremy Wohlwend</a>
|
<a href=/people/t/tao-lei/>Tao Lei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4104><div class="card-body p-3 small">Response suggestion is an important task for building <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer conversation systems</a>. Recent approaches to conversation modeling have introduced new model architectures with impressive results, but relatively little attention has been paid to whether these models would be practical in a production setting. In this paper, we describe the unique challenges of building a production retrieval-based conversation system, which selects outputs from a whitelist of candidate responses. To address these challenges, we propose a dual encoder architecture which performs rapid inference and scales well with the size of the whitelist. We also introduce and compare two methods for generating <a href=https://en.wikipedia.org/wiki/Whitelisting>whitelists</a>, and we carry out a comprehensive analysis of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and <a href=https://en.wikipedia.org/wiki/Whitelisting>whitelists</a>. Experimental results on a large, proprietary help desk chat dataset, including both offline metrics and a human evaluation, indicate production-quality performance and illustrate key lessons about conversation modeling in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4107/>DSTC7 Task 1 : Noetic End-to-End Response Selection<span class=acl-fixed-case>DSTC</span>7 Task 1: Noetic End-to-End Response Selection</a></strong><br><a href=/people/c/chulaka-gunasekara/>Chulaka Gunasekara</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4107><div class="card-body p-3 small">Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> provided two new resources that presented different challenges : one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem : (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4108/>End-to-End Neural Context Reconstruction in Chinese Dialogue<span class=acl-fixed-case>C</span>hinese Dialogue</a></strong><br><a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/r/rui-qiao/>Rui Qiao</a>
|
<a href=/people/h/haocheng-qin/>Haocheng Qin</a>
|
<a href=/people/a/amy-sun/>Amy Sun</a>
|
<a href=/people/l/luchen-tan/>Luchen Tan</a>
|
<a href=/people/k/kun-xiong/>Kun Xiong</a>
|
<a href=/people/m/ming-li/>Ming Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4108><div class="card-body p-3 small">We tackle the problem of context reconstruction in Chinese dialogue, where the task is to replace <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>, zero pronouns, and other referring expressions with their referent nouns so that sentences can be processed in isolation without context. Following a standard decomposition of the context reconstruction task into referring expression detection and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, we propose a novel end-to-end architecture for separately and jointly accomplishing this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Key features of this model include POS and position encoding using CNNs and a novel pronoun masking mechanism. One perennial problem in building such models is the paucity of training data, which we address by augmenting previously-proposed methods to generate a large amount of realistic training data. The combination of more data and better <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yields <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> higher than the state-of-the-art method in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and end-to-end context reconstruction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4115/>Relevant and Informative Response Generation using <a href=https://en.wikipedia.org/wiki/Pointwise_mutual_information>Pointwise Mutual Information</a></a></strong><br><a href=/people/j/junya-takayama/>Junya Takayama</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4115><div class="card-body p-3 small">A sequence-to-sequence model tends to generate generic responses with little information for input utterances. To solve this problem, we propose a neural model that generates relevant and informative responses. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has simple architecture to enable easy application to existing neural dialogue models. Specifically, using positive pointwise mutual information, it first identifies <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> that frequently co-occur in responses given an utterance. Then, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> encourages the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> to use the <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> for response generation. Experiment results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> successfully diversifies responses relative to previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4116/>Responsive and Self-Expressive Dialogue Generation</a></strong><br><a href=/people/k/kozo-chikai/>Kozo Chikai</a>
|
<a href=/people/j/junya-takayama/>Junya Takayama</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4116><div class="card-body p-3 small">A neural conversation model is a promising approach to develop dialogue systems with the ability of <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a>. It allows training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in an end-to-end manner without complex rule design nor <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. However, as a side effect, the neural model tends to generate safe but uninformative and insensitive responses like OK and I do n&#8217;t know. Such replies are called generic responses and regarded as a critical problem for user-engagement of dialogue systems. For a more engaging chit-chat experience, we propose a neural conversation model that generates responsive and self-expressive replies. Specifically, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates domain-aware and sentiment-rich responses. Experiments empirically confirmed that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> outperformed the sequence-to-sequence model ; 68.1 % of our responses were domain-aware with <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment polarities</a>, which was only 2.7 % for responses generated by the sequence-to-sequence model.</div></div></div><hr><div id=w19-42><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-42.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-42/>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4200/>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4201/>AX Semantics’ Submission to the SIGMORPHON 2019 Shared Task<span class=acl-fixed-case>AX</span> Semantics’ Submission to the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task</a></strong><br><a href=/people/a/andreas-madsack/>Andreas Madsack</a>
|
<a href=/people/r/robert-weissgraeber/>Robert Weißgraeber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4201><div class="card-body p-3 small">This paper describes the AX Semantics&#8217; submission to the SIGMORPHON 2019 shared task on morphological reinflection. We implemented two systems, both tackling the task for all languages in one codebase, without any underlying language specific features. The first one is an encoder-decoder model using AllenNLP ; the second system uses the same <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> modified by a custom trainer that trains only with the target language resources after a specific threshold. We especially focused on building an <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> using AllenNLP with out-of-the-box methods to facilitate easy operation and reuse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4207/>ITIST at the SIGMORPHON 2019 Shared Task : Sparse Two-headed Models for Inflection<span class=acl-fixed-case>IT</span>–<span class=acl-fixed-case>IST</span> at the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task: Sparse Two-headed Models for Inflection</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4207><div class="card-body p-3 small">This paper presents the Instituto de TelecomunicaesInstituto Superior Tcnico submission to Task 1 of the SIGMORPHON 2019 Shared Task. Our models combine sparse sequence-to-sequence models with a two-headed attention mechanism that learns separate attention distributions for the <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemma</a> and inflectional tags. Among submissions to Task 1, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> rank second and third. Despite the low data setting of the task (only 100 in-language training examples), they learn plausible inflection patterns and often concentrate all probability mass into a small set of hypotheses, making <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> exact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4208/>CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology<span class=acl-fixed-case>CMU</span>-01 at the <span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task on Crosslinguality and Context in Morphology</a></strong><br><a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/g/gayatri-bhat/>Gayatri Bhat</a>
|
<a href=/people/d/david-r-mortensen/>David R. Mortensen</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4208><div class="card-body p-3 small">This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatization</a> in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, <a href=https://en.wikipedia.org/wiki/Case_(disambiguation)>Case</a>, etc.) independently. However, most <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> are under-resourced, thus making it challenging to train <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a> for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4210/>THOMAS : The Hegemonic OSU Morphological Analyzer using Seq2seq<span class=acl-fixed-case>THOMAS</span>: The Hegemonic <span class=acl-fixed-case>OSU</span> Morphological Analyzer using Seq2seq</a></strong><br><a href=/people/b/byung-doh-oh/>Byung-Doh Oh</a>
|
<a href=/people/p/pranav-maneriker/>Pranav Maneriker</a>
|
<a href=/people/n/nanjiang-jiang/>Nanjiang Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4210><div class="card-body p-3 small">This paper describes the OSU submission to the SIGMORPHON 2019 shared task, Crosslinguality and Context in <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphology</a>. Our system addresses the contextual morphological analysis subtask of Task 2, which is to produce the morphosyntactic description (MSD) of each fully inflected word within a given sentence. We frame this as a sequence generation task and employ a neural encoder-decoder (seq2seq) architecture to generate the sequence of MSD tags given the encoded representation of each token. Follow-up analyses reveal that our system most significantly improves performance on morphologically complex languages whose inflected word forms typically have longer MSD tag sequences. In addition, our system seems to capture the structured correlation between MSD tags, such as that between the verb tag and TAM-related tags.<i>contextual morphological analysis</i> subtask of Task 2, which is to produce the morphosyntactic description (MSD) of each fully inflected word within a given sentence. We frame this as a sequence generation task and employ a neural encoder-decoder (seq2seq) architecture to generate the sequence of MSD tags given the encoded representation of each token. Follow-up analyses reveal that our system most significantly improves performance on morphologically complex languages whose inflected word forms typically have longer MSD tag sequences. In addition, our system seems to capture the structured correlation between MSD tags, such as that between the &#8220;verb&#8221; tag and TAM-related tags.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4211/>Sigmorphon 2019 Task 2 system description paper : <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphological analysis</a> in context for many languages, with supervision from only a few</a></strong><br><a href=/people/b/brad-aiken/>Brad Aiken</a>
|
<a href=/people/j/jared-kelly/>Jared Kelly</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/s/suleyman-olcay-polat/>Suleyman Olcay Polat</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/r/rodney-nielsen/>Rodney Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4211><div class="card-body p-3 small">This paper presents the UNT HiLT+Ling system for the Sigmorphon 2019 shared Task 2 : <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphological Analysis</a> and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatization</a> in Context. Our core approach focuses on the morphological tagging task ; <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> are treated as secondary tasks. Given the highly multilingual nature of the task, we propose an approach which makes minimal use of the supplied training data, in order to be extensible to languages without labeled training data for the morphological inflection task. Specifically, we use a parallel Bible corpus to align contextual embeddings at the <a href=https://en.wikipedia.org/wiki/Chapters_and_verses_of_the_Bible>verse level</a>. The aligned verses are used to build cross-language translation matrices, which in turn are used to map between embedding spaces for the various languages. Finally, we use sets of inflected forms, primarily from a high-resource language, to induce vector representations for individual UniMorph tags. Morphological analysis is performed by matching <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a> to <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for individual tokens. While our <a href=https://en.wikipedia.org/wiki/System>system</a> results are dramatically below the average system submitted for the shared task evaluation campaign, our method is (we suspect) unique in its minimal reliance on labeled training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4212/>UDPipe at SIGMORPHON 2019 : Contextualized Embeddings, Regularization with Morphological Categories, Corpora Merging<span class=acl-fixed-case>UDP</span>ipe at <span class=acl-fixed-case>SIGMORPHON</span> 2019: Contextualized Embeddings, Regularization with Morphological Categories, Corpora Merging</a></strong><br><a href=/people/m/milan-straka/>Milan Straka</a>
|
<a href=/people/j/jana-strakova/>Jana Straková</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4212><div class="card-body p-3 small">We present our contribution to the SIGMORPHON 2019 Shared Task : Crosslinguality and Context in Morphology, Task 2 : contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pretrained contextualized embeddings (BERT) as additional inputs to the network ; secondly, we use individual morphological features as regularization ; and finally, we merge the selected corpora of the same language. In the lemmatization task, our <a href=https://en.wikipedia.org/wiki/System>system</a> exceeds all the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> by a wide margin with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>lemmatization accuracy</a> 95.78 (second best was 95.00, third 94.46). In the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> placed tightly second : our <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis accuracy</a> was 93.19, the winning <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s 93.23.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4214/>A Little Linguistics Goes a Long Way : Unsupervised Segmentation with Limited Language Specific Guidance</a></strong><br><a href=/people/a/alexander-erdmann/>Alexander Erdmann</a>
|
<a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/m/mai-oudah/>Mai Oudah</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4214><div class="card-body p-3 small">We present de-lexical segmentation, a linguistically motivated alternative to greedy or other unsupervised methods, requiring only minimal language specific input. Our technique involves creating a small grammar of closed-class affixes which can be written in a few hours. The <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> over generates analyses for word forms attested in a raw corpus which are disambiguated based on features of the linguistic base proposed for each form. Extending the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> to cover orthographic, morpho-syntactic or lexical variation is simple, making it an ideal solution for challenging corpora with noisy, dialect-inconsistent, or otherwise non-standard content. In two evaluations, we consistently outperform competitive unsupervised baselines and approach the performance of state-of-the-art <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> trained on large amounts of data, providing evidence for the value of linguistic input during preprocessing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4215/>Equiprobable mappings in weighted constraint grammars</a></strong><br><a href=/people/a/arto-anttila/>Arto Anttila</a>
|
<a href=/people/s/scott-borgeson/>Scott Borgeson</a>
|
<a href=/people/g/giorgio-magri/>Giorgio Magri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4215><div class="card-body p-3 small">We show that <a href=https://en.wikipedia.org/wiki/MaxEnt>MaxEnt</a> is so rich that it can distinguish between any two different <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> : there always exists a nonnegative weight vector which assigns them different <a href=https://en.wikipedia.org/wiki/MaxEnt>MaxEnt probabilities</a>. Stochastic HG instead does admit equiprobable mappings and we give a complete formal characterization of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4216/>Unbounded Stress in Subregular Phonology</a></strong><br><a href=/people/y/yiding-hao/>Yiding Hao</a>
|
<a href=/people/s/samuel-andersson/>Samuel Andersson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4216><div class="card-body p-3 small">This paper situates culminative unbounded stress systems within the subregular hierarchy for <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a>. While Baek (2018) has argued that such systems can be uniformly understood as input tier-based strictly local constraints, we show here that default-to-opposite-side and default-to-same-side stress systems belong to distinct subregular classes when they are viewed as functions that assign primary stress to underlying forms. While the former system can be captured by input tier-based input strictly local functions, a subsequential function class that we define here, the latter system is not subsequential, though it is weakly deterministic according to McCollum et al.&#8217;s (2018) non-interaction criterion. Our results motivate the extension of recently proposed subregular language classes to subregular functions and argue in favor of McCollum et al&#8217;s definition of weak determinism over that of Heinz and Lai (2013).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4218/>Convolutional neural networks for low-resource morpheme segmentation : baseline or state-of-the-art?</a></strong><br><a href=/people/a/alexey-sorokin/>Alexey Sorokin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4218><div class="card-body p-3 small">We apply <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> to the task of shallow morpheme segmentation using low-resource datasets for 5 different languages. We show that both in fully supervised and semi-supervised settings our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> beats previous state-of-the-art approaches. We argue that <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> reflect local nature of morpheme segmentation better than other semi-supervised approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4222/>Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages</a></strong><br><a href=/people/r/ramy-eskander/>Ramy Eskander</a>
|
<a href=/people/j/judith-l-klavans/>Judith Klavans</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4222><div class="card-body p-3 small">Polysynthetic languages pose a challenge for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> due to the root-morpheme complexity and to the word class squish. In addition, many of these <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic languages</a> are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the <a href=https://en.wikipedia.org/wiki/Uto-Aztecan_languages>Uto-Aztecan family</a>. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4226 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4226/>The SIGMORPHON 2019 Shared Task : Morphological Analysis in Context and Cross-Lingual Transfer for <a href=https://en.wikipedia.org/wiki/Inflection>Inflection</a><span class=acl-fixed-case>SIGMORPHON</span> 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/s/sabrina-j-mielke/>Sabrina J. Mielke</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4226><div class="card-body p-3 small">The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years&#8217; inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> and morphological feature analysis in context. All submissions featured a neural component and built on either this year&#8217;s strong baselines or highly ranked systems from previous years&#8217; shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.</div></div></div><hr><div id=w19-43><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-43.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-43/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4300/>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</a></strong><br><a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4301/>Deep Generalized Canonical Correlation Analysis</a></strong><br><a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/b/biman-gujral/>Biman Gujral</a>
|
<a href=/people/d/dee-ann-reisinger/>Dee Ann Reisinger</a>
|
<a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/r/raman-arora/>Raman Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4301><div class="card-body p-3 small">We present Deep Generalized Canonical Correlation Analysis (DGCCA) a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> are maximally informative of each other. While methods for nonlinear two view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks : <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic transcription</a> from acoustic & articulatory measurements, recommending hashtags and recommending friends on a dataset of Twitter users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4305/>Multilingual NMT with a Language-Independent Attention Bridge<span class=acl-fixed-case>NMT</span> with a Language-Independent Attention Bridge</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4305><div class="card-body p-3 small">In this paper, we propose an architecture for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> from each language for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and develops into a language-agnostic meaning representation that can efficiently be used for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We present a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4306/>Efficient <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> with Automatic Relevance Determination in Recurrent Neural Networks</a></strong><br><a href=/people/m/maxim-kodryan/>Maxim Kodryan</a>
|
<a href=/people/a/artem-grachev/>Artem Grachev</a>
|
<a href=/people/d/dmitry-ignatov/>Dmitry Ignatov</a>
|
<a href=/people/d/dmitry-vetrov/>Dmitry Vetrov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4306><div class="card-body p-3 small">Reduction of the number of parameters is one of the most important goals in <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a>. In this article we propose an adaptation of Doubly Stochastic Variational Inference for Automatic Relevance Determination (DSVI-ARD) for neural networks compression. We find this method to be especially useful in language modeling tasks, where large number of parameters in the input and output layers is often excessive. We also show that DSVI-ARD can be applied together with encoder-decoder weight tying allowing to achieve even better sparsity and performance. Our experiments demonstrate that more than 90 % of the weights in both encoder and decoder layers can be removed with a minimal quality loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4310/>Specializing Distributional Vectors of All Words for Lexical Entailment</a></strong><br><a href=/people/a/aishwarya-kamath/>Aishwarya Kamath</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4310><div class="card-body p-3 small">Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g. WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first post-processing method that specializes vectors of all vocabulary words including those unseen in the resources for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep feed-forward neural network</a> : its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4311/>Composing Noun Phrase Vector Representations</a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/v/valeria-de-paiva/>Valeria de Paiva</a>
|
<a href=/people/r/richard-crouch/>Richard Crouch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4311><div class="card-body p-3 small">Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks. While there seems to be a consensus about the usefulness of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and how to learn them, it is still unclear which <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> can capture the meaning of phrases or even whole sentences. Recent work has shown that simple operations outperform more complex <a href=https://en.wikipedia.org/wiki/Deep_learning>deep architectures</a>. In this work, we propose two novel <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> for computing noun phrase vector representations. First, we propose that the semantic and not the syntactic contribution of each component of a <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrase</a> should be considered, so that the resulting composed vectors express more of the phrase meaning. Second, the composition process of the two phrase vectors should apply suitable dimensions&#8217; selection in a way that specific semantic features captured by the phrase&#8217;s meaning become more salient. Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets. Our results show that these <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4312 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4312" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4312/>Towards Robust Named Entity Recognition for Historic German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/s/stefan-schweter/>Stefan Schweter</a>
|
<a href=/people/j/johannes-baiter/>Johannes Baiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4312><div class="card-body p-3 small">In this paper we study the influence of using language model pre-training for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Historic German. We achieve new state-of-the-art results using carefully chosen training data for <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. For a low-resource domain like <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer-learning</a> with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4313/>On Evaluating Embedding Models for Knowledge Base Completion</a></strong><br><a href=/people/y/yanjie-wang/>Yanjie Wang</a>
|
<a href=/people/d/daniel-ruffinelli/>Daniel Ruffinelli</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/s/samuel-broscheit/>Samuel Broscheit</a>
|
<a href=/people/c/christian-meilicke/>Christian Meilicke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4313><div class="card-body p-3 small">Knowledge graph embedding models have recently received significant attention in the literature. These models learn latent semantic representations for the entities and relations in a given <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> ; the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> can be used to infer missing knowledge. In this paper, we study the question of how well recent <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> perform for the task of knowledge base completion, i.e., the task of inferring new facts from an <a href=https://en.wikipedia.org/wiki/Complete_knowledge_base>incomplete knowledge base</a>. We argue that the entity ranking protocol, which is currently used to evaluate knowledge graph embedding models, is not suitable to answer this question since only a subset of the model predictions are evaluated. We propose an alternative entity-pair ranking protocol that considers all model predictions as a whole and is thus more suitable to the task. We conducted an experimental study on standard datasets and found that the performance of popular embeddings models was unsatisfactory under the new protocol, even on datasets that are generally considered to be too easy. Moreover, we found that a simple rule-based model often provided superior performance. Our findings suggest that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4314/>Constructive Type-Logical Supertagging With Self-Attention Networks</a></strong><br><a href=/people/k/konstantinos-kogkalidis/>Konstantinos Kogkalidis</a>
|
<a href=/people/m/michael-moortgat/>Michael Moortgat</a>
|
<a href=/people/t/tejaswini-deoskar/>Tejaswini Deoskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4314><div class="card-body p-3 small">We propose a novel application of self-attention networks towards <a href=https://en.wikipedia.org/wiki/Grammar_induction>grammar induction</a>. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of the grammar&#8217;s type system along with its <a href=https://en.wikipedia.org/wiki/Denotational_semantics>denotational semantics</a>. This lifts the <a href=https://en.wikipedia.org/wiki/Closed-world_assumption>closed world assumption</a> commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4317/>An Empirical Study on Pre-trained Embeddings and <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Bot Detection</a></strong><br><a href=/people/a/andres-garcia-silva/>Andres Garcia-Silva</a>
|
<a href=/people/c/cristian-berrio/>Cristian Berrio</a>
|
<a href=/people/j/jose-manuel-gomez-perez/>José Manuel Gómez-Pérez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4317><div class="card-body p-3 small">Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are learned from large and well-formed text corpora from e.g. encyclopedic resources, <a href=https://en.wikipedia.org/wiki/Book>books</a> or <a href=https://en.wikipedia.org/wiki/News>news</a>. However, a significant amount of the text to be analyzed nowadays is Web data, often from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper we consider the research question : How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>? To answer this question, we focus on bot detection in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4318/>Probing Multilingual Sentence Representations With X-Probe<span class=acl-fixed-case>X</span>-Probe</a></strong><br><a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4318><div class="card-body p-3 small">This paper extends the task of probing sentence representations for linguistic insight in a multilingual domain. In doing so, we make two contributions : first, we provide datasets for multilingual probing, derived from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, in five languages, viz. English, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Second, we evaluate six sentence encoders for each language, each trained by mapping sentence representations to English sentence representations, using sentences in a parallel corpus. We discover that cross-lingually mapped representations are often better at retaining certain linguistic information than <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> derived from English encoders trained on natural language inference (NLI) as a downstream task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4320/>Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4320><div class="card-body p-3 small">In this paper, we propose Multilingual Meta-Embeddings (MME), an effective method to learn multilingual representations by leveraging monolingual pre-trained embeddings. MME learns to utilize information from these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> via a self-attention mechanism without explicit language identification. We evaluate the proposed embedding method on the code-switching English-Spanish Named Entity Recognition dataset in a multilingual and cross-lingual setting. The experimental results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4321/>Investigating Sub-Word Embedding Strategies for the Morphologically Rich and Free Phrase-Order Hungarian<span class=acl-fixed-case>H</span>ungarian</a></strong><br><a href=/people/b/balint-dobrossy/>Bálint Döbrössy</a>
|
<a href=/people/m/marton-makrai/>Márton Makrai</a>
|
<a href=/people/b/balazs-tarjan/>Balázs Tarján</a>
|
<a href=/people/g/gyorgy-szaszak/>György Szaszák</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4321><div class="card-body p-3 small">For morphologically rich languages, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> provide less consistent semantic representations due to higher variance in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>word forms</a>. Moreover, these <a href=https://en.wikipedia.org/wiki/Language>languages</a> often allow for less constrained word order, which further increases variance. For the highly agglutinative Hungarian, semantic accuracy of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> measured on word analogy tasks drops by 50-75 % compared to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We observed that <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learn morphosyntax quite well instead. Therefore, we explore and evaluate several sub-word unit based embedding strategies character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor) to boost semantic consistency in Hungarian word vectors. The effect of changing <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding dimension</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>context window size</a> have also been considered. Morphological analysis based lemmatization was found to be the best strategy to improve embeddings&#8217; semantic accuracy, whereas adding character n-grams was found consistently counterproductive in this regard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4322/>A Self-Training Approach for Short Text Clustering</a></strong><br><a href=/people/a/amir-hadifar/>Amir Hadifar</a>
|
<a href=/people/l/lucas-sterckx/>Lucas Sterckx</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4322><div class="card-body p-3 small">Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations of the short texts. Low-dimensional continuous representations or <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> can counter that sparseness problem : their high representational power is exploited in deep clustering algorithms. While deep clustering has been studied extensively in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, relatively little work has focused on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. The method we propose, learns discriminative features from both an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> and a <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a>, then uses assignments from a <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering algorithm</a> as supervision to update weights of the encoder network. Experiments on three short text datasets empirically validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4323 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4323/>Improving <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>Word Embeddings</a> Using Kernel PCA<span class=acl-fixed-case>PCA</span></a></strong><br><a href=/people/v/vishwani-gupta/>Vishwani Gupta</a>
|
<a href=/people/s/sven-giesselbach/>Sven Giesselbach</a>
|
<a href=/people/s/stefan-ruping/>Stefan Rüping</a>
|
<a href=/people/c/christian-bauckhage/>Christian Bauckhage</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4323><div class="card-body p-3 small">Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections ; however, they fail to do so with small datasets. Extensions such as <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data. In this paper, we introduce a new approach to warm-start embedding models with morphological information, in order to reduce <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> and enhance their performance. We use word embeddings generated using both <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices. This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities. Evaluating our models on word similarity and analogy tasks in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time. Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies. Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4324" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4324/>Assessing Incrementality in Sequence-to-Sequence Models</a></strong><br><a href=/people/d/dennis-ulmer/>Dennis Ulmer</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/e/elia-bruni/>Elia Bruni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4324><div class="card-body p-3 small">Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. The most recent successes are predominantly due to the use of different variations of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language : continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to assess the behavior of RNNs with and without an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and identify key differences in the way the different model types process sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4325/>On Committee Representations of Adversarial Learning Models for Question-Answer Ranking</a></strong><br><a href=/people/s/sparsh-gupta/>Sparsh Gupta</a>
|
<a href=/people/v/vitor-carvalho/>Vitor Carvalho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4325><div class="card-body p-3 small">Adversarial training is a process in <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> that explicitly trains models on <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial inputs</a> (inputs designed to deceive or trick the learning process) in order to make it more robust or accurate. In this paper we investigate how representing adversarial training models as <a href=https://en.wikipedia.org/wiki/Committee>committees</a> can be used to effectively improve the performance of Question-Answer (QA) Ranking. We start by empirically probing the effects of adversarial training over multiple QA ranking algorithms, including the state-of-the-art Multihop Attention Network model. We evaluate these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> on several benchmark datasets and observe that, while adversarial training is beneficial to most baseline algorithms, there are cases where it may lead to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and performance degradation. We investigate the causes of such degradation, and then propose a new representation procedure for this adversarial learning problem, based on committee learning, that not only is capable of consistently improving all baseline algorithms, but also outperforms the previous state-of-the-art algorithm by as much as 6 % in NDCG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4327 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4327/>Best Practices for Learning Domain-Specific Cross-Lingual Embeddings</a></strong><br><a href=/people/l/lena-shakurova/>Lena Shakurova</a>
|
<a href=/people/b/beata-nyari/>Beata Nyari</a>
|
<a href=/people/c/chao-li/>Chao Li</a>
|
<a href=/people/m/mihai-rotaru/>Mihai Rotaru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4327><div class="card-body p-3 small">Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>linear projection</a> from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4329 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4329/>Learning Word Embeddings without Context Vectors</a></strong><br><a href=/people/a/alexey-zobnin/>Alexey Zobnin</a>
|
<a href=/people/e/evgenia-elistratova/>Evgenia Elistratova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4329><div class="card-body p-3 small">Most word embedding algorithms such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> or <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> construct two sort of vectors : for words and for contexts. Naive use of vectors of only one sort leads to poor results. We suggest using indefinite inner product in skip-gram negative sampling algorithm. This allows us to use only one sort of vectors without loss of quality. Our context-free cf algorithm performs on par with SGNS on word similarity datasets</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4331 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4331/>Modality-based Factorization for Multimodal Fusion</a></strong><br><a href=/people/e/elham-j-barezi/>Elham J. Barezi</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4331><div class="card-body p-3 small">We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (M+1)-way tensor to consider the high-order relationships between M modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> avoiding <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. We have applied this method to three different multimodal datasets in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, personality trait recognition, and <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>. We are able to recognize relationships and relative importance of different modalities in these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and achieves a 1 % to 4 % improvement on several evaluation measures compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for all three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.<i>M</i>+1)-way tensor to consider the high-order relationships between <i>M</i> modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks.</div></div></div><hr><div id=w19-44><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-44.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-44/>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4400/>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</a></strong><br><a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4401/>The many dimensions of <a href=https://en.wikipedia.org/wiki/Algorithmic_fairness>algorithmic fairness</a> in educational applications</a></strong><br><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/k/klaus-zechner/>Klaus Zechner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4401><div class="card-body p-3 small">The issues of <a href=https://en.wikipedia.org/wiki/Algorithmic_fairness>algorithmic fairness</a> and <a href=https://en.wikipedia.org/wiki/Bias>bias</a> have recently featured prominently in many publications highlighting the fact that training the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for maximum performance may often result in predictions that are biased against various groups. Educational applications based on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Speech_processing>speech processing technologies</a> often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> as other machine learning systems. Yet such <a href=https://en.wikipedia.org/wiki/System>systems</a> can have high impact on people&#8217;s lives especially when deployed as part of <a href=https://en.wikipedia.org/wiki/Test_(assessment)>high-stakes tests</a>. In this paper we discuss different definitions of <a href=https://en.wikipedia.org/wiki/Fair_division>fairness</a> and possible ways to apply them to <a href=https://en.wikipedia.org/wiki/Educational_technology>educational applications</a>. We then use simulated and real data to consider how test-takers&#8217; native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of <a href=https://en.wikipedia.org/wiki/Equity_(economics)>fairness</a> may require different solutions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4404/>Computationally Modeling the Impact of Task-Appropriate Language Complexity and Accuracy on Human Grading of German Essays<span class=acl-fixed-case>G</span>erman Essays</a></strong><br><a href=/people/z/zarah-weiss/>Zarah Weiss</a>
|
<a href=/people/a/anja-riemenschneider/>Anja Riemenschneider</a>
|
<a href=/people/p/pauline-schroter/>Pauline Schröter</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4404><div class="card-body p-3 small">Computational linguistic research on the <a href=https://en.wikipedia.org/wiki/Language_complexity>language complexity</a> of student writing typically involves human ratings as a gold standard. However, educational science shows that teachers find it difficult to identify and cleanly separate <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, different aspects of <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>, <a href=https://en.wikipedia.org/wiki/Content_(media)>contents</a>, and <a href=https://en.wikipedia.org/wiki/Structure>structure</a>. In this paper, we therefore explore the use of computational linguistic methods to investigate how task-appropriate complexity and accuracy relate to the grading of overall performance, content performance, and language performance as assigned by teachers. Based on texts written by students for the official school-leaving state examination (Abitur), we show that teachers successfully assign higher language performance grades to essays with higher task-appropriate language complexity and properly separate this from content scores. Yet, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> impacts teacher assessment for all grading rubrics, also the content score, overemphasizing the role of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our analysis is based on broad computational linguistic modeling of German language complexity and an innovative theory- and data-driven feature aggregation method inferring task-appropriate language complexity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4405/>Analysing Rhetorical Structure as a Key Feature of Summary Coherence</a></strong><br><a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/t/tamara-sladoljev-agejev/>Tamara Sladoljev-Agejev</a>
|
<a href=/people/s/svjetlana-kolic-vehovec/>Svjetlana Kolić Vehovec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4405><div class="card-body p-3 small">We present a model for automatic scoring of coherence based on comparing the rhetorical structure (RS) of college student summaries in L2 (English) against expert summaries. Coherence is conceptualised as a construct consisting of the rhetorical relation and its arguments. Comparison with expert-assigned scores shows that RS scores correlate with both <a href=https://en.wikipedia.org/wiki/Group_cohesiveness>cohesion</a> and <a href=https://en.wikipedia.org/wiki/Group_cohesiveness>coherence</a>. Furthermore, RS scores improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for cohesion score prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4406/>The BEA-2019 Shared Task on Grammatical Error Correction<span class=acl-fixed-case>BEA</span>-2019 Shared Task on Grammatical Error Correction</a></strong><br><a href=/people/c/christopher-bryant/>Christopher Bryant</a>
|
<a href=/people/m/mariano-felice/>Mariano Felice</a>
|
<a href=/people/o/oistein-e-andersen/>Øistein E. Andersen</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4406><div class="card-body p-3 small">This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write&Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of <a href=https://en.wikipedia.org/wiki/Track_(navigation)>tracks</a>, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F_0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4407" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4407/>A Benchmark Corpus of English Misspellings and a Minimally-supervised Model for Spelling Correction<span class=acl-fixed-case>E</span>nglish Misspellings and a Minimally-supervised Model for Spelling Correction</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/m/michael-fried/>Michael Fried</a>
|
<a href=/people/a/alla-rozovskaya/>Alla Rozovskaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4407><div class="card-body p-3 small">Spelling correction has attracted a lot of attention in the NLP community. However, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English language learners</a>. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our <a href=https://en.wikipedia.org/wiki/Data>data</a> : 88.12 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1 %). Furthermore, this approach allows easy portability to <a href=https://en.wikipedia.org/wiki/Domain_name>new domains</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on data from a medical domain and demonstrate that it rivals the performance of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained and tuned on in-domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4409/>Regression or classification? Automated Essay Scoring for Norwegian<span class=acl-fixed-case>N</span>orwegian</a></strong><br><a href=/people/s/stig-johan-berggren/>Stig Johan Berggren</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4409><div class="card-body p-3 small">In this paper we present first results for the task of <a href=https://en.wikipedia.org/wiki/Automated_essay_scoring>Automated Essay Scoring</a> for <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian learner language</a>. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> or <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for joint prediction of essay scoring and native language identification. We find that a GRU-based attention model trained in a single-task setting performs best at the AES task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4411/>How to account for mispellings : Quantifying the benefit of character representations in neural content scoring models</a></strong><br><a href=/people/b/brian-riordan/>Brian Riordan</a>
|
<a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/r/robert-pugh/>Robert Pugh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4411><div class="card-body p-3 small">Character-based representations in neural models have been claimed to be a tool to overcome spelling variation in in word token-based input. We examine this claim in neural models for content scoring. We formulate precise hypotheses about the possible effects of adding <a href=https://en.wikipedia.org/wiki/Character_(computing)>character representations</a> to word-based models and test these hypotheses on large-scale real world content scoring datasets. We find that, while character representations may provide small performance gains in general, their effectiveness in accounting for spelling variation may be limited. We show that spelling correction can provide larger gains than character representations, and that spelling correction improves the performance of models with character representations. With these insights, we report a new state of the art on the ASAP-SAS content scoring dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4412" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4412/>The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction</a></strong><br><a href=/people/d/dimitris-alikaniotis/>Dimitris Alikaniotis</a>
|
<a href=/people/v/vipul-raheja/>Vipul Raheja</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4412><div class="card-body p-3 small">Recent work on Grammatical Error Correction (GEC) has highlighted the importance of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4415/>Erroneous data generation for Grammatical Error Correction</a></strong><br><a href=/people/s/shuyao-xu/>Shuyao Xu</a>
|
<a href=/people/j/jiehao-zhang/>Jiehao Zhang</a>
|
<a href=/people/j/jin-chen/>Jin Chen</a>
|
<a href=/people/l/long-qin/>Long Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4415><div class="card-body p-3 small">It has been demonstrated that the utilization of a monolingual corpus in neural Grammatical Error Correction (GEC) systems can significantly improve the system performance. The previous state-of-the-art neural GEC system is an ensemble of four Transformer models pretrained on a large amount of Wikipedia Edits. The Singsound GEC system follows a similar approach but is equipped with a sophisticated erroneous data generating component. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved an <a href=https://en.wikipedia.org/wiki/Formal_grammar>F0:5</a> of 66.61 in the BEA 2019 Shared Task : Grammatical Error Correction. With our novel erroneous data generating component, the Singsound neural GEC system yielded an M2 of 63.2 on the CoNLL-2014 benchmark (8.4 % relative improvement over the previous state-of-the-art system).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4416/>The LAIX Systems in the BEA-2019 GEC Shared Task<span class=acl-fixed-case>LAIX</span> Systems in the <span class=acl-fixed-case>BEA</span>-2019 <span class=acl-fixed-case>GEC</span> Shared Task</a></strong><br><a href=/people/r/ruobing-li/>Ruobing Li</a>
|
<a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/y/yefei-zha/>Yefei Zha</a>
|
<a href=/people/y/yonghong-yu/>Yonghong Yu</a>
|
<a href=/people/s/shiman-guo/>Shiman Guo</a>
|
<a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/h/hui-lin/>Hui Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4416><div class="card-body p-3 small">In this paper, we describe two <a href=https://en.wikipedia.org/wiki/System>systems</a> we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble systems</a> to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4421/>The BLCU System in the BEA 2019 Shared Task<span class=acl-fixed-case>BLCU</span> System in the <span class=acl-fixed-case>BEA</span> 2019 Shared Task</a></strong><br><a href=/people/l/liner-yang/>Liner Yang</a>
|
<a href=/people/c/chencheng-wang/>Chencheng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4421><div class="card-body p-3 small">This paper describes the BLCU Group submissions to the Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC). The task is to detect and correct <a href=https://en.wikipedia.org/wiki/Error_(linguistics)>grammatical errors</a> that occurred in essays. We participate in 2 tracks including the Restricted Track and the Unrestricted Track. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on a Transformer model architecture. We integrate many effective <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> proposed in recent years. Such as, Byte Pair Encoding, model ensemble, checkpoints average and <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checker</a>. We also corrupt the public monolingual data to further improve the performance of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. On the test data of the BEA 2019 Shared Task, our <a href=https://en.wikipedia.org/wiki/System>system</a> yields F0.5 = 58.62 and 59.50, ranking twelfth and fourth respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4424/>Neural and FST-based approaches to grammatical error correction<span class=acl-fixed-case>FST</span>-based approaches to grammatical error correction</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4424><div class="card-body p-3 small">In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>system pipeline</a> that utilises both <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection and correction models</a>. The input text is first corrected by two complementary neural machine translation systems : one using convolutional networks and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, and another using a neural Transformer-based system. Training is performed on publicly available data, along with artificial examples generated through <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>. The n-best lists of these two <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> are then combined and scored using a <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>finite state transducer (FST)</a>. Finally, an unsupervised re-ranking system is applied to the n-best output of the <a href=https://en.wikipedia.org/wiki/Finite-state_machine>FST</a>. The re-ranker uses a number of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection features</a> to re-rank the FST n-best list and identify the final 1-best correction hypothesis. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 66.75 % F 0.5 on error correction (ranking 4th), and 82.52 % F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4425/>Improving Precision of Grammatical Error Correction with a Cheat Sheet</a></strong><br><a href=/people/m/mengyang-qiu/>Mengyang Qiu</a>
|
<a href=/people/x/xuejiao-chen/>Xuejiao Chen</a>
|
<a href=/people/m/maggie-liu/>Maggie Liu</a>
|
<a href=/people/k/krishna-parvathala/>Krishna Parvathala</a>
|
<a href=/people/a/apurva-patil/>Apurva Patil</a>
|
<a href=/people/j/jungyeul-park/>Jungyeul Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4425><div class="card-body p-3 small">In this paper, we explore two approaches of generating error-focused phrases and examine whether these <a href=https://en.wikipedia.org/wiki/Phrase>phrases</a> can lead to better performance in <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>grammatical error correction</a> for the restricted track of BEA 2019 Shared Task on GEC. Our results show that phrases directly extracted from GEC corpora outperform phrases from statistical machine translation phrase table by a large margin. Appending error+context phrases to the original GEC corpora yields comparably high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. We also explore the generation of artificial syntactic error sentences using error+context phrases for the unrestricted track. The additional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> greatly facilitates syntactic error correction (e.g., verb form) and contributes to better overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4428 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4428" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4428/>Evaluation of automatic collocation extraction methods for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a></a></strong><br><a href=/people/v/vishal-bhalla/>Vishal Bhalla</a>
|
<a href=/people/k/klara-klimcikova/>Klara Klimcikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4428><div class="card-body p-3 small">A number of methods have been proposed to automatically extract collocations, i.e., conventionalized lexical combinations, from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>. However, the attempts to evaluate and compare <a href=https://en.wikipedia.org/wiki/Them_(band)>them</a> with a specific application in mind lag behind. This paper compares three end-to-end resources for collocation learning, all of which used the same corpus but different methods. Adopting a gold-standard evaluation method, the results show that the method of dependency parsing outperforms regex-over-pos in collocation identification. The lexical association measures (AMs) used for collocation ranking perform about the same overall but differently for individual collocation types. Further analysis has also revealed that there are considerable differences between other commonly used <a href=https://en.wikipedia.org/wiki/Amplitude_modulation>AMs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4429/>Anglicized Words and Misspelled Cognates in <a href=https://en.wikipedia.org/wiki/Native_Language_Identification>Native Language Identification</a></a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4429><div class="card-body p-3 small">In this paper, we present experiments that estimate the impact of specific lexical choices of people writing in a second language (L2). In particular, we look at misspelled words that indicate lexical uncertainty on the part of the author, and separate them into three categories : misspelled cognates, L2-ed (in our case, anglicized) words, and all other spelling errors. We test the assumption that such <a href=https://en.wikipedia.org/wiki/Error_(linguistics)>errors</a> contain clues about the native language of an essay&#8217;s author through the task of native language identification. The results of the experiments show that the information brought by each of these <a href=https://en.wikipedia.org/wiki/Category_(mathematics)>categories</a> is complementary. We also note that while the distribution of such <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> changes with the proficiency level of the writer, their contribution towards native language identification remains significant at all levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4430 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4430/>Linguistically-Driven Strategy for Concept Prerequisites Learning on Italian<span class=acl-fixed-case>I</span>talian</a></strong><br><a href=/people/a/alessio-miaschi/>Alessio Miaschi</a>
|
<a href=/people/c/chiara-alzetta/>Chiara Alzetta</a>
|
<a href=/people/f/franco-alberto-cardillo/>Franco Alberto Cardillo</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4430><div class="card-body p-3 small">We present a new concept prerequisite learning method for Learning Object (LO) ordering that exploits only linguistic features extracted from textual educational resources. The method was tested in a cross- and in- domain scenario both for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Additionally, we performed experiments based on a incremental training strategy to study the impact of the training set size on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performances. The paper also introduces ITA-PREREQ, to the best of our knowledge the first Italian dataset annotated with prerequisite relations between pairs of educational concepts, and describe the automatic strategy devised to build it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4431/>Grammatical-Error-Aware Incorrect Example Retrieval System for Learners of <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> as a Second Language<span class=acl-fixed-case>J</span>apanese as a Second Language</a></strong><br><a href=/people/m/mio-arai/>Mio Arai</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4431><div class="card-body p-3 small">Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. Even if a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners need to know whether their query includes errors or not. Considering the usability of retrieving incorrect examples, our proposed method uses a large-scale corpus and presents correct expressions along with incorrect expressions using a grammatical error detection system so that the learner do not need to be aware of how to search for the examples. Intrinsic and extrinsic evaluations indicate that our method improves accuracy of example sentence retrieval and quality of learner&#8217;s writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4432/>Toward Automated Content Feedback Generation for Non-native Spontaneous Speech</a></strong><br><a href=/people/s/su-youn-yoon/>Su-Youn Yoon</a>
|
<a href=/people/c/ching-ni-hsieh/>Ching-Ni Hsieh</a>
|
<a href=/people/k/klaus-zechner/>Klaus Zechner</a>
|
<a href=/people/m/matthew-mulholland/>Matthew Mulholland</a>
|
<a href=/people/y/yuan-wang/>Yuan Wang</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4432><div class="card-body p-3 small">In this study, we developed an <a href=https://en.wikipedia.org/wiki/Algorithm>automated algorithm</a> to provide feedback about the specific content of non-native English speakers&#8217; spoken responses. The responses were spontaneous speech, elicited using integrated tasks where the language learners listened to and/or read passages and integrated the core content in their spoken responses. Our models detected the absence of key points considered to be important in a spoken response to a particular test question, based on two different models : (a) a model using word-embedding based content features and (b) a state-of-the art short response scoring engine using traditional n-gram based features. Both <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> achieved a substantially improved performance over the majority baseline, and the combination of the two <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> achieved a significant further improvement. In particular, the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> were robust to automated speech recognition (ASR) errors, and performance based on the ASR word hypotheses was comparable to that based on manual transcriptions. The <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of the best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the questions included in the train set were 0.80 and 0.68, respectively. Finally, we discussed possible approaches to generating targeted feedback about the content of a language learner&#8217;s response, based on automatically detected missing key points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4435 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4435/>Curio SmartChat : A system for Natural Language Question Answering for Self-Paced K-12 Learning<span class=acl-fixed-case>S</span>mart<span class=acl-fixed-case>C</span>hat : A system for Natural Language Question Answering for Self-Paced K-12 Learning</a></strong><br><a href=/people/s/srikrishna-raamadhurai/>Srikrishna Raamadhurai</a>
|
<a href=/people/r/ryan-baker/>Ryan Baker</a>
|
<a href=/people/v/vikraman-poduval/>Vikraman Poduval</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4435><div class="card-body p-3 small">During learning, students often have questions which they would benefit from responses to in real time. In class, a student can ask a question to a teacher. During homework, or even in class if the student is shy, it can be more difficult to receive a rapid response. In this work, we introduce Curio SmartChat, an automated question answering system for middle school Science topics. Our <a href=https://en.wikipedia.org/wiki/System>system</a> has now been used by around 20,000 students who have so far asked over 100,000 questions. We present data on the challenge created by students&#8217; <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> and spelling mistakes, and discuss our <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s approach and degree of effectiveness at disambiguating questions that the <a href=https://en.wikipedia.org/wiki/System>system</a> is initially unsure about. We also discuss the prevalence of student small talk not related to science topics, the pluses and minuses of this behavior, and how a <a href=https://en.wikipedia.org/wiki/System>system</a> should respond to these conversational acts. We conclude with discussions and point to directions for potential future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4436/>Supporting content evaluation of student summaries by Idea Unit embedding</a></strong><br><a href=/people/m/marcello-gecchele/>Marcello Gecchele</a>
|
<a href=/people/h/hiroaki-yamada/>Hiroaki Yamada</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a>
|
<a href=/people/y/yasuyo-sawaki/>Yasuyo Sawaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4436><div class="card-body p-3 small">This paper discusses the computer-assisted content evaluation of summaries. We propose a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> to make a correspondence between the segments of the source text and its summary. As a unit of the segment, we adopt Idea Unit (IU) which is proposed in <a href=https://en.wikipedia.org/wiki/Applied_linguistics>Applied Linguistics</a>. Introducing IUs enables us to make a correspondence even for the sentences that contain multiple ideas. The IU correspondence is made based on the similarity between vector representations of IU. An evaluation experiment with two source texts and 20 summaries showed that the proposed method is more robust against rephrased expressions than the conventional ROUGE-based baselines. Also, the proposed method outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in recall. We im-plemented the proposed method in a GUI toolSegment Matcher that aids teachers to estab-lish a link between corresponding IUs acrossthe summary and source text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4437" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4437/>On Understanding the Relation between Expert Annotations of Text Readability and Target Reader Comprehension</a></strong><br><a href=/people/s/sowmya-vajjala/>Sowmya Vajjala</a>
|
<a href=/people/i/ivana-lucic/>Ivana Lucic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4437><div class="card-body p-3 small">Automatic readability assessment aims to ensure that readers read texts that they can comprehend. However, <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> are typically trained on texts created from the perspective of the text writer, not the target reader. There is little experimental research on the relationship between expert annotations of readability, reader&#8217;s language proficiency, and different levels of <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. To address this gap, we conducted a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> in which over a 100 participants read texts of different reading levels and answered questions created to test three forms of <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a>. Our results indicate that more than readability annotation or reader proficiency, it is the type of comprehension question asked that shows differences between reader responses-inferential questions were difficult for users of all levels of proficiency across reading levels. The data collected from this study will be released with this paper, which will, for the first time, provide a collection of 45 reader bench marked texts to evaluate readability assessment systems developed for <a href=https://en.wikipedia.org/wiki/Adult_learner>adult learners</a> of <a href=https://en.wikipedia.org/wiki/English_language>English</a>. It can also potentially be useful for the development of question generation approaches in <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems research</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4440 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4440/>Analyzing Linguistic Complexity and Accuracy in Academic Language Development of <a href=https://en.wikipedia.org/wiki/German_language>German</a> across Elementary and Secondary School<span class=acl-fixed-case>G</span>erman across Elementary and Secondary School</a></strong><br><a href=/people/z/zarah-weiss/>Zarah Weiss</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4440><div class="card-body p-3 small">We track the development of writing complexity and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in German students&#8217; early academic language development from first to eighth grade. Combining an empirically broad approach to linguistic complexity with the high-quality error annotation included in the Karlsruhe Children&#8217;s Text corpus (Lavalley et al. 2015) used, we construct models of German academic language development that successfully identify the student&#8217;s grade level. We show that <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> for the early years rely more on accuracy development, whereas development in <a href=https://en.wikipedia.org/wiki/Secondary_school>secondary school</a> is better characterized by increasingly complex language in all domains : <a href=https://en.wikipedia.org/wiki/Linguistic_system>linguistic system</a>, language use, and human sentence processing characteristics. We demonstrate the generalizability and robustness of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using such a broad complexity feature set across writing topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4442/>Learning Outcomes and Their Relatedness in a Medical Curriculum</a></strong><br><a href=/people/s/sneha-mondal/>Sneha Mondal</a>
|
<a href=/people/t/tejas-dhamecha/>Tejas Dhamecha</a>
|
<a href=/people/s/shantanu-godbole/>Shantanu Godbole</a>
|
<a href=/people/s/smriti-pathak/>Smriti Pathak</a>
|
<a href=/people/r/red-mendoza/>Red Mendoza</a>
|
<a href=/people/k/k-gayathri-wijayarathna/>K Gayathri Wijayarathna</a>
|
<a href=/people/n/nabil-zary/>Nabil Zary</a>
|
<a href=/people/s/swarnadeep-saha/>Swarnadeep Saha</a>
|
<a href=/people/m/malolan-chetlur/>Malolan Chetlur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4442><div class="card-body p-3 small">A typical medical curriculum is organized in a hierarchy of instructional objectives called Learning Outcomes (LOs) ; a few thousand LOs span five years of study. Gaining a thorough understanding of the <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a> requires learners to recognize and apply related LOs across years, and across different parts of the curriculum. However, given the large scope of the curriculum, manually labeling related LOs is tedious, and almost impossible to scale. In this paper, we build a <a href=https://en.wikipedia.org/wiki/System>system</a> that learns relationships between LOs, and we achieve up to human-level performance in the LO relationship extraction task. We then present an application where the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is employed to build a map of related LOs and Learning Resources (LRs) pertaining to a virtual patient case. We believe that our system can help medical students grasp the curriculum better, within classroom as well as in Intelligent Tutoring Systems (ITS) settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4446 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4446/>Equity Beyond Bias in Language Technologies for Education</a></strong><br><a href=/people/e/elijah-mayfield/>Elijah Mayfield</a>
|
<a href=/people/m/michael-madaio/>Michael Madaio</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/d/david-gerritsen/>David Gerritsen</a>
|
<a href=/people/b/brittany-mclaughlin/>Brittany McLaughlin</a>
|
<a href=/people/e/ezekiel-dixon-roman/>Ezekiel Dixon-Román</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4446><div class="card-body p-3 small">There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We present case studies in a range of topics like <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a>, <a href=https://en.wikipedia.org/wiki/Computer-assisted_language_learning>computer-assisted language learning</a>, automated essay scoring, and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in classrooms, and provide an actionable agenda for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4447/>From Receptive to Productive : Learning to Use Confusing Words through Automatically Selected Example Sentences</a></strong><br><a href=/people/c/chieh-yang-huang/>Chieh-Yang Huang</a>
|
<a href=/people/y/yi-ting-huang/>Yi-Ting Huang</a>
|
<a href=/people/m/meihua-chen/>MeiHua Chen</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4447><div class="card-body p-3 small">Knowing how to use words appropriately has been a key to improving language proficiency. Previous studies typically discuss how students learn receptively to select the correct candidate from a set of confusing words in the fill-in-the-blank task where specific context is given. In this paper, we go one step further, assisting students to learn to use confusing words appropriately in a productive task : sentence translation. We leverage the GiveMe-Example system, which suggests example sentences for each confusing word, to achieve this goal. In this study, students learn to differentiate the confusing words by reading the example sentences, and then choose the appropriate word(s) to complete the sentence translation task. Results show students made substantial progress in terms of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a>. In addition, highly proficient students better managed to learn confusing words. In view of the influence of the first language on learners, we further propose an effective approach to improve the quality of the suggested sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4450 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4450/>Automated Essay Scoring with Discourse-Aware Neural Models</a></strong><br><a href=/people/f/farah-nadeem/>Farah Nadeem</a>
|
<a href=/people/h/huy-nguyen/>Huy Nguyen</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4450><div class="card-body p-3 small">Automated essay scoring systems typically rely on hand-crafted features to predict essay quality, but such systems are limited by the cost of <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. Neural networks offer an alternative to <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>, but they typically require more annotated data. This paper explores network structures, contextualized embeddings and pre-training strategies aimed at capturing discourse characteristics of essays. Experiments on three essay scoring tasks show benefits from all three <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> in different combinations, with simpler <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> being more effective when less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4452.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4452 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4452 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4452" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4452/>Rubric Reliability and Annotation of Content and Argument in Source-Based Argument Essays</a></strong><br><a href=/people/y/yanjun-gao/>Yanjun Gao</a>
|
<a href=/people/a/alex-driban/>Alex Driban</a>
|
<a href=/people/b/brennan-xavier-mcmanus/>Brennan Xavier McManus</a>
|
<a href=/people/e/elena-musi/>Elena Musi</a>
|
<a href=/people/p/patricia-davies/>Patricia Davies</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca J. Passonneau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4452><div class="card-body p-3 small">We present a unique dataset of student source-based argument essays to facilitate research on the relations between <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>, <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation skills</a>, and <a href=https://en.wikipedia.org/wiki/Educational_assessment>assessment</a>. Two classroom writing assignments were given to college students in a STEM major, accompanied by a carefully designed rubric. The paper presents a reliability study of the <a href=https://en.wikipedia.org/wiki/Rubric>rubric</a>, showing it to be highly reliable, and initial annotation on content and argumentation annotation of the essays.</div></div></div><hr><div id=w19-45><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-45.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-45/>Proceedings of the 6th Workshop on Argument Mining</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4500/>Proceedings of the 6th Workshop on Argument Mining</a></strong><br><a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4502/>A Cascade Model for Proposition Extraction in Argumentation</a></strong><br><a href=/people/y/yohan-jo/>Yohan Jo</a>
|
<a href=/people/j/jacky-visser/>Jacky Visser</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4502><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to tackle a fundamental but understudied problem in computational argumentation : proposition extraction. Propositions are the basic units of an argument and the primary building blocks of most <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining systems</a>. However, they are usually substituted by argumentative discourse units obtained via surface-level text segmentation, which may yield text segments that lack semantic information necessary for subsequent argument mining processes. In contrast, our cascade model aims to extract complete propositions by handling <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a>, <a href=https://en.wikipedia.org/wiki/Text_segmentation>text segmentation</a>, reported speech, questions, <a href=https://en.wikipedia.org/wiki/Imperative_mood>imperatives</a>, missing subjects, and revision. We formulate each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as a <a href=https://en.wikipedia.org/wiki/Computational_problem>computational problem</a> and test various <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using a corpus of the 2016 <a href=https://en.wikipedia.org/wiki/United_States_presidential_debates>U.S. presidential debates</a>. We show promising performance for some <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and discuss main challenges in proposition extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4503/>Dissecting Content and Context in Argumentative Relation Analysis</a></strong><br><a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4503><div class="card-body p-3 small">When assessing relations between argumentative units (e.g., support or attack), computational systems often exploit disclosing indicators or markers that are not part of elementary argumentative units (EAUs) themselves, but are gained from their context (position in paragraph, preceding tokens, etc.). We show that this dependency is much stronger than previously assumed. In fact, we show that by completely masking the EAU text spans and only feeding information from their context, a competitive system may function even better. We argue that an argument analysis system that relies more on discourse context than the argument&#8217;s content is unsafe, since it can easily be tricked. To alleviate this issue, we separate argumentative units from their context such that the <a href=https://en.wikipedia.org/wiki/System>system</a> is forced to model and rely on an EAU&#8217;s content. We show that the resulting classification system is more robust, and argue that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are better suited for predicting argumentative relations across documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4506/>The Swedish PoliGraph : A Semantic Graph for Argument Mining of Swedish Parliamentary Data<span class=acl-fixed-case>S</span>wedish <span class=acl-fixed-case>P</span>oli<span class=acl-fixed-case>G</span>raph: A Semantic Graph for Argument Mining of <span class=acl-fixed-case>S</span>wedish Parliamentary Data</a></strong><br><a href=/people/s/stian-rodven-eide/>Stian Rødven Eide</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4506><div class="card-body p-3 small">As part of a larger project on <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> of Swedish parliamentary data, we have created a semantic graph that, together with named entity recognition and resolution (NER), should make it easier to establish connections between arguments in a given debate. The <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a> is essentially a semantic database that keeps track of Members of Parliament (MPs), in particular their presence in the parliament and activity in debates, but also party affiliation and participation in commissions. The hope is that the Swedish PoliGraph will enable us to perform named entity resolution on debates in the <a href=https://en.wikipedia.org/wiki/Riksdag>Swedish parliament</a> with a high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, with the aim of determining to whom an argument is directed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4507/>Towards Effective Rebuttal : <a href=https://en.wikipedia.org/wiki/Listening_comprehension>Listening Comprehension</a> Using Corpus-Wide Claim Mining</a></strong><br><a href=/people/t/tamar-lavee/>Tamar Lavee</a>
|
<a href=/people/m/matan-orbach/>Matan Orbach</a>
|
<a href=/people/l/lili-kotlerman/>Lili Kotlerman</a>
|
<a href=/people/y/yoav-kantor/>Yoav Kantor</a>
|
<a href=/people/s/shai-gretz/>Shai Gretz</a>
|
<a href=/people/l/lena-dankin/>Lena Dankin</a>
|
<a href=/people/m/michal-jacovi/>Michal Jacovi</a>
|
<a href=/people/y/yonatan-bilu/>Yonatan Bilu</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4507><div class="card-body p-3 small">Engaging in a live debate requires, among other things, the ability to effectively rebut arguments claimed by your opponent. In particular, this requires identifying these arguments. Here, we suggest doing so by automatically mining claims from a corpus of news articles containing billions of sentences, and searching for them in a given speech. This raises the question of whether such claims indeed correspond to those made in spoken speeches. To this end, we collected a large dataset of 400 speeches in English discussing 200 controversial topics, mined claims for each topic, and asked annotators to identify the mined claims mentioned in each speech. Results show that in the vast majority of speeches debaters indeed make use of such claims. In addition, we present several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for the automatic detection of mined claims in <a href=https://en.wikipedia.org/wiki/Public_speaking>speeches</a>, forming the basis for future work. All collected data is freely available for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4509/>Is It Worth the Attention? A Comparative Evaluation of Attention Layers for Argument Unit Segmentation</a></strong><br><a href=/people/m/maximilian-spliethover/>Maximilian Spliethöver</a>
|
<a href=/people/j/jonas-klaff/>Jonas Klaff</a>
|
<a href=/people/h/hendrik-heuer/>Hendrik Heuer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4509><div class="card-body p-3 small">Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4512/>The Utility of Discourse Parsing Features for Predicting Argumentation Structure</a></strong><br><a href=/people/f/freya-hewett/>Freya Hewett</a>
|
<a href=/people/r/roshan-prakash-rane/>Roshan Prakash Rane</a>
|
<a href=/people/n/nina-harlacher/>Nina Harlacher</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4512><div class="card-body p-3 small">Research on argumentation mining from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> has frequently discussed relationships to discourse parsing, but few empirical results are available so far. One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the &#8216;argumentative microtexts&#8217; (Peldszus and Stede, 2016a). While results on perusing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the <a href=https://en.wikipedia.org/wiki/Microtext>microtexts</a>, we find that PDTB features can indeed improve its performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4516/>Categorizing Comparative Sentences</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/a/alexander-bondarenko/>Alexander Bondarenko</a>
|
<a href=/people/m/mirco-franzek/>Mirco Franzek</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4516><div class="card-body p-3 small">We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> has better NLP libraries than <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB Python</a>, better, <a href=https://en.wikipedia.org/wiki/MATLAB>MATLAB</a>). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27 % of the sentences contain an oriented comparison in the sense of better or worse). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85 % in our experimental evaluation. The model can be used to extract comparative sentences for pro / con argumentation in comparative / argument search engines or debating technologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4517 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4517/>Ranking Passages for Argument Convincingness</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/adam-ferguson/>Adam Ferguson</a>
|
<a href=/people/t/timothy-j-hazen/>Timothy J. Hazen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4517><div class="card-body p-3 small">In data ranking applications, pairwise annotation is often more consistent than cardinal annotation for learning ranking models. We examine this in a case study on ranking text passages for argument convincingness. Our task is to choose text passages that provide the highest-quality, most-convincing arguments for opposing sides of a topic. Using data from a deployed system within the <a href=https://en.wikipedia.org/wiki/Bing_(search_engine)>Bing search engine</a>, we construct a pairwise-labeled dataset for argument convincingness that is substantially more comprehensive in topical coverage compared to existing public resources. We detail the process of extracting topical passages for queries submitted to a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a>, creating annotated sets of passages aligned to different stances on a topic, and assessing argument convincingness of passages using pairwise annotation. Using a state-of-the-art convincingness model, we evaluate several methods for using pairwise-annotated data examples to train models for ranking passages. Our results show pairwise training outperforms training that regresses to a target score for each passage. Our results also show a simple &#8216;win-rate&#8217; score is a better regression target than the previously proposed page-rank target. Lastly, addressing the need to filter noisy crowd-sourced annotations when constructing a dataset, we show that filtering for transitivity within pairwise annotations is more effective than filtering based on annotation confidence measures for individual examples.</div></div></div><hr><div id=w19-46><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-46.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-46/>Proceedings of the Fourth Arabic Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4600/>Proceedings of the Fourth Arabic Natural Language Processing Workshop</a></strong><br><a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich Belguith</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a>
|
<a href=/people/i/imed-zitouni/>Imed Zitouni</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4601" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4601/>Incremental Domain Adaptation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> in Low-Resource Settings</a></strong><br><a href=/people/m/marimuthu-kalimuthu/>Marimuthu Kalimuthu</a>
|
<a href=/people/m/michael-barz/>Michael Barz</a>
|
<a href=/people/d/daniel-sonntag/>Daniel Sonntag</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4601><div class="card-body p-3 small">We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>. In this paper, we propose a novel query strategy for selecting unlabeled samples from a new domain based on <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. We accelerate the <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning process</a> of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50 % of the annotation costs without loss in <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>, demonstrating the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4603/>POS Tagging for Improving Code-Switching Identification in Arabic<span class=acl-fixed-case>POS</span> Tagging for Improving Code-Switching Identification in <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/a/ali-elkahky/>Ali Elkahky</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4603><div class="card-body p-3 small">When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language. This paper explores the possibility of utilizing this pattern in improving code-switching identification between <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>Modern Standard Arabic (MSA)</a> and <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian Arabic (EA)</a>. We try to answer the question of how strong is the POS signal in word-level code-switching identification. We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9 % on the development set and 1.0 % on the test set. We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the <a href=https://en.wikipedia.org/wiki/Dialect>dialectal language</a> while the majority of content words come from the <a href=https://en.wikipedia.org/wiki/Standard_language>standard language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4604/>Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects<span class=acl-fixed-case>A</span>rabic Dialects</a></strong><br><a href=/people/h/hala-mulki/>Hala Mulki</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/m/mourad-gridach/>Mourad Gridach</a>
|
<a href=/people/i/ismail-babaoglu/>Ismail Babaoğlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4604><div class="card-body p-3 small">Arabic sentiment analysis models have employed compositional embedding features to represent the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialectal content</a>. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural frameworks</a>. With the <a href=https://en.wikipedia.org/wiki/Free_word_order>free word order</a> and the varying syntax nature across the different <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a>, a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis system</a> developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of several <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a>. The proposed <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4610/>Constrained Sequence-to-sequence Semitic Root Extraction for Enriching Word Embeddings<span class=acl-fixed-case>S</span>emitic Root Extraction for Enriching Word Embeddings</a></strong><br><a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/x/xingyu-fu/>Xingyu Fu</a>
|
<a href=/people/a/aseel-addawood/>Aseel Addawood</a>
|
<a href=/people/n/nahil-sobh/>Nahil Sobh</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4610><div class="card-body p-3 small">In this paper, we tackle the problem of <a href=https://en.wikipedia.org/wiki/Root_(linguistics)>root extraction</a> from words in the <a href=https://en.wikipedia.org/wiki/Semitic_languages>Semitic language family</a>. A challenge in applying natural language processing techniques to these languages is the data sparsity problem that arises from their rich internal morphology, where the substructure is inherently non-concatenative and morphemes are interdigitated in <a href=https://en.wikipedia.org/wiki/Word_formation>word formation</a>. While previous automated methods have relied on human-curated rules or multiclass classification, they have not fully leveraged the various combinations of regular, sequential concatenative morphology within the words and the internal interleaving within templatic stems of roots and patterns. To address this, we propose a constrained sequence-to-sequence root extraction method. Experimental results show our constrained model outperforms a variety of methods at <a href=https://en.wikipedia.org/wiki/Root_extraction>root extraction</a>. Furthermore, by enriching word embeddings with resulting decompositions, we show improved results on word analogy, word similarity, and language modeling tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4611/>En-Ar Bilingual Word Embeddings without <a href=https://en.wikipedia.org/wiki/Word_alignment>Word Alignment</a> : Factors Effects</a></strong><br><a href=/people/t/taghreed-alqaisi/>Taghreed Alqaisi</a>
|
<a href=/people/s/simon-okeefe/>Simon O’Keefe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4611><div class="card-body p-3 small">This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA). We investigate the effect of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> and embedding size on the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a>. Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the ATB and D0 schemes in all different training settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4614.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4614 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4614/>Assessing Arabic Weblog Credibility via Deep Co-learning<span class=acl-fixed-case>A</span>rabic Weblog Credibility via Deep Co-learning</a></strong><br><a href=/people/c/chadi-helwe/>Chadi Helwe</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a>
|
<a href=/people/a/ayman-al-zaatari/>Ayman Al Zaatari</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4614><div class="card-body p-3 small">Assessing the credibility of <a href=https://en.wikipedia.org/wiki/Online_content>online content</a> has garnered a lot of attention lately. We focus on one such type of <a href=https://en.wikipedia.org/wiki/Online_content>online content</a>, namely <a href=https://en.wikipedia.org/wiki/Blog>weblogs</a> or <a href=https://en.wikipedia.org/wiki/Blog>blogs</a> for short. Some recent work attempted the task of automatically assessing the credibility of <a href=https://en.wikipedia.org/wiki/Blog>blogs</a>, typically via <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. However, in the case of Arabic blogs, there are hardly any datasets available that can be used to train robust <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> for this difficult task. To overcome the lack of sufficient training data, we propose deep co-learning, a semi-supervised end-to-end deep learning approach to assess the credibility of Arabic blogs. In deep co-learning, multiple weak deep neural network classifiers are trained using a small labeled dataset, and each using a different view of the data. Each one of these <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> is then used to classify unlabeled data, and its prediction is used to train the other <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> in a semi-supervised fashion. We evaluate our deep co-learning approach on an Arabic blogs dataset, and we report significant improvements in performance compared to many baselines including fully-supervised deep learning models as well as ensemble models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4616/>Construction and Annotation of the Jordan Comprehensive Contemporary Arabic Corpus (JCCA)<span class=acl-fixed-case>J</span>ordan Comprehensive Contemporary <span class=acl-fixed-case>A</span>rabic Corpus (<span class=acl-fixed-case>JCCA</span>)</a></strong><br><a href=/people/m/majdi-sawalha/>Majdi Sawalha</a>
|
<a href=/people/f/faisal-alshargi/>Faisal Alshargi</a>
|
<a href=/people/a/abdallah-alshdaifat/>Abdallah AlShdaifat</a>
|
<a href=/people/s/sane-yagi/>Sane Yagi</a>
|
<a href=/people/m/mohammad-a-qudah/>Mohammad A. Qudah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4616><div class="card-body p-3 small">To compile a modern dictionary that catalogues the words in currency, and to study linguistic patterns in the contemporary language, it is necessary to have a corpus of authentic texts that reflect current usage of the language. Although there are numerous Arabic corpora, none claims to be representative of the language in terms of the combination of geographical region, genre, subject matter, mode, and medium. This paper describes a 100-million-word corpus that takes the <a href=https://en.wikipedia.org/wiki/British_National_Corpus>British National Corpus (BNC)</a> as a model. The aim of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is to be balanced, annotated, comprehensive, and representative of contemporary Arabic as written and spoken in Arab countries today. It will be different from most others in not being heavily-dominated by the news or in mixing the classical with the modern. In this paper is an outline of the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> adopted for the design, construction, and annotation of this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. DIWAN (Alshargi and Rambow, 2015) was used to annotate a one-million-word snapshot of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. DIWAN is a dialectal word annotation tool, but we upgraded it by adding a new tag-set that is based on traditional <a href=https://en.wikipedia.org/wiki/Arabic_grammar>Arabic grammar</a> and by adding the roots and morphological patterns of nouns and verbs. Moreover, the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> we constructed covers the major spoken varieties of Arabic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4621/>Mazajak : An Online Arabic Sentiment Analyser<span class=acl-fixed-case>M</span>azajak: An Online <span class=acl-fixed-case>A</span>rabic Sentiment Analyser</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4621><div class="card-body p-3 small">Sentiment analysis (SA) is one of the most useful <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. Literature is flooding with many papers and systems addressing this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, but most of the work is focused on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In this paper, we present Mazajak, an online system for Arabic SA. The system is based on a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a>, which achieves state-of-the-art results on many Arabic dialect datasets including SemEval 2017 and ASTD. The availability of such <a href=https://en.wikipedia.org/wiki/System>system</a> should assist various applications and research that rely on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> as a tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4622/>The MADAR Shared Task on Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>MADAR</span> Shared Task on <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/s/sabit-hassan/>Sabit Hassan</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4622><div class="card-body p-3 small">In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. This shared task was organized as part of The Fourth Arabic Natural Language Processing Workshop, collocated with ACL 2019. The shared task includes two subtasks : the MADAR Travel Domain Dialect Identification subtask (Subtask 1) and the MADAR Twitter User Dialect Identification subtask (Subtask 2). This shared task is the first to target a large set of <a href=https://en.wikipedia.org/wiki/Dialect>dialect labels</a> at the city and country levels. The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project. A total of 21 teams from 15 countries participated in the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4623 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4623/>ZCU-NLP at MADAR 2019 : Recognizing Arabic Dialects<span class=acl-fixed-case>ZCU</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>MADAR</span> 2019: Recognizing <span class=acl-fixed-case>A</span>rabic Dialects</a></strong><br><a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/s/stephen-taylor/>Stephen Taylor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4623><div class="card-body p-3 small">In this paper, we present our <a href=https://en.wikipedia.org/wiki/System>systems</a> for the MADAR Shared Task : Arabic Fine-Grained Dialect Identification. The shared task consists of two subtasks. The goal of Subtask 1 (S-1) is to detect an Arabic city dialect in a given text and the goal of Subtask2 (S-2) is to predict the country of origin of a Twitter user by using tweets posted by the user. In S-1, our proposed <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a>. We use <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are later used as an input for other <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>. We also experiment with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a>, but these experiments showed that simpler <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> are more successful. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 0.658 macro F1-score and our rank is 6th out of 19 teams in S-1 and 7th in S-2 with 0.475 macro F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4624/>Simple But Not Nave : Fine-Grained Arabic Dialect Identification Using Only N-Grams<span class=acl-fixed-case>A</span>rabic Dialect Identification Using Only N-Grams</a></strong><br><a href=/people/s/sohaila-eltanbouly/>Sohaila Eltanbouly</a>
|
<a href=/people/m/may-bashendy/>May Bashendy</a>
|
<a href=/people/t/tamer-elsayed/>Tamer Elsayed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4624><div class="card-body p-3 small">This paper presents the participation of Qatar University team in MADAR shared task, which addresses the problem of sentence-level fine-grained Arabic Dialect Identification over 25 different Arabic dialects in addition to the Modern Standard Arabic. Arabic Dialect Identification is not a trivial task since different dialects share some <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>, e.g., utilizing the same <a href=https://en.wikipedia.org/wiki/Character_structure>character set</a> and some <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabularies</a>. We opted to adopt a very simple approach in terms of extracted features and classification models ; we only utilize word and character n-grams as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, and Na ve Bayes models as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Surprisingly, the simple <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approach</a> achieved non-na ve performance. The official results, reported on a <a href=https://en.wikipedia.org/wiki/Test_set>held-out testing set</a>, show that the dialect of a given sentence can be identified at an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 64.58 % by our best submitted run.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4625 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4625/>LIUM-MIRACL Participation in the MADAR Arabic Dialect Identification Shared Task<span class=acl-fixed-case>LIUM</span>-<span class=acl-fixed-case>MIRACL</span> Participation in the <span class=acl-fixed-case>MADAR</span> <span class=acl-fixed-case>A</span>rabic Dialect Identification Shared Task</a></strong><br><a href=/people/s/sameh-kchaou/>Saméh Kchaou</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich-Belguith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4625><div class="card-body p-3 small">This paper describes the joint participation of the LIUM and MIRACL Laboratories at the Arabic dialect identification challenge of the MADAR Shared Task (Bouamor et al., 2019) conducted during the Fourth Arabic Natural Language Processing Workshop (WANLP 2019). We participated to the Travel Domain Dialect Identification subtask. We built several <a href=https://en.wikipedia.org/wiki/System>systems</a> and explored different techniques including conventional <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning algorithms</a>. Deep learning approaches did not perform well on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We experimented several <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification systems</a> and we were able to identify the <a href=https://en.wikipedia.org/wiki/Dialect>dialect</a> of an input sentence with an F1-score of 65.41 % on the official test set using only the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> supplied by the shared task organizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4627 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4627/>MICHAEL : Mining Character-level Patterns for Arabic Dialect Identification (MADAR Challenge)<span class=acl-fixed-case>MICHAEL</span>: Mining Character-level Patterns for <span class=acl-fixed-case>A</span>rabic Dialect Identification (<span class=acl-fixed-case>MADAR</span> Challenge)</a></strong><br><a href=/people/d/dhaou-ghoul/>Dhaou Ghoul</a>
|
<a href=/people/g/gael-lejeune/>Gaël Lejeune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4627><div class="card-body p-3 small">We present MICHAEL, a simple lightweight method for automatic Arabic Dialect Identification on the MADAR travel domain Dialect Identification (DID). MICHAEL uses simple <a href=https://en.wikipedia.org/wiki/Character_(computing)>character-level features</a> in order to perform a pre-processing free classification. More precisely, Character N-grams extracted from the original sentences are used to train a Multinomial Naive Bayes classifier. This system achieved an official score (accuracy) of 53.25 % with 1 = N=3 but showed a much better result with character 4-grams (62.17 % accuracy).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4628.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4628 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4628 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4628/>Arabic Dialect Identification for <a href=https://en.wikipedia.org/wiki/Travel>Travel</a> and Twitter Text<span class=acl-fixed-case>A</span>rabic Dialect Identification for Travel and <span class=acl-fixed-case>T</span>witter Text</a></strong><br><a href=/people/p/pruthwik-mishra/>Pruthwik Mishra</a>
|
<a href=/people/v/vandan-mujadia/>Vandan Mujadia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4628><div class="card-body p-3 small">This paper presents the results of the experiments done as a part of MADAR Shared Task in WANLP 2019 on Arabic Fine-Grained Dialect Identification. Dialect Identification is one of the prominent tasks in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural language processing</a> where the subsequent <a href=https://en.wikipedia.org/wiki/Modular_programming>language modules</a> can be improved based on it. We explored the use of different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> like char, word n-gram, language model probabilities, etc on different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Results show that these <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> help to improve dialect classification accuracy. Results also show that traditional machine learning classifier tends to perform better when compared to <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> on this task in a low resource setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4629 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4629/>Mawdoo3 AI at MADAR Shared Task : Arabic Tweet Dialect Identification<span class=acl-fixed-case>AI</span> at <span class=acl-fixed-case>MADAR</span> Shared Task: <span class=acl-fixed-case>A</span>rabic Tweet Dialect Identification</a></strong><br><a href=/people/b/bashar-talafha/>Bashar Talafha</a>
|
<a href=/people/w/wael-farhan/>Wael Farhan</a>
|
<a href=/people/a/ahmed-altakrouri/>Ahmed Altakrouri</a>
|
<a href=/people/h/hussein-al-natsheh/>Hussein Al-Natsheh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4629><div class="card-body p-3 small">Arabic dialect identification is an inherently complex problem, as Arabic dialect taxonomy is convoluted and aims to dissect a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous space</a> rather than a <a href=https://en.wikipedia.org/wiki/Discrete_space>discrete one</a>. In this work, we present machine and deep learning approaches to predict 21 fine-grained dialects form a set of given tweets per user. We adopted numerous feature extraction methods most of which showed improvement in the final model, such as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, <a href=https://en.wikipedia.org/wiki/Tf-idf>Tf-idf</a>, and other tweet features. Our results show that a simple LinearSVC can outperform any complex deep learning model given a set of curated features. With a relatively complex user voting mechanism, we were able to achieve a Macro-Averaged F1-score of 71.84 % on MADAR shared subtask-2. Our best submitted <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> ranked second out of all participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4633.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4633 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4633 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4633/>The SMarT Classifier for Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>SM</span>ar<span class=acl-fixed-case>T</span> Classifier for <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/k/karima-meftouh/>Karima Meftouh</a>
|
<a href=/people/k/karima-abidi/>Karima Abidi</a>
|
<a href=/people/s/salima-harrat/>Salima Harrat</a>
|
<a href=/people/k/kamel-smaili/>Kamel Smaili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4633><div class="card-body p-3 small">This paper describes the approach adopted by the SMarT research group to build a dialect identification system in the framework of the Madar shared task on Arabic fine-grained dialect identification. We experimented several approaches, but we finally decided to use a <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Multinomial Naive Bayes classifier</a> based on word and character ngrams in addition to the language model probabilities. We achieved a score of 67.73 % in terms of Macro accuracy and a macro-averaged F1-score of 67.31 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4634.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4634 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4634 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4634/>JHU System Description for the MADAR Arabic Dialect Identification Shared Task<span class=acl-fixed-case>JHU</span> System Description for the <span class=acl-fixed-case>MADAR</span> <span class=acl-fixed-case>A</span>rabic Dialect Identification Shared Task</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a>
|
<a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4634><div class="card-body p-3 small">Our submission to the MADAR shared task on Arabic dialect identification employed a language modeling technique called Prediction by Partial Matching, an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models. We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2. Interestingly, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundant information</a>, and future work may focus on encouraging decorrelated learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4636 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4636/>A Character Level Convolutional BiLSTM for Arabic Dialect Identification<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> for <span class=acl-fixed-case>A</span>rabic Dialect Identification</a></strong><br><a href=/people/m/mohamed-elaraby/>Mohamed Elaraby</a>
|
<a href=/people/a/ahmed-zahran/>Ahmed Zahran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4636><div class="card-body p-3 small">In this paper, we describe CU-RAISA teamcontribution to the 2019Madar shared task2, which focused on Twitter User fine-grained dialect identification. Among par-ticipating teams, our system ranked the4th(with 61.54 %) F1-Macro measure. Our sys-tem is trained using a character level convo-lutional bidirectional long-short-term memorynetwork trained on 2k users&#8217; data. We showthat training on concatenated user tweets asinput is further superior to training on usertweets separately and assign user&#8217;s label on themode of user&#8217;s tweets&#8217; predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4638.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4638 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4638 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4638/>Team JUST at the MADAR Shared Task on Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>JUST</span> at the <span class=acl-fixed-case>MADAR</span> Shared Task on <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/b/bashar-talafha/>Bashar Talafha</a>
|
<a href=/people/a/ali-fadel/>Ali Fadel</a>
|
<a href=/people/m/mahmoud-al-ayyoub/>Mahmoud Al-Ayyoub</a>
|
<a href=/people/y/yaser-jararweh/>Yaser Jararweh</a>
|
<a href=/people/m/mohammad-al-smadi/>Mohammad AL-Smadi</a>
|
<a href=/people/p/patrick-juola/>Patrick Juola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4638><div class="card-body p-3 small">In this paper, we describe our team&#8217;s effort on the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. The task requires building a <a href=https://en.wikipedia.org/wiki/System>system</a> capable of differentiating between 25 different <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> in addition to <a href=https://en.wikipedia.org/wiki/Modern_Standard_Arabic>MSA</a>. Our <a href=https://en.wikipedia.org/wiki/Tactic_(method)>approach</a> is simple. After preprocessing the data, we use Data Augmentation (DA) to enlarge the training data six times. We then build a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and extract n-gram word-level and character-level TF-IDF features and feed them into an MNB classifier. Despite its simplicity, the resulting model performs really well producing the 4th highest <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> and region-level accuracy and the 5th highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, <a href=https://en.wikipedia.org/wiki/Precision_recall>recall</a>, city-level accuracy and country-level accuracy among the participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4639.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4639 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4639 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4639/>QC-GO Submission for MADAR Shared Task : Arabic Fine-Grained Dialect Identification<span class=acl-fixed-case>QC</span>-<span class=acl-fixed-case>GO</span> Submission for <span class=acl-fixed-case>MADAR</span> Shared Task: <span class=acl-fixed-case>A</span>rabic Fine-Grained Dialect Identification</a></strong><br><a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/m/mohamed-eldesouki/>Mohamed Eldesouki</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4639><div class="card-body p-3 small">This paper describes the QC-GO team submission to the MADAR Shared Task Subtask 1 (travel domain dialect identification) and Subtask 2 (Twitter user location identification). In our participation in both subtasks, we explored a number of approaches and system combinations to obtain the best performance for both tasks. These include <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural nets</a> and <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>. Since individual <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> suffer from various shortcomings, the combination of different approaches was able to fill some of these gaps. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves F1-Scores of 66.1 % and 67.0 % on the development sets for Subtasks 1 and 2 respectively.</div></div></div><hr><div id=w19-47><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-47.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-47/>Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4700/>Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</a></strong><br><a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a>
|
<a href=/people/l/lars-borin/>Lars Borin</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4704/>Evaluation of Semantic Change of Harm-Related Concepts in Psychology</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/s/sean-murphy/>Sean Murphy</a>
|
<a href=/people/n/nicholas-haslam/>Nicholas Haslam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4704><div class="card-body p-3 small">The paper focuses on diachronic evaluation of semantic changes of harm-related concepts in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a>. More specifically, we investigate a hypothesis that certain concepts such as <a href=https://en.wikipedia.org/wiki/Addiction>addiction</a>, <a href=https://en.wikipedia.org/wiki/Bullying>bullying</a>, <a href=https://en.wikipedia.org/wiki/Harassment>harassment</a>, <a href=https://en.wikipedia.org/wiki/Prejudice>prejudice</a>, and <a href=https://en.wikipedia.org/wiki/Psychological_trauma>trauma</a> became broader during the last four decades. We evaluate semantic changes using two <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> : an LSA-based model from Sagi et al. (2009) and a diachronic adaptation of <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> from Hamilton et al. (2016), that are trained on a large corpus of journal abstracts covering the period of 1980 2019. Several <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> showed evidence of broadening. Addiction moved from physiological dependency on a substance to include psychological dependency on <a href=https://en.wikipedia.org/wiki/Video_game_culture>gaming</a> and the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a>. Similarly, <a href=https://en.wikipedia.org/wiki/Harassment>harassment</a> and <a href=https://en.wikipedia.org/wiki/Psychological_trauma>trauma</a> shifted towards more psychological meanings. On the other hand, <a href=https://en.wikipedia.org/wiki/Bullying>bullying</a> has transformed into a more victim-related concept and expanded to new areas such as <a href=https://en.wikipedia.org/wiki/Workplace_bullying>workplaces</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4707/>GASC : Genre-Aware Semantic Change for Ancient Greek<span class=acl-fixed-case>GASC</span>: Genre-Aware Semantic Change for <span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>G</span>reek</a></strong><br><a href=/people/v/valerio-perrone/>Valerio Perrone</a>
|
<a href=/people/m/marco-palma/>Marco Palma</a>
|
<a href=/people/s/simon-hengchen/>Simon Hengchen</a>
|
<a href=/people/a/alessandro-vatri/>Alessandro Vatri</a>
|
<a href=/people/j/jim-q-smith/>Jim Q. Smith</a>
|
<a href=/people/b/barbara-mcgillivray/>Barbara McGillivray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4707><div class="card-body p-3 small">Word meaning changes over time, depending on linguistic and extra-linguistic factors. Associating a word&#8217;s correct meaning in its historical context is a central challenge in diachronic research, and is relevant to a range of NLP tasks, including <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> and <a href=https://en.wikipedia.org/wiki/Semantic_search>semantic search</a> in historical texts. Bayesian models for <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> have emerged as a powerful tool to address this challenge, providing explicit and interpretable representations of <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change phenomena</a>. However, while corpora typically come with rich metadata, existing models are limited by their inability to exploit <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> (such as text genre) beyond the document time-stamp. This is particularly critical in the case of ancient languages, where lack of data and long diachronic span make it harder to draw a clear distinction between <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> (the fact that a word has several senses) and <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> (the process of acquiring, losing, or changing senses), and current systems perform poorly on these languages. We develop GASC, a dynamic semantic change model that leverages categorical metadata about the texts&#8217; genre to boost inference and uncover the evolution of meanings in Ancient Greek corpora. In a new evaluation framework, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves improved <a href=https://en.wikipedia.org/wiki/Predictive_analytics>predictive performance</a> compared to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4709/>A Method to Automatically Identify Diachronic Variation in Collocations.</a></strong><br><a href=/people/m/marcos-garcia/>Marcos Garcia</a>
|
<a href=/people/m/marcos-garcia-salido/>Marcos García Salido</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4709><div class="card-body p-3 small">This paper introduces a novel method to track collocational variations in diachronic corpora that can identify several changes undergone by these phraseological combinations and to propose alternative solutions found in later periods. The <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> consists of extracting syntactically-related candidates of collocations and ranking them using statistical association measures. Then, starting from the first period of the corpus, the system tracks each combination over time, verifying different types of historical variation such as the loss of one or both lemmas, the disappearance of the collocation, or its diachronic frequency trend. Using a distributional semantics strategy, it also proposes other linguistic structures which convey similar meanings to those extinct collocations. A case study on historical corpora of <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> shows that the system speeds up and facilitates the finding of some diachronic changes and phraseological shifts that are harder to identify without using automated methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4710/>Written on Leaves or in Stones? : Computational Evidence for the Era of Authorship of Old Thai Prose<span class=acl-fixed-case>T</span>hai Prose</a></strong><br><a href=/people/a/attapol-rutherford/>Attapol Rutherford</a>
|
<a href=/people/s/santhawat-thanyawong/>Santhawat Thanyawong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4710><div class="card-body p-3 small">We aim to provide computational evidence for the era of authorship of two important old Thai texts : Traiphumikatha and Pumratchatham. The era of authorship of these two books is still an ongoing debate among Thai literature scholars. Analysis of old Thai texts present a challenge for standard natural language processing techniques, due to the lack of corpora necessary for building old Thai word and syllable segmentation. We propose an accurate and interpretable model to classify each segment as one of the three eras of authorship (Sukhothai, Ayuddhya, or Rattanakosin) without sophisticated linguistic preprocessing. Contrary to previous hypotheses, our <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a> suggests that both books were written during the <a href=https://en.wikipedia.org/wiki/Sukhothai_Kingdom>Sukhothai era</a>. Moreover, the second half of the Pumratchtham is uncharacteristic of the <a href=https://en.wikipedia.org/wiki/Sukhothai_Kingdom>Sukhothai era</a>, which may have confounded literary scholars in the past. Further, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> reveals that the most indicative linguistic changes stem from unidirectional grammaticalized words and polyfunctional words, which show up as most dominant features in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4711/>Identifying Temporal Trends Based on Perplexity and Clustering : Are We Looking at Language Change?</a></strong><br><a href=/people/s/sidsel-boldsen/>Sidsel Boldsen</a>
|
<a href=/people/m/manex-agirrezabal/>Manex Agirrezabal</a>
|
<a href=/people/p/patrizia-paggio/>Patrizia Paggio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4711><div class="card-body p-3 small">In this work we propose a data-driven methodology for identifying temporal trends in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of medieval charters</a>. We have used perplexities derived from RNNs as a distance measure between documents and then, performed <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> on those distances. We argue that <a href=https://en.wikipedia.org/wiki/Perplexity>perplexities</a> calculated by such <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are representative of temporal trends. The <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> produced using the <a href=https://en.wikipedia.org/wiki/K-Means_algorithm>K-Means algorithm</a> give an insight of the differences in language in different time periods at least partly due to <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>. We suggest that the temporal distribution of the individual clusters might provide a more nuanced picture of temporal trends compared to discrete bins, thus providing better results when used in a classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4713/>Predicting Historical Phonetic Features using Deep Neural Networks : A Case Study of the Phonetic System of Proto-Indo-European<span class=acl-fixed-case>P</span>roto-<span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>E</span>uropean</a></strong><br><a href=/people/f/frederik-hartmann/>Frederik Hartmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4713><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a> lacks the possibility to empirically assess its assumptions regarding the phonetic systems of past languages and language stages since most current methods rely on comparative tools to gain insights into phonetic features of sounds in proto- or ancestor languages. The paper at hand presents a computational method based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to predict <a href=https://en.wikipedia.org/wiki/Phonetics>phonetic features</a> of historical sounds where the exact quality is unknown and to test the overall coherence of reconstructed historical phonetic features. The method utilizes the principles of coarticulation, local predictability and statistical phonological constraints to predict <a href=https://en.wikipedia.org/wiki/Phoneme>phonetic features</a> by the features of their immediate phonetic environment. The validity of this method will be assessed using New High German phonetic data and its specific application to <a href=https://en.wikipedia.org/wiki/Historical_linguistics>diachronic linguistics</a> will be demonstrated in a case study of the phonetic system Proto-Indo-European.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4714/>ParHistVis : Visualization of Parallel Multilingual Historical Data<span class=acl-fixed-case>P</span>ar<span class=acl-fixed-case>H</span>ist<span class=acl-fixed-case>V</span>is: Visualization of Parallel Multilingual Historical Data</a></strong><br><a href=/people/a/aikaterini-lida-kalouli/>Aikaterini-Lida Kalouli</a>
|
<a href=/people/r/rebecca-kehlbeck/>Rebecca Kehlbeck</a>
|
<a href=/people/r/rita-sevastjanova/>Rita Sevastjanova</a>
|
<a href=/people/k/katharina-kaiser/>Katharina Kaiser</a>
|
<a href=/people/g/georg-a-kaiser/>Georg A. Kaiser</a>
|
<a href=/people/m/miriam-butt/>Miriam Butt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4714><div class="card-body p-3 small">The study of <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> through <a href=https://en.wikipedia.org/wiki/Parallel_corpora>parallel corpora</a> can be advantageous for the analysis of complex interactions between time, text domain and language. Often, those advantages can not be fully exploited due to the sparse but high-dimensional nature of such historical data. To tackle this challenge, we introduce ParHistVis : a novel, free, easy-to-use, interactive visualization tool for parallel, multilingual, diachronic and synchronic linguistic data. We illustrate the suitability of the components of the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> based on a use case of word order change in Romance wh-interrogatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4716/>DiaHClust : an Iterative Hierarchical Clustering Approach for Identifying Stages in Language Change<span class=acl-fixed-case>D</span>ia<span class=acl-fixed-case>HC</span>lust: an Iterative Hierarchical Clustering Approach for Identifying Stages in Language Change</a></strong><br><a href=/people/c/christin-schatzle/>Christin Schätzle</a>
|
<a href=/people/h/hannah-booth/>Hannah Booth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4716><div class="card-body p-3 small">Language change is often assessed against a set of pre-determined time periods in order to be able to trace its diachronic trajectory. This is problematic, since a pre-determined periodization might obscure significant developments and lead to false assumptions about the data. Moreover, these <a href=https://en.wikipedia.org/wiki/Time>time periods</a> can be based on factors which are either arbitrary or non-linguistic, e.g., dividing the corpus data into equidistant stages or taking into account language-external events. Addressing this problem, in this paper we present a data-driven approach to <a href=https://en.wikipedia.org/wiki/Periodization>periodization</a> : &#8216;DiaHClust&#8217;. DiaHClust is based on iterative hierarchical clustering and offers a multi-layered perspective on change from text-level to broader time periods. We demonstrate the usefulness of DiaHClust via a case study investigating <a href=https://en.wikipedia.org/wiki/Syntactic_change>syntactic change</a> in <a href=https://en.wikipedia.org/wiki/Icelandic_language>Icelandic</a>, modelling the syntactic system of the language in terms of vectors of syntactic change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4718.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4718 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4718 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4718/>Times Are Changing : Investigating the Pace of Language Change in Diachronic Word Embeddings</a></strong><br><a href=/people/s/stephanie-brandl/>Stephanie Brandl</a>
|
<a href=/people/d/david-lassner/>David Lassner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4718><div class="card-body p-3 small">We propose Word Embedding Networks, a novel method that is able to learn word embeddings of individual data slices while simultaneously aligning and ordering them without feeding temporal information a priori to the model. This gives us the opportunity to analyse the dynamics in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on a large scale in a purely data-driven manner. In experiments on two different newspaper corpora, the New York Times (English) and die Zeit (German), we were able to show that time actually determines the dynamics of semantic change. However, there is by no means a uniform evolution, but instead times of faster and times of slower change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4720 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4720/>Studying Laws of Semantic Divergence across Languages using Cognate Sets</a></strong><br><a href=/people/a/ana-uban/>Ana Uban</a>
|
<a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4720><div class="card-body p-3 small">Semantic divergence in related languages is a key concern of <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. Intra-lingual semantic shift has been previously studied in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>, but this can only provide a limited picture of the evolution of word meanings, which often develop in a multilingual environment. In this paper we investigate <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> across languages by measuring the <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a> of cognate words in multiple languages. By comparing current meanings of cognates in different languages, we hope to uncover information about their previous meanings, and about how they diverged in their respective languages from their common original etymon. We further study the properties of their semantic divergence, by analyzing how the features of words such as frequency and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> are related to the divergence in their meaning, and thus make the first steps towards formulating laws of cross-lingual semantic change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4721.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4721 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4721 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4721" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4721/>Detecting Syntactic Change Using a Neural Part-of-Speech Tagger</a></strong><br><a href=/people/w/william-merrill/>William Merrill</a>
|
<a href=/people/g/gigi-stark/>Gigi Stark</a>
|
<a href=/people/r/robert-frank/>Robert Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4721><div class="card-body p-3 small">We train a diachronic long short-term memory (LSTM) part-of-speech tagger on a large corpus of American English from the 19th, 20th, and 21st centuries. We analyze the <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a>&#8217;s ability to implicitly learn temporal structure between years, and the extent to which this knowledge can be transferred to date new sentences. The learned year embeddings show a strong linear correlation between their first principal component and time. We show that temporal information encoded in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be used to predict novel sentences&#8217; years of composition relatively well. Comparisons to a feedforward baseline suggest that the temporal change learned by the LSTM is syntactic rather than purely lexical. Thus, our results suggest that our <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a> is implicitly learning to model <a href=https://en.wikipedia.org/wiki/Syntactic_change>syntactic change</a> in <a href=https://en.wikipedia.org/wiki/American_English>American English</a> over the course of the 19th, 20th, and early 21st centuries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4723 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4723/>Spatio-Temporal Prediction of Dialectal Variant Usage</a></strong><br><a href=/people/p/peter-jeszenszky/>Péter Jeszenszky</a>
|
<a href=/people/p/panote-siriaraya/>Panote Siriaraya</a>
|
<a href=/people/p/philipp-stoeckle/>Philipp Stoeckle</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4723><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Frequency_distribution>distribution</a> of most <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>dialectal variants</a> have not only spatial but also temporal patterns. Based on the &#8216;apparent time hypothesis&#8217;, much of dialect change is happening through younger speakers accepting innovations. Thus, synchronic diversity can be interpreted diachronically. With the assumption of the &#8216;contact effect&#8217;, i.e. contact possibility (contact and isolation) between speaker communities being responsible for <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>, and the apparent time hypothesis, we aim to predict the usage of dialectal variants. In this paper we model the contact possibility based on two of the most important factors in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> to be affecting <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> : <a href=https://en.wikipedia.org/wiki/Ageing>age</a> and <a href=https://en.wikipedia.org/wiki/Distance>distance</a>. The first steps of the approach involve modeling contact possibility using a <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic predictor</a>, taking the age of respondents into account. We test the global, and the local role of age for variation where the local level means spatial subsets around each survey site, chosen based on k nearest neighbors. The prediction approach is tested on Swiss German syntactic survey data, featuring multiple respondents from different age cohorts at survey sites. The results show the relative success of the logistic prediction approach and the limitations of the <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, therefore further proposals are made to develop the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>.<i>global</i>, and the <i>local</i> role of age for variation where the local level means spatial subsets around each survey site, chosen based on <i>k</i> nearest neighbors. The prediction approach is tested on Swiss German syntactic survey data, featuring multiple respondents from different age cohorts at survey sites. The results show the relative success of the logistic prediction approach and the limitations of the method, therefore further proposals are made to develop the methodology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4724 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4724" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4724/>One-to-X Analogical Reasoning on Word Embeddings : a Case for Diachronic Armed Conflict Prediction from News Texts<span class=acl-fixed-case>X</span> Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4724><div class="card-body p-3 small">We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type &#8216;location : armed-group&#8217; based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of <a href=https://en.wikipedia.org/wiki/Trigonometric_functions>cosine distance</a> to decrease the number of false positives ; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4726.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4726 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4726 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4726/>Semantic Change in the Language of UK Parliamentary Debates<span class=acl-fixed-case>UK</span> Parliamentary Debates</a></strong><br><a href=/people/g/gavin-abercrombie/>Gavin Abercrombie</a>
|
<a href=/people/r/riza-theresa-batista-navarro/>Riza Batista-Navarro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4726><div class="card-body p-3 small">We investigate changes in the meanings of words used in the <a href=https://en.wikipedia.org/wiki/Parliament_of_the_United_Kingdom>UK Parliament</a> across two different epochs. We use <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to explore changes in the distribution of words of interest and uncover words that appear to have undergone <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic transformation</a> in the intervening period, and explore different ways of obtaining target words for this purpose. We find that <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic changes</a> are generally in line with those found in other corpora, and little evidence that <a href=https://en.wikipedia.org/wiki/Parliamentary_language>parliamentary language</a> is more static than <a href=https://en.wikipedia.org/wiki/General_English>general English</a>. It also seems that words with senses that have been recorded in the <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a> as having fallen into disuse do not undergo semantic changes in this domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4727.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4727 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4727 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4727" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4727/>Semantic Change and Emerging Tropes In a Large Corpus of New High German Poetry<span class=acl-fixed-case>N</span>ew <span class=acl-fixed-case>H</span>igh <span class=acl-fixed-case>G</span>erman Poetry</a></strong><br><a href=/people/t/thomas-haider/>Thomas Haider</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4727><div class="card-body p-3 small">Due to its semantic succinctness and novelty of expression, <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a> is a great test-bed for semantic change analysis. However, so far there is a scarcity of large diachronic corpora. Here, we provide a large corpus of <a href=https://en.wikipedia.org/wiki/German_poetry>German poetry</a> which consists of about 75k <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> with more than 11 million tokens, with <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> ranging from the 16th to early 20th century. We then track <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> in this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by investigating the rise of <a href=https://en.wikipedia.org/wiki/Trope_(literature)>tropes</a> (&#8216;love is magic&#8217;) over time and detecting change points of meaning, which we find to occur particularly within the <a href=https://en.wikipedia.org/wiki/German_Romanticism>German Romantic period</a>. Additionally, through <a href=https://en.wikipedia.org/wiki/Self-similarity>self-similarity</a>, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4728 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4728/>Conceptual Change and Distributional Semantic Models : an Exploratory Study on Pitfalls and Possibilities</a></strong><br><a href=/people/p/pia-sommerauer/>Pia Sommerauer</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4728><div class="card-body p-3 small">Studying <a href=https://en.wikipedia.org/wiki/Conceptual_change>conceptual change</a> using embedding models has become increasingly popular in the Digital Humanities community while critical observations about them have received less attention. This paper investigates what the impact of known pitfalls can be on the conclusions drawn in a digital humanities study through the use case of <a href=https://en.wikipedia.org/wiki/Racism>Racism</a>. In addition, we suggest an approach for modeling a <a href=https://en.wikipedia.org/wiki/Complexity>complex concept</a> in terms of words and relations representative of the <a href=https://en.wikipedia.org/wiki/Conceptual_system>conceptual system</a>. Our results show that different models created from the same data yield different results, but also indicate that using different model architectures, comparing different corpora and comparing to control words and relations can help to identify which results are solid and which may be due to artefact. We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4730.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4730 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4730 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4730/>Towards Automatic Variant Analysis of Ancient Devotional Texts</a></strong><br><a href=/people/a/amir-hazem/>Amir Hazem</a>
|
<a href=/people/b/beatrice-daille/>Béatrice Daille</a>
|
<a href=/people/d/dominique-stutzmann/>Dominique Stutzmann</a>
|
<a href=/people/j/jacob-currie/>Jacob Currie</a>
|
<a href=/people/c/christine-jacquin/>Christine Jacquin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4730><div class="card-body p-3 small">We address in this paper the issue of text reuse in liturgical manuscripts of the middle ages. More specifically, we study variant readings of the Obsecro Te prayer, part of the devotional Books of Hours often used by Christians as guidance for their daily prayers. We aim at automatically extracting and categorising pairs of words and expressions that exhibit <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>variant relations</a>. For this purpose, we adopt a linguistic classification that allows to better characterize the <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>variants</a> than edit operations. Then, we study the evolution of Obsecro Te texts from a temporal and geographical axis. Finally, we contrast several unsupervised state-of-the-art approaches for the automatic extraction of Obsecro Te variants. Based on the manual observation of 772 Obsecro Te copies which show more than 21,000 variants, we show that the proposed methodology is helpful for an automatic study of variants and may serve as basis to analyze and to depict useful information from devotional texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4731.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4731 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4731 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4731/>Understanding the Evolution of Circular Economy through Language Change</a></strong><br><a href=/people/s/sampriti-mahanty/>Sampriti Mahanty</a>
|
<a href=/people/f/frank-boons/>Frank Boons</a>
|
<a href=/people/j/julia-handl/>Julia Handl</a>
|
<a href=/people/r/riza-theresa-batista-navarro/>Riza Theresa Batista-Navarro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4731><div class="card-body p-3 small">In this study, we propose to focus on understanding the evolution of a specific scientific conceptthat of Circular Economy (CE)by analysing how the language used in academic discussions has changed semantically. It is worth noting that the meaning and central theme of this <a href=https://en.wikipedia.org/wiki/Concept>concept</a> has remained the same ; however, we hypothesise that it has undergone <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> by way of additional layers being added to the <a href=https://en.wikipedia.org/wiki/Concept>concept</a>. We have shown that semantic change in language is a reflection of shifts in scientific ideas, which in turn help explain the evolution of a <a href=https://en.wikipedia.org/wiki/Concept>concept</a>. Focusing on the CE concept, our analysis demonstrated that the change over time in the language used in academic discussions of CE is indicative of the way in which the <a href=https://en.wikipedia.org/wiki/Concept>concept</a> evolved and expanded.</div></div></div><hr><div id=w19-48><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-48.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-48/>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4800/>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></strong><br><a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4802" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4802/>Sentiment Analysis Is Not Solved ! Assessing and Probing Sentiment Classification</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4802><div class="card-body p-3 small">Neural methods for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> have led to quantitative improvements over previous approaches, but these advances are not always accompanied with a thorough analysis of the qualitative differences. Therefore, it is not clear what outstanding conceptual challenges for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> remain. In this work, we attempt to discover what challenges still prove a problem for sentiment classifiers for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and to provide a challenging <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We collect the subset of sentences that an (oracle) ensemble of state-of-the-art sentiment classifiers misclassify and then annotate them for 18 linguistic and paralinguistic phenomena, such as <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modality</a>, etc. Finally, we provide a case study that demonstrates the usefulness of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to probe the performance of a given sentiment classifier with respect to <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic phenomena</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4805/>Multi-Granular Text Encoding for Self-Explaining Categorization</a></strong><br><a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/l/lin-pan/>Lin Pan</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/y/yousef-el-kurdi/>Yousef El-Kurdi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4805><div class="card-body p-3 small">Self-explaining text categorization requires a <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> to make a prediction along with supporting evidence. A popular type of evidence is sub-sequences extracted from the input text which are sufficient for the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to make the prediction. In this work, we define multi-granular ngrams as basic units for explanation, and organize all <a href=https://en.wikipedia.org/wiki/Ngram>ngrams</a> into a hierarchical structure, so that shorter <a href=https://en.wikipedia.org/wiki/Ngram>ngrams</a> can be reused while computing longer <a href=https://en.wikipedia.org/wiki/Ngram>ngrams</a>. We leverage the tree-structured LSTM to learn a context-independent representation for each unit via parameter sharing. Experiments on <a href=https://en.wikipedia.org/wiki/Medical_classification>medical disease classification</a> show that our model is more accurate, efficient and compact than the BiLSTM and CNN baselines. More importantly, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can extract intuitive multi-granular evidence to support its predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4806/>The Meaning of Most for Visual Question Answering Models</a></strong><br><a href=/people/a/alexander-kuhnle/>Alexander Kuhnle</a>
|
<a href=/people/a/ann-copestake/>Ann Copestake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4806><div class="card-body p-3 small">The correct interpretation of quantifier statements in the context of a <a href=https://en.wikipedia.org/wiki/Visual_system>visual scene</a> requires non-trivial <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference mechanisms</a>. For the example of most, we discuss two <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> which rely on fundamentally different <a href=https://en.wikipedia.org/wiki/Cognition>cognitive concepts</a>. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber&#8217;s law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4809/>Detecting Political Bias in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>News Articles</a> Using Headline Attention</a></strong><br><a href=/people/r/rama-rohit-reddy-gangula/>Rama Rohit Reddy Gangula</a>
|
<a href=/people/s/suma-reddy-duggenpudi/>Suma Reddy Duggenpudi</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4809><div class="card-body p-3 small">Language is a powerful tool which can be used to state the facts as well as express our views and perceptions. Most of the times, we find a subtle bias towards or against someone or something. When it comes to <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, media houses and journalists are known to create bias by shrewd means such as misinterpreting reality and distorting viewpoints towards some parties. This misinterpretation on a large scale can lead to the production of <a href=https://en.wikipedia.org/wiki/News_bias>biased news</a> and <a href=https://en.wikipedia.org/wiki/Conspiracy_theory>conspiracy theories</a>. Automating bias detection in newspaper articles could be a good challenge for research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We proposed a headline attention network for this bias detection. Our model has two distinctive characteristics : (i) it has a structure that mirrors a person&#8217;s way of reading a news article (ii) it has attention mechanism applied on the article based on its headline, enabling it to attend to more critical content to predict bias. As the required datasets were not available, we created a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprising of 1329 <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> collected from various <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_India>Telugu newspapers</a> and marked them for bias towards a particular political party. The experiments conducted on it demonstrated that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms various baseline methods by a substantial margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4810/>Testing the Generalization Power of Neural Network Models across NLI Benchmarks<span class=acl-fixed-case>NLI</span> Benchmarks</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4810><div class="card-body p-3 small">Neural network models have been very successful in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, with the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> reaching 90 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in some benchmarks. However, the success of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> turns out to be largely benchmark specific. We show that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on a natural language inference dataset drawn from one benchmark fail to perform well in others, even if the notion of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> assumed in these <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> is the same or similar. We train six high performing neural network models on different datasets and show that each one of these has problems of generalizing when we replace the original test set with a test set taken from another corpus designed for the same task. In light of these results, we argue that most of the current neural network models are not able to generalize well in the task of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>. We find that using large pre-trained language models helps with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> when the datasets are similar enough. Our results also highlight that the current NLI datasets do not cover the different nuances of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> extensively enough.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4811" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4811/>Character Eyes : Seeing Language through Character-Level Taggers</a></strong><br><a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/m/marc-marone/>Marc Marone</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4811><div class="card-body p-3 small">Character-level models have been used extensively in recent years in NLP tasks as both supplements and replacements for closed-vocabulary token-level word representations. In one popular architecture, character-level LSTMs are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (POS) tags. In this work, we examine the behavior of POS taggers across languages from the perspective of individual hidden units within the character LSTM. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4812.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4812 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4812 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4812/>Faithful Multimodal Explanation for Visual Question Answering</a></strong><br><a href=/people/j/jialin-wu/>Jialin Wu</a>
|
<a href=/people/r/raymond-mooney/>Raymond Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4812><div class="card-body p-3 small">AI systems&#8217; ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a> such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods using both automated metrics and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4813" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4813/>Evaluating Recurrent Neural Network Explanations</a></strong><br><a href=/people/l/leila-arras/>Leila Arras</a>
|
<a href=/people/a/ahmed-osman/>Ahmed Osman</a>
|
<a href=/people/k/klaus-robert-muller/>Klaus-Robert Müller</a>
|
<a href=/people/w/wojciech-samek/>Wojciech Samek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4813><div class="card-body p-3 small">Recently, several methods have been proposed to explain the predictions of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNNs)</a>, in particular of <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a>. The goal of these methods is to understand the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a>&#8217;s decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction. In previous works, some of these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> were not yet compared to one another, or were evaluated only qualitatively. We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations. Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4816 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4816" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4816/>Modeling Paths for Explainable Knowledge Base Completion</a></strong><br><a href=/people/j/josua-stadelmaier/>Josua Stadelmaier</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4816><div class="card-body p-3 small">A common approach in knowledge base completion (KBC) is to learn representations for entities and relations in order to infer missing facts by generalizing existing ones. A shortcoming of standard models is that they do not explain their predictions to make them verifiable easily to human inspection. In this paper, we propose the Context Path Model (CPM) which generates explanations for new facts in KBC by providing sets of context paths as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a <a href=https://en.wikipedia.org/wiki/Wrapper_function>wrapper</a> that can be applied on top of various existing KBC models. We evaluate <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness.<i>context paths</i> as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a wrapper that can be applied on top of various existing KBC models. We evaluate it for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4817 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4817/>Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a><span class=acl-fixed-case>F</span>rench and <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/p/paola-merlo/>Paola Merlo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4817><div class="card-body p-3 small">The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising <a href=https://en.wikipedia.org/wiki/Language>human languages</a>. Results are at present inconclusive. In this paper, we extend previous work on <a href=https://en.wikipedia.org/wiki/Long-distance_dependencies>long-distance dependencies</a> in three ways. We manipulate <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to translate them in a space that is attuned to the linguistic properties under study. We extend the <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> to <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> and to <a href=https://en.wikipedia.org/wiki/Formal_language>new languages</a>. We confirm previous negative results : <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> do not unequivocally encode fine-grained linguistic properties of long-distance dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4819.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4819 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4819 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4819/>Hierarchical Representation in Neural Language Models : Suppression and Recovery of Expectations</a></strong><br><a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a>
|
<a href=/people/r/richard-futrell/>Richard Futrell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4819><div class="card-body p-3 small">Work using artificial languages as training input has shown that LSTMs are capable of inducing the stack-like data structures required to represent <a href=https://en.wikipedia.org/wiki/Context-free_language>context-free</a> and certain mildly context-sensitive languages formal language classes which correspond in theory to the hierarchical structures of natural language. Here we present a suite of experiments probing whether neural language models trained on linguistic data induce these stack-like data structures and deploy them while incrementally predicting words. We study two natural language phenomena : center embedding sentences and syntactic island constraints on the fillergap dependency. In order to properly predict words in these structures, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4821.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4821 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4821 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4821" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4821/>An LSTM Adaptation Study of (Un)grammaticality<span class=acl-fixed-case>LSTM</span> Adaptation Study of (Un)grammaticality</a></strong><br><a href=/people/s/shammur-absar-chowdhury/>Shammur Absar Chowdhury</a>
|
<a href=/people/r/roberto-zamparelli/>Roberto Zamparelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4821><div class="card-body p-3 small">We propose a novel approach to the study of how <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural network</a> perceive the distinction between grammatical and ungrammatical sentences, a crucial task in the growing field of synthetic linguistics. The method is based on performance measures of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> trained on corpora and fine-tuned with either <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>grammatical or ungrammatical sentences</a>, then applied to (different types of) <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>grammatical or ungrammatical sentences</a>. The results show that both in the difficult and highly symmetrical task of detecting subject islands and in the more open CoLA dataset, grammatical sentences give rise to better scores than ungrammatical ones, possibly because they can be better integrated within the body of linguistic structural knowledge that the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> has accumulated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4825.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4825 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4825 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-4825" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-4825/>Open Sesame : Getting inside BERT’s Linguistic Knowledge<span class=acl-fixed-case>BERT</span>’s Linguistic Knowledge</a></strong><br><a href=/people/y/yongjie-lin/>Yongjie Lin</a>
|
<a href=/people/y/yi-chern-tan/>Yi Chern Tan</a>
|
<a href=/people/r/robert-frank/>Robert Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4825><div class="card-body p-3 small">How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT&#8217;s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT&#8217;s representation of subject-verb agreement and <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphor-antecedent dependencies</a> through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT&#8217;s representations do indeed model linguistically relevant aspects of <a href=https://en.wikipedia.org/wiki/Hierarchical_structure>hierarchical structure</a>, though they do not appear to show the sharp sensitivity to <a href=https://en.wikipedia.org/wiki/Hierarchical_structure>hierarchical structure</a> that is found in human processing of reflexive anaphora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4826.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4826 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4826 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4826/>GEval : Tool for Debugging NLP Datasets and Models<span class=acl-fixed-case>GE</span>val: Tool for Debugging <span class=acl-fixed-case>NLP</span> Datasets and Models</a></strong><br><a href=/people/f/filip-gralinski/>Filip Graliński</a>
|
<a href=/people/a/anna-wroblewska/>Anna Wróblewska</a>
|
<a href=/people/t/tomasz-stanislawek/>Tomasz Stanisławek</a>
|
<a href=/people/k/kamil-grabowski/>Kamil Grabowski</a>
|
<a href=/people/t/tomasz-gorecki/>Tomasz Górecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4826><div class="card-body p-3 small">This paper presents a simple but general and effective method to debug the output of machine learning (ML) supervised models, including <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> looks for <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that lower the evaluation metric in such a way that it can not be ascribed to chance (as measured by their p-values). Using this method implemented as MLEval tool you can find : (1) anomalies in test sets, (2) issues in preprocessing, (3) problems in the ML model itself. It can give you an insight into what can be improved in the datasets and/or the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The same <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can be used to compare ML models or different versions of the same <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We present the <a href=https://en.wikipedia.org/wiki/Tool>tool</a>, the theory behind it and use cases for text-based models of various types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4827.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-4827 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-4827 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4827/>From Balustrades to Pierre Vinken : Looking for Syntax in Transformer Self-Attentions</a></strong><br><a href=/people/d/david-marecek/>David Mareček</a>
|
<a href=/people/r/rudolf-rosa/>Rudolf Rosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-4827><div class="card-body p-3 small">We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> to existing constituency treebanks, both manually and by computing <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and recall.</div></div></div><hr><div id=w19-49><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-49.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-49/>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-4900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-4900/>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</a></strong><br><a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p></div><hr><div id=w19-50><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-50.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-50/>Proceedings of the 18th BioNLP Workshop and Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5000/>Proceedings of the 18th BioNLP Workshop and Shared Task</a></strong><br><a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5002/>Learning from the Experience of Doctors : Automated Diagnosis of Appendicitis Based on Clinical Notes</a></strong><br><a href=/people/s/steven-kester-yuwono/>Steven Kester Yuwono</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a>
|
<a href=/people/k/kee-yuan-ngiam/>Kee Yuan Ngiam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5002><div class="card-body p-3 small">The objective of this work is to develop an automated diagnosis system that is able to predict the probability of appendicitis given a free-text emergency department (ED) note and additional structured information (e.g., lab test results). Our clinical corpus consists of about 180,000 ED notes based on ten years of patient visits to the Accident and Emergency (A&E) Department of the National University Hospital (NUH), Singapore. We propose a novel neural network approach that learns to diagnose <a href=https://en.wikipedia.org/wiki/Appendicitis>acute appendicitis</a> based on doctors&#8217; free-text ED notes without any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. On a test set of 2,000 ED notes with equal number of appendicitis (positive) and non-appendicitis (negative) diagnosis and in which all the negative ED notes only consist of abdominal-related diagnosis, our model is able to achieve a promising F_0.5-score of 0.895 while <a href=https://en.wikipedia.org/wiki/Emergency_department>ED doctors</a> achieve F_0.5-score of 0.900. Visualization shows that our model is able to learn important features, signs, and symptoms of patients from unstructured free-text ED notes, which will help doctors to make better diagnosis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5003/>A Paraphrase Generation System for EHR Question Answering<span class=acl-fixed-case>EHR</span> Question Answering</a></strong><br><a href=/people/s/sarvesh-soni/>Sarvesh Soni</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5003><div class="card-body p-3 small">This paper proposes a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and method for automatically generating paraphrases for clinical questions relating to patient-specific information in electronic health records (EHRs). Crowdsourcing is used to collect 10,578 unique questions across 946 semantically distinct paraphrase clusters. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is then used with a deep learning-based question paraphrasing method utilizing variational autoencoder and LSTM encoder / decoder. The ultimate use of such a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is to improve the performance of automatic question answering methods for <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5006/>Transfer Learning in Biomedical Natural Language Processing : An Evaluation of BERT and ELMo on Ten Benchmarking Datasets<span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>ELM</span>o on Ten Benchmarking Datasets</a></strong><br><a href=/people/y/yifan-peng/>Yifan Peng</a>
|
<a href=/people/s/shankai-yan/>Shankai Yan</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5006><div class="card-body p-3 small">Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> consists of five tasks with ten <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on <a href=https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor>BERT</a> and ELMo and find that the <a href=https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor>BERT model</a> pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp / BLUE_Benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5007/>Combining Structured and Free-text Electronic Medical Record Data for Real-time Clinical Decision Support</a></strong><br><a href=/people/e/emilia-apostolova/>Emilia Apostolova</a>
|
<a href=/people/t/tony-wang/>Tony Wang</a>
|
<a href=/people/t/tim-tschampel/>Tim Tschampel</a>
|
<a href=/people/i/ioannis-koutroulis/>Ioannis Koutroulis</a>
|
<a href=/people/t/tom-velez/>Tom Velez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5007><div class="card-body p-3 small">The goal of this work is to utilize Electronic Medical Record (EMR) data for real-time Clinical Decision Support (CDS). We present a deep learning approach to combining in real time available diagnosis codes (ICD codes) and free-text notes : Patient Context Vectors. Patient Context Vectors are created by averaging ICD code embeddings, and by predicting the same from free-text notes via a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a>. The Patient Context Vectors were then simply appended to available structured data (vital signs and lab results) to build prediction models for a specific condition. Experiments on predicting ARDS, a rare and complex condition, demonstrate the utility of Patient Context Vectors as a means of summarizing the patient history and overall condition, and improve significantly the prediction model results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5010/>Deep Contextualized Biomedical Abbreviation Expansion</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/j/jinling-liu/>Jinling Liu</a>
|
<a href=/people/x/xinghua-lu/>Xinghua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5010><div class="card-body p-3 small">Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications, such as <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> and question answering systems. In this paper, we present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model. DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple <a href=https://en.wikipedia.org/wiki/Heuristic>heuristic</a>. Then it utilizes BioELMo to extract the contextualized features of words, and feed those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to abbreviation-specific bidirectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions. Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It also surpasses human performance for expanding a sample abbreviation, and remains robust in imbalanced, low-resources and clinical settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5011/>RNN Embeddings for Identifying Difficult to Understand Medical Words<span class=acl-fixed-case>RNN</span> Embeddings for Identifying Difficult to Understand Medical Words</a></strong><br><a href=/people/h/hanna-pylieva/>Hanna Pylieva</a>
|
<a href=/people/a/artem-chernodub/>Artem Chernodub</a>
|
<a href=/people/n/natalia-grabar/>Natalia Grabar</a>
|
<a href=/people/t/thierry-hamon/>Thierry Hamon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5011><div class="card-body p-3 small">Patients and their families often require a better understanding of medical information provided by doctors. We currently address this issue by improving the identification of difficult to understand medical words. We introduce novel embeddings received from RNN-FrnnMUTE (French RNN Medical Understandability Text Embeddings) which allow to reach up to 87.0 F1 score in identification of difficult words. We also note that adding pre-trained FastText word embeddings to the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature set</a> substantially improves the performance of the model which classifies words according to their difficulty. We study the generalizability of different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> through three cross-validation scenarios which allow testing <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> in real-world conditions : understanding of medical words by new users, and classification of new unseen words by the automatic models. The RNN-FrnnMUTE embeddings and the categorization code are being made available for the research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5012/>A distantly supervised dataset for <a href=https://en.wikipedia.org/wiki/Data_extraction>automated data extraction</a> from diagnostic studies</a></strong><br><a href=/people/c/christopher-norman/>Christopher Norman</a>
|
<a href=/people/m/mariska-leeflang/>Mariska Leeflang</a>
|
<a href=/people/r/rene-spijker/>René Spijker</a>
|
<a href=/people/e/evangelos-kanoulas/>Evangelos Kanoulas</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5012><div class="card-body p-3 small">Systematic reviews are important in <a href=https://en.wikipedia.org/wiki/Evidence-based_medicine>evidence based medicine</a>, but are expensive to produce. Automating or semi-automating the data extraction of index test, target condition, and reference standard from articles has the potential to decrease the cost of conducting <a href=https://en.wikipedia.org/wiki/Systematic_review>systematic reviews</a> of diagnostic test accuracy, but relevant training data is not available. We create a distantly supervised dataset of approximately 90,000 sentences, and let two experts manually annotate a small subset of around 1,000 sentences for evaluation. We evaluate the performance of BioBERT and logistic regression for <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> the sentences, and compare the performance for distant and direct supervision. Our results suggest that distant supervision can work as well as, or better than direct supervision on this problem, and that distantly trained models can perform as well as, or better than human annotators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5015 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5015/>A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics</a></strong><br><a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/r/ross-sparks/>Ross Sparks</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a>
|
<a href=/people/c/c-raina-macintyre/>C Raina MacIntyre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5015><div class="card-body p-3 small">Distributed representations of text can be used as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> when training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>statistical classifier</a>. These <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems : influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and <a href=https://en.wikipedia.org/wiki/FLAIR>FLAIR</a> are better than Word2Vec, <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a> and the two adapted using the MESH ontology. There is an improvement of 2-4 % in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> when these context-based representations are used instead of word-based representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5021/>Annotating Temporal Information in Clinical Notes for Timeline Reconstruction : Towards the Definition of Calendar Expressions</a></strong><br><a href=/people/n/natalia-viani/>Natalia Viani</a>
|
<a href=/people/h/hegler-tissot/>Hegler Tissot</a>
|
<a href=/people/a/ariane-bernardino/>Ariane Bernardino</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5021><div class="card-body p-3 small">To automatically analyse complex trajectory information enclosed in clinical text (e.g. timing of symptoms, duration of treatment), it is important to understand the related temporal aspects, anchoring each event on an absolute point in time. In the clinical domain, few temporally annotated corpora are currently available. Moreover, underlying annotation schemas-which mainly rely on the TimeML standard-are not necessarily easily applicable for applications such as patient timeline reconstruction. In this work, we investigated how temporal information is documented in clinical text by annotating a corpus of medical reports with time expressions (TIMEXes), based on <a href=https://en.wikipedia.org/wiki/TimeML>TimeML</a>. The developed <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is available to the NLP community. Starting from our annotations, we analysed the suitability of the TimeML TIMEX schema for capturing timeline information, identifying challenges and possible solutions. As a result, we propose a novel annotation schema that could be useful for timeline reconstruction : CALendar EXpression (CALEX).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5023 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5023/>Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding<span class=acl-fixed-case>PIO</span> Element Detection in Medical Text Using Contextualized Embedding</a></strong><br><a href=/people/h/hichem-mezaoui/>Hichem Mezaoui</a>
|
<a href=/people/i/isuru-gunasekara/>Isuru Gunasekara</a>
|
<a href=/people/a/aleksandr-gontcharov/>Aleksandr Gontcharov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5023><div class="card-body p-3 small">In this paper, we presented an improved <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to extract PIO elements, from abstracts of medical papers, that reduces <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>. The proposed technique was used to build a dataset of PIO elements that we call <a href=https://en.wikipedia.org/wiki/Piconet>PICONET</a>. We further proposed a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a>, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context. Furthermore, to enhance the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the model, we have investigated an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>text features</a> were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> when using the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with one of the base learners and the case where we combine the base learners along with the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. We obtained the highest score in terms of <a href=https://en.wikipedia.org/wiki/Analysis_of_covariance>AUC</a> when we combine the base learners. The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to <a href=https://en.wikipedia.org/wiki/Medical_literature>full medical articles</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5025 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5025/>Can Character Embeddings Improve Cause-of-Death Classification for Verbal Autopsy Narratives?</a></strong><br><a href=/people/z/zhaodong-yan/>Zhaodong Yan</a>
|
<a href=/people/s/serena-jeblee/>Serena Jeblee</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5025><div class="card-body p-3 small">We present two models for combining word and character embeddings for cause-of-death classification of verbal autopsy reports using the text of the narratives. We find that for smaller datasets (500 to 1000 records), adding character information to the model improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, making character-based CNNs a promising method for automated verbal autopsy coding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5026/>Is artificial data useful for biomedical Natural Language Processing algorithms?</a></strong><br><a href=/people/z/zixu-wang/>Zixu Wang</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/s/sumithra-velupillai/>Sumithra Velupillai</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5026><div class="card-body p-3 small">A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility. This problem can be addressed by generating medical data artificially. Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited. We propose a generic <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to guide the generation of clinical text with key phrases. We use the artificial data as additional training data in two key biomedical NLP tasks : <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and temporal relation extraction. We show that artificially generated training data used in conjunction with real training data can lead to performance boosts for data-greedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5027 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5027" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5027/>ChiMed : A Chinese Medical Corpus for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a><span class=acl-fixed-case>C</span>hi<span class=acl-fixed-case>M</span>ed: A <span class=acl-fixed-case>C</span>hinese Medical Corpus for Question Answering</a></strong><br><a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/w/weicheng-ma/>Weicheng Ma</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/y/yan-song/>Yan Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5027><div class="card-body p-3 small">Question answering (QA) is a challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>, especially when it is applied to specific domains. While <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained in the <a href=https://en.wikipedia.org/wiki/Domain_(biology)>general domain</a> can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such <a href=https://en.wikipedia.org/wiki/Data>data</a> are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed ; second we annotate a small fraction of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to check the quality of the answers ; third, we extract two datasets from the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and use them for the relevancy prediction task and the adoption prediction task. Several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark models</a> are applied to the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, producing good results for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5038 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5038/>Extracting relations between outcomes and <a href=https://en.wikipedia.org/wiki/Statistical_significance>significance levels</a> in Randomized Controlled Trials (RCTs) publications<span class=acl-fixed-case>RCT</span>s) publications</a></strong><br><a href=/people/a/anna-koroleva/>Anna Koroleva</a>
|
<a href=/people/p/patrick-paroubek/>Patrick Paroubek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5038><div class="card-body p-3 small">Randomized controlled trials assess the effects of an experimental intervention by comparing it to a control intervention with regard to some variables-trial outcomes. Statistical hypothesis testing is used to test if the <a href=https://en.wikipedia.org/wiki/Design_of_experiments>experimental intervention</a> is superior to the <a href=https://en.wikipedia.org/wiki/Scientific_control>control</a>. Statistical significance is typically reported for the measured outcomes and is an important characteristic of the results. We propose a machine learning approach to automatically extract reported outcomes, <a href=https://en.wikipedia.org/wiki/Statistical_significance>significance levels</a> and the relation between them. We annotated a corpus of 663 sentences with 2,552 outcome-significance level relations (1,372 positive and 1,180 negative relations). We compared several <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, using a manually crafted feature set, and a number of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. The best performance (F-measure of 94 %) was shown by the BioBERT fine-tuned model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5039 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5039/>Overview of the MEDIQA 2019 Shared Task on Textual Inference, <a href=https://en.wikipedia.org/wiki/Question_answering>Question Entailment</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a><span class=acl-fixed-case>MEDIQA</span> 2019 Shared Task on Textual Inference, Question Entailment and Question Answering</a></strong><br><a href=/people/a/asma-ben-abacha/>Asma Ben Abacha</a>
|
<a href=/people/c/chaitanya-shivade/>Chaitanya Shivade</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5039><div class="card-body p-3 small">This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks : Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98 % in the NLI task, 74.9 % in the RQE task, and 78.3 % in the QA task. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and the participants&#8217; approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> in the medical domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5043 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5043/>Surf at MEDIQA 2019 : Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model<span class=acl-fixed-case>MEDIQA</span> 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model</a></strong><br><a href=/people/j/jiin-nam/>Jiin Nam</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5043><div class="card-body p-3 small">While deep learning techniques have shown promising results in many natural language processing (NLP) tasks, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has not been widely applied to the clinical domain. The lack of <a href=https://en.wikipedia.org/wiki/Data_set>large datasets</a> and the pervasive use of <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific language</a> (i.e. abbreviations and acronyms) in the clinical domain causes slower progress in NLP tasks than that of the general NLP tasks. To fill this gap, we employ word / subword-level based models that adopt large-scale data-driven methods such as pre-trained language models and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> in analyzing text for the clinical domain. Empirical results demonstrate the superiority of the proposed <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>methods</a> by achieving 90.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>medical domain natural language inference task</a>. Furthermore, we inspect the independent strengths of the proposed approaches in quantitative and qualitative manners. This analysis will help researchers to select necessary components in building <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5044 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5044/>WTMED at MEDIQA 2019 : A Hybrid Approach to Biomedical Natural Language Inference<span class=acl-fixed-case>WTMED</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: A Hybrid Approach to Biomedical Natural Language Inference</a></strong><br><a href=/people/z/zhaofeng-wu/>Zhaofeng Wu</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/s/sicong-huang/>Sicong Huang</a>
|
<a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5044><div class="card-body p-3 small">Natural language inference (NLI) is challenging, especially when <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is applied to technical domains such as <a href=https://en.wikipedia.org/wiki/Biomedical_sciences>biomedical settings</a>. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5045 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5045/>KU_ai at MEDIQA 2019 : Domain-specific Pre-training and Transfer Learning for Medical NLI<span class=acl-fixed-case>KU</span>_ai at <span class=acl-fixed-case>MEDIQA</span> 2019: Domain-specific Pre-training and Transfer Learning for Medical <span class=acl-fixed-case>NLI</span></a></strong><br><a href=/people/c/cemil-cengiz/>Cemil Cengiz</a>
|
<a href=/people/u/ulas-sert/>Ulaş Sert</a>
|
<a href=/people/d/deniz-yuret/>Deniz Yuret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5045><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> and results submitted for the Natural Language Inference (NLI) track of the MEDIQA 2019 Shared Task. As KU_ai team, we used BERT as our baseline model and pre-processed the MedNLI dataset to mitigate the negative impact of de-identification artifacts. Moreover, we investigated different pre-training and transfer learning approaches to improve the performance. We show that pre-training the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> on rich biomedical corpora has a significant effect in teaching the model domain-specific language. In addition, training the model on large NLI datasets such as MultiNLI and SNLI helps in learning task-specific reasoning. Finally, we ensembled our highest-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and achieved 84.7 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the unseen test dataset and ranked 10th out of 17 teams in the official results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5048 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5048/>Dr. Quad at MEDIQA 2019 : Towards Textual Inference and Question Entailment using contextualized representations<span class=acl-fixed-case>D</span>r.<span class=acl-fixed-case>Q</span>uad at <span class=acl-fixed-case>MEDIQA</span> 2019: Towards Textual Inference and Question Entailment using contextualized representations</a></strong><br><a href=/people/v/vinayshekhar-bannihatti-kumar/>Vinayshekhar Bannihatti Kumar</a>
|
<a href=/people/a/ashwin-srinivasan/>Ashwin Srinivasan</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/j/james-route/>James Route</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5048><div class="card-body p-3 small">This paper presents the submissions by TeamDr. Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on the prior work Liu et al. (2019) which uses a multi-task objective function for <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> through <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> is a powerful strategy for addressing challenges posed specialized domains such as <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5049 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5049/>Sieg at MEDIQA 2019 : Multi-task Neural Ensemble for Biomedical Inference and Entailment<span class=acl-fixed-case>MEDIQA</span> 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment</a></strong><br><a href=/people/s/sai-abishek-bhaskar/>Sai Abishek Bhaskar</a>
|
<a href=/people/r/rashi-rungta/>Rashi Rungta</a>
|
<a href=/people/j/james-route/>James Route</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5049><div class="card-body p-3 small">This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomedical domain. Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH. We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn better biomedical feature representations. Our final models for the NLI and RQE tasks achieve the 4th and 2nd rank on the shared-task leaderboard respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5052 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5052/>MSIT_SRIB at MEDIQA 2019 : Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain.<span class=acl-fixed-case>MSIT</span>_<span class=acl-fixed-case>SRIB</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain.</a></strong><br><a href=/people/s/sahil-chopra/>Sahil Chopra</a>
|
<a href=/people/a/ankita-gupta/>Ankita Gupta</a>
|
<a href=/people/a/anupama-kaushik/>Anupama Kaushik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5052><div class="card-body p-3 small">In this paper, we present Biomedical Multi-Task Deep Neural Network (Bio-MTDNN) on the NLI task of MediQA 2019 challenge. Bio-MTDNN utilizes transfer learning based paradigm where not only the source and target domains are different but also the source and target tasks are varied, although related. Further, Bio-MTDNN integrates knowledge from external sources such as clinical databases (UMLS) enhancing its performance on the clinical domain. Our proposed method outperformed the official baseline and other prior models (such as ESIM and Infersent on dev set) by a considerable margin as evident from our experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5056 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5056/>IITP at MEDIQA 2019 : Systems Report for Natural Language Inference, Question Entailment and Question Answering<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: Systems Report for Natural Language Inference, Question Entailment and Question Answering</a></strong><br><a href=/people/d/dibyanayan-bandyopadhyay/>Dibyanayan Bandyopadhyay</a>
|
<a href=/people/b/baban-gain/>Baban Gain</a>
|
<a href=/people/t/tanik-saikh/>Tanik Saikh</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5056><div class="card-body p-3 small">This paper presents the experiments accomplished as a part of our participation in the MEDIQA challenge, an (Abacha et al., 2019) shared task. We participated in all the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> defined in this particular <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are viz. i. Natural Language Inference (NLI) ii. Recognizing Question Entailment(RQE) and their application in medical Question Answering (QA). We submitted runs using multiple deep learning based systems (runs) for each of these three tasks. We submitted five <a href=https://en.wikipedia.org/wiki/System>system</a> results in each of the NLI and RQE tasks, and four system results for the QA task. The <a href=https://en.wikipedia.org/wiki/System>systems</a> yield encouraging results in all the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The highest performance obtained in NLI, RQE and QA tasks are 81.8 %, 53.2 %, and 71.7 %, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5057 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5057/>LasigeBioTM at MEDIQA 2019 : Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition<span class=acl-fixed-case>L</span>asige<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>TM</span> at <span class=acl-fixed-case>MEDIQA</span> 2019: Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition</a></strong><br><a href=/people/a/andre-lamurias/>Andre Lamurias</a>
|
<a href=/people/f/francisco-m-couto/>Francisco M Couto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5057><div class="card-body p-3 small">Biomedical Question Answering (QA) aims at providing automated answers to user questions, regarding a variety of biomedical topics. For example, these questions may ask for related to <a href=https://en.wikipedia.org/wiki/Disease>diseases</a>, <a href=https://en.wikipedia.org/wiki/Drug>drugs</a>, <a href=https://en.wikipedia.org/wiki/Symptom>symptoms</a>, or <a href=https://en.wikipedia.org/wiki/Medical_procedure>medical procedures</a>. Automated biomedical QA systems could improve the retrieval of information necessary to answer these questions. The MEDIQA challenge consisted of three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> concerning various aspects of biomedical QA. This challenge aimed at advancing approaches to Natural Language Inference (NLI) and Recognizing Question Entailment (RQE), which would then result in enhanced approaches to biomedical QA. Our approach explored a common Transformer-based architecture that could be applied to each <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. This approach shared the same pre-trained weights, but which were then fine-tuned for each task using the provided training data. Furthermore, we augmented the training data with external datasets and enriched the question and answer texts using MER, a named entity recognition tool. Our approach obtained high levels of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, in particular on the NLI task, which classified pairs of text according to their relation. For the QA task, we obtained higher Spearman&#8217;s rank correlation values using the entities recognized by MER.</div></div></div><hr><div id=w19-51><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-51.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-51/>Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5100/>Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra Escartín</a>
|
<a href=/people/f/francis-bond/>Francis Bond</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5101/>When the whole is greater than the sum of its parts : Multiword expressions and <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a></a></strong><br><a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5101><div class="card-body p-3 small">Multiword expressions (MWEs) feature prominently in the mental lexicon of native speakers (Jackendoff, 1997) in all languages and domains, from informal to technical contexts (Biber et al., 1999) with about four MWEs being produced per minute of discourse (Glucksberg, 1989). MWEs come in all shapes and forms, including idioms like rock the boat (as cause problems or disturb a situation) and compound nouns like monkey business (as dishonest behaviour). Their accurate detection and understanding may often require more than knowledge about individual words and how they can be combined (Fillmore, 1979), as they may display various degrees of idiosyncrasy, including lexical, syntactic, semantic and statistical (Sag et al., 2002 ; Baldwin and Kim, 2010), which provide new challenges and opportunities for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a> (Constant et al., 2017). For instance, while for some combinations the meaning can be inferred from their parts like olive oil (oil made of olives) this is not always the case, as in dark horse (meaning an unknown candidate who unexpectedly succeeds), and when processing a sentence some of the challenges are to identify which words form an expression (Ramisch, 2015), and whether the expression is idiomatic (Cordeiro et al., 2019). In this talk I will give an overview of advances on the identification and treatment of multiword expressions, in particular concentrating on techniques for identifying their degree of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5102/>Hear about Verbal Multiword Expressions in the Bulgarian and the Romanian Wordnets Straight from the Horse’s Mouth<span class=acl-fixed-case>B</span>ulgarian and the <span class=acl-fixed-case>R</span>omanian Wordnets Straight from the Horse’s Mouth</a></strong><br><a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/i/ivelina-stoyanova/>Ivelina Stoyanova</a>
|
<a href=/people/s/svetlozara-leseva/>Svetlozara Leseva</a>
|
<a href=/people/m/maria-mitrofan/>Maria Mitrofan</a>
|
<a href=/people/t/tsvetana-dimitrova/>Tsvetana Dimitrova</a>
|
<a href=/people/m/maria-todorova/>Maria Todorova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5102><div class="card-body p-3 small">In this paper we focus on verbal multiword expressions (VMWEs) in <a href=https://en.wikipedia.org/wiki/Bulgarian_language>Bulgarian</a> and Romanian as reflected in the wordnets of the two languages. The <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of VMWEs relies on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> defined within the PARSEME Cost Action. After outlining the properties of various types of VMWEs, a cross-language comparison is drawn, aimed to highlight the similarities and the differences between <a href=https://en.wikipedia.org/wiki/Bulgarian_language>Bulgarian</a> and Romanian with respect to the <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a> and distribution of VMWEs. The contribution of this work is in outlining essential features of the description and classification of VMWEs and the cross-language comparison at the lexical level, which is essential for the understanding of the need for uniform annotation guidelines and a viable procedure for validation of the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5103/>The Romanian Corpus Annotated with Verbal Multiword Expressions<span class=acl-fixed-case>R</span>omanian Corpus Annotated with Verbal Multiword Expressions</a></strong><br><a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/m/mihaela-cristescu/>Mihaela Cristescu</a>
|
<a href=/people/m/mihaela-plamada-onofrei/>Mihaela Onofrei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5103><div class="card-body p-3 small">This paper reports on the Romanian journalistic corpus annotated with verbal multiword expressions following the PARSEME guidelines. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is sentence split, tokenized, part-of-speech tagged, lemmatized, syntactically annotated and verbal multiword expressions are identified and classified. It offers insights into the frequency of such Romanian word combinations and allows for their characterization. We offer data about the types of verbal multiword expressions in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and some of their characteristics, such as internal structure, diversity in the corpus, average length, productivity of the verbs. This is a language resource that is important per se, as well as for the task of automatic multiword expressions identification, which can be further used in other systems. It was already used as training and test material in the shared tasks for the automatic identification of verbal multiword expressions organized by PARSEME.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5106/>Unsupervised Compositional Translation of Multiword Expressions</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a>
|
<a href=/people/m/marcos-garcia/>Marcos Garcia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5106><div class="card-body p-3 small">This article describes a dependency-based strategy that uses compositional distributional semantics and cross-lingual word embeddings to translate multiword expressions (MWEs). Our unsupervised approach performs <a href=https://en.wikipedia.org/wiki/Translation>translation</a> as a process of word contextualization by taking into account lexico-syntactic contexts and selectional preferences. This <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> is suited to translate phraseological combinations and phrases whose constituent words are lexically restricted by each other. Several experiments in adjective-noun and verb-object compounds show that mutual contextualization (co-compositionality) clearly outperforms other compositional methods. The paper also contributes with a new freely available dataset of English-Spanish MWEs used to validate the proposed compositional strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5110/>Without <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, multiword expression identification will never fly : A position statement</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/s/silvio-cordeiro/>Silvio Cordeiro</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5110><div class="card-body p-3 small">Because most multiword expressions (MWEs), especially verbal ones, are semantically non-compositional, their automatic identification in running text is a prerequisite for semantically-oriented downstream applications. However, recent developments, driven notably by the PARSEME shared task on automatic identification of verbal MWEs, show that this task is harder than related tasks, despite recent contributions both in multilingual corpus annotation and in computational models. In this paper, we analyse possible reasons for this state of affairs. They lie in the nature of the MWE phenomenon, as well as in its <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional properties</a>. We also offer a comparative analysis of the state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a>, which exhibit particularly strong sensitivity to unseen data. On this basis, we claim that, in order to make strong headway in MWE identification, the community should bend its mind into coupling identification of MWEs with their discovery, via syntactic MWE lexicons. Such lexicons need not necessarily achieve a linguistically complete modelling of MWEs&#8217; behavior, but they should provide minimal morphosyntactic information to cover some potential uses, so as to complement existing MWE-annotated corpora. We define requirements for such minimal NLP-oriented lexicon, and we propose a roadmap for the MWE community driven by these requirements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5112/>Semantic Modelling of Adjective-Noun Collocations Using FrameNet<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/y/yana-strakatova/>Yana Strakatova</a>
|
<a href=/people/e/erhard-hinrichs/>Erhard Hinrichs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5112><div class="card-body p-3 small">In this paper we argue that <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>Frame Semantics</a> (Fillmore, 1982) provides a good framework for semantic modelling of adjective-noun collocations. More specifically, the notion of a frame is rich enough to account for nouns from different semantic classes and to model semantic relations that hold between an adjective and a <a href=https://en.wikipedia.org/wiki/Noun>noun</a> in terms of Frame Elements. We have substantiated these findings by considering a sample of adjective-noun collocations from <a href=https://en.wikipedia.org/wiki/German_language>German</a> such as enger Freund &#8216;close friend&#8217; and starker Regen &#8216;heavy rain&#8217;. The data sample is taken from different semantic fields identified in the German wordnet GermaNet (Hamp and Feldweg, 1997 ; Henrich and Hinrichs, 2010). The study is based on the electronic dictionary DWDS (Klein and Geyken, 2010) and uses the collocation extraction tool Wortprofil (Geyken et al., 2009). The FrameNet modelling is based on the online resource available at http://framenet.icsi.berkeley.edu. Since FrameNets are available for a range of typologically different languages, it is feasible to extend the current case study to other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5114/>Confirming the Non-compositionality of Idioms for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/a/alyssa-hwang/>Alyssa Hwang</a>
|
<a href=/people/c/christopher-hidey/>Christopher Hidey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5114><div class="card-body p-3 small">An <a href=https://en.wikipedia.org/wiki/Idiom>idiom</a> is defined as a non-compositional multiword expression, one whose meaning can not be deduced from the definitions of the component words. This definition does not explicitly define the compositionality of an <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom&#8217;s sentiment</a> ; this paper aims to determine whether the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> of the component words of an <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom</a> is related to the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> of that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiom</a>. We use the Dictionary of Affect in Language augmented by <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> to give each idiom in the Sentiment Lexicon of IDiomatic Expressions (SLIDE) a component-wise sentiment score and compare it to the phrase-level sentiment label crowdsourced by the creators of SLIDE. We find that there is no discernible relation between these two measures of idiom sentiment. This supports the hypothesis that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> are not compositional for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> along with <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and motivates further work in handling <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5115/>IDION : A database for Modern Greek multiword expressions<span class=acl-fixed-case>IDION</span>: A database for <span class=acl-fixed-case>M</span>odern <span class=acl-fixed-case>G</span>reek multiword expressions</a></strong><br><a href=/people/s/stella-markantonatou/>Stella Markantonatou</a>
|
<a href=/people/p/panagiotis-minos/>Panagiotis Minos</a>
|
<a href=/people/g/george-zakis/>George Zakis</a>
|
<a href=/people/v/vassiliki-moutzouri/>Vassiliki Moutzouri</a>
|
<a href=/people/m/maria-chantou/>Maria Chantou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5115><div class="card-body p-3 small">We report on the ongoing development of IDION, a web resource of richly documented multiword expressions (MWEs) of Modern Greek addressed to the human user and to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. IDION contains about 2000 verb MWEs (VMWEs) of which about 850 are fully documented as regards their syntactic flexibility, their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and the semantic relations with other VMWEs. Sets of synonymous MWEs are defined in a bottom-up manner revealing the conceptual organization of the MG VMWE domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5118/>Evaluating Automatic Term Extraction Methods on Individual Documents</a></strong><br><a href=/people/a/antonio-sajatovic/>Antonio Šajatović</a>
|
<a href=/people/m/maja-buljan/>Maja Buljan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/b/bojana-dalbelo-basic/>Bojana Dalbelo Bašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5118><div class="card-body p-3 small">Automatic Term Extraction (ATE) extracts terminology from <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific corpora</a>. ATE is used in many NLP tasks, including <a href=https://en.wikipedia.org/wiki/Computer-aided_translation>Computer Assisted Translation</a>, where it is typically applied to individual documents rather than the entire corpus. While corpus-level ATE has been extensively evaluated, it is not obvious how the results transfer to document-level ATE. To fill this gap, we evaluate 16 state-of-the-art ATE methods on full-length documents from three different domains, on both corpus and document levels. Unlike existing studies, our evaluation is more realistic as we take into account all gold terms. We show that no single method is best in corpus-level ATE, but C-Value and KeyConceptRelatendess surpass others in document-level ATE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5119 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5119/>Cross-lingual Transfer Learning and <a href=https://en.wikipedia.org/wiki/Multitask_learning>Multitask Learning</a> for Capturing Multiword Expressions</a></strong><br><a href=/people/s/shiva-taslimipoor/>Shiva Taslimipoor</a>
|
<a href=/people/o/omid-rohanian/>Omid Rohanian</a>
|
<a href=/people/l/le-an-ha/>Le An Ha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5119><div class="card-body p-3 small">Recent developments in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> have prompted a surge of interest in the application of multitask and transfer learning to NLP problems. In this study, we explore for the first time, the application of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning (TRL)</a> and <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning (MTL)</a> to the identification of Multiword Expressions (MWEs). For MTL, we exploit the shared syntactic information between MWE and dependency parsing models to jointly train a single model on both tasks. We specifically predict two types of labels : MWE and dependency parse. Our neural MTL architecture utilises the supervision of dependency parsing in lower layers and predicts MWE tags in upper layers. In the TRL scenario, we overcome the scarcity of data by learning a model on a larger MWE dataset and transferring the knowledge to a resource-poor setting in another language. In both scenarios, the resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieved higher performance compared to standard neural approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5120/>Ilfhocail : A Lexicon of Irish MWEs<span class=acl-fixed-case>I</span>lfhocail: A Lexicon of <span class=acl-fixed-case>I</span>rish <span class=acl-fixed-case>MWE</span>s</a></strong><br><a href=/people/a/abigail-walsh/>Abigail Walsh</a>
|
<a href=/people/t/teresa-lynn/>Teresa Lynn</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5120><div class="card-body p-3 small">This paper describes the categorisation of Irish MWEs, and the construction of the first version of a lexicon of Irish MWEs for NLP purposes (Ilfhocail, meaning &#8216;Multiwords&#8217;), collected from a number of resources. For the purposes of <a href=https://en.wikipedia.org/wiki/Quality_assurance>quality assurance</a>, 530 entries of this <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> were examined and manually annotated for <a href=https://en.wikipedia.org/wiki/Product_lifecycle>POS information</a> and <a href=https://en.wikipedia.org/wiki/Product_lifecycle>MWE category</a>.</div></div></div><hr><div id=w19-52><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-52.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-52/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5200/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5203/>Incorporating <a href=https://en.wikipedia.org/wiki/Source_code>Source Syntax</a> into Transformer-Based Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5203><div class="card-body p-3 small">Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018 ; Tang et al., 2018 ; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> into the Transformer architecture without modifying it. We introduce two methods : a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into twenty target languages, showing consistent improvements of 1.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5205/>Generalizing Back-Translation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/miguel-graca/>Miguel Graça</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/j/julian-schamper/>Julian Schamper</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5205><div class="card-body p-3 small">Back-translation data augmentation by translating target monolingual data is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling-based approaches</a> and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German-English news translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5208.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5208.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5208/>The Effect of Translationese in Machine Translation Test Sets</a></strong><br><a href=/people/m/mike-zhang/>Mike Zhang</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5208><div class="card-body p-3 small">The effect of translationese has been studied in the field of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>, mostly with respect to training data. We study in depth the effect of translationese on <a href=https://en.wikipedia.org/wiki/Test_data>test data</a>, using the test sets from the last three editions of WMT&#8217;s news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems ; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5209/>Customizing Neural Machine Translation for <a href=https://en.wikipedia.org/wiki/Subtitling>Subtitling</a></a></strong><br><a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/p/patrick-wilken/>Patrick Wilken</a>
|
<a href=/people/y/yota-georgakopoulou/>Yota Georgakopoulou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5209><div class="card-body p-3 small">In this work, we customized a neural machine translation system for translation of subtitles in the domain of <a href=https://en.wikipedia.org/wiki/Entertainment>entertainment</a>. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> learned from human segmentation decisions. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a <a href=https://en.wikipedia.org/wiki/Documentary_film>documentary</a> and a sitcom). It showed a notable productivity increase of up to 37 % as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5210/>Integration of Dubbing Constraints into <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/a/ashutosh-saboo/>Ashutosh Saboo</a>
|
<a href=/people/t/timo-baumann/>Timo Baumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5210><div class="card-body p-3 small">Translation systems aim to perform a meaning-preserving conversion of linguistic material (typically text but also speech) from a source to a target language (and, to a lesser degree, the corresponding socio-cultural contexts). Dubbing, i.e., the lip-synchronous translation and revoicing of speech adds to this constraints about the close matching of phonetic and resulting visemic synchrony characteristics of source and target material. There is an inherent conflict between a <a href=https://en.wikipedia.org/wiki/Translation>translation</a>&#8217;s meaning preservation and &#8216;dubbability&#8217; and the resulting trade-off can be controlled by weighing the synchrony constraints. We introduce our work, which to the best of our knowledge is the first of its kind, on integrating synchrony constraints into the machine translation paradigm. We present first results for the integration of synchrony constraints into encoder decoder-based neural machine translation and show that considerably more &#8216;dubbable&#8217; translations can be achieved with only a small impact on <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a>, and dubbability improves more steeply than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> degrades.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5211/>Widening the Representation Bottleneck in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Lexical Shortcuts</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5211><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> is a state-of-the-art neural translation model that uses <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. This enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical information</a> passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5212.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5212/>A High-Quality Multilingual Dataset for Structured Documentation Translation</a></strong><br><a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/r/raffaella-buschiazzo/>Raffaella Buschiazzo</a>
|
<a href=/people/j/james-bradbury/>James Bradbury</a>
|
<a href=/people/t/teresa-marshall/>Teresa Marshall</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5212><div class="card-body p-3 small">This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These <a href=https://en.wikipedia.org/wiki/Web_page>Web pages</a> have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from <a href=https://en.wikipedia.org/wiki/English_language>English</a>, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 16 translation settings. Our experiments show that learning to translate with the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>XML tags</a> improves translation accuracy, and the <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> accurately generates <a href=https://en.wikipedia.org/wiki/XML>XML structures</a>. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>.</div></div></div><hr><div id=w19-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-53/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5300/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5302.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5302/>Results of the WMT19 Metrics Shared Task : Segment-Level and Strong MT Systems Pose Big Challenges<span class=acl-fixed-case>WMT</span>19 Metrics Shared Task: Segment-Level and Strong <span class=acl-fixed-case>MT</span> Systems Pose Big Challenges</a></strong><br><a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5302><div class="card-body p-3 small">This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. 13 research groups submitted 24 metrics, 10 of which are reference-less metrics and constitute submissions to the joint task with WMT19 Quality Estimation Task, QE as a Metric. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> correlates with the WMT19 official manual ranking, and segment level, how well the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5303/>Findings of the First Shared Task on Machine Translation Robustness</a></strong><br><a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/p/paul-michel/>Paul Michel</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5303><div class="card-body p-3 small">We share the findings of the first shared task on improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>models&#8217; robustness</a> to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved large improvements over <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, with the best improvement having +22.33 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a>. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson&#8217;s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> using compare-mt, which revealed their salient differences in handling challenges in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Such analysis provides additional insights when there is occasional disagreement between <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, e.g. systems better at producing <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial expressions</a> received higher score from <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5304/>The University of Edinburgh’s Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/r/rachel-bawden/>Rachel Bawden</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/f/faheem-kirefu/>Faheem Kirefu</a>
|
<a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5304><div class="card-body p-3 small">The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions : <a href=https://en.wikipedia.org/wiki/English_language>EnglishGujarati</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>EnglishChinese</a>, <a href=https://en.wikipedia.org/wiki/German_language>GermanEnglish</a>, and <a href=https://en.wikipedia.org/wiki/Czech_language>EnglishCzech</a>. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For EnglishGujarati, we also explored <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised MT</a> with cross-lingual language model pre-training, and translation pivoting through <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. For <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to and from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, we investigated character-based tokenisation vs. sub-word segmentation of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese text</a>. For GermanEnglish, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. For EnglishCzech, we compared different preprocessing and tokenisation regimes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5305/>GTCOM Neural Machine Translation Systems for WMT19<span class=acl-fixed-case>GTCOM</span> Neural Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a>
|
<a href=/people/c/conghu-yuan/>Conghu Yuan</a>
|
<a href=/people/q/qingming-liu/>Qingming Liu</a>
|
<a href=/people/b/baoyong-fan/>Baoyong Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5305><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT19 shared news translation task. We participate in six directions : English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English. Further, we get the best BLEU scores in the directions of English to <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a> and Lithuanian to English (28.2 and 36.3 respectively) among all the participants. The submitted systems mainly focus on <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, knowledge distillation and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> to build a competitive model for this task. Also, we apply <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to filter <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>, back-translated data and parallel data. The techniques we apply for data filtering include filtering by <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a>, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5309/>DBMS-KU Interpolation for WMT19 News Translation Task<span class=acl-fixed-case>DBMS</span>-<span class=acl-fixed-case>KU</span> Interpolation for <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/s/sari-dewi-budiwati/>Sari Dewi Budiwati</a>
|
<a href=/people/a/al-hafiz-akbar-maulana-siagian/>Al Hafiz Akbar Maulana Siagian</a>
|
<a href=/people/t/tirana-noor-fatyanosa/>Tirana Noor Fatyanosa</a>
|
<a href=/people/m/masayoshi-aritsugi/>Masayoshi Aritsugi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5309><div class="card-body p-3 small">This paper presents the participation of DBMS-KU Interpolation system in WMT19 shared task, namely, Kazakh-English language pair. We examine the use of <a href=https://en.wikipedia.org/wiki/Interpolation>interpolation method</a> using a different language model order. Our Interpolation system combines a direct translation with <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> as a pivot language. We use 3-gram and 5-gram language model orders to perform the language translation in this work. To reduce noise in the pivot translation process, we prune the phrase table of source-pivot and pivot-target. Our experimental results show that our Interpolation system outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>Baseline</a> in terms of <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>BLEU-cased score</a> by +0.5 and +0.1 points in Kazakh-English and English-Kazakh, respectively. In particular, using the 5-gram language model order in our system could obtain better BLEU-cased score than utilizing the 3-gram one. Interestingly, we found that by employing the Interpolation system could reduce the perplexity score of English-Kazakh when using 3-gram language model order.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5310/>Lingua Custodia at WMT’19 : Attempts to Control Terminology<span class=acl-fixed-case>WMT</span>’19: Attempts to Control Terminology</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5310><div class="card-body p-3 small">This paper describes Lingua Custodia&#8217;s submission to the WMT&#8217;19 news shared task for German-to-French on the topic of the EU elections. We report experiments on the adaptation of the terminology of a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> to a specific topic, aimed at providing more accurate translations of specific entities like <a href=https://en.wikipedia.org/wiki/Political_party>political parties</a> and <a href=https://en.wikipedia.org/wiki/Personal_name>person names</a>, given that the shared task provided no in-domain training parallel data dealing with the restricted topic. Our primary submission to the shared task uses backtranslation generated with a type of decoding allowing the insertion of constraints in the output in order to guarantee the correct translation of specific terms that are not necessarily observed in the data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5311.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5311/>The TALP-UPC Machine Translation Systems for WMT19 News Translation Task : Pivoting Techniques for Low Resource MT<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 News Translation Task: Pivoting Techniques for Low Resource <span class=acl-fixed-case>MT</span></a></strong><br><a href=/people/n/noe-casas/>Noe Casas</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5311><div class="card-body p-3 small">In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for <a href=https://en.wikipedia.org/wiki/Kazakh_language>Kazakh-English</a>. Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively. Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5315/>UdS-DFKI Participation at WMT 2019 : Low-Resource (en-gu) and Coreference-Aware (en-de) Systems<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DFKI</span> Participation at <span class=acl-fixed-case>WMT</span> 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems</a></strong><br><a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5315><div class="card-body p-3 small">This paper describes the UdS-DFKI submission to the WMT2019 news translation task for GujaratiEnglish (low-resourced pair) and GermanEnglish (document-level evaluation). Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5316 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5316/>The IIIT-H Gujarati-English Machine Translation System for WMT19<span class=acl-fixed-case>IIIT</span>-<span class=acl-fixed-case>H</span> <span class=acl-fixed-case>G</span>ujarati-<span class=acl-fixed-case>E</span>nglish Machine Translation System for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/v/vikrant-goyal/>Vikrant Goyal</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5316><div class="card-body p-3 small">This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the GujaratiEnglish news translation shared task of WMT19. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is basedon encoder-decoder framework with <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5317/>Kingsoft’s Neural Machine Translation System for WMT19<span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/x/xinze-guo/>Xinze Guo</a>
|
<a href=/people/c/chang-liu/>Chang Liu</a>
|
<a href=/people/x/xiaolong-li/>Xiaolong Li</a>
|
<a href=/people/y/yiran-wang/>Yiran Wang</a>
|
<a href=/people/g/guoliang-li/>Guoliang Li</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a>
|
<a href=/people/z/zhitao-xu/>Zhitao Xu</a>
|
<a href=/people/l/liuyi-yang/>Liuyi Yang</a>
|
<a href=/people/l/li-ma/>Li Ma</a>
|
<a href=/people/c/changliang-li/>Changliang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5317><div class="card-body p-3 small">This paper describes the Kingsoft AI Lab&#8217;s submission to the WMT2019 news translation shared task. We participated in two language directions : <a href=https://en.wikipedia.org/wiki/English_language>English-Chinese</a> and <a href=https://en.wikipedia.org/wiki/Standard_Chinese>Chinese-English</a>. For both language directions, we trained several variants of Transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data. The best <a href=https://en.wikipedia.org/wiki/Translation>translation</a> result was obtained with <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensemble</a> and reranking techniques. According to automatic metrics (BLEU) our Chinese-English system reached the second highest score, and our English-Chinese system reached the second highest score for this subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5320.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5320/>The MLLP-UPV Supervised Machine Translation Systems for WMT19 News Translation Task<span class=acl-fixed-case>MLLP</span>-<span class=acl-fixed-case>UPV</span> Supervised Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/j/javier-iranzo-sanchez/>Javier Iranzo-Sánchez</a>
|
<a href=/people/g/goncal-garces-diaz-munio/>Gonçal Garcés Díaz-Munío</a>
|
<a href=/people/j/jorge-civera/>Jorge Civera</a>
|
<a href=/people/a/alfons-juan/>Alfons Juan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5320><div class="card-body p-3 small">This paper describes the participation of the MLLP research group of the Universitat Politcnica de Valncia in the WMT 2019 News Translation Shared Task. In this edition, we have submitted <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> for the German English and German French language pairs, participating in both directions of each pair. Our submitted systems, based on the Transformer architecture, make ample use of data filtering, synthetic data and <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> through <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5321.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5321/>Microsoft Translator at WMT 2019 : Towards Large-Scale Document-Level Neural Machine Translation<span class=acl-fixed-case>M</span>icrosoft Translator at <span class=acl-fixed-case>WMT</span> 2019: Towards Large-Scale Document-Level Neural Machine Translation</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5321><div class="card-body p-3 small">This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> seems to mainly help with translationese input. We explore <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning techniques</a>, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5322.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5322/>CUNI Submission for Low-Resource Languages in WMT News 2019<span class=acl-fixed-case>CUNI</span> Submission for Low-Resource Languages in <span class=acl-fixed-case>WMT</span> News 2019</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5322><div class="card-body p-3 small">This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages : Gujarati-English and Kazakh-English. We participated in both language pairs in both translation directions. Our system combines <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a different high-resource language pair followed by training on backtranslated monolingual data. Thanks to the simultaneous training in both directions, we can iterate the backtranslation process. We are using the Transformer model in a constrained submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5324/>A Comparison on Fine-grained Pre-trained Embeddings for the WMT19Chinese-English News Translation Task<span class=acl-fixed-case>WMT</span>19<span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish News Translation Task</a></strong><br><a href=/people/z/zhenhao-li/>Zhenhao Li</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5324><div class="card-body p-3 small">This paper describes our submission to the WMT 2019 Chinese-English (zh-en) news translation shared task. Our systems are based on RNN architectures with pre-trained embeddings which utilize character and sub-character information. We compare <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with these different granularity levels using different evaluating metics. We find that a finer granularity embeddings can help the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> according to character level evaluation and that the pre-trained embeddings can also be beneficial for <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performance marginally when the training data is limited.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5327 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5327/>Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring</a></strong><br><a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5327><div class="card-body p-3 small">This paper describes CAiRE&#8217;s submission to the unsupervised machine translation track of the WMT&#8217;19 news shared task from <a href=https://en.wikipedia.org/wiki/German_language>German</a> to <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> separately, and they are aligned using <a href=https://en.wikipedia.org/wiki/MUSE>MUSE</a> (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5330 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5330/>NICT’s Unsupervised Neural and Statistical Machine Translation Systems for the WMT19 News Translation Task<span class=acl-fixed-case>NICT</span>’s Unsupervised Neural and Statistical Machine Translation Systems for the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/h/haipeng-sun/>Haipeng Sun</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5330><div class="card-body p-3 small">This paper presents the NICT&#8217;s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction : <a href=https://en.wikipedia.org/wiki/German_language>German-Czech</a>. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (constraint&#8217;), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5333 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5333" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5333/>Facebook FAIR’s WMT19 News Translation Task Submission<span class=acl-fixed-case>F</span>acebook <span class=acl-fixed-case>FAIR</span>’s <span class=acl-fixed-case>WMT</span>19 News Translation Task Submission</a></strong><br><a href=/people/n/nathan-ng/>Nathan Ng</a>
|
<a href=/people/k/kyra-yee/>Kyra Yee</a>
|
<a href=/people/a/alexei-baevski/>Alexei Baevski</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5333><div class="card-body p-3 small">This paper describes Facebook FAIR&#8217;s submission to the WMT19 shared news translation task. We participate in four language directions, <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> and <a href=https://en.wikipedia.org/wiki/English_language>English-Russian</a> in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on domain-specific data, then decode using noisy channel model reranking. Our <a href=https://en.wikipedia.org/wiki/System>system</a> improves on our previous <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction EnglishRussian.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5335 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5335.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5335/>Tilde’s Machine Translation Systems for WMT 2019<span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/m/marcis-pinnis/>Marcis Pinnis</a>
|
<a href=/people/r/rihards-krislauks/>Rihards Krišlauks</a>
|
<a href=/people/m/matiss-rikters/>Matīss Rikters</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5335><div class="card-body p-3 small">The paper describes the development process of Tilde&#8217;s NMT systems for the WMT 2019 shared task on news translation. We trained systems for the English-Lithuanian and Lithuanian-English translation directions in constrained and unconstrained tracks. We build upon the best <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of the previous year&#8217;s competition and combine them with recent advancements in the field. We also present a new method to ensure source domain adherence in back-translated data. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved a shared first place in human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5336 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5336.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5336/>Apertium-fin-engRule-based Shallow Machine Translation for WMT 2019 Shared Task<span class=acl-fixed-case>WMT</span> 2019 Shared Task</a></strong><br><a href=/people/t/tommi-a-pirinen/>Tommi Pirinen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5336><div class="card-body p-3 small">In this paper we describe a rule-based, bi-directional machine translation system for the FinnishEnglish language pair. The baseline system was based on the existing data of FinnWordNet, omorfi and apertium-eng. We have built the <a href=https://en.wikipedia.org/wiki/Disambiguation>disambiguation</a>, lexical selection and translation rules by hand. The <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a> have been developed based on the shared task data. We describe in this article the use of the shared task data as a kind of a test-driven development workflow in RBMT development and show that it suits perfectly to a modern software engineering continuous integration workflow of RBMT and yields big increases to BLEU scores with minimal effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5337 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5337/>English-Czech Systems in WMT19 : Document-Level Transformer<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>zech Systems in <span class=acl-fixed-case>WMT</span>19: Document-Level Transformer</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/d/dominik-machacek/>Dominik Macháček</a>
|
<a href=/people/m/michal-auersperger/>Michal Auersperger</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5337><div class="card-body p-3 small">We describe our NMT systems submitted to the WMT19 shared task in EnglishCzech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2 T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2 T implementation, this document-level-trained system achieves a +0.6 BLEU improvement (p 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we can not draw any conclusions from this week evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5340 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5340/>CUED@WMT19 : EWC&LMs<span class=acl-fixed-case>CUED</span>@<span class=acl-fixed-case>WMT</span>19:<span class=acl-fixed-case>EWC</span>&<span class=acl-fixed-case>LM</span>s</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5340><div class="card-body p-3 small">Two techniques provide the fabric of the Cambridge University Engineering Department&#8217;s (CUED) entry to the WMT19 evaluation campaign : elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5342 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5342/>University of Tartu’s Multilingual Multi-domain WMT19 News Translation Shared Task Submission<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>T</span>artu’s Multilingual Multi-domain <span class=acl-fixed-case>WMT</span>19 News Translation Shared Task Submission</a></strong><br><a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/e/elizaveta-korotkova/>Elizaveta Korotkova</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5342><div class="card-body p-3 small">This paper describes the University of Tartu&#8217;s submission to the news translation shared task of WMT19, where the core idea was to train a single multilingual system to cover several language pairs of the shared task and submit its results. We only used the constrained data from the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>. We describe our approach and its results and discuss the technical issues we faced.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5344 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5344/>The LMU Munich Unsupervised Machine Translation System for WMT19<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation System for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5344><div class="card-body p-3 small">We describe LMU Munich&#8217;s machine translation system for GermanCzech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only from both languages. The final model is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised neural model</a> using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5345 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5345/>Combining Local and Document-Level Context : The LMU Munich Neural Machine Translation System at WMT19<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Neural Machine Translation System at <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5345><div class="card-body p-3 small">We describe LMU Munich&#8217;s machine translation system for EnglishGerman translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this <a href=https://en.wikipedia.org/wiki/Physical_model>model</a> by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5347 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5347/>The University of Helsinki Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/a/arvi-hurskainen/>Arvi Hurskainen</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5347><div class="card-body p-3 small">In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish-English</a>. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5352 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5352.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5352/>A Test Suite and Manual Evaluation of Document-Level NMT at WMT19<span class=acl-fixed-case>NMT</span> at <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/k/katerina-rysova/>Kateřina Rysová</a>
|
<a href=/people/m/magdalena-rysova/>Magdaléna Rysová</a>
|
<a href=/people/t/tomas-musil/>Tomáš Musil</a>
|
<a href=/people/l/lucie-polakova/>Lucie Poláková</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5352><div class="card-body p-3 small">As the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5355 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5355.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5355" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5355/>SAO WMT19 Test Suite : Machine Translation of Audit Reports<span class=acl-fixed-case>SAO</span> <span class=acl-fixed-case>WMT</span>19 Test Suite: Machine Translation of Audit Reports</a></strong><br><a href=/people/t/tereza-vojtechova/>Tereza Vojtěchová</a>
|
<a href=/people/m/michal-novak/>Michal Novák</a>
|
<a href=/people/m/milos-kloucek/>Miloš Klouček</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5355><div class="card-body p-3 small">This paper describes a machine translation test set of documents from the auditing domain and its use as one of the test suites in the WMT19 News Translation Task for translation directions involving <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5356 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5356/>WMDO : Fluency-based Word Mover’s Distance for Machine Translation Evaluation<span class=acl-fixed-case>WMDO</span>: Fluency-based Word Mover’s Distance for Machine Translation Evaluation</a></strong><br><a href=/people/j/julian-chow/>Julian Chow</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5356><div class="card-body p-3 small">We propose <a href=https://en.wikipedia.org/wiki/WMDO>WMDO</a>, a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> based on distance between distributions in the semantic vector space. Matching in the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> has been investigated for translation evaluation, but the constraints of a translation&#8217;s word order have not been fully explored. Building on the Word Mover&#8217;s Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5357 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5357/>Meteor++ 2.0 : Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation</a></strong><br><a href=/people/y/yinuo-guo/>Yinuo Guo</a>
|
<a href=/people/j/junfeng-hu/>Junfeng Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5357><div class="card-body p-3 small">This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical level</a> and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> outperforms all previous versions of Meteor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5358 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5358/>YiSi-a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources<span class=acl-fixed-case>Y</span>i<span class=acl-fixed-case>S</span>i - a Unified Semantic <span class=acl-fixed-case>MT</span> Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources</a></strong><br><a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5358><div class="card-body p-3 small">We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1&#8217;s scores with human judgment is made by using contextual embeddings in multilingual BERTBidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5359.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5359 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5359 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5359/>EED : Extended Edit Distance Measure for Machine Translation<span class=acl-fixed-case>EED</span>: Extended Edit Distance Measure for Machine Translation</a></strong><br><a href=/people/p/peter-stanchev/>Peter Stanchev</a>
|
<a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5359><div class="card-body p-3 small">Over the years a number of machine translation metrics have been developed in order to evaluate the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine-generated translations</a>. Metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and TER have been used for decades. However, with the rapid progress of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, the need for better <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> is growing. This paper proposes an extension of the <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a>, which achieves better human correlation, whilst remaining fast, flexible and easy to understand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5360 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5360/>Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/r/ryoma-yoshimura/>Ryoma Yoshimura</a>
|
<a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5360><div class="card-body p-3 small">In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose an improved version of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence BLEU</a> using filtered pseudo-references. We propose a method to filter pseudo-references by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudo-references filtered by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (SentBLEU) baselines with a single reference and with unfiltered pseudo-references.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5361 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5361/>Naver Labs Europe’s Systems for the WMT19 Machine Translation Robustness Task<span class=acl-fixed-case>E</span>urope’s Systems for the <span class=acl-fixed-case>WMT</span>19 Machine Translation Robustness Task</a></strong><br><a href=/people/a/alexandre-berard/>Alexandre Berard</a>
|
<a href=/people/i/ioan-calapodescu/>Ioan Calapodescu</a>
|
<a href=/people/c/claude-roux/>Claude Roux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5361><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>systems</a> that we submitted to the WMT19 Machine Translation robustness task. This task aims to improve MT&#8217;s robustness to noise found on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, like <a href=https://en.wikipedia.org/wiki/Informal_language>informal language</a>, spelling mistakes and other orthographic variations. The organizers provide parallel data extracted from a <a href=https://en.wikipedia.org/wiki/Social_media>social media website</a> in two language pairs : French-English and Japanese-English (one for each language direction). The goal is to obtain the best scores on unseen test sets from the same source, according to automatic metrics (BLEU) and human evaluation. We propose one single and one ensemble system for each translation direction. Our <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble models</a> ranked first in all language pairs, according to BLEU evaluation. We discuss the pre-processing choices that we made, and present our solutions for <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to noise and domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5363 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5363/>System Description : The Submission of FOKUS to the WMT 19 Robustness Task<span class=acl-fixed-case>FOKUS</span> to the <span class=acl-fixed-case>WMT</span> 19 Robustness Task</a></strong><br><a href=/people/c/cristian-grozea/>Cristian Grozea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5363><div class="card-body p-3 small">This paper describes the systems of Fraunhofer FOKUS for the WMT 2019 machine translation robustness task. We have made submissions to the EN-FR, FR-EN, and JA-EN language pairs. The first two were made with a baseline translator, trained on clean data for the WMT 2019 biomedical translation task. These baselines improved over the baselines from the MTNT paper by 2 to 4 BLEU points, but where not trained on the same data. The last one used the same model class and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a>, with induced typos in the training data to increase the <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5364 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5364/>CUNI System for the WMT19 Robustness Task<span class=acl-fixed-case>CUNI</span> System for the <span class=acl-fixed-case>WMT</span>19 Robustness Task</a></strong><br><a href=/people/j/jindrich-helcl/>Jindřich Helcl</a>
|
<a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5364><div class="card-body p-3 small">We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5365 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5365/>NTT’s Machine Translation Systems for WMT19 Robustness Task<span class=acl-fixed-case>NTT</span>’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 Robustness Task</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5365><div class="card-body p-3 small">This paper describes NTT&#8217;s submission to the WMT19 robustness task. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> mainly focuses on translating <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy text</a> (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as <a href=https://en.wikipedia.org/wiki/News>news</a>. Our submission combined techniques including utilization of a synthetic corpus, <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, and a <a href=https://en.wikipedia.org/wiki/Placeholder_name>placeholder mechanism</a>, which significantly improved over the previous <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> with special placeholder tokens during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, improves <a href=https://en.wikipedia.org/wiki/Translation>translation accuracy</a> even with noisy texts.</div></div></div><hr><div id=w19-54><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-54.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-54/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5400/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5404/>Findings of the WMT 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions<span class=acl-fixed-case>WMT</span> 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions</a></strong><br><a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5404><div class="card-body p-3 small">Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2 % and 10 % of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5405/>RTM Stacking Results for Machine Translation Performance Prediction<span class=acl-fixed-case>RTM</span> Stacking Results for Machine Translation Performance Prediction</a></strong><br><a href=/people/e/ergun-bicici/>Ergun Biçici</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5405><div class="card-body p-3 small">We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5407/>QE BERT : Bilingual BERT Using Multi-task Learning for Neural Quality Estimation<span class=acl-fixed-case>QE</span> <span class=acl-fixed-case>BERT</span>: Bilingual <span class=acl-fixed-case>BERT</span> Using Multi-task Learning for Neural Quality Estimation</a></strong><br><a href=/people/h/hyun-kim/>Hyun Kim</a>
|
<a href=/people/j/joon-ho-lim/>Joon-Ho Lim</a>
|
<a href=/people/h/hyun-ki-kim/>Hyun-Ki Kim</a>
|
<a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5407><div class="card-body p-3 small">For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5408/>MIPT System for World-Level Quality Estimation<span class=acl-fixed-case>MIPT</span> System for World-Level Quality Estimation</a></strong><br><a href=/people/m/mikhail-mosyagin/>Mikhail Mosyagin</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5408><div class="card-body p-3 small">We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation. We start with a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> similar to Shef-bRNN, which we modify by using conditional random fields for sequence labelling. Additionally, we use a different approach for labelling gaps and source words. We further develop this model by including features from different sources such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, baseline features for the task and transformer encoders. We evaluate the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the English-German dataset for the corresponding <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5409/>NJU Submissions for the WMT19 Quality Estimation Shared Task<span class=acl-fixed-case>NJU</span> Submissions for the <span class=acl-fixed-case>WMT</span>19 Quality Estimation Shared Task</a></strong><br><a href=/people/h/hou-qi/>Hou Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5409><div class="card-body p-3 small">In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair. We develop two approaches based on a two-stage neural QE model consisting of a <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a> and a quality estimator. More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions ; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks. To efficiently train these two-stage models, a joint learning training method is applied. Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5410/>Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings</a></strong><br><a href=/people/e/elizaveta-yankovskaya/>Elizaveta Yankovskaya</a>
|
<a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5410><div class="card-body p-3 small">We propose the use of pre-trained embeddings as features of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also <a href=https://en.wikipedia.org/wiki/Log_probability>log probability</a> of any <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) system</a>. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5411/>SOURCE : SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation<span class=acl-fixed-case>SOURCE</span>: <span class=acl-fixed-case>SOUR</span>ce-Conditional Elmo-style Model for Machine Translation Quality Estimation</a></strong><br><a href=/people/j/junpei-zhou/>Junpei Zhou</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/z/zecong-hu/>Zecong Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5411><div class="card-body p-3 small">Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> and adopt a bi-directional translation-like strategy. The <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategy</a> is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>, which makes the pre-training slightly harder, can bring improvements for <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a>. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5416/>Effort-Aware Neural Automatic Post-Editing</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5416><div class="card-body p-3 small">For this round of the WMT 2019 APE shared task, our submission focuses on addressing the over-correction problem in <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the <a href=https://en.wikipedia.org/wiki/System>system</a> about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5417.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5417/>UdS Submission for the WMT 19 Automatic Post-Editing Task<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span> Submission for the <span class=acl-fixed-case>WMT</span> 19 Automatic Post-Editing Task</a></strong><br><a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/q/qiuhui-liu/>Qiuhui Liu</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5417><div class="card-body p-3 small">In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>, implement this in our own transformer model and explore joint training of the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE task</a> with a de-noising encoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5418/>Terminology-Aware Segmentation and Domain Feature for the WMT19 Biomedical Translation Task<span class=acl-fixed-case>WMT</span>19 Biomedical Translation Task</a></strong><br><a href=/people/c/casimiro-pio-carrino/>Casimiro Pio Carrino</a>
|
<a href=/people/b/bardia-rafieian/>Bardia Rafieian</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5418><div class="card-body p-3 small">In this work, we give a description of the TALP-UPC systems submitted for the WMT19 Biomedical Translation Task. Our proposed strategy is NMT model-independent and relies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the <a href=https://en.wikipedia.org/wiki/Term_(logic)>terms information</a> at a token level. Finally, we trained the Transformer model with this terms-informed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5420/>Huawei’s NMT Systems for the WMT 2019 Biomedical Translation Task<span class=acl-fixed-case>NMT</span> Systems for the <span class=acl-fixed-case>WMT</span> 2019 Biomedical Translation Task</a></strong><br><a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/j/jianfeng-liu/>Jianfeng Liu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5420><div class="card-body p-3 small">This paper describes Huawei&#8217;s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering EnglishChinese, EnglishFrench and EnglishGerman language pairs. Our submitted systems achieve the best BLEU scores on EnglishFrench and EnglishGerman language pairs according to the official evaluation results. In the EnglishChinese translation task, our <a href=https://en.wikipedia.org/wiki/System>systems</a> are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5421/>UCAM Biomedical Translation at WMT19 : Transfer Learning Multi-domain Ensembles<span class=acl-fixed-case>UCAM</span> Biomedical Translation at <span class=acl-fixed-case>WMT</span>19: Transfer Learning Multi-domain Ensembles</a></strong><br><a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5421><div class="card-body p-3 small">The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of <a href=https://en.wikipedia.org/wiki/English_language>English-Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5425/>Machine Translation from an Intercomprehension Perspective</a></strong><br><a href=/people/y/yu-chen/>Yu Chen</a>
|
<a href=/people/t/tania-avgustinova/>Tania Avgustinova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5425><div class="card-body p-3 small">Within the first shared task on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between similar languages, we present our first attempts on Czech to Polish machine translation from an intercomprehension perspective. We propose methods based on the <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> of the two languages, taking advantage of their orthographic and phonological similarity, in the hope to improve over our baselines. The <a href=https://en.wikipedia.org/wiki/Translation>translation</a> results are evaluated using <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. On this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, none of our <a href=https://en.wikipedia.org/wiki/Proposal_(business)>proposals</a> could outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on the final test set. The current setups are rather preliminary, and there are several potential improvements we can try in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5426/>Utilizing Monolingual Data in NMT for Similar Languages : Submission to Similar Language Translation Task<span class=acl-fixed-case>NMT</span> for Similar Languages: Submission to Similar Language Translation Task</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5426><div class="card-body p-3 small">This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi-Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5427/>Neural Machine Translation : Hindi-Nepali<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>N</span>epali</a></strong><br><a href=/people/s/sahinur-rahman-laskar/>Sahinur Rahman Laskar</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5427><div class="card-body p-3 small">With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5429/>Panlingua-KMI MT System for Similar Language Translation Task at WMT 2019<span class=acl-fixed-case>KMI</span> <span class=acl-fixed-case>MT</span> System for Similar Language Translation Task at <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/akanksha-bansal/>Akanksha Bansal</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5429><div class="card-body p-3 small">The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-Hindi</a>, 1 PBSMT for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi-Nepali</a> and 2 NMT systems were developed for <a href=https://en.wikipedia.org/wiki/Nepali_language>NepaliHindi</a>. The results show that PBSMT could be an effective method for developing MT systems for <a href=https://en.wikipedia.org/wiki/Lingua_franca>closely-related languages</a>. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5430 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5430/>UDSDFKI Submission to the WMT2019 CzechPolish Similar Language Translation Shared Task<span class=acl-fixed-case>UDS</span>–<span class=acl-fixed-case>DFKI</span> Submission to the <span class=acl-fixed-case>WMT</span>2019 <span class=acl-fixed-case>C</span>zech–<span class=acl-fixed-case>P</span>olish Similar Language Translation Shared Task</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5430><div class="card-body p-3 small">In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and Nepali, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our <a href=https://en.wikipedia.org/wiki/System>system</a> in translating from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and comment on the impact of out-of-domain test data in the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a>. UDS-DFKI achieved competitive performance ranking second among ten teams in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to Polish translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5431/>Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation</a></strong><br><a href=/people/m/michael-przystupa/>Michael Przystupa</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5431><div class="card-body p-3 small">We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> on three low-resource, similar language pairs : <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech Polish</a>, and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi Nepali</a>. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish Portuguese and Czech Polish translation, whereas LSTMs with global attention worked best on Hindi Nepali translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5433 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5433.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5433/>Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data</a></strong><br><a href=/people/a/amittai-axelrod/>Amittai Axelrod</a>
|
<a href=/people/a/anish-kumar/>Anish Kumar</a>
|
<a href=/people/s/steve-sloto/>Steve Sloto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5433><div class="card-body p-3 small">We introduce a purely monolingual approach to <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering</a> for parallel data from a noisy corpus in a low-resource scenario. Our work is inspired by Junczysdowmunt:2018, but we relax the requirements to allow for cases where no parallel data is available. Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection Axelrod:2017, and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems. Our approach is featherweight, and runs end-to-end on a standard laptop in three hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5434 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5434/>NRC Parallel Corpus Filtering System for WMT 2019<span class=acl-fixed-case>NRC</span> Parallel Corpus Filtering System for <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5434><div class="card-body p-3 small">We describe the National Research Council Canada team&#8217;s submissions to the parallel corpus filtering task at the Fourth Conference on <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5436/>Quality and Coverage : The AFRL Submission to the WMT19 Parallel Corpus Filtering for Low-Resource Conditions Task<span class=acl-fixed-case>AFRL</span> Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering for Low-Resource Conditions Task</a></strong><br><a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5436><div class="card-body p-3 small">The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering a noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are the relatively low-resource language pairs of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> and the relative benefits of different options.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5437/>Webinterpret Submission to the WMT2019 Shared Task on Parallel Corpus Filtering<span class=acl-fixed-case>WMT</span>2019 Shared Task on Parallel Corpus Filtering</a></strong><br><a href=/people/j/jesus-gonzalez-rubio/>Jesús González-Rubio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5437><div class="card-body p-3 small">This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019). Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5439 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5439/>Filtering of Noisy Parallel Corpora Based on Hypothesis Generation</a></strong><br><a href=/people/z/zuzanna-parcheta/>Zuzanna Parcheta</a>
|
<a href=/people/g/german-sanchis-trilles/>Germán Sanchis-Trilles</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5439><div class="card-body p-3 small">The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems. In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model. We train translation models in both language pairs : NepaliEnglish and SinhalaEnglish using provided parallel corpora. We select the training subset for three language pairs (Nepali, <a href=https://en.wikipedia.org/wiki/Sinhala_language>Sinhala</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to English) jointly using bilingual cross-entropy selection to create the best possible translation model for both language pairs. Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair. We compute the smoothed BLEU score between the target sentence and generated hypothesis. In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score. These <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> are based on <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, source and target similarity and source language detection. We compare our results with the baseline published on the shared task website, which uses the Zipporah model, over which we achieve significant improvements in one of the conditions in the shared task. The designed <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering system</a> is domain independent and all experiments are conducted using <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5441/>The University of Helsinki Submission to the WMT19 Parallel Corpus Filtering Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering Task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5441><div class="card-body p-3 small">This paper describes the University of Helsinki Language Technology group&#8217;s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of <a href=https://en.wikipedia.org/wiki/Filter_(software)>filters</a> to remove the &#8216;bad&#8217; quality sentences. Then, we produced scores for each sentence by weighting these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with a <a href=https://en.wikipedia.org/wiki/Statistical_model>classification model</a>. This methodology allowed us to build a simple and reliable <a href=https://en.wikipedia.org/wiki/System>system</a> that is easily adaptable to other language pairs.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>