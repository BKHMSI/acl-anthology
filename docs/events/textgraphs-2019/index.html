<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Graph-based Methods for Natural Language Processing (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Graph-based Methods for Natural Language Processing (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#d19-53>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li></ul></div></div><div id=d19-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-53/>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5300/>Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5302/>Relation Prediction for Unseen-Entities Using Entity-Word Graphs</a></strong><br><a href=/people/y/yuki-tagawa/>Yuki Tagawa</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a>
|
<a href=/people/t/takayuki-yamamoto/>Takayuki Yamamoto</a>
|
<a href=/people/k/keiichi-nemoto/>Keiichi Nemoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5302><div class="card-body p-3 small">Knowledge graphs (KGs) are generally used for various <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. However, as <a href=https://en.wikipedia.org/wiki/Knowledge_graph>KGs</a> still miss some information, it is necessary to develop Knowledge Graph Completion (KGC) methods. Most KGC researches do not focus on the Out-of-KGs entities (Unseen-entities), we need a method that can predict the relation for the entity pairs containing Unseen-entities to automatically add new entities to the KGs. In this study, we focus on relation prediction and propose a method to learn entity representations via a graph structure that uses Seen-entities, Unseen-entities and words as nodes created from the descriptions of all entities. In the experiments, our method shows a significant improvement in the relation prediction for the entity pairs containing Unseen-entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5304.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5304/>Neural Speech Translation using <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>Lattice Transformations</a> and Graph Networks</a></strong><br><a href=/people/d/daniel-beck/>Daniel Beck</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5304><div class="card-body p-3 small">Speech translation systems usually follow a pipeline approach, using word lattices as an <a href=https://en.wikipedia.org/wiki/Intermediate_representation>intermediate representation</a>. However, previous work assume access to the original <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a> used to train the ASR system, which can limit applicability in real scenarios. In this work we propose an approach for <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a> through lattice transformations and neural models based on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph networks</a>. Experimental results show that our approach reaches competitive performance without relying on <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>transcriptions</a>, while also being orders of magnitude faster than previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5305 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5305/>Using Graphs for Word Embedding with Enhanced Semantic Relations</a></strong><br><a href=/people/m/matan-zuckerman/>Matan Zuckerman</a>
|
<a href=/people/m/mark-last/>Mark Last</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5305><div class="card-body p-3 small">Word embedding algorithms have become a common tool in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. While some, like Word2Vec, are based on sequential text input, others are utilizing a <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph representation of text</a>. In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, named WordGraph2Vec, or in short WG2V, which combines the two approaches to gain the benefits of both. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> uses a <a href=https://en.wikipedia.org/wiki/Directed_graph>directed word graph</a> to provide additional information for sequential text input algorithms. Our experiments on benchmark datasets show that text classification algorithms are nearly as accurate with WG2V as with other word embedding models while preserving more stable accuracy rankings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5306 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5306/>Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks</a></strong><br><a href=/people/m/mokanarangan-thayaparan/>Mokanarangan Thayaparan</a>
|
<a href=/people/m/marco-valentino/>Marco Valentino</a>
|
<a href=/people/v/viktor-schlegel/>Viktor Schlegel</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5306><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have resulted in models that surpass human performance when the answer is contained in a single, continuous passage of text. However, complex Question Answering (QA) typically requires multi-hop reasoning-i.e. the integration of supporting facts from different sources, to infer the correct answer. This paper proposes Document Graph Network (DGN), a message passing architecture for the identification of supporting facts over a graph-structured representation of text. The evaluation on HotpotQA shows that DGN obtains competitive results when compared to a reading comprehension baseline operating on raw text, confirming the relevance of structured representations for supporting multi-hop reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5307 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5307/>Essentia : Mining Domain-specific Paraphrases with Word-Alignment Graphs<span class=acl-fixed-case>E</span>ssentia: Mining Domain-specific Paraphrases with Word-Alignment Graphs</a></strong><br><a href=/people/d/danni-ma/>Danni Ma</a>
|
<a href=/people/c/chen-chen/>Chen Chen</a>
|
<a href=/people/b/behzad-golshan/>Behzad Golshan</a>
|
<a href=/people/w/wang-chiew-tan/>Wang-Chiew Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5307><div class="card-body p-3 small">Paraphrases are important linguistic resources for a wide variety of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Many techniques for automatic paraphrase mining from general corpora have been proposed. While these techniques are successful at discovering <a href=https://en.wikipedia.org/wiki/Paraphrase>generic paraphrases</a>, they often fail to identify <a href=https://en.wikipedia.org/wiki/Paraphrase>domain-specific paraphrases</a> (e.g., <a href=https://en.wikipedia.org/wiki/Employment>staff</a>, concierge in the hospitality domain). This is because current techniques are often based on <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>, while domain-specific corpora are too small to fit <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>. In this paper, we present an unsupervised graph-based technique to mine paraphrases from a small set of sentences that roughly share the same topic or intent. Our system, Essentia, relies on word-alignment techniques to create a word-alignment graph that merges and organizes tokens from input sentences. The resulting <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is then used to generate candidate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>. We demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> obtains high quality paraphrases, as evaluated by crowd workers. We further show that the majority of the identified <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> are domain-specific and thus complement existing paraphrase databases.<fixed-case>staff, concierge</fixed-case> in the hospitality domain). This is because current techniques are often based on statistical methods, while domain-specific corpora are too small to fit statistical methods. In this paper, we present an unsupervised graph-based technique to mine paraphrases from a small set of sentences that roughly share the same topic or intent. Our system, Essentia, relies on word-alignment techniques to create a word-alignment graph that merges and organizes tokens from input sentences. The resulting graph is then used to generate candidate paraphrases. We demonstrate that our system obtains high quality paraphrases, as evaluated by crowd workers. We further show that the majority of the identified paraphrases are domain-specific and thus complement existing paraphrase databases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5308 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5308.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-5308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-5308/>Layerwise Relevance Visualization in Convolutional Text Graph Classifiers</a></strong><br><a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/m/marc-hubner/>Marc Hübner</a>
|
<a href=/people/d/david-harbecke/>David Harbecke</a>
|
<a href=/people/c/christoph-alt/>Christoph Alt</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5308><div class="card-body p-3 small">Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a>, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on <a href=https://en.wikipedia.org/wiki/Intermediate_state>intermediate states</a>. In this work, we present a novel method that traces and visualizes <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that contribute to a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification decision</a> in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph structure</a>. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5310 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5310.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5310/>ASU at TextGraphs 2019 Shared Task : Explanation ReGeneration using Language Models and Iterative Re-Ranking<span class=acl-fixed-case>ASU</span> at <span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs 2019 Shared Task: Explanation <span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>G</span>eneration using Language Models and Iterative Re-Ranking</a></strong><br><a href=/people/p/pratyay-banerjee/>Pratyay Banerjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5310><div class="card-body p-3 small">In this work we describe the system from Natural Language Processing group at Arizona State University for the TextGraphs 2019 Shared Task. The task focuses on Explanation Regeneration, an intermediate step towards general multi-hop inference on large graphs. Our approach consists of modeling the explanation regeneration task as a learning to rank problem, for which we use state-of-the-art language models and explore dataset preparation techniques. We utilize an iterative reranking based approach to further improve the <a href=https://en.wikipedia.org/wiki/Ranking>rankings</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> secured 2nd rank in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with a mean average precision (MAP) of 41.3 % on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5313 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5313/>Chains-of-Reasoning at TextGraphs 2019 Shared Task : Reasoning over Chains of Facts for Explainable Multi-hop Inference<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs 2019 Shared Task: Reasoning over Chains of Facts for Explainable Multi-hop Inference</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/ameya-godbole/>Ameya Godbole</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/s/shehzaad-dhuliawala/>Shehzaad Dhuliawala</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5313><div class="card-body p-3 small">This paper describes our submission to the shared task on Multi-hop Inference Explanation Regeneration in TextGraphs workshop at EMNLP 2019 (Jansen and Ustalov, 2019). Our <a href=https://en.wikipedia.org/wiki/System>system</a> identifies chains of facts relevant to explain an answer to an elementary science examination question. To counter the problem of &#8216;spurious chains&#8217; leading to &#8216;semantic drifts&#8217;, we train a <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> that uses contextualized representation of facts to score its relevance for explaining an answer to a question. Our <a href=https://en.wikipedia.org/wiki/System>system</a> was ranked first w.r.t the mean average precision (MAP) metric outperforming the second best <a href=https://en.wikipedia.org/wiki/System>system</a> by 14.95 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5318 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-5318.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-5318/>Graph-Based Semi-Supervised Learning for Natural Language Understanding</a></strong><br><a href=/people/z/zimeng-qiu/>Zimeng Qiu</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/x/xiaochun-ma/>Xiaochun Ma</a>
|
<a href=/people/w/william-campbell/>William Campbell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5318><div class="card-body p-3 small">Semi-supervised learning is an efficient method to augment training data automatically from unlabeled data. Development of many natural language understanding (NLU) applications has a challenge where unlabeled data is relatively abundant while labeled data is rather limited. In this work, we propose transductive graph-based semi-supervised learning models as well as their inductive variants for NLU. We evaluate the approach&#8217;s applicability using publicly available NLU data and models. In order to find similar utterances and construct a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, we use a paraphrase detection model. Results show that applying the inductive graph-based semi-supervised learning can improve the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error rate</a> of the NLU model by 5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5319 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5319/>Graph Enhanced Cross-Domain Text-to-SQL Generation<span class=acl-fixed-case>SQL</span> Generation</a></strong><br><a href=/people/s/siyu-huo/>Siyu Huo</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/j/jie-chen/>Jie Chen</a>
|
<a href=/people/m/maria-chang/>Maria Chang</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5319><div class="card-body p-3 small">Semantic parsing is a fundamental problem in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, as it involves the mapping of natural language to structured forms such as <a href=https://en.wikipedia.org/wiki/Executable>executable queries</a> or <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>logic-like knowledge representations</a>. Existing deep learning approaches for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> have shown promise on a variety of benchmark data sets, particularly on text-to-SQL parsing. However, most text-to-SQL parsers do not generalize to unseen data sets in different domains. In this paper, we propose a new cross-domain learning scheme to perform text-to-SQL translation and demonstrate its use on Spider, a large-scale cross-domain text-to-SQL data set. We improve upon a state-of-the-art Spider model, SyntaxSQLNet, by constructing a graph of column names for all databases and using graph neural networks to compute their embeddings. The resulting embeddings offer better cross-domain representations and <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a>, as evidenced by substantial improvement on the Spider data set compared to SyntaxSQLNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5320 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5320/>Reasoning Over Paths via Knowledge Base Completion</a></strong><br><a href=/people/s/saatviga-sudhahar/>Saatviga Sudhahar</a>
|
<a href=/people/a/andrea-pierleoni/>Andrea Pierleoni</a>
|
<a href=/people/i/ian-roberts/>Ian Roberts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5320><div class="card-body p-3 small">Reasoning over paths in large scale knowledge graphs is an important problem for many applications. In this paper we discuss a simple approach to automatically build and rank paths between a source and target entity pair with learned embeddings using a knowledge base completion model (KBC). We assembled a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> by mining the available <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical scientific literature</a> and extracted a set of high frequency paths to use for <a href=https://en.wikipedia.org/wiki/Data_validation>validation</a>. We demonstrate that our method is able to effectively rank a list of known paths between a pair of entities and also come up with plausible paths that are not present in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. For a given entity pair we are able to reconstruct the highest ranking path 60 % of the time within the top 10 ranked paths and achieve 49 % mean average precision. Our approach is compositional since any KBC model that can produce vector representations of entities can be used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5321 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5321/>Node Embeddings for Graph Merging : Case of Knowledge Graph Construction</a></strong><br><a href=/people/i/ida-szubert/>Ida Szubert</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5321><div class="card-body p-3 small">Combining two <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> requires merging the nodes which are counterparts of each other. In this process errors occur, resulting in incorrect merging or incorrect failure to merge. We find a high prevalence of such errors when using AskNET, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for building Knowledge Graphs from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a>. AskNET node matching method uses <a href=https://en.wikipedia.org/wiki/String_similarity>string similarity</a>, which we propose to replace with vector embedding similarity. We explore graph-based and word-based embedding models and show an overall <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> of from 56 % to 23.6 %, with a reduction of over a half in both types of incorrect node matching.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5322 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5322/>DBee : A Database for Creating and Managing Knowledge Graphs and Embeddings<span class=acl-fixed-case>DB</span>ee: A Database for Creating and Managing Knowledge Graphs and Embeddings</a></strong><br><a href=/people/v/viktor-schlegel/>Viktor Schlegel</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5322><div class="card-body p-3 small">This paper describes DBee, a <a href=https://en.wikipedia.org/wiki/Database>database</a> to support the construction of data-intensive AI applications. DBee provides a unique <a href=https://en.wikipedia.org/wiki/Data_model>data model</a> which operates jointly over large-scale knowledge graphs (KGs) and embedding vector spaces (VSs). This model supports queries which exploit the semantic properties of both types of representations (KGs and VSs). Additionally, DBee aims to facilitate the construction of KGs and VSs, by providing a library of generators, which can be used to create, integrate and transform data into KGs and VSs.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>