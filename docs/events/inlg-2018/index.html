<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Natural Language Generation Conference (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Natural Language Generation Conference (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-65>Proceedings of the 11th International Conference on Natural Language Generation</a>
<span class="badge badge-info align-middle ml-1">40&nbsp;papers</span></li><li><a class=align-middle href=#w18-66>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-67>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&NLG)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w18-69>Proceedings of the Workshop on NLG for Humanâ€“Robot Interaction</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#w18-70>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li></ul></div></div><div id=w18-65><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-65.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-65/>Proceedings of the 11th International Conference on Natural Language Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6500/>Proceedings of the 11th International Conference on Natural Language Generation</a></strong><br><a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/m/martijn-goudbeek/>Martijn Goudbeek</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6501/>Deep Graph Convolutional Encoders for <a href=https://en.wikipedia.org/wiki/Structured_data>Structured Data</a> to Text Generation</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6501><div class="card-body p-3 small">Most previous work on neural text generation from <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured data</a> relies on standard sequence-to-sequence methods. These approaches linearise the input graph to be fed to a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. In this paper, we propose an alternative <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> based on graph convolutional networks that directly exploits the input structure. We report results on two graph-to-sequence datasets that empirically show the benefits of explicitly encoding the input graph structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6502/>Describing a Knowledge Base</a></strong><br><a href=/people/q/qingyun-wang/>Qingyun Wang</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/z/zhiying-jiang/>Zhiying Jiang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6502><div class="card-body p-3 small">We aim to automatically generate <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a> about an input structured knowledge base (KB). We build our generation framework based on a pointer network which can copy facts from the input KB, and add two attention mechanisms : (i) slot-aware attention to capture the association between a slot type and its corresponding slot value ; and (ii) a new table position self-attention to capture the inter-dependencies among related slots. For evaluation, besides standard metrics including <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>, and ROUGE, we propose a <a href=https://en.wikipedia.org/wiki/Binary_logarithm>KB reconstruction based metric</a> by extracting a <a href=https://en.wikipedia.org/wiki/Binary_logarithm>KB</a> from the generation output and comparing it with the input KB. We also create a new <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> which includes 106,216 pairs of structured KBs and their corresponding <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a> for two distinct entity types. Experiments show that our approach significantly outperforms <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art methods</a>. The reconstructed KB achieves 68.8 %-72.6 % F-score.<i>slot-aware attention</i> to capture the association between a slot type and its corresponding slot value; and (ii) a new <i>table position self-attention</i> to capture the inter-dependencies among related slots. For evaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we propose a <i>KB reconstruction</i> based metric by extracting a KB from the generation output and comparing it with the input KB. We also create a new data set which includes 106,216 pairs of structured KBs and their corresponding natural language descriptions for two distinct entity types. Experiments show that our approach significantly outperforms state-of-the-art methods. The reconstructed KB achieves 68.8% - 72.6% F-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6506" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6506/>SimpleNLG-ZH : a Linguistic Realisation Engine for <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a><span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span>-<span class=acl-fixed-case>ZH</span>: a Linguistic Realisation Engine for <span class=acl-fixed-case>M</span>andarin</a></strong><br><a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6506><div class="card-body p-3 small">We introduce SimpleNLG-ZH, a realisation engine for <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a> that follows the software design paradigm of SimpleNLG (Gatt and Reiter, 2009). We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from <a href=https://en.wikipedia.org/wiki/English_language>English</a> and other languages for which SimpleNLG engines have been built. The <a href=https://en.wikipedia.org/wiki/System>system</a> was evaluated by regenerating expressions from a body of test sentences and a corpus of human-authored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6507/>Adapting SimpleNLG to Galician language<span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span> to <span class=acl-fixed-case>G</span>alician language</a></strong><br><a href=/people/a/andrea-cascallar-fuentes/>Andrea Cascallar-Fuentes</a>
|
<a href=/people/a/alejandro-ramos-soto/>Alejandro Ramos-Soto</a>
|
<a href=/people/a/alberto-bugarin-diz/>Alberto BugarÃ­n Diz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6507><div class="card-body p-3 small">In this paper, we describe SimpleNLG-GL, an adaptation of the linguistic realisation SimpleNLG library for the <a href=https://en.wikipedia.org/wiki/Galician_language>Galician language</a>. This <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> is derived from SimpleNLG-ES, the English-Spanish version of this <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a>. It has been tested using a battery of examples which covers the most common rules for Galician.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6510/>Stylistically User-Specific Generation</a></strong><br><a href=/people/a/abdurrisyad-fikri/>Abdurrisyad Fikri</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6510><div class="card-body p-3 small">Recent neural models for response generation show good results in terms of general responses. In real conversations, however, depending on the speaker / responder, similar utterances should require different responses. In this study, we attempt to consider individual user&#8217;s information in adjusting the notable sequence-to-sequence (seq2seq) model for more diverse, user-specific responses. We assume that we need user-specific features to adjust the response and we argue that some selected representative words from the users are suitable for this task. Furthermore, we prove that even for unseen or unknown users, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can provide more diverse and interesting responses, while maintaining correlation with input utterances. Experimental results with human evaluation show that our model can generate more interesting responses than the popular seq2seqmodel and achieve higher relevance with input utterances than our baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6512/>Treat the <a href=https://en.wikipedia.org/wiki/System>system</a> like a human student : Automatic naturalness evaluation of generated text without reference texts</a></strong><br><a href=/people/i/isabel-groves/>Isabel Groves</a>
|
<a href=/people/y/ye-tian/>Ye Tian</a>
|
<a href=/people/i/ioannis-douratsos/>Ioannis Douratsos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6512><div class="card-body p-3 small">The current most popular method for automatic Natural Language Generation (NLG) evaluation is comparing generated text with human-written reference sentences using a metrics system, which has drawbacks around <a href=https://en.wikipedia.org/wiki/Reliability_(computer_networking)>reliability</a> and <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a>. We draw inspiration from second language (L2) assessment and extract a set of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> to predict human judgments of sentence naturalness. Our experiment using a small dataset showed that the feature-based approach yields promising results, with the added potential of providing <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> into the source of the problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6513/>Content Aware Source Code Change Description Generation</a></strong><br><a href=/people/p/pablo-loyola/>Pablo Loyola</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a>
|
<a href=/people/f/fumiko-satoh/>Fumiko Satoh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6513><div class="card-body p-3 small">We propose to study the generation of descriptions from source code changes by integrating the messages included on code commits and the intra-code documentation inside the source in the form of docstrings. Our hypothesis is that although both types of descriptions are not directly aligned in semantic terms one explaining a change and the other the actual functionality of the code being modified there could be certain common ground that is useful for the generation. To this end, we propose an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that uses the source code-docstring relationship to guide the description generation. We discuss the results of the approach comparing against a baseline based on a sequence-to-sequence model, using standard automatic natural language generation metrics as well as with a human study, thus offering a comprehensive view of the feasibility of the approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6514" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6514/>Improving Context Modelling in Multimodal Dialogue Generation</a></strong><br><a href=/people/s/shubham-agarwal/>Shubham Agarwal</a>
|
<a href=/people/o/ondrej-dusek/>OndÅ™ej DuÅ¡ek</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6514><div class="card-body p-3 small">In this work, we investigate the task of textual response generation in a multimodal task-oriented dialogue system. Our work is based on the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) in the fashion domain. We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics. We also showcase the shortcomings of current vision and language models by performing an <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> on our <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6515" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6515/>Generating Market Comments Referring to External Resources</a></strong><br><a href=/people/t/tatsuya-aoki/>Tatsuya Aoki</a>
|
<a href=/people/a/akira-miyazawa/>Akira Miyazawa</a>
|
<a href=/people/t/tatsuya-ishigaki/>Tatsuya Ishigaki</a>
|
<a href=/people/k/keiichi-goshima/>Keiichi Goshima</a>
|
<a href=/people/k/kasumi-aoki/>Kasumi Aoki</a>
|
<a href=/people/i/ichiro-kobayashi/>Ichiro Kobayashi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6515><div class="card-body p-3 small">Comments on a <a href=https://en.wikipedia.org/wiki/Stock_market>stock market</a> often include the reason or cause of changes in stock prices, such as <a href=https://en.wikipedia.org/wiki/Nikkei_225>Nikkei</a> turns lower as yen&#8217;s rise hits exporters. Generating such informative sentences requires capturing the relationship between different resources, including a target stock price. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for automatically generating such informative market comments that refer to <a href=https://en.wikipedia.org/wiki/Resource_(economics)>external resources</a>. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> through an automatic metric in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and human evaluation done by an expert in finance. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> both in BLEU scores and <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6518 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6518" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6518/>Automatic Opinion Question Generation</a></strong><br><a href=/people/y/yllias-chali/>Yllias Chali</a>
|
<a href=/people/t/tina-baghaee/>Tina Baghaee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6518><div class="card-body p-3 small">We study the problem of opinion question generation from sentences with the help of community-based question answering systems. For this purpose, we use a sequence to sequence attentional model, and we adopt coverage mechanism to prevent sentences from repeating themselves. Experimental results on the Amazon question / answer dataset show an improvement in automatic evaluation metrics as well as human evaluations from the state-of-the-art question generation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6519 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6519/>Modelling Pro-drop with the Rational Speech Acts Model</a></strong><br><a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6519><div class="card-body p-3 small">We extend the classic Referring Expressions Generation task by considering zero pronouns in pro-drop languages such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, modelling their use by means of the Bayesian Rational Speech Acts model (Frank and Goodman, 2012). By assuming that highly salient referents are most likely to be referred to by <a href=https://en.wikipedia.org/wiki/Zero_pronoun>zero pronouns</a> (i.e., <a href=https://en.wikipedia.org/wiki/Pro-drop_language>pro-drop</a> is more likely for salient referents than the less salient ones), the model offers an attractive explanation of a phenomenon not previously addressed probabilistically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6521 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6521" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6521/>Enriching the WebNLG corpus<span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>NLG</span> corpus</a></strong><br><a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/d/diego-moussallem/>Diego Moussallem</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a>
|
<a href=/people/s/sander-wubben/>Sander Wubben</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6521><div class="card-body p-3 small">This paper describes the enrichment of WebNLG corpus (Gardent et al., 2017a, b), with the aim to further extend its usefulness as a resource for evaluating common NLG tasks, including Discourse Ordering, <a href=https://en.wikipedia.org/wiki/Lexicalization>Lexicalization</a> and Referring Expression Generation. We also produce a silver-standard German translation of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to enable the exploitation of NLG approaches to other languages than <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The enriched corpus is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6523/>Template-based multilingual football reports generation using <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> as a knowledge base<span class=acl-fixed-case>W</span>ikidata as a knowledge base</a></strong><br><a href=/people/l/lorenzo-gatti/>Lorenzo Gatti</a>
|
<a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/m/mariet-theune/>MariÃ«t Theune</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6523><div class="card-body p-3 small">This paper presents a new version of a football reports generation system called PASS. The original <a href=https://en.wikipedia.org/wiki/Software_versioning>version</a> generated <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch text</a> and relied on a limited hand-crafted knowledge base. We describe how, in a short amount of time, we extended PASS to produce English texts, exploiting <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> as a large-scale source of multilingual knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6526/>Japanese Advertising Slogan Generator using <a href=https://en.wikipedia.org/wiki/Case_frame>Case Frame</a> and Word Vector<span class=acl-fixed-case>J</span>apanese Advertising Slogan Generator using Case Frame and Word Vector</a></strong><br><a href=/people/k/kango-iwama/>Kango Iwama</a>
|
<a href=/people/y/yoshinobu-kano/>Yoshinobu Kano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6526><div class="card-body p-3 small">There has been many works published for automatic sentence generation of a variety of domains. However, there would be still no single <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> available at present that can generate sentences for all of domains. Each domain will require a suitable generation method. We focus on automatic generation of <a href=https://en.wikipedia.org/wiki/Advertising_slogan>Japanese advertisement slogans</a> in this paper. We use our advertisement slogan database, case frame information, and word vector information. We employed our <a href=https://en.wikipedia.org/wiki/System>system</a> to apply for a copy competition for human copywriters, where our <a href=https://en.wikipedia.org/wiki/Advertising_slogan>advertisement slogan</a> was left as a finalist. Our <a href=https://en.wikipedia.org/wiki/System>system</a> could be regarded as the world first system that generates <a href=https://en.wikipedia.org/wiki/Slogan>slogans</a> in a practical level, as an advertising agency already employs our <a href=https://en.wikipedia.org/wiki/System>system</a> in their business.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6527/>Underspecified Universal Dependency Structures as Inputs for Multilingual Surface Realisation<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Structures as Inputs for Multilingual Surface Realisation</a></strong><br><a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/a/anja-belz/>Anja Belz</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6527><div class="card-body p-3 small">In this paper, we present the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> used in the Shallow and Deep Tracks of the First Multilingual Surface Realisation Shared Task (SR&#8217;18). For the Shallow Track, data in ten languages has been released : <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. For the Deep Track, data in three languages is made available : <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. We describe in detail how the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> were derived from the Universal Dependencies V2.0, and report on an evaluation of the Deep Track input quality. In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6528.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6528 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6528 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6528/>LSTM Hypertagging<span class=acl-fixed-case>LSTM</span> Hypertagging</a></strong><br><a href=/people/r/reid-fu/>Reid Fu</a>
|
<a href=/people/m/michael-white/>Michael White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6528><div class="card-body p-3 small">Hypertagging, or supertagging for surface realization, is the process of assigning <a href=https://en.wikipedia.org/wiki/Lexical_category>lexical categories</a> to nodes in an input semantic graph. Previous work has shown that hypertagging significantly increases realization speed and quality by reducing the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> of the <a href=https://en.wikipedia.org/wiki/Realizer>realizer</a>. Building on recent work using LSTMs to improve accuracy on supertagging for parsing, we develop an LSTM hypertagging method for OpenCCG, an open source NLP toolkit for CCG. Our results show significant improvements in both hypertagging accuracy and downstream realization performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6530 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6530/>Generating E-Commerce Product Titles and Predicting their Quality<span class=acl-fixed-case>E</span>-Commerce Product Titles and Predicting their Quality</a></strong><br><a href=/people/j/jose-g-c-de-souza/>JosÃ© G. Camargo de Souza</a>
|
<a href=/people/m/michael-kozielski/>Michael Kozielski</a>
|
<a href=/people/p/prashant-mathur/>Prashant Mathur</a>
|
<a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6530><div class="card-body p-3 small">E-commerce platforms present products using titles that summarize product information. These titles can not be created by hand, therefore an algorithmic solution is required. The task of automatically generating these <a href=https://en.wikipedia.org/wiki/Title_(publishing)>titles</a> given noisy user provided titles is one way to achieve the goal. The setting requires the generation process to be fast and the generated title to be both human-readable and concise. Furthermore, we need to understand if such generated titles are usable. As such, we propose approaches that (i) automatically generate product titles, (ii) predict their quality. Our approach scales to millions of products and both automatic and human evaluations performed on real-world data indicate our approaches are effective and applicable to existing e-commerce scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6531 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6531/>Designing and testing the messages produced by a virtual dietitian</a></strong><br><a href=/people/l/luca-anselma/>Luca Anselma</a>
|
<a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6531><div class="card-body p-3 small">This paper presents a project about the automatic generation of persuasive messages in the context of the <a href=https://en.wikipedia.org/wiki/Diet_(nutrition)>diet management</a>. In the first part of the paper we introduce the basic mechanisms related to data interpretation and content selection for a numerical data-to-text generation architecture. In the second part of the paper we discuss a number of factors influencing the design of the <a href=https://en.wikipedia.org/wiki/Message>messages</a>. In particular, we consider the design of the aggregation procedure. Finally, we present the results of a human-based evaluation concerning this design factor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6533 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6533/>Automatically Generating Questions about Novel Metaphors in Literature</a></strong><br><a href=/people/n/natalie-parde/>Natalie Parde</a>
|
<a href=/people/r/rodney-nielsen/>Rodney Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6533><div class="card-body p-3 small">The automatic generation of stimulating questions is crucial to the development of intelligent cognitive exercise applications. We developed an approach that generates appropriate Questioning the Author queries based on novel <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> in diverse syntactic relations in literature. We show that the generated questions are comparable to human-generated questions in terms of naturalness, <a href=https://en.wikipedia.org/wiki/Sensibility>sensibility</a>, and depth, and score slightly higher than human-generated questions in terms of clarity. We also show that questions generated about novel <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> are rated as cognitively deeper than questions generated about non- or conventional metaphors, providing evidence that metaphor novelty can be leveraged to promote cognitive exercise.<i>Questioning the Author</i> queries based on novel metaphors in diverse syntactic relations in literature. We show that the generated questions are comparable to human-generated questions in terms of naturalness, sensibility, and depth, and score slightly higher than human-generated questions in terms of clarity. We also show that questions generated about novel metaphors are rated as cognitively deeper than questions generated about non- or conventional metaphors, providing evidence that metaphor novelty can be leveraged to promote cognitive exercise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6534/>A Master-Apprentice Approach to Automatic Creation of Culturally Satirical Movie Titles</a></strong><br><a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a>
|
<a href=/people/m/mika-hamalainen/>Mika HÃ¤mÃ¤lÃ¤inen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6534><div class="card-body p-3 small">Satire has played a role in indirectly expressing critique towards an authority or a person from time immemorial. We present an autonomously creative master-apprentice approach consisting of a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> and an NMT model to produce humorous and culturally apt satire out of movie titles automatically. Furthermore, we evaluate the <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> in terms of its creativity and its output. We provide a solid definition for <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a> to maximize the objectiveness of the evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6536 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6536/>Neural Generation of Diverse Questions using Answer Focus, Contextual and Linguistic Features</a></strong><br><a href=/people/v/vrindavan-harrison/>Vrindavan Harrison</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6536><div class="card-body p-3 small">Question Generation is the task of automatically creating questions from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual input</a>. In this work we present a new Attentional EncoderDecoder Recurrent Neural Network model for automatic question generation. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> incorporates linguistic features and an additional <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embedding</a> to capture meaning at both sentence and word levels. The linguistic features are designed to capture information related to <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, word case, and entity coreference resolution. In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state of the art results of 19.98 Bleu_4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin. A human evaluation also shows that the added <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> improve the quality of the generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6537 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6537/>Evaluation methodologies in Automatic Question Generation 2013-2018</a></strong><br><a href=/people/j/jacopo-amidei/>Jacopo Amidei</a>
|
<a href=/people/p/paul-piwek/>Paul Piwek</a>
|
<a href=/people/a/alistair-willis/>Alistair Willis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6537><div class="card-body p-3 small">In the last few years Automatic Question Generation (AQG) has attracted increasing interest. In this paper we survey the evaluation methodologies used in <a href=https://en.wikipedia.org/wiki/AQG>AQG</a>. Based on a sample of 37 papers, our research shows that the <a href=https://en.wikipedia.org/wiki/System>systems</a>&#8217; development has not been accompanied by similar developments in the <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> used for the <a href=https://en.wikipedia.org/wiki/System>systems&#8217; evaluation</a>. Indeed, in the papers we examine here, we find a wide variety of both intrinsic and extrinsic evaluation methodologies. Such diverse evaluation practices make it difficult to reliably compare the quality of different <a href=https://en.wikipedia.org/wiki/Electricity_generation>generation systems</a>. Our study suggests that, given the rapidly increasing level of research in the area, a common framework is urgently needed to compare the performance of AQG systems and NLG systems more generally.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6538 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6538/>Task Proposal : The TL;DR Challenge<span class=acl-fixed-case>TL</span>;<span class=acl-fixed-case>DR</span> Challenge</a></strong><br><a href=/people/s/shahbaz-syed/>Shahbaz Syed</a>
|
<a href=/people/m/michael-volske/>Michael VÃ¶lske</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/n/nedim-lipka/>Nedim Lipka</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich SchÃ¼tze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6538><div class="card-body p-3 small">The TL;DR challenge fosters research in abstractive summarization of informal text, the largest and fastest-growing source of textual data on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, which has been overlooked by summarization research so far. The challenge owes its name to the frequent practice of social media users to supplement long posts with a TL;DRfor too long ; did n&#8217;t readfollowed by a short summary as a courtesy to those who would otherwise reply with the exact same abbreviation to indicate they did not care to read a post for its apparent length. Posts featuring TL;DR summaries form an excellent ground truth for summarization, and by tapping into this resource for the first time, we have mined millions of training examples from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, opening the door to all kinds of generative models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6541 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6541" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6541/>BENGAL : An <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>Automatic Benchmark Generator</a> for Entity Recognition and Linking<span class=acl-fixed-case>BENGAL</span>: An Automatic Benchmark Generator for Entity Recognition and Linking</a></strong><br><a href=/people/a/axel-cyrille-ngonga-ngomo/>Axel-Cyrille Ngonga Ngomo</a>
|
<a href=/people/m/michael-roder/>Michael RÃ¶der</a>
|
<a href=/people/d/diego-moussallem/>Diego Moussallem</a>
|
<a href=/people/r/ricardo-usbeck/>Ricardo Usbeck</a>
|
<a href=/people/r/rene-speck/>RenÃ© Speck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6541><div class="card-body p-3 small">The manual creation of gold standards for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> is time- and resource-intensive. Moreover, recent works show that such <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standards</a> contain a large proportion of mistakes in addition to being difficult to maintain. We hence present <a href=https://en.wikipedia.org/wiki/Bengal>Bengal</a>, a novel automatic generation of such <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standards</a> as a complement to manually created benchmarks. The main advantage of our <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> is that they can be readily generated at any time. They are also cost-effective while being guaranteed to be free of annotation errors. We compare the performance of 11 tools on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> generated by <a href=https://en.wikipedia.org/wiki/Bengal>Bengal</a> and on 16 <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> created manually. We show that our approach can be ported easily across languages by presenting results achieved by 4 tools on both <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Overall, our results suggest that our automatic benchmark generation approach can create varied benchmarks that have characteristics similar to those of existing <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> is open-source. Our experimental results are available at and the code at.<url>http://faturl.com/bengalexpinlg</url> and the code at <url>https://github.com/dice-group/BENGAL</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6543 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6543/>Handling Rare Items in Data-to-Text Generation</a></strong><br><a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6543><div class="card-body p-3 small">Neural approaches to data-to-text generation generally handle rare input items using either delexicalisation or a copy mechanism. We investigate the relative impact of these two methods on two datasets (E2E and WebNLG) and using two evaluation settings. We show (i) that rare items strongly impact performance ; (ii) that combining delexicalisation and <a href=https://en.wikipedia.org/wiki/Copying>copying</a> yields the strongest improvement ; (iii) that <a href=https://en.wikipedia.org/wiki/Copying>copying</a> underperforms for rare and unseen items and (iv) that the impact of these two mechanisms greatly varies depending on how the dataset is constructed and on how it is split into train, dev and test.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6545/>Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization : A Pilot Study</a></strong><br><a href=/people/j/jianmin-zhang/>Jianmin Zhang</a>
|
<a href=/people/j/jiwei-tan/>Jiwei Tan</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6545><div class="card-body p-3 small">Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine tuning</a>. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6546 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6546/>Toward Bayesian Synchronous Tree Substitution Grammars for Sentence Planning<span class=acl-fixed-case>B</span>ayesian Synchronous Tree Substitution Grammars for Sentence Planning</a></strong><br><a href=/people/d/david-m-howcroft/>David M. Howcroft</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6546><div class="card-body p-3 small">Developing conventional natural language generation systems requires extensive attention from human experts in order to craft complex sets of sentence planning rules. We propose a Bayesian nonparametric approach to learn sentence planning rules by inducing synchronous tree substitution grammars for pairs of text plans and morphosyntactically-specified dependency trees. Our system is able to learn rules which can be used to generate novel texts after training on small datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6547 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6547/>The Task Matters : Comparing Image Captioning and Task-Based Dialogical Image Description</a></strong><br><a href=/people/n/nikolai-ilinykh/>Nikolai Ilinykh</a>
|
<a href=/people/s/sina-zarriess/>Sina ZarrieÃŸ</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6547><div class="card-body p-3 small">Image captioning models are typically trained on <a href=https://en.wikipedia.org/wiki/Data>data</a> that is collected from people who are asked to describe an image, without being given any further task context. As we argue here, this context independence is likely to cause problems for transferring to task settings in which image description is bound by task demands. We demonstrate that careful design of <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> is required to obtain image descriptions which are contextually bounded to a particular meta-level task. As a task, we use MeetUp !, a text-based communication game where two players have the goal of finding each other in a visual environment. To reach this goal, the players need to describe images representing their current location. We analyse a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> from this <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>domain</a> and show that the nature of image descriptions found in MeetUp ! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6548 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6548/>Generating Summaries of Sets of Consumer Products : Learning from Experiments</a></strong><br><a href=/people/k/kittipitch-kuptavanich/>Kittipitch Kuptavanich</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/k/kees-van-deemter/>Kees Van Deemter</a>
|
<a href=/people/a/advaith-siddharthan/>Advaith Siddharthan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6548><div class="card-body p-3 small">We explored the task of creating a textual summary describing a large set of objects characterised by a small number of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> using an e-commerce dataset. When a set of consumer products is large and varied, it can be difficult for a consumer to understand how the products in the set differ ; consequently, it can be challenging to choose the most suitable product from the set. To assist consumers, we generated high-level summaries of product sets. Two generation algorithms are presented, discussed, and evaluated with human users. Our evaluation results suggest a positive contribution to consumers&#8217; understanding of the domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6549.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6549 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6549 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6549/>Neural sentence generation from <a href=https://en.wikipedia.org/wiki/Formal_semantics_(linguistics)>formal semantics</a></a></strong><br><a href=/people/k/kana-manome/>Kana Manome</a>
|
<a href=/people/m/masashi-yoshikawa/>Masashi Yoshikawa</a>
|
<a href=/people/h/hitomi-yanaka/>Hitomi Yanaka</a>
|
<a href=/people/p/pascual-martinez-gomez/>Pascual MartÃ­nez-GÃ³mez</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6549><div class="card-body p-3 small">Sequence-to-sequence models have shown strong performance in a wide range of NLP tasks, yet their applications to sentence generation from logical representations are underdeveloped. In this paper, we present a sequence-to-sequence model for generating sentences from logical meaning representations based on event semantics. We use a semantic parsing system based on Combinatory Categorial Grammar (CCG) to obtain data annotated with logical formulas. We augment our sequence-to-sequence model with masking for <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a> to constrain output sentences. We also propose a novel evaluation method for generation using Recognizing Textual Entailment (RTE). Combining <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and generation, we test whether or not the output sentence entails the original text and vice versa. Experiments showed that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperformed a baseline with respect to both BLEU scores and accuracies in RTE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6550.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6550 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6550 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6550" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6550/>Talking about other people : an endless range of possibilities</a></strong><br><a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6550><div class="card-body p-3 small">Image description datasets, such as Flickr30 K and MS COCO, show a high degree of variation in the ways that crowd-workers talk about the world. Although this gives us a rich and diverse collection of data to work with, it also introduces uncertainty about how the world should be described. This paper shows the extent of this <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> in the PEOPLE-domain. We present a <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> of different ways to talk about other people. This <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a> serves as a reference point to think about how other people should be described, and can be used to classify and compute statistics about labels applied to people.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6551 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6551/>Meteorologists and Students : A resource for language grounding of geographical descriptors</a></strong><br><a href=/people/a/alejandro-ramos-soto/>Alejandro Ramos-Soto</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/j/jose-m-alonso/>Jose Alonso</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6551><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_(computing)>data resource</a> which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as <a href=https://en.wikipedia.org/wiki/Polygon_(computer_graphics)>polygons</a> on a map by two groups of human subjects : teenage students and expert meteorologists.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6553 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6553/>Neural Transition-based Syntactic Linearization</a></strong><br><a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6553><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Linearization>linearization</a> is to find a grammatical order given a set of words. Traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> use <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>. Syntactic linearization systems, which generate a sentence along with its <a href=https://en.wikipedia.org/wiki/Syntactic_tree>syntactic tree</a>, have shown state-of-the-art performance. Recent work shows that a multilayer LSTM language model outperforms competitive statistical syntactic linearization systems without using <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6557.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6557 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6557 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6557/>E2E NLG Challenge : Neural Models vs. Templates<span class=acl-fixed-case>E</span>2<span class=acl-fixed-case>E</span> <span class=acl-fixed-case>NLG</span> Challenge: Neural Models vs. Templates</a></strong><br><a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6557><div class="card-body p-3 small">E2E NLG Challenge is a shared task on generating restaurant descriptions from sets of key-value pairs. This paper describes the results of our participation in the challenge. We develop a simple, yet effective neural encoder-decoder model which produces fluent restaurant descriptions and outperforms a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. We further analyze the data provided by the organizers and conclude that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> can also be approached with a template-based model developed in just a few hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6558 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6558/>The E2E NLG Challenge : A Tale of Two Systems<span class=acl-fixed-case>E</span>2<span class=acl-fixed-case>E</span> <span class=acl-fixed-case>NLG</span> Challenge: A Tale of Two Systems</a></strong><br><a href=/people/c/charese-smiley/>Charese Smiley</a>
|
<a href=/people/e/elnaz-davoodi/>Elnaz Davoodi</a>
|
<a href=/people/d/dezhao-song/>Dezhao Song</a>
|
<a href=/people/f/frank-schilder/>Frank Schilder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6558><div class="card-body p-3 small">This paper presents the two systems we entered into the 2017 E2E NLG Challenge : TemplGen, a templated-based system and SeqGen, a neural network-based system. Through the automatic evaluation, SeqGen achieved competitive results compared to the template-based approach and to other participating <a href=https://en.wikipedia.org/wiki/System>systems</a> as well. In addition to the automatic evaluation, in this paper we present and discuss the human evaluation results of our two <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6560.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6560 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6560 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6560/>Multi-Language Surface Realisation as REST API based NLG Microservice<span class=acl-fixed-case>REST</span> <span class=acl-fixed-case>API</span> based <span class=acl-fixed-case>NLG</span> Microservice</a></strong><br><a href=/people/a/andreas-madsack/>Andreas Madsack</a>
|
<a href=/people/j/johanna-heininger/>Johanna Heininger</a>
|
<a href=/people/n/nyamsuren-davaasambuu/>Nyamsuren Davaasambuu</a>
|
<a href=/people/v/vitaliia-voronik/>Vitaliia Voronik</a>
|
<a href=/people/m/michael-kaufl/>Michael KÃ¤ufl</a>
|
<a href=/people/r/robert-weissgraeber/>Robert WeiÃŸgraeber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6560><div class="card-body p-3 small">We present a readily available <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> that solves the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology component</a> for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>surface realizers</a> in 10 languages (e.g., English, German and Finnish) for any topic and is available as <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>REST API</a>. This can be used to add <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> to any kind of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLG application</a> (e.g., a multi-language chatbot), without requiring <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistic knowledge</a> by the integrator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6561.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6561 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6561 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6561/>Statistical NLG for Generating the Content and Form of Referring Expressions<span class=acl-fixed-case>NLG</span> for Generating the Content and Form of Referring Expressions</a></strong><br><a href=/people/x/xiao-li/>Xiao Li</a>
|
<a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6561><div class="card-body p-3 small">This paper argues that a new generic approach to statistical NLG can be made to perform Referring Expression Generation (REG) successfully. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> does not only select attributes and values for referring to a target referent, but also performs <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Realisation</a>, generating an actual <a href=https://en.wikipedia.org/wiki/Noun_phrase>Noun Phrase</a>. Our evaluations suggest that the attribute selection aspect of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> exceeds classic REG algorithms, while the <a href=https://en.wikipedia.org/wiki/Noun_phrase>Noun Phrases</a> generated are as similar to those in a previously developed corpus as were <a href=https://en.wikipedia.org/wiki/Noun_phrase>Noun Phrases</a> produced by a new set of human speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6562 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6562/>Specificity measures and reference</a></strong><br><a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/n/nicolas-marin/>NicolÃ¡s MarÃ­n</a>
|
<a href=/people/g/gustavo-rivas-gervilla/>Gustavo Rivas-Gervilla</a>
|
<a href=/people/d/daniel-sanchez-cisneros/>Daniel SÃ¡nchez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6562><div class="card-body p-3 small">In this paper we study empirically the validity of measures of referential success for referring expressions involving <a href=https://en.wikipedia.org/wiki/Gradualism>gradual properties</a>. More specifically, we study the ability of several measures of referential success to predict the success of a user in choosing the right object, given a referring expression. Experimental results indicate that certain fuzzy measures of success are able to predict <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>human accuracy</a> in reference resolution. Such measures are therefore suitable for the estimation of the success or otherwise of a <a href=https://en.wikipedia.org/wiki/Referring_expression>referring expression</a> produced by a generation algorithm, especially in case the properties in a domain can not be assumed to have crisp denotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6563 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6563/>Decoding Strategies for Neural Referring Expression Generation</a></strong><br><a href=/people/s/sina-zarriess/>Sina ZarrieÃŸ</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6563><div class="card-body p-3 small">RNN-based sequence generation is now widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and NLG (natural language generation). Most work focusses on how to train RNNs, even though also decoding is not necessarily straightforward : previous work on neural MT found seq2seq models to radically prefer short candidates, and has proposed a number of beam search heuristics to deal with this. In this work, we assess decoding strategies for referring expression generation with neural models. Here, expression length is crucial : output should neither contain too much or too little information, in order to be pragmatically adequate. We find that most beam search heuristics developed for MT do not generalize well to referring expression generation (REG), and do not generally outperform greedy decoding. We observe that beam search heuristics for termination seem to override the model&#8217;s knowledge of what a good stopping point is. Therefore, we also explore a recent approach called trainable decoding, which uses a small network to modify the RNN&#8217;s hidden state for better <a href=https://en.wikipedia.org/wiki/Decoding_methods>decoding</a> results. We find this approach to consistently outperform greedy decoding for REG.</div></div></div><hr><div id=w18-66><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-66.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-66/>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6600/>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (<span class=acl-fixed-case>CC</span>-<span class=acl-fixed-case>NLG</span> 2018)</a></strong><br><a href=/people/h/hugo-goncalo-oliveira/>Hugo GonÃ§alo Oliveira</a>
|
<a href=/people/b/ben-burtenshaw/>Ben Burtenshaw</a>
|
<a href=/people/r/raquel-hervas/>Raquel HervÃ¡s</a></span></p></div><hr><div id=w18-67><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-67.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-67/>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&NLG)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6700/>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2<span class=acl-fixed-case>IS</span>&<span class=acl-fixed-case>NLG</span>)</a></strong><br><a href=/people/j/jose-m-alonso/>Jose M. Alonso</a>
|
<a href=/people/a/alejandro-catala/>Alejandro Catala</a>
|
<a href=/people/m/mariet-theune/>MariÃ«t Theune</a></span></p></div><hr><div id=w18-69><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-69.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-69/>Proceedings of the Workshop on NLG for Humanâ€“Robot Interaction</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6900/>Proceedings of the Workshop on <span class=acl-fixed-case>NLG</span> for Humanâ€“Robot Interaction</a></strong><br><a href=/people/m/mary-ellen-foster/>Mary Ellen Foster</a>
|
<a href=/people/h/hendrik-buschmeier/>Hendrik Buschmeier</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6906/>Being data-driven is not enough : Revisiting interactive instruction giving as a challenge for NLG<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/s/sina-zarriess/>Sina ZarrieÃŸ</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6906><div class="card-body p-3 small">Modeling traditional NLG tasks with data-driven techniques has been a major focus of research in NLG in the past decade. We argue that existing modeling techniques are mostly tailored to textual data and are not sufficient to make NLG technology meet the requirements of <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> which target fluid interaction and collaboration in the real world. We revisit interactive instruction giving as a challenge for datadriven NLG and, based on insights from previous GIVE challenges, propose that instruction giving should be addressed in a setting that involves visual grounding and <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a>. These basic design decisions will require NLG frameworks that are capable of monitoring their environment as well as timing and revising their verbal output. We believe that these are core capabilities for making NLG technology transferrable to interactive systems.</div></div></div><hr><div id=w18-70><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-70.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-70/>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-7000/>Proceedings of the 1st Workshop on Automatic Text Adaptation (<span class=acl-fixed-case>ATA</span>)</a></strong><br><a href=/people/a/arne-jonsson/>Arne JÃ¶nsson</a>
|
<a href=/people/e/evelina-rennes/>Evelina Rennes</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a>
|
<a href=/people/s/sanja-stajner/>Sanja Stajner</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a></span></p></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>