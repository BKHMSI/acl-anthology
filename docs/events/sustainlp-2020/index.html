<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Simple and Efficient Natural Language Processing (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Simple and Efficient Natural Language Processing (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020sustainlp-1>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li></ul></div></div><div id=2020sustainlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.sustainlp-1/>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.0/>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/g/goran-glavas/>Goran Glava≈°</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939419 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.1/>Knowing Right from Wrong : Should We Use More Complex <a href=https://en.wikipedia.org/wiki/Mathematical_model>Models</a> for Automatic Short-Answer Scoring in Bahasa Indonesia?<span class=acl-fixed-case>B</span>ahasa <span class=acl-fixed-case>I</span>ndonesia?</a></strong><br><a href=/people/a/ali-akbar-septiandri/>Ali Akbar Septiandri</a>
|
<a href=/people/y/yosef-ardhito-winatmoko/>Yosef Ardhito Winatmoko</a>
|
<a href=/people/i/ilham-firdausi-putra/>Ilham Firdausi Putra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--1><div class="card-body p-3 small">We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring : single classical, <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classical</a>, and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble models</a> and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2 % difference in <a href=https://en.wikipedia.org/wiki/F-number>F1</a> compared to the deep learning approach with 1/18 time for model training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939422 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.3/>Learning Informative Representations of Biomedical Relations with Latent Variable Models</a></strong><br><a href=/people/h/harshil-shah/>Harshil Shah</a>
|
<a href=/people/j/julien-fauqueur/>Julien Fauqueur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--3><div class="card-body p-3 small">Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a <a href=https://en.wikipedia.org/wiki/Point_estimation>point estimate</a> to represent the <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> ; this is then used as the input to a relation classifier. However, the <a href=https://en.wikipedia.org/wiki/Binary_relation>relation</a> expressed in text between a pair of biomedical entities is often more complex than can be captured by a <a href=https://en.wikipedia.org/wiki/Point_estimate>point estimate</a>. To address this issue, we propose a <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable model</a> with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.4.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939423 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.4/>End to End Binarized Neural Networks for Text Classification</a></strong><br><a href=/people/k/kumar-shridhar/>Kumar Shridhar</a>
|
<a href=/people/h/harshil-jain/>Harshil Jain</a>
|
<a href=/people/a/akshat-agarwal/>Akshat Agarwal</a>
|
<a href=/people/d/denis-kleyko/>Denis Kleyko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--4><div class="card-body p-3 small">Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a> pose high requirements for <a href=https://en.wikipedia.org/wiki/Computer_hardware>computing hardware</a> and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is one way of addressing the issue of the increasing <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a>. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and text classification with a larger dataset. On the considered <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, the proposed <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> achieves comparable to the state-of-the-art results while utilizing 20-40 % lesser <a href=https://en.wikipedia.org/wiki/Computer_memory>memory</a> and training time compared to the benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939426 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.5/>Exploring the Boundaries of Low-Resource BERT Distillation<span class=acl-fixed-case>BERT</span> Distillation</a></strong><br><a href=/people/m/moshe-wasserblat/>Moshe Wasserblat</a>
|
<a href=/people/o/oren-pereg/>Oren Pereg</a>
|
<a href=/people/p/peter-izsak/>Peter Izsak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--5><div class="card-body p-3 small">In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on devices with limited resources is challenging due to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>&#8217; large <a href=https://en.wikipedia.org/wiki/Computation>computational consumption</a> and <a href=https://en.wikipedia.org/wiki/Computer_memory>memory requirements</a>. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, <a href=https://en.wikipedia.org/wiki/Load_(computing)>computational load</a> and <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a>. In this paper we test the boundaries of BERT model distillation in terms of <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a>, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.6.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939427 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.6/>Efficient Estimation of Influence of a Training Instance</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/s/sho-yokoi/>Sho Yokoi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--6><div class="card-body p-3 small">Understanding the influence of a training instance on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> leads to improving <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. However, it is difficult and inefficient to evaluate the influence, which shows how a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s prediction would be changed if a training instance were not used. In this paper, we propose an efficient <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for estimating the influence. Our method is inspired by dropout, which zero-masks a <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> and prevents the <a href=https://en.wikipedia.org/wiki/Subnetwork>sub-network</a> from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939429 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.7/>Efficient Inference For Neural Machine Translation</a></strong><br><a href=/people/y/yi-te-hsu/>Yi-Te Hsu</a>
|
<a href=/people/s/sarthak-garg/>Sarthak Garg</a>
|
<a href=/people/y/yi-hsiu-liao/>Yi-Hsiu Liao</a>
|
<a href=/people/i/ilya-chatsviorkin/>Ilya Chatsviorkin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--7><div class="card-body p-3 small">Large Transformer models have achieved state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speed</a> without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109 % and 84 % speedup on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a> and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> respectively and reduce the number of parameters by 25 % while maintaining the same translation quality in terms of BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.sustainlp-1.8.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939430 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.8/>Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm</a></strong><br><a href=/people/a/alicia-tsai/>Alicia Tsai</a>
|
<a href=/people/l/laurent-el-ghaoui/>Laurent El Ghaoui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--8><div class="card-body p-3 small">We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated <a href=https://en.wikipedia.org/wiki/Frank-Wolfe_algorithm>Frank-Wolfe algorithm</a>. To generate a summary with k sentences, the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and works especially well when combined with <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for highly paraphrased summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939432 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.10/>A Two-stage Model for Slot Filling in Low-resource Settings : Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings</a></strong><br><a href=/people/c/cennet-oguz/>Cennet Oguz</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--10><div class="card-body p-3 small">Learning-based slot filling-a key component of spoken language understanding systems-typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as <a href=https://en.wikipedia.org/wiki/ELMO>ELMO</a> and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939433 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.11/>Early Exiting BERT for Efficient Document Ranking<span class=acl-fixed-case>BERT</span> for Efficient Document Ranking</a></strong><br><a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/r/rodrigo-nogueira/>Rodrigo Nogueira</a>
|
<a href=/people/y/yaoliang-yu/>Yaoliang Yu</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--11><div class="card-body p-3 small">Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for <a href=https://en.wikipedia.org/wiki/Document_ranking>document ranking</a>. With a slight modification, BERT becomes a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with multiple output paths, and each inference sample can exit early from these <a href=https://en.wikipedia.org/wiki/Path_(graph_theory)>paths</a>. In this way, <a href=https://en.wikipedia.org/wiki/Computation>computation</a> can be effectively allocated among samples, and overall <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>system latency</a> is significantly reduced while the original <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x <a href=https://en.wikipedia.org/wiki/Time_complexity>inference speedup</a> with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939436 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.14/>A Little Bit Is Worse Than None : Ranking with Limited Training Data</a></strong><br><a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--14><div class="card-body p-3 small">Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from <a href=https://en.wikipedia.org/wiki/Keyword_search>keyword search</a>. In this work, we tackle the challenge of fine-tuning these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using corpus-specific labeled data from sources such as TREC. We first answer the question : How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that some labeled in-domain data can be worse than none at all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939438 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.16/>Load What You Need : Smaller Versions of Mutililingual BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/amine-abdaoui/>Amine Abdaoui</a>
|
<a href=/people/c/camille-pradel/>Camille Pradel</a>
|
<a href=/people/g/gregoire-sigel/>Gr√©goire Sigel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--16><div class="card-body p-3 small">Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> is often a drawback for their deployment in <a href=https://en.wikipedia.org/wiki/Real-time_computing>real production applications</a>. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that keep comparable results, while reducing up to 45 % of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> induced a 1.7 % to 6 % drop in the overall accuracy on the XNLI data set. The presented <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and code are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939441 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.sustainlp-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.19/>Towards Accurate and Reliable Energy Measurement of NLP Models<span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/q/qingqing-cao/>Qingqing Cao</a>
|
<a href=/people/a/aruna-balasubramanian/>Aruna Balasubramanian</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--19><div class="card-body p-3 small">Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. We conduct energy measurement experiments with four different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for a <a href=https://en.wikipedia.org/wiki/Question_answering>question answering task</a>. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and <a href=https://en.wikipedia.org/wiki/Energy_consumption>energy consumption</a>. We release the code and data at https://github.com/csarron/sustainlp2020-energy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.sustainlp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--sustainlp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.sustainlp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.sustainlp-1.24/>Overview of the SustaiNLP 2020 Shared Task<span class=acl-fixed-case>S</span>ustai<span class=acl-fixed-case>NLP</span> 2020 Shared Task</a></strong><br><a href=/people/a/alex-wang/>Alex Wang</a>
|
<a href=/people/t/thomas-wolf/>Thomas Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--sustainlp-1--24><div class="card-body p-3 small">We describe the SustaiNLP 2020 shared task : efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> as well as energy consumed in making predictions on the test sets. We describe the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, its organization, and the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a>. Across the six submissions to the shared task, participants achieved efficiency gains of 20 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>