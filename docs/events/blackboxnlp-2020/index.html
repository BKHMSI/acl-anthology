<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020blackboxnlp-1>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li></ul></div></div><div id=2020blackboxnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.blackboxnlp-1/>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.0/>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></strong><br><a href=/people/a/afra-alishahi/>Afra Alishahi</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupa≈Ça</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.6/>Leveraging Extracted Model Adversaries for Improved Black Box Attacks</a></strong><br><a href=/people/n/naveen-jafer-nizar/>Naveen Jafer Nizar</a>
|
<a href=/people/a/ari-kobren/>Ari Kobren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--6><div class="card-body p-3 small">We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our <a href=https://en.wikipedia.org/wiki/Stake_(Latter_Day_Saints)>approach</a> is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANYa white box attackperformed on the approximate model by 25 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>, and the ADDSENT attacka black box attackby 11 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939764 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.14/>The elephant in the interpretability room : Why use attention as explanation when we have <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>saliency methods</a>?</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/k/katja-filippova/>Katja Filippova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--14><div class="card-body p-3 small">There is a recent surge of interest in using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> as explanation of model predictions, with mixed evidence on whether <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be used as such. While <a href=https://en.wikipedia.org/wiki/Attention>attention</a> conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.16.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.blackboxnlp-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.16/>Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation</a></strong><br><a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--16><div class="card-body p-3 small">We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods : the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on <a href=https://en.wikipedia.org/wiki/Logical_consequence>lexical entailment</a> and <a href=https://en.wikipedia.org/wiki/Negation>negation</a>. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing <a href=https://en.wikipedia.org/wiki/Negation>negation</a>, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the <a href=https://en.wikipedia.org/wiki/Causal_model>causal dynamics</a> of the model mirror the <a href=https://en.wikipedia.org/wiki/Causal_model>causal dynamics</a> of this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.19.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939765 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.19/>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</a></strong><br><a href=/people/r/rajiv-movva/>Rajiv Movva</a>
|
<a href=/people/j/jason-zhao/>Jason Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--19><div class="card-body p-3 small">Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for <a href=https://en.wikipedia.org/wiki/NMT>NMT</a> while maintaining <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, it is unclear how such pruning techniques affect a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more <a href=https://en.wikipedia.org/wiki/Code>encoding</a>. Attention mechanisms remain remarkably consistent as sparsity increases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939766 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.21/>BERTs of a feather do not generalize together : Large variability in generalization across models with similar test set performance<span class=acl-fixed-case>BERT</span>s of a feather do not generalize together: Large variability in generalization across models with similar test set performance</a></strong><br><a href=/people/r/r-thomas-mccoy/>R. Thomas McCoy</a>
|
<a href=/people/j/junghyun-min/>Junghyun Min</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--21><div class="card-body p-3 small">If the same neural network architecture is trained multiple times on the same <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ranging between 83.6 % and 84.8 %. In stark contrast, the same <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> varied widely in their <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> performance. For example, on the simple case of subject-object swap (e.g., determining that the doctor visited the lawyer does not entail the lawyer visited the doctor), <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ranged from 0.0 % to 66.2 %. Such variation is likely due to the presence of many <a href=https://en.wikipedia.org/wiki/Maxima_and_minima>local minima</a> in the loss surface that are equally attractive to a low-bias learner such as a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> ; decreasing the variability may therefore require models with stronger <a href=https://en.wikipedia.org/wiki/Inductive_reasoning>inductive biases</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.blackboxnlp-1.22.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.22/>Second-Order NLP Adversarial Examples<span class=acl-fixed-case>NLP</span> Adversarial Examples</a></strong><br><a href=/people/j/john-morris/>John Morris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--22><div class="card-body p-3 small">Adversarial example generation methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> rely on models like <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of a <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> to second-order adversarial examples. To generate this <a href=https://en.wikipedia.org/wiki/Curve>curve</a>, we design an <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial attack</a> to run directly on the semantic similarity models. We test on two <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.blackboxnlp-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.25/>Investigating Novel Verb Learning in BERT : Selectional Preference Classes and Alternation-Based Syntactic Generalization<span class=acl-fixed-case>BERT</span>: Selectional Preference Classes and Alternation-Based Syntactic Generalization</a></strong><br><a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--25><div class="card-body p-3 small">Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT&#8217;s few-shot learning capabilities for two aspects of English verbs : alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb / object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias : verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.blackboxnlp-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--blackboxnlp-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.blackboxnlp-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.blackboxnlp-1.29/>Defining Explanation in an AI Context<span class=acl-fixed-case>AI</span> Context</a></strong><br><a href=/people/t/tejaswani-verma/>Tejaswani Verma</a>
|
<a href=/people/c/christoph-lingenfelder/>Christoph Lingenfelder</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--blackboxnlp-1--29><div class="card-body p-3 small">With the increase in the use of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a>, a need for explanation systems arises. Building an <a href=https://en.wikipedia.org/wiki/Explanation>explanation system</a> requires a definition of <a href=https://en.wikipedia.org/wiki/Explanation>explanation</a>. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a>, <a href=https://en.wikipedia.org/wiki/Philosophy>philosophy</a>, and <a href=https://en.wikipedia.org/wiki/Cognitive_science>cognitive sciences</a>. We study multiple perspectives and aspects of explainability of recommendations or predictions made by <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a>, and provide a generic definition of <a href=https://en.wikipedia.org/wiki/Explanation>explanation</a>. The proposed <a href=https://en.wikipedia.org/wiki/Definition>definition</a> is ambitious and challenging to apply. With the intention to bridge the gap between theory and application, we also propose a possible architecture of an automated explanation system based on our definition of explanation.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>