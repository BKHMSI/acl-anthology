<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Transactions of the Association for Computational Linguistics (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Transactions of the Association for Computational Linguistics (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#q17-1>Transactions of the Association for Computational Linguistics, Volume 5</a>
<span class="badge badge-info align-middle ml-1">31&nbsp;papers</span></li></ul></div></div><div id=q17-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/Q17-1/>Transactions of the Association for Computational Linguistics, Volume 5</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1000/>Transactions of the Association for Computational Linguistics, Volume 5</a></strong><br><a href=/people/l/lillian-lee/>Lillian Lee</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957075 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1001/>Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels</a></strong><br><a href=/people/a/alison-smith/>Alison Smith</a>
|
<a href=/people/t/tak-yeon-lee/>Tak Yeon Lee</a>
|
<a href=/people/f/forough-poursabzi-sangdeh/>Forough Poursabzi-Sangdeh</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/n/niklas-elmqvist/>Niklas Elmqvist</a>
|
<a href=/people/l/leah-findlater/>Leah Findlater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1001><div class="card-body p-3 small">Probabilistic topic models are important tools for <a href=https://en.wikipedia.org/wiki/Index_(publishing)>indexing</a>, summarizing, and analyzing large document collections by their themes. However, promoting end-user understanding of topics remains an open research problem. We compare labels generated by users given four topic visualization techniquesword lists, word lists with bars, word clouds, and network graphsagainst each other and against automatically generated labels. Our basis of comparison is participant ratings of how well labels describe documents from the topic. Our study has two phases : a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics&#8217; documents. Although all <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualizations</a> produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure. Automatic labels lag behind user-created labels, but our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of manually labeled topics highlights <a href=https://en.wikipedia.org/wiki/Pattern>linguistic patterns</a> (e.g., <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernyms</a>, phrases) that can be used to improve automatic topic labeling algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954554 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1002/>Visually Grounded and Textual Semantic Models Differentially Decode Brain Activity Associated with Concrete and Abstract Nouns</a></strong><br><a href=/people/a/andrew-j-anderson/>Andrew J. Anderson</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1002><div class="card-body p-3 small">Important advances have recently been made using computational semantic models to decode brain activity patterns associated with <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> ; however, this work has almost exclusively focused on concrete nouns. How well these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> extend to decoding <a href=https://en.wikipedia.org/wiki/Noun>abstract nouns</a> is largely unknown. We address this question by applying state-of-the-art computational models to decode functional Magnetic Resonance Imaging (fMRI) activity patterns, elicited by participants reading and imagining a diverse set of both concrete and abstract nouns. One of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> we use is linguistic, exploiting the recent word2vec skipgram approach trained on <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. The <a href=https://en.wikipedia.org/wiki/Second>second</a> is visually grounded, using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep convolutional neural networks</a> trained on <a href=https://en.wikipedia.org/wiki/Google_Images>Google Images</a>. Dual coding theory considers concrete concepts to be encoded in the brain both linguistically and visually, and abstract concepts only linguistically. Splitting the fMRI data according to human concreteness ratings, we indeed observe that both models significantly decode the most concrete nouns ; however, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> are sufficiently advanced to assist in investigating the representational structure of <a href=https://en.wikipedia.org/wiki/Concept>abstract concepts</a> in the brain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958123 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1003/>Modeling Semantic Expectation : Using Script Knowledge for Referent Prediction</a></strong><br><a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1003><div class="card-body p-3 small">Recent research in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> has provided increasing evidence that humans predict upcoming content. Prediction also affects <a href=https://en.wikipedia.org/wiki/Perception>perception</a> and might be a key to robustness in <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>human language processing</a>. In this paper, we investigate the factors that affect human prediction by building a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that <a href=https://en.wikipedia.org/wiki/Predictability>predictability</a> influences referring expression type but do not find evidence for such an effect.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/Q17-1004/>Shift-Reduce Constituent Parsing with Neural Lookahead Features</a></strong><br><a href=/people/j/jiangming-liu/>Jiangming Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1004><div class="card-body p-3 small">Transition-based models can be fast and accurate for <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent parsing</a>. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which consists of a sequence of non-local constituents. On the other hand, during incremental parsing, <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent information</a> on the right hand side of the current word is not utilized, which is a relative weakness of <a href=https://en.wikipedia.org/wiki/Shift-reduce_parsing>shift-reduce parsing</a>. To address this limitation, we leverage a fast neural model to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>lookahead features</a>. In particular, we build a bidirectional LSTM model, which leverages full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as <a href=https://en.wikipedia.org/wiki/Compiler-compiler>lookahead features</a>. The resulting <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> gives 1.3 % absolute improvement in WSJ and 2.3 % in CTB compared to the baseline, giving the highest reported accuracies for fully-supervised parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/Q17-1005.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951458 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1005/>A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit</a></strong><br><a href=/people/y/yin-wen-chang/>Yin-Wen Chang</a>
|
<a href=/people/m/michael-collins/>Michael Collins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1005><div class="card-body p-3 small">Decoding of phrase-based translation models in the general case is known to be NP-complete, by a reduction from the traveling salesman problem (Knight, 1999). In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. However, the impact on <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> after imposing such a <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> is not well studied. In this paper, we describe a <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming algorithm</a> for phrase-based decoding with a fixed distortion limit. The runtime of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is O(nd!lhd+1) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> makes use of a novel <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> that gives a new perspective on decoding of phrase-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952732 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1006/>A Generative Model of Phonotactics</a></strong><br><a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/a/adam-albright/>Adam Albright</a>
|
<a href=/people/p/peter-graff/>Peter Graff</a>
|
<a href=/people/t/timothy-odonnell/>Timothy J. O’Donnell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1006><div class="card-body p-3 small">We present a probabilistic model of phonotactics, the set of well-formed phoneme sequences in a language. Unlike most computational models of phonotactics (Hayes and Wilson, 2008 ; Goldsmith and Riggle, 2012), we take a fully generative approach, modeling a process where forms are built up out of subparts by phonologically-informed structure building operations. We learn an inventory of subparts by applying stochastic memoization (Johnson et al., 2007 ; Goodman et al., 2008) to a generative process for phonemes structured as an and-or graph, based on concepts of feature hierarchy from <a href=https://en.wikipedia.org/wiki/Generative_phonology>generative phonology</a> (Clements, 1985 ; Dresher, 2009). Subparts are combined in a way that allows tier-based feature interactions. We evaluate our models&#8217; ability to capture phonotactic distributions in the lexicons of 14 languages drawn from the WOLEX corpus (Graff, 2012). Our full <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> robustly assigns higher probabilities to held-out forms than a sophisticated N-gram model for all languages. We also present novel analyses that probe model behavior in more detail.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951517 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1007/>Context Gates for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/z/zhengdong-lu/>Zhengdong Lu</a>
|
<a href=/people/x/xiaohua-liu/>Xiaohua Liu</a>
|
<a href=/people/h/hang-li/>Hang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1007><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>, generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the <a href=https://en.wikipedia.org/wiki/Adequality>adequacy</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1009/>Automatically Tagging Constructions of Causation and Their Slot-Fillers</a></strong><br><a href=/people/j/jesse-dunietz/>Jesse Dunietz</a>
|
<a href=/people/l/lori-levin/>Lori Levin</a>
|
<a href=/people/j/jaime-g-carbonell/>Jaime Carbonell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1009><div class="card-body p-3 small">This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case. Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that <a href=https://en.wikipedia.org/wiki/Causality>causation</a> can take on. We therefore base our approach on the concept of <a href=https://en.wikipedia.org/wiki/Grammatical_construction>constructions</a> from the <a href=https://en.wikipedia.org/wiki/Paradigm>linguistic paradigm</a> known as Construction Grammar (CxG). In <a href=https://en.wikipedia.org/wiki/CxG>CxG</a>, a construction is a form / function pairing that can rely on arbitrary linguistic and semantic features. Rather than codifying all aspects of each construction&#8217;s form, as some attempts to employ <a href=https://en.wikipedia.org/wiki/CxG>CxG</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have done, we propose methods that offload that problem to <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. We describe two supervised approaches for tagging causal constructions and their arguments. Both approaches combine automatically induced pattern-matching rules with <a href=https://en.wikipedia.org/wiki/Statistical_classification>statistical classifiers</a> that learn the subtler parameters of the constructions. Our results show that these approaches are promising : they significantly outperform nave baselines for both construction recognition and cause and effect head matches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954994 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1012/>Head-Lexicalized Bidirectional Tree LSTMs<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/z/zhiyang-teng/>Zhiyang Teng</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1012><div class="card-body p-3 small">Sequential LSTMs have been extended to model <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree structures</a>, giving competitive results for a number of tasks. Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes. This is different from sequential LSTMs, which contain references to input words for each <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a>. In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node. In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTMs in structure. Experiments show that both <a href=https://en.wikipedia.org/wiki/Extension_(semantics)>extensions</a> give better representations of <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree structures</a>. Our final model gives the best results on the Stanford Sentiment Treebank and highly competitive results on the TREC question type classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235035 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1013/>Nonparametric Bayesian Semi-supervised Word Segmentation<span class=acl-fixed-case>B</span>ayesian Semi-supervised Word Segmentation</a></strong><br><a href=/people/r/ryo-fujii/>Ryo Fujii</a>
|
<a href=/people/r/ryo-domoto/>Ryo Domoto</a>
|
<a href=/people/d/daichi-mochihashi/>Daichi Mochihashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1013><div class="card-body p-3 small">This paper presents a novel hybrid generative / discriminative model of word segmentation based on nonparametric Bayesian methods. Unlike ordinary discriminative word segmentation which relies only on labeled data, our <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised model</a> also leverages a huge amounts of unlabeled text to automatically learn new words, and further constrains them by using a labeled data to segment non-standard texts such as those found in <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking services</a>. Specifically, our hybrid model combines a discriminative classifier (CRF ; Lafferty et al. (2001) and unsupervised word segmentation (NPYLM ; Mochihashi et al. (2009)), with a transparent exchange of information between these two model structures within the semi-supervised framework (JESS-CM ; Suzuki and Isozaki (2008)). We confirmed that it can appropriately segment non-standard texts like those in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Weibo</a> and has nearly state-of-the-art accuracy on standard datasets in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and <a href=https://en.wikipedia.org/wiki/Thai_language>Thai</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233899 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1014/>Joint Modeling of Topics, <a href=https://en.wikipedia.org/wiki/Citation>Citations</a>, and Topical Authority in Academic Corpora</a></strong><br><a href=/people/j/jooyeon-kim/>Jooyeon Kim</a>
|
<a href=/people/d/dongwoo-kim/>Dongwoo Kim</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1014><div class="card-body p-3 small">Much of scientific progress stems from previously published findings, but searching through the vast sea of scientific publications is difficult. We often rely on metrics of scholarly authority to find the prominent authors but these <a href=https://en.wikipedia.org/wiki/Authority>authority indices</a> do not differentiate <a href=https://en.wikipedia.org/wiki/Authority>authority</a> based on research topics. We present Latent Topical-Authority Indexing (LTAI) for jointly modeling the topics, <a href=https://en.wikipedia.org/wiki/Citation>citations</a>, and topical authority in a corpus of academic papers. Compared to previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, <a href=https://en.wikipedia.org/wiki/LTAI>LTAI</a> differs in two main aspects. First, it explicitly models the generative process of the <a href=https://en.wikipedia.org/wiki/Citation>citations</a>, rather than treating the <a href=https://en.wikipedia.org/wiki/Citation>citations</a> as given. Second, it models each author&#8217;s influence on citations of a paper based on the topics of the cited papers, as well as the citing papers. We fit LTAI into four academic corpora : CORA, <a href=https://en.wikipedia.org/wiki/Arxiv>Arxiv Physics</a>, <a href=https://en.wikipedia.org/wiki/Proceedings_of_the_National_Academy_of_Sciences_of_the_United_States_of_America>PNAS</a>, and <a href=https://en.wikipedia.org/wiki/Citeseer>Citeseer</a>. We compare the performance of LTAI against various baselines, starting with the <a href=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>latent Dirichlet allocation</a>, to the more advanced models including author-link topic model and dynamic author citation topic model. The results show that LTAI achieves improved accuracy over other similar <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> when predicting words, <a href=https://en.wikipedia.org/wiki/Citation>citations</a> and authors of publications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955039 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1015/>Pushing the Limits of Translation Quality Estimation</a></strong><br><a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/f/fabio-kepler/>Fabio N. Kepler</a>
|
<a href=/people/r/ramon-fernandez-astudillo/>Ramón Astudillo</a>
|
<a href=/people/c/chris-hokamp/>Chris Hokamp</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1015><div class="card-body p-3 small">Translation quality estimation is a task of growing importance in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, due to its potential to reduce post-editing human effort in disruptive ways. However, this potential is currently limited by the relatively low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of existing <a href=https://en.wikipedia.org/wiki/System>systems</a>. In this paper, we achieve remarkable improvements by exploiting synergies between the related tasks of word-level quality estimation and automatic post-editing. First, we stack a new, carefully engineered, neural model into a rich feature-based word-level quality estimation system. Then, we use the output of an automatic post-editing system as an extra feature, obtaining striking results on WMT16 : a word-level FMULT1 score of 57.47 % (an absolute gain of +7.95 % over the current state of the art), and a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation score</a> of 65.56 % for sentence-level HTER prediction (an absolute gain of +13.36 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953410 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1016/>Winning on the Merits : The Joint Effects of Content and Style on Debate Outcomes</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/n/nick-beauchamp/>Nick Beauchamp</a>
|
<a href=/people/s/sarah-shugars/>Sarah Shugars</a>
|
<a href=/people/k/kechen-qin/>Kechen Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1016><div class="card-body p-3 small">Debate and <a href=https://en.wikipedia.org/wiki/Deliberation>deliberation</a> play essential roles in <a href=https://en.wikipedia.org/wiki/Politics>politics</a> and <a href=https://en.wikipedia.org/wiki/Government>government</a>, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of <a href=https://en.wikipedia.org/wiki/Debate>debate</a> that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model&#8217;s combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74 % accuracy, significantly outperforming linguistic features alone (66 %). Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> finds that winning sides employ stronger arguments, and allows us to identify the <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a> associated with strong or weak arguments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1017/>Domain-Targeted, High Precision Knowledge Extraction</a></strong><br><a href=/people/b/bhavana-dalvi/>Bhavana Dalvi Mishra</a>
|
<a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1017><div class="card-body p-3 small">Our goal is to construct a domain-targeted, high precision knowledge base (KB), containing general (subject, predicate, object) statements about the world, in support of a downstream question-answering (QA) application. Despite recent advances in information extraction (IE) techniques, no suitable resource for our task already exists ; existing resources are either too noisy, too named-entity centric, or too incomplete, and typically have not been constructed with a clear scope or purpose. To address these, we have created a domain-targeted, high precision knowledge extraction pipeline, leveraging <a href=https://en.wikipedia.org/wiki/Open_IE>Open IE</a>, <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, and a novel canonical schema learning algorithm (called CASI), that produces high precision knowledge targeted to a particular domain-in our case, elementary science. To measure the KB&#8217;s coverage of the target domain&#8217;s knowledge (its comprehensiveness with respect to science) we measure <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> with respect to an independent corpus of domain text, and show that our pipeline produces output with over 80 % precision and 23 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> with respect to that target, a substantially higher coverage of tuple-expressible science knowledge than other comparable resources. We have made the KB publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/Q17-1018.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952125 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1018/>Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling</a></strong><br><a href=/people/g/gabor-berend/>Gábor Berend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1018><div class="card-body p-3 small">In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> has favorable generalization properties as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> retains over 89.8 % of its average POS tagging accuracy when trained at 1.2 % of the total available training data, i.e. 150 sentences per language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/276419865 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1020/>Cross-Lingual Syntactic Transfer with Limited Resources</a></strong><br><a href=/people/m/mohammad-sadegh-rasooli/>Mohammad Sadegh Rasooli</a>
|
<a href=/people/m/michael-collins/>Michael Collins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1020><div class="card-body p-3 small">We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps : 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser ; 2) a method for transferring lexical information from a target language to source language treebanks ; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015). Experiments show improvements over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in several languages used in previous work, in a setting where the only source of translation data is the <a href=https://en.wikipedia.org/wiki/Bible>Bible</a>, a considerably smaller corpus than the <a href=https://en.wikipedia.org/wiki/Europarl_Corpus>Europarl corpus</a> used in previous work. Results using the <a href=https://en.wikipedia.org/wiki/Europarl_corpus>Europarl corpus</a> as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets from the Universal Dependencies corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952612 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1021/>Overcoming <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>Language Variation</a> in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> with Social Attention</a></strong><br><a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1021><div class="card-body p-3 small">Variation in <a href=https://en.wikipedia.org/wiki/Language>language</a> is ubiquitous, particularly in newer forms of <a href=https://en.wikipedia.org/wiki/Writing>writing</a> such as <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Fortunately, variation is not random ; it is often linked to social properties of the author. In this paper, we show how to exploit <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> to make <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> more robust to social language variation. The key idea is linguistic homophily : the tendency of socially linked individuals to use <a href=https://en.wikipedia.org/wiki/Language>language</a> in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author&#8217;s position in the <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. This has the effect of smoothing the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification function</a> across the <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the accuracies of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and on review data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1022/>Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints</a></strong><br><a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/d/diarmuid-o-seaghdha/>Diarmuid Ó Séaghdha</a>
|
<a href=/people/i/ira-leviant/>Ira Leviant</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/s/steve-young/>Steve Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1022><div class="card-body p-3 small">We present Attract-Repel, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Attract-Repel facilitates the use of constraints from mono- and cross-lingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource ones. The effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages. We next show that Attract-Repel-specialized vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show that cross-lingual vector spaces produced by our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> facilitate the training of multilingual DST models, which brings further performance improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230459 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1023/>Colors in Context : A Pragmatic Neural Model for Grounded Language Understanding</a></strong><br><a href=/people/w/will-monroe/>Will Monroe</a>
|
<a href=/people/r/robert-x-d-hawkins/>Robert X.D. Hawkins</a>
|
<a href=/people/n/noah-goodman/>Noah D. Goodman</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1023><div class="card-body p-3 small">We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases : when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233299 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1024/>Google’s Multilingual Neural Machine Translation System : Enabling Zero-Shot Translation<span class=acl-fixed-case>G</span>oogle’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a></strong><br><a href=/people/m/melvin-johnson/>Melvin Johnson</a>
|
<a href=/people/m/mike-schuster/>Mike Schuster</a>
|
<a href=/people/q/quoc-le/>Quoc V. Le</a>
|
<a href=/people/m/maxim-krikun/>Maxim Krikun</a>
|
<a href=/people/y/yonghui-wu/>Yonghui Wu</a>
|
<a href=/people/z/zhifeng-chen/>Zhifeng Chen</a>
|
<a href=/people/n/nikhil-thorat/>Nikhil Thorat</a>
|
<a href=/people/f/fernanda-viegas/>Fernanda Viégas</a>
|
<a href=/people/m/martin-wattenberg/>Martin Wattenberg</a>
|
<a href=/people/g/greg-corrado/>Greg Corrado</a>
|
<a href=/people/m/macduff-hughes/>Macduff Hughes</a>
|
<a href=/people/j/jeffrey-dean/>Jeffrey Dean</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1024><div class="card-body p-3 small">We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. On the WMT&#8217;14 benchmarks, a single multilingual model achieves comparable performance for <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>EnglishFrench</a> and surpasses state-of-theart results for <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>EnglishGerman</a>. Similarly, a single multilingual model surpasses state-of-the-art results for <a href=https://en.wikipedia.org/wiki/French_language>FrenchEnglish</a> and <a href=https://en.wikipedia.org/wiki/German_language>GermanEnglish</a> on WMT&#8217;14 and WMT&#8217;15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952859 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1025/>Unsupervised Learning of Morphological Forests</a></strong><br><a href=/people/j/jiaming-luo/>Jiaming Luo</a>
|
<a href=/people/k/karthik-narasimhan/>Karthik Narasimhan</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1025><div class="card-body p-3 small">This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edge-wise properties reflecting single-step morphological derivations, along with global distributional properties of the entire <a href=https://en.wikipedia.org/wiki/Forest>forest</a>. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting <a href=https://en.wikipedia.org/wiki/Loss_function>objective</a> is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks : <a href=https://en.wikipedia.org/wiki/Root-finding_algorithm>root detection</a>, clustering of morphological families, and <a href=https://en.wikipedia.org/wiki/Segmentation_(biology)>segmentation</a>. Our experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields consistent gains in all three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> compared with the best published results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955246 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1026/>Fully Character-Level Neural Machine Translation without Explicit Segmentation</a></strong><br><a href=/people/j/jason-lee/>Jason Lee</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/t/thomas-hofmann/>Thomas Hofmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1026><div class="card-body p-3 small">Most existing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT&#8217;15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1027/>Ordinal Common-sense Inference</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1027><div class="card-body p-3 small">Humans have the capacity to draw common-sense inferences from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> : various things that are likely but not certain to hold based on established <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>, and are rarely stated explicitly. We propose an evaluation of automated common-sense inference based on an extension of recognizing textual entailment : predicting ordinal human responses on the subjective likelihood of an inference holding in a given context. We describe a framework for extracting common-sense knowledge from corpora, which is then used to construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this ordinal entailment task. We train a neural sequence-to-sequence model on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, which we use to score and generate possible inferences. Further, we annotate subsets of previously established datasets via our ordinal annotation protocol in order to then analyze the distinctions between these and what we have constructed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1028" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/Q17-1028/>Learning Distributed Representations of Texts and Entities from Knowledge Base</a></strong><br><a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/h/hideaki-takeda/>Hideaki Takeda</a>
|
<a href=/people/y/yoshiyasu-takefuji/>Yoshiyasu Takefuji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1028><div class="card-body p-3 small">We describe a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to predict entities that are relevant to the text. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is designed to be generic with the ability to address various NLP tasks with ease. We train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using a large corpus of texts and their entity annotations extracted from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We evaluated the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> on three important NLP tasks (i.e., sentence textual similarity, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. Our code and trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are publicly available for further academic research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1030/>Evaluating Low-Level Speech Features Against Human Perceptual Data</a></strong><br><a href=/people/c/caitlin-richter/>Caitlin Richter</a>
|
<a href=/people/n/naomi-feldman/>Naomi H. Feldman</a>
|
<a href=/people/h/harini-salgado/>Harini Salgado</a>
|
<a href=/people/a/aren-jansen/>Aren Jansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1030><div class="card-body p-3 small">We introduce a method for measuring the correspondence between low-level speech features and <a href=https://en.wikipedia.org/wiki/Perception>human perception</a>, using a cognitive model of speech perception implemented directly on speech recordings. We evaluate two speaker normalization techniques using this method and find that in both cases, <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>speech features</a> that are normalized across speakers predict human data better than unnormalized speech features, consistent with previous research. Results further reveal differences across <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization methods</a> in how well each predicts human data. This work provides a new framework for evaluating low-level representations of speech on their match to <a href=https://en.wikipedia.org/wiki/Perception>human perception</a>, and lays the groundwork for creating more ecologically valid models of <a href=https://en.wikipedia.org/wiki/Speech_perception>speech perception</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/277673914 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1032/>Unsupervised Acquisition of Comprehensive Multiword Lexicons using Competition in an n-gram Lattice</a></strong><br><a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1032><div class="card-body p-3 small">We present a new model for acquiring comprehensive multiword lexicons from large corpora based on competition among n-gram candidates. In contrast to the standard approach of simple ranking by association measure, in our model n-grams are arranged in a lattice structure based on subsumption and overlap relationships, with nodes inhibiting other nodes in their vicinity when they are selected as a lexical item. We show how the configuration of such a <a href=https://en.wikipedia.org/wiki/Lattice_(group)>lattice</a> can be optimized tractably, and demonstrate using annotations of sampled n-grams that our method consistently outperforms alternatives by at least 0.05 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> across several corpora and languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/285803652 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1033" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1033/>Replicability Analysis for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> : Testing Significance with Multiple Datasets</a></strong><br><a href=/people/r/rotem-dror/>Rotem Dror</a>
|
<a href=/people/g/gili-baumer/>Gili Baumer</a>
|
<a href=/people/m/marina-bogomolov/>Marina Bogomolov</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1033><div class="card-body p-3 small">With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications : multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/Q17-1035/>Joint Prediction of Word Alignment with Alignment Types</a></strong><br><a href=/people/a/anahita-mansouri-bigvand/>Anahita Mansouri Bigvand</a>
|
<a href=/people/t/te-bu/>Te Bu</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1035><div class="card-body p-3 small">Current word alignment models do not distinguish between different types of alignment links. In this paper, we provide a new probabilistic model for <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> where word alignments are associated with linguistically motivated alignment types. We propose a novel task of joint prediction of word alignment and alignment types and propose novel <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning algorithms</a> for this task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> without alignment types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/276406923 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1036" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1036/>Aspect-augmented Adversarial Networks for Domain Adaptation</a></strong><br><a href=/people/y/yuan-zhang/>Yuan Zhang</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/t/tommi-jaakkola/>Tommi Jaakkola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1036><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural method</a> for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> between two (source and target) <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification tasks</a> or aspects over the same domain. Rather than training on target labels, we use a few <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, yielding an improvement of 27 % on a pathology dataset and 5 % on a review dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/Q17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-Q17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-Q17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/276403824 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=Q17-1037" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/Q17-1037/>Anchored Correlation Explanation : Topic Modeling with Minimal Domain Knowledge</a></strong><br><a href=/people/r/ryan-j-gallagher/>Ryan J. Gallagher</a>
|
<a href=/people/k/kyle-reing/>Kyle Reing</a>
|
<a href=/people/d/david-kale/>David Kale</a>
|
<a href=/people/g/greg-ver-steeg/>Greg Ver Steeg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-Q17-1037><div class="card-body p-3 small">While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a>, they often require detailed assumptions and careful specification of hyperparameters. Such model complexity issues only compound when trying to generalize <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> to incorporate <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>human input</a>. We introduce Correlation Explanation (CorEx), an alternative approach to <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> that does not assume an underlying <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a>, and instead learns maximally informative topics through an information-theoretic framework. This <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> naturally generalizes to hierarchical and semi-supervised extensions with no additional modeling assumptions. In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> to be promoted with minimal human intervention. Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semi-supervised variants of LDA.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>