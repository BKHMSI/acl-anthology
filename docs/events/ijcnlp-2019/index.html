<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Joint Conference on Natural Language Processing (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Joint Conference on Natural Language Processing (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#d19-1>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a>
<span class="badge badge-info align-middle ml-1">332&nbsp;papers</span></li><li><a class=align-middle href=#d19-2>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#d19-3>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li></ul></div></div><div id=d19-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-1/>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1000/>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></strong><br><a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1002.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1002/>Attention is not not Explanation</a></strong><br><a href=/people/s/sarah-wiegreffe/>Sarah Wiegreffe</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1002><div class="card-body p-3 small">Attention mechanisms play a central role in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>, especially within <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN) models</a>. Recently, there has been increasing interest in whether or not the intermediate representations offered by these <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> may be used to explain the reasoning for a model&#8217;s prediction, and consequently reach insights regarding the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s decision-making process. A recent paper claims that &#8216;Attention is not Explanation&#8217; (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one&#8217;s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when / whether <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be used as explanation : a simple uniform-weights baseline ; a variance calibration based on multiple random seed runs ; a diagnostic framework using frozen weights from pretrained models ; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> in <a href=https://en.wikipedia.org/wiki/Neural_circuit>RNN models</a>. We show that even when reliable adversarial distributions can be found, they do n&#8217;t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1003.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1003/>Practical Obstacles to Deploying <a href=https://en.wikipedia.org/wiki/Active_learning>Active Learning</a></a></strong><br><a href=/people/d/david-lowell/>David Lowell</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary C. Lipton</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1003><div class="card-body p-3 small">Active learning (AL) is a widely-used <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training strategy</a> for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>AL strategies</a>. Moreover, AL couples the training dataset with the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1005.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1005/>Knowledge Enhanced Contextual Word Representations</a></strong><br><a href=/people/m/matthew-e-peters/>Matthew E. Peters</a>
|
<a href=/people/m/mark-neumann/>Mark Neumann</a>
|
<a href=/people/r/robert-logan/>Robert Logan</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/v/vidur-joshi/>Vidur Joshi</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1005><div class="card-body p-3 small">Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linkers</a> and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and a subset of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>. KnowBert&#8217;s runtime is comparable to BERT&#8217;s and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> scales to large KBs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1006/>How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings<span class=acl-fixed-case>C</span>omparing the Geometry of <span class=acl-fixed-case>BERT</span>, <span class=acl-fixed-case>ELM</span>o, and <span class=acl-fixed-case>GPT</span>-2 Embeddings</a></strong><br><a href=/people/k/kawin-ethayarajh/>Kawin Ethayarajh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1006><div class="card-body p-3 small">Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this <a href=https://en.wikipedia.org/wiki/Self-similarity>self-similarity</a> is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5 % of the variance in a word&#8217;s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1008.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1008/>Correlations between Word Vector Sets</a></strong><br><a href=/people/v/vitalii-zhelezniak/>Vitalii Zhelezniak</a>
|
<a href=/people/a/april-shen/>April Shen</a>
|
<a href=/people/d/daniel-busbridge/>Daniel Busbridge</a>
|
<a href=/people/a/aleksandar-savkov/>Aleksandar Savkov</a>
|
<a href=/people/n/nils-hammerla/>Nils Hammerla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1008><div class="card-body p-3 small">Similarity measures based purely on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are comfortably competing with much more sophisticated deep learning and expert-engineered systems on unsupervised semantic textual similarity (STS) tasks. In contrast to commonly used <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>geometric approaches</a>, we treat a <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>single word embedding</a> as e.g. 300 observations from a <a href=https://en.wikipedia.org/wiki/Scalar_(mathematics)>scalar random variable</a>. Using this paradigm, we first illustrate that similarities derived from elementary pooling operations and classic correlation coefficients yield excellent results on standard STS benchmarks, outperforming many recently proposed methods while being much faster and trivial to implement. Next, we demonstrate how to avoid pooling operations altogether and compare sets of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> directly via correlation operators between reproducing kernel Hilbert spaces. Just like <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a> is used to compare individual word vectors, we introduce a novel application of the centered kernel alignment (CKA) as a natural generalisation of squared cosine similarity for sets of word vectors. Likewise, CKA is very easy to implement and enjoys very strong empirical results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1010.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1010/>Guided Dialog Policy Learning : Reward Estimation for Multi-Domain Task-Oriented Dialog</a></strong><br><a href=/people/r/ryuichi-takanobu/>Ryuichi Takanobu</a>
|
<a href=/people/h/hanlin-zhu/>Hanlin Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1010><div class="card-body p-3 small">Dialog policy decides what and how a task-oriented dialog system will respond, and plays a vital role in delivering effective conversations. Many studies apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> to learn a dialog policy with the reward function which requires elaborate design and pre-specified user goals. With the growing needs to handle complex goals across multiple domains, such manually designed reward functions are not affordable to deal with the complexity of real-world tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm based on Adversarial Inverse Reinforcement Learning for joint reward estimation and policy optimization in multi-domain task-oriented dialog. The proposed approach estimates the <a href=https://en.wikipedia.org/wiki/Reward_system>reward signal</a> and infers the <a href=https://en.wikipedia.org/wiki/Goal>user goal</a> in the <a href=https://en.wikipedia.org/wiki/Dialogue>dialog sessions</a>. The reward estimator evaluates the state-action pairs so that it can guide the dialog policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset show that the dialog policy guided by the learned reward function achieves remarkably higher task success than state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1011/>Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots</a></strong><br><a href=/people/c/chunyuan-yuan/>Chunyuan Yuan</a>
|
<a href=/people/w/wei-zhou/>Wei Zhou</a>
|
<a href=/people/m/mingming-li/>Mingming Li</a>
|
<a href=/people/s/shangwen-lv/>Shangwen Lv</a>
|
<a href=/people/f/fuqing-zhu/>Fuqing Zhu</a>
|
<a href=/people/j/jizhong-han/>Jizhong Han</a>
|
<a href=/people/s/songlin-hu/>Songlin Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1011><div class="card-body p-3 small">Multi-turn retrieval-based conversation is an important task for building intelligent dialogue systems. Existing works mainly focus on matching candidate responses with every context utterance on multiple levels of granularity, which ignore the side effect of using excessive context information. Context utterances provide abundant information for extracting more matching features, but it also brings <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise signals</a> and <a href=https://en.wikipedia.org/wiki/Information_overload>unnecessary information</a>. In this paper, we will analyze the side effect of using too many context utterances and propose a multi-hop selector network (MSN) to alleviate the problem. Specifically, <a href=https://en.wikipedia.org/wiki/MSN>MSN</a> firstly utilizes a multi-hop selector to select the relevant utterances as context. Then, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> matches the filtered context with the candidate response and obtains a matching score. Experimental results show that <a href=https://en.wikipedia.org/wiki/MSN>MSN</a> outperforms some state-of-the-art methods on three public multi-turn dialogue datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1012/>MoEL : Mixture of Empathetic Listeners<span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>EL</span>: Mixture of Empathetic Listeners</a></strong><br><a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1012><div class="card-body p-3 small">Previous research on empathetic dialogue systems has mostly focused on generating responses given certain emotions. However, being empathetic not only requires the ability of generating emotional responses, but more importantly, requires the understanding of user emotions and replying appropriately. In this paper, we propose a novel end-to-end approach for modeling empathy in dialogue systems : Mixture of Empathetic Listeners (MoEL). Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> first captures the <a href=https://en.wikipedia.org/wiki/Emotion>user emotions</a> and outputs an emotion distribution. Based on this, MoEL will softly combine the output states of the appropriate Listener(s), which are each optimized to react to certain emotions, and generate an empathetic response. Human evaluations on EMPATHETIC-DIALOGUES dataset confirm that MoEL outperforms multitask training baseline in terms of <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>, <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a>, and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Furthermore, the case study on generated responses of different Listeners shows high interpretability of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1013/>Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever<span class=acl-fixed-case>KB</span> Retriever</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/yangming-li/>Yangming Li</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1013><div class="card-body p-3 small">Querying the knowledge base (KB) has long been a challenge in the end-to-end task-oriented dialogue system. Previous sequence-to-sequence (Seq2Seq) dialogue generation work treats the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB query</a> as an attention over the entire <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a>, without the guarantee that the generated entities are consistent with each other. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> which queries the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> in two steps to improve the consistency of generated entities. In the first step, inspired by the observation that a response can usually be supported by a single <a href=https://en.wikipedia.org/wiki/Row_(database)>KB row</a>, we introduce a KB retrieval component which explicitly returns the most relevant <a href=https://en.wikipedia.org/wiki/Row_(database)>KB row</a> given a dialogue history. The retrieval result is further used to filter the irrelevant entities in a Seq2Seq response generation model to improve the consistency among the output entities. In the second step, we further perform the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to address the most correlated KB column. Two methods are proposed to make the training feasible without labeled retrieval data, which include distant supervision and Gumbel-Softmax technique. Experiments on two publicly available task oriented dialog datasets show the effectiveness of our model by outperforming the baseline systems and producing entity-consistent responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1015/>DialogueGCN : A Graph Convolutional Neural Network for Emotion Recognition in Conversation<span class=acl-fixed-case>D</span>ialogue<span class=acl-fixed-case>GCN</span>: A Graph Convolutional Neural Network for Emotion Recognition in Conversation</a></strong><br><a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/n/niyati-chhaya/>Niyati Chhaya</a>
|
<a href=/people/a/alexander-gelbukh/>Alexander Gelbukh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1015><div class="card-body p-3 small">Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as <a href=https://en.wikipedia.org/wiki/Health_care>health-care</a>, education, and <a href=https://en.wikipedia.org/wiki/Human_resources>human resources</a>. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>. Through the <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph network</a>, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1018/>Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects</a></strong><br><a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/j/jiacheng-li/>Jiacheng Li</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1018><div class="card-body p-3 small">Several recent works have considered the problem of generating reviews (or &#8216;tips&#8217;) as a form of explanation as to why a recommendation might match a customer&#8217;s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users&#8217; decision-making process. We seek to introduce new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to address the recommendation justification task. In terms of data, we first propose an &#8216;extractive&#8217; approach to identify review segments which justify users&#8217; intentions ; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data : (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is capable of generating convincing and diverse justifications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1019/>Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning</a></strong><br><a href=/people/k/kaisong-song/>Kaisong Song</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/w/wei-gao/>Wei Gao</a>
|
<a href=/people/j/jun-lin/>Jun Lin</a>
|
<a href=/people/l/lujun-zhao/>Lujun Zhao</a>
|
<a href=/people/j/jiancheng-wang/>Jiancheng Wang</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/x/xiaozhong-liu/>Xiaozhong Liu</a>
|
<a href=/people/q/qiong-zhang/>Qiong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1019><div class="card-body p-3 small">Customers ask questions and customer service staffs answer their questions, which is the basic <a href=https://en.wikipedia.org/wiki/Service_model>service model</a> via multi-turn customer service (CS) dialogues on <a href=https://en.wikipedia.org/wiki/E-commerce>E-commerce platforms</a>. Existing studies fail to provide comprehensive service satisfaction analysis, namely satisfaction polarity classification (e.g., well satisfied, met and unsatisfied) and sentimental utterance identification (e.g., positive, neutral and negative). In this paper, we conduct a pilot study on the task of service satisfaction analysis (SSA) based on multi-turn CS dialogues. We propose an extensible Context-Assisted Multiple Instance Learning (CAMIL) model to predict the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiments</a> of all the customer utterances and then aggregate those <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiments</a> into service satisfaction polarity. After that, we propose a novel Context Clue Matching Mechanism (CCMM) to enhance the representations of all customer utterances with their matched context clues, i.e., sentiment and reasoning clues. We construct two CS dialogue datasets from a top E-commerce platform. Extensive experimental results are presented and contrasted against a few previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to demonstrate the efficacy of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1022.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1022/>Improving <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a> with Knowledge-attention</a></strong><br><a href=/people/p/pengfei-li/>Pengfei Li</a>
|
<a href=/people/k/kezhi-mao/>Kezhi Mao</a>
|
<a href=/people/x/xuefeng-yang/>Xuefeng Yang</a>
|
<a href=/people/q/qi-li/>Qi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1022><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> have been proven to be effective in many NLP tasks, majority of them are data-driven. We propose a novel knowledge-attention encoder which incorporates prior knowledge from external lexical resources into <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> for relation extraction task. Furthermore, we present three effective ways of integrating knowledge-attention with self-attention to maximize the utilization of both knowledge and data. The proposed relation extraction system is end-to-end and fully attention-based. Experiment results show that the proposed knowledge-attention mechanism has complementary strengths with self-attention, and our integrated models outperform existing CNN, RNN, and self-attention based models. State-of-the-art performance is achieved on TACRED, a complex and large-scale relation extraction dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1024.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1024/>Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion</a></strong><br><a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/k/kwunping-lai/>Kwunping Lai</a>
|
<a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1024><div class="card-body p-3 small">For large-scale knowledge graphs (KGs), recent research has been focusing on the large proportion of infrequent relations which have been ignored by previous studies. For example few-shot learning paradigm for <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> has been investigated. In this work, we further advocate that handling uncommon entities is inevitable when dealing with infrequent relations. Therefore, we propose a meta-learning framework that aims at handling infrequent relations with few-shot learning and uncommon entities by using textual descriptions. We design a novel <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to better extract key information from textual descriptions. Besides, we also develop a novel <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> in our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to enhance the performance by generating extra triplets during the training stage. Experiments are conducted on two datasets from real-world KGs, and the results show that our framework outperforms previous methods when dealing with infrequent relations and their accompanying uncommon entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1025" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1025/>Low-Resource Name Tagging Learned with Weakly Labeled Data</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/z/zikun-hu/>Zikun Hu</a>
|
<a href=/people/t/tat-seng-chua/>Tat-seng Chua</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1025><div class="card-body p-3 small">Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively : (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics ; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two <a href=https://en.wikipedia.org/wiki/Module_(computer_science)>modules</a> are combined via <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>shared parameters</a>. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6 % and 7.8 % F1 gains on average) as well as <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1026.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1026/>Learning Dynamic Context Augmentation for Global Entity Linking</a></strong><br><a href=/people/x/xiyuan-yang/>Xiyuan Yang</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/s/sheng-lin/>Sheng Lin</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1026><div class="card-body p-3 small">Despite of the recent success of collective entity linking (EL) methods, these global inference methods may yield sub-optimal results when the all-mention coherence assumption breaks, and often suffer from high computational cost at the inference stage, due to the complex search space. In this paper, we propose a simple yet effective solution, called Dynamic Context Augmentation (DCA), for collective EL, which requires only one pass through the mentions in a document. DCA sequentially accumulates context information to make efficient, collective inference, and can cope with different local EL models as a plug-and-enhance module. We explore both supervised and reinforcement learning strategies for learning the DCA model. Extensive experiments show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with different learning settings, base models, <a href=https://en.wikipedia.org/wiki/Decision-making>decision orders</a> and <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1028/>Learning to Bootstrap for Entity Set Expansion</a></strong><br><a href=/people/l/lingyong-yan/>Lingyong Yan</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/b/ben-he/>Ben He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1028><div class="card-body p-3 small">Bootstrapping for Entity Set Expansion (ESE) aims at iteratively acquiring new instances of a specific target category. Traditional bootstrapping methods often suffer from two problems : 1) delayed feedback, i.e., the pattern evaluation relies on both its direct extraction quality and extraction quality in later iterations. 2) sparse supervision, i.e., only few seed entities are used as the <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. To address the above two problems, we propose a novel bootstrapping method combining the Monte Carlo Tree Search (MCTS) algorithm with a deep similarity network, which can efficiently estimate delayed feedback for pattern evaluation and adaptively score entities given sparse supervision signals. Experimental results confirm the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1029/>Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text</a></strong><br><a href=/people/t/tianwen-jiang/>Tianwen Jiang</a>
|
<a href=/people/t/tong-zhao/>Tong Zhao</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/n/nitesh-chawla/>Nitesh Chawla</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1029><div class="card-body p-3 small">Condition is essential in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific statement</a>. Without the conditions (e.g., equipment, environment) that were precisely specified, facts (e.g., observations) in the statements may no longer be valid. Existing ScienceIE methods, which aim at extracting factual tuples from scientific text, do not consider the conditions. In this work, we propose a new sequence labeling framework (as well as a new tag schema) to jointly extract the fact and condition tuples from statement sentences. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> has (1) a multi-output module to generate one or multiple tuples and (2) a multi-input module to feed in multiple types of signals as sequences. It improves <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> relatively by 4.2 % on BioNLP2013 and by 6.2 % on a new bio-text dataset for tuple extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1030/>Cross-lingual Structure Transfer for Relation and Event Extraction</a></strong><br><a href=/people/a/ananya-subburathinam/>Ananya Subburathinam</a>
|
<a href=/people/d/di-lu/>Di Lu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a>
|
<a href=/people/a/avirup-sil/>Avirup Sil</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1030><div class="card-body p-3 small">The identification of complex semantic structures such as <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity relations</a>, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of cross-lingual structure transfer techniques for these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> demonstrate that our approach achieves performance comparable to state-of-the-art <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> trained on up to 3,000 manually annotated mentions : up to 62.6 % F-score for Relation Extraction, and 63.1 % F-score for Event Argument Role Labeling. The event argument role labeling model transferred from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> achieves similar performance as the model trained from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1032.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1032" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1032/>Doc2EDAG : An End-to-End Document-level Framework for Chinese Financial Event Extraction<span class=acl-fixed-case>D</span>oc2<span class=acl-fixed-case>EDAG</span>: An End-to-End Document-level Framework for <span class=acl-fixed-case>C</span>hinese Financial Event Extraction</a></strong><br><a href=/people/s/shun-zheng/>Shun Zheng</a>
|
<a href=/people/w/wei-cao/>Wei Cao</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/j/jiang-bian/>Jiang Bian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1032><div class="card-body p-3 small">Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as <a href=https://en.wikipedia.org/wiki/Finance>finance</a>, <a href=https://en.wikipedia.org/wiki/Legislation>legislation</a>, <a href=https://en.wikipedia.org/wiki/Health>health</a>, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1034.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1034/>A Boundary-aware Neural Model for Nested Named Entity Recognition</a></strong><br><a href=/people/c/changmeng-zheng/>Changmeng Zheng</a>
|
<a href=/people/y/yi-cai/>Yi Cai</a>
|
<a href=/people/j/jingyun-xu/>Jingyun Xu</a>
|
<a href=/people/h/ho-fung-leung/>Ho-fung Leung</a>
|
<a href=/people/g/guandong-xu/>Guandong Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1034><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, it is common that many entities contain other entities inside them. Most existing works on named entity recognition (NER) only deal with flat entities but ignore nested ones. We propose a boundary-aware neural model for nested NER which leverages entity boundaries to predict entity categorical labels. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can locate entities precisely by detecting boundaries using sequence labeling models. Based on the detected boundaries, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> utilizes the boundary-relevant regions to predict entity categorical labels, which can decrease computation cost and relieve error propagation problem in layered sequence labeling model. We introduce <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> to capture the dependencies of entity boundaries and their categorical labels, which helps to improve the performance of identifying entities. We conduct our experiments on GENIA dataset and the experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms other state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1035/>Learning the Extraction Order of Multiple Relational Facts in a Sentence with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/x/xiangrong-zeng/>Xiangrong Zeng</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/d/daojian-zeng/>Daojian Zeng</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1035><div class="card-body p-3 small">The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works did n&#8217;t consider the extraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> into a sequence-to-sequence model. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could generate <a href=https://en.wikipedia.org/wiki/Relational_model>relational facts</a> freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1040" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1040/>EntEval : A Holistic Evaluation Benchmark for Entity Representations<span class=acl-fixed-case>E</span>nt<span class=acl-fixed-case>E</span>val: A Holistic Evaluation Benchmark for Entity Representations</a></strong><br><a href=/people/m/mingda-chen/>Mingda Chen</a>
|
<a href=/people/z/zewei-chu/>Zewei Chu</a>
|
<a href=/people/y/yang-chen/>Yang Chen</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1040><div class="card-body p-3 small">Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity representations</a>. In this work, we propose EntEval : a test suite of diverse tasks that require nontrivial understanding of entities including <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity typing</a>, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity similarity</a>, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity relation prediction</a>, and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity disambiguation</a>. In addition, we develop <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training techniques</a> for learning better <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity representations</a> by using natural hyperlink annotations in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We identify effective objectives for incorporating the contextual information in <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> into state-of-the-art pretrained language models (Peters et al., 2018) and show that they improve strong baselines on multiple EntEval tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1044/>Label-Specific Document Representation for Multi-Label Text Classification</a></strong><br><a href=/people/l/lin-xiao/>Lin Xiao</a>
|
<a href=/people/x/xin-huang/>Xin Huang</a>
|
<a href=/people/b/boli-chen/>Boli Chen</a>
|
<a href=/people/l/liping-jing/>Liping Jing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1044><div class="card-body p-3 small">Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a Label-Specific Attention Network (LSAN) to learn a label-specific document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to seamlessly integrate the above two parts, an adaptive fusion strategy is proposed, which can effectively output the comprehensive label-specific document representation to build multi-label text classifier. Extensive experimental results demonstrate that LSAN consistently outperforms the state-of-the-art methods on four different datasets, especially on the prediction of low-frequency labels. The code and hyper-parameter settings are released to facilitate other researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1050.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1050/>Linking artificial and human neural representations of language</a></strong><br><a href=/people/j/jon-gauthier/>Jon Gauthier</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1050><div class="card-body p-3 small">What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1051.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1051/>Neural Text Summarization : A Critical Evaluation</a></strong><br><a href=/people/w/wojciech-kryscinski/>Wojciech Kryscinski</a>
|
<a href=/people/n/nitish-shirish-keskar/>Nitish Shirish Keskar</a>
|
<a href=/people/b/bryan-mccann/>Bryan McCann</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1051><div class="card-body p-3 small">Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> has stagnated. We critically evaluate key ingredients of the current research setup : <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, evaluation metrics, and <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, and highlight three primary shortcomings : 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1053.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1053/>MoverScore : Text Generation Evaluating with <a href=https://en.wikipedia.org/wiki/Contextualization>Contextualized Embeddings</a> and <a href=https://en.wikipedia.org/wiki/Earth_mover_distance>Earth Mover Distance</a><span class=acl-fixed-case>M</span>over<span class=acl-fixed-case>S</span>core: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/c/christian-m-meyer/>Christian M. Meyer</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1053><div class="card-body p-3 small">A robust evaluation metric has a profound impact on the development of text generation systems. A desirable <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> compares system output against references based on their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Closed_captioning>image captioning</a>, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> combining contextualized representations with a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>distance measure</a> perform the best. Such <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> also demonstrate strong generalization capability across tasks. For ease-of-use we make our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> available as <a href=https://en.wikipedia.org/wiki/Web_service>web service</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1055.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1055/>Sentence-Level Content Planning and Style Specification for Neural Text Generation</a></strong><br><a href=/people/x/xinyu-hua/>Xinyu Hua</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1055><div class="card-body p-3 small">Building effective text generation systems requires three critical components : content selection, text planning, and surface realization, and traditionally they are tackled as separate problems. Recent all-in-one style neural generation models have made impressive progress, yet they often produce outputs that are incoherent and unfaithful to the input. To address these issues, we present an end-to-end trained two-step generation model, where a sentence-level content planner first decides on the keyphrases to cover as well as a desired language style, followed by a surface realization decoder that generates relevant and coherent text. For experiments, we consider three tasks from domains with diverse topics and varying language styles : persuasive argument construction from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, paragraph generation for normal and simple versions of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, and abstract generation for scientific articles. Automatic evaluation shows that our <a href=https://en.wikipedia.org/wiki/System>system</a> can significantly outperform competitive comparisons. Human judges further rate our <a href=https://en.wikipedia.org/wiki/System>system</a> generated text as more fluent and correct, compared to the generations by its variants that do not consider <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>language style</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1056.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1056" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1056/>Translate and Label ! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling</a></strong><br><a href=/people/a/angel-daza/>Angel Daza</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1056><div class="card-body p-3 small">We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> offers a flexible method for leveraging SRL data in multiple languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1058/>VerbAtlas : a Novel Large-Scale Verbal Semantic Resource and Its Application to <a href=https://en.wikipedia.org/wiki/Semantic_Role_Labeling>Semantic Role Labeling</a><span class=acl-fixed-case>V</span>erb<span class=acl-fixed-case>A</span>tlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling</a></strong><br><a href=/people/a/andrea-di-fabio/>Andrea Di Fabio</a>
|
<a href=/people/s/simone-conia/>Simone Conia</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1058><div class="card-body p-3 small">We present VerbAtlas, a new, hand-crafted lexical-semantic resource whose goal is to bring together all verbal synsets from <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> into semantically-coherent frames. The <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a> define a common, prototypical argument structure while at the same time providing new concept-specific information. In contrast to <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a>, which defines enumerative semantic roles, VerbAtlas comes with an explicit, cross-frame set of semantic roles linked to selectional preferences expressed in terms of WordNet synsets, and is the first resource enriched with semantic information about implicit, shadow, and default arguments. We demonstrate the effectiveness of VerbAtlas in the task of dependency-based Semantic Role Labeling and show how its integration into a high-performance system leads to improvements on both the in-domain and out-of-domain test sets of CoNLL-2009. VerbAtlas is available at http://verbatlas.org.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1059.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1059" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1059/>Parameter-free Sentence Embedding via <a href=https://en.wikipedia.org/wiki/Orthogonal_basis>Orthogonal Basis</a></a></strong><br><a href=/people/z/ziyi-yang/>Ziyi Yang</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1059><div class="card-body p-3 small">We propose a simple and robust non-parameterized approach for building <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a>. Inspired by the <a href=https://en.wikipedia.org/wiki/Gram&#8211;Schmidt_process>Gram-Schmidt Process</a> in geometric theory, we build an <a href=https://en.wikipedia.org/wiki/Orthogonal_basis>orthogonal basis</a> of the <a href=https://en.wikipedia.org/wiki/Linear_subspace>subspace</a> spanned by a word and its surrounding context in a sentence. We model the <a href=https://en.wikipedia.org/wiki/Semantics>semantic meaning</a> of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the word&#8217;s novel semantic meaning which shall be introduced as a new <a href=https://en.wikipedia.org/wiki/Basis_(linear_algebra)>basis vector</a> perpendicular to this existing <a href=https://en.wikipedia.org/wiki/Linear_subspace>subspace</a>. Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representations. This approach requires zero parameters, along with efficient <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> performance. We evaluate our approach on 11 downstream NLP tasks. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows superior performance compared with non-parameterized alternatives and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1061/>Extracting Possessions from <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> : Images Complement Language</a></strong><br><a href=/people/d/dhivya-chinnappa/>Dhivya Chinnappa</a>
|
<a href=/people/s/srikala-murugan/>Srikala Murugan</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1061><div class="card-body p-3 small">This paper describes a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and experiments to determine whether authors of tweets possess the objects they tweet about. We work with 5,000 tweets and show that both <a href=https://en.wikipedia.org/wiki/Human>humans</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> benefit from <a href=https://en.wikipedia.org/wiki/Image>images</a> in addition to <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. We also introduce a simple yet effective <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> to incorporate <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> into any <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> beyond weights from pretrained networks. Specifically, we consider the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a> identified in an <a href=https://en.wikipedia.org/wiki/Image>image</a> as an additional textual input, and leverage pretrained word embeddings as usually done with regular text. Experimental results show this novel <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> is beneficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1062.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1062/>Learning to Speak and Act in a Fantasy Text Adventure Game</a></strong><br><a href=/people/j/jack-urbanek/>Jack Urbanek</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/s/siddharth-karamcheti/>Siddharth Karamcheti</a>
|
<a href=/people/s/saachi-jain/>Saachi Jain</a>
|
<a href=/people/s/samuel-humeau/>Samuel Humeau</a>
|
<a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/t/tim-rocktaschel/>Tim Rocktschel</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/a/arthur-szlam/>Arthur Szlam</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1062><div class="card-body p-3 small">We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the <a href=https://en.wikipedia.org/wiki/Game>game</a>. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>. We analyze the ingredients necessary for successful grounding in this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a>, and how each of these factors relate to agents that can talk and act successfully.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1063.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1063" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1063/>Help, Anna ! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</a></strong><br><a href=/people/k/khanh-nguyen/>Khanh Nguyen</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daum III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1063><div class="card-body p-3 small">Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop <a href=https://en.wikipedia.org/wiki/Help_(TV_series)>Help</a>, Anna ! (HANNA), an interactive photo-realistic simulator in which an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> fulfills object-finding tasks by requesting and interpreting natural language-and-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of <a href=https://en.wikipedia.org/wiki/Decision-making>decision-making</a>, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1064/>Incorporating Visual Semantics into Sentence Representations within a Grounded Space</a></strong><br><a href=/people/p/patrick-bordes/>Patrick Bordes</a>
|
<a href=/people/e/eloi-zablocki/>Eloi Zablocki</a>
|
<a href=/people/l/laure-soulier/>Laure Soulier</a>
|
<a href=/people/b/benjamin-piwowarski/>Benjamin Piwowarski</a>
|
<a href=/people/p/patrick-gallinari/>Patrick Gallinari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1064><div class="card-body p-3 small">Language grounding is an active field aiming at enriching textual representations with <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a>. Generally, textual and visual elements are embedded in the same <a href=https://en.wikipedia.org/wiki/Representation_space>representation space</a>, which implicitly assumes a one-to-one correspondence between modalities. This hypothesis does not hold when representing words, and becomes problematic when used to learn sentence representations the focus of this paper as a visual scene can be described by a wide variety of sentences. To overcome this limitation, we propose to transfer <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> to textual representations by learning an intermediate representation space : the grounded space. We further propose two new complementary objectives ensuring that (1) sentences associated with the same visual content are close in the grounded space and (2) similarities between related elements are preserved across modalities. We show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on classification and semantic relatedness tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1065.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1065/>Neural Naturalist : Generating Fine-Grained Image Comparisons</a></strong><br><a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/c/christine-kaeser-chen/>Christine Kaeser-Chen</a>
|
<a href=/people/p/piyush-sharma/>Piyush Sharma</a>
|
<a href=/people/s/serge-belongie/>Serge Belongie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1065><div class="card-body p-3 small">We introduce the new Birds-to-Words dataset of 41k sentences describing fine-grained differences between photographs of birds. The language collected is highly detailed, while remaining understandable to the everyday observer (e.g., <a href=https://en.wikipedia.org/wiki/Heart_(symbol)>heart-shaped face</a>, squat body). Paragraph-length descriptions naturally adapt to varying levels of taxonomic and visual distancedrawn from a novel stratified sampling approachwith the appropriate level of detail. We propose a new model called Neural Naturalist that uses a joint image encoding and comparative module to generate comparative language, and evaluate the results with humans who must use the descriptions to distinguish real images. Our results indicate promising potential for neural models to explain differences in visual embedding space using <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, as well as a concrete path for <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to aid citizen scientists in their effort to preserve biodiversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1066/>Fine-Grained Evaluation for Entity Linking</a></strong><br><a href=/people/h/henry-rosales-mendez/>Henry Rosales-Mndez</a>
|
<a href=/people/a/aidan-hogan/>Aidan Hogan</a>
|
<a href=/people/b/barbara-poblete/>Barbara Poblete</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1066><div class="card-body p-3 small">The Entity Linking (EL) task identifies entity mentions in a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and associates them with an unambiguous identifier in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>Knowledge Base</a>. While much work has been done on the topic, we first present the results of a survey that reveal a lack of consensus in the community regarding what forms of mentions in a text and what forms of links the EL task should consider. We argue that no one definition of the Entity Linking task fits all, and rather propose a fine-grained categorization of different types of entity mentions and links. We then re-annotate three EL benchmark datasets ACE2004, KORE50, and VoxEL with respect to these categories. We propose a fuzzy recall metric to address the lack of consensus and conclude with fine-grained evaluation results comparing a selection of online EL systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1068/>Neural Cross-Lingual Event Detection with Minimal Parallel Resources</a></strong><br><a href=/people/j/jian-liu/>Jian Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1068><div class="card-body p-3 small">The scarcity in annotated data poses a great challenge for event detection (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge between different languages to boost performance. However, previous cross-lingual methods for ED demonstrated a heavy dependency on <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel resources</a>, which might limit their applicability. In this paper, we propose a new method for cross-lingual ED, demonstrating a minimal dependency on <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel resources</a>. Specifically, to construct a lexical mapping between different languages, we devise a context-dependent translation method ; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual co-training. The efficiency of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1069" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1069/>KnowledgeNet : A Benchmark Dataset for Knowledge Base Population<span class=acl-fixed-case>K</span>nowledge<span class=acl-fixed-case>N</span>et: A Benchmark Dataset for Knowledge Base Population</a></strong><br><a href=/people/f/filipe-mesquita/>Filipe Mesquita</a>
|
<a href=/people/m/matteo-cannaviccio/>Matteo Cannaviccio</a>
|
<a href=/people/j/jordan-schmidek/>Jordan Schmidek</a>
|
<a href=/people/p/paramita-mirza/>Paramita Mirza</a>
|
<a href=/people/d/denilson-barbosa/>Denilson Barbosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1069><div class="card-body p-3 small">KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, relation extraction). We discuss five baseline approaches, where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79 % (0.28). However, our best <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> is far from reaching <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a> (0.82), indicating our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is challenging. The KnowledgeNet dataset and baselines are available at https://github.com/diffbot/knowledge-net</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1071/>Explicit Cross-lingual Pre-training for Unsupervised Machine Translation</a></strong><br><a href=/people/s/shuo-ren/>Shuo Ren</a>
|
<a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1071><div class="card-body p-3 small">Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the cross-lingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pairs, we propose a new pre-training model called Cross-lingual Masked Language Model (CMLM), which randomly chooses source n-grams in the input text stream and predicts their translation candidates at each time step. Experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can incorporate beneficial cross-lingual information into pre-trained models. Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1072.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1072/>Latent Part-of-Speech Sequences for Neural Machine Translation</a></strong><br><a href=/people/x/xuewen-yang/>Xuewen Yang</a>
|
<a href=/people/y/yingru-liu/>Yingru Liu</a>
|
<a href=/people/d/dongliang-xie/>Dongliang Xie</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1072><div class="card-body p-3 small">Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> through <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> introduces additional complexity in <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, as the models need to marginalize over the latent syntactic structures. To avoid this, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often resort to <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a> which only allows them to explore a limited portion of the latent space. In this work, we introduce a new <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable model</a>, LaSyn, that captures the co-dependence between <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1073" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1073/>Improving <a href=https://en.wikipedia.org/wiki/Back-translation>Back-Translation</a> with Uncertainty-based Confidence Estimation</a></strong><br><a href=/people/s/shuo-wang/>Shuo Wang</a>
|
<a href=/people/y/yang-liu/>Yang Liu</a>
|
<a href=/people/c/chao-wang/>Chao Wang</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1073><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> is simple and effective in exploiting abundant monolingual corpora to improve low-resource neural machine translation (NMT), the synthetic bilingual corpora generated by NMT models trained on limited authentic bilingual data are inevitably noisy. In this work, we propose to quantify the confidence of NMT model predictions based on model uncertainty. With word- and sentence-level confidence measures based on <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>, it is possible for <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> to better cope with <a href=https://en.wikipedia.org/wiki/Noise>noise</a> in synthetic bilingual corpora. Experiments on Chinese-English and English-German translation tasks show that uncertainty-based confidence estimation significantly improves the performance of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1076/>Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages</a></strong><br><a href=/people/m/masud-moshtaghi/>Masud Moshtaghi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1076><div class="card-body p-3 small">Enabling cross-lingual NLP tasks by leveraging multilingual word embedding has recently attracted much attention. An important motivation is to support lower resourced languages, however, most efforts focus on demonstrating the effectiveness of the techniques using embeddings derived from similar languages to English with large parallel content. In this study, we first describe the general requirements for the success of these techniques and then present a noise tolerant piecewise linear technique to learn a non-linear mapping between two monolingual word embedding vector spaces. We evaluate our approach on inferring bilingual dictionaries. We show that our <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in lower resourced settings with an average of 3.7 % improvement of precision @10 across 14 mostly low resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1079/>Multi-agent Learning for Neural Machine Translation</a></strong><br><a href=/people/t/tianchi-bi/>Tianchi Bi</a>
|
<a href=/people/h/hao-xiong/>Hao Xiong</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1079><div class="card-body p-3 small">Conventional Neural Machine Translation (NMT) models benefit from the training with an additional <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>, e.g., dual learning, and bidirectional decoding with one agent decod- ing from left to right and the other decoding in the opposite direction. In this paper, we extend the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training framework</a> to the multi-agent sce- nario by introducing diverse agents in an in- teractive updating process. At training time, each agent learns advanced knowledge from others, and they work together to improve translation quality. Experimental results on NIST Chinese-English, IWSLT 2014 German- English, WMT 2014 English-German and large-scale Chinese-English translation tasks indicate that our approach achieves absolute improvements over the strong baseline sys- tems and shows competitive performance on all tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1080/>Pivot-based Transfer Learning for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> between Non-English Languages<span class=acl-fixed-case>E</span>nglish Languages</a></strong><br><a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/p/petre-petrov/>Petre Petrov</a>
|
<a href=/people/p/pavel-petrushkov/>Pavel Petrushkov</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1080><div class="card-body p-3 small">We present effective pre-training strategies for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> using <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> involving a <a href=https://en.wikipedia.org/wiki/Pivot_language>pivot language</a>, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training : 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> greatly outperform multilingual models up to +2.6 % BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot / zero-resource scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1084/>A Discriminative Neural Model for Cross-Lingual Word Alignment</a></strong><br><a href=/people/e/elias-stengel-eskin/>Elias Stengel-Eskin</a>
|
<a href=/people/t/tzu-ray-su/>Tzu-ray Su</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1084><div class="card-body p-3 small">We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (1.7K5 K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (1127 F1). We evaluate the model extrinsically on data projection for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese NER</a>, showing that our alignments lead to higher performance when used to project NER tags from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1085/>One Model to Learn Both : Zero Pronoun Prediction and Translation</a></strong><br><a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1085><div class="card-body p-3 small">Zero pronouns (ZPs) are frequently omitted in <a href=https://en.wikipedia.org/wiki/Pro-drop_language>pro-drop languages</a>, but should be recalled in <a href=https://en.wikipedia.org/wiki/Pro-drop_language>non-pro-drop languages</a>. This discourse phenomenon poses a significant challenge for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> when translating texts from pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware ZP translation approach for neural MT models. Specifically, we jointly learn to predict and translate ZPs in an end-to-end manner, allowing both components to interact with each other. In addition, we employ hierarchical neural networks to exploit discourse-level context, which is beneficial for ZP prediction and thus <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>. Experimental results on both Chinese-English and Japanese-English data show that our approach significantly and accumulatively improves both <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance and ZP prediction accuracy over not only baseline but also previous works using external ZP prediction models. Extensive analyses confirm that the performance improvement comes from the alleviation of different kinds of errors especially caused by subjective ZPs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1086.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1086/>Dynamic Past and Future for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xinyu-dai/>Xin-Yu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1086><div class="card-body p-3 small">Previous studies have shown that neural machine translation (NMT) models can benefit from explicitly modeling translated () and untranslated () source contents as recurrent states (CITATION). However, this less interpretable recurrent process hinders its power to model the dynamic updating of and contents during decoding. In this paper, we propose to model the dynamic principles by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely Guided Dynamic Routing, where the translating status at each decoding step guides the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both <a href=https://en.wikipedia.org/wiki/Reverse_Polish_notation>Rnmt</a> and <a href=https://en.wikipedia.org/wiki/Reverse_Polish_notation>Transformer</a> by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.<i>dynamic principles</i> by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely <i>Guided Dynamic Routing</i>, where the translating status at each decoding step <i>guides</i> the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both Rnmt and Transformer by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1088.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1088/>Towards Understanding <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Word Importance</a></strong><br><a href=/people/s/shilin-he/>Shilin He</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1088><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> has advanced the state-of-the-art on various language pairs, the interpretability of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>NMT</a> remains unsatisfactory. In this work, we propose to address this gap by focusing on understanding the input-output behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on identifying input words with higher influence on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models. Furthermore, our analysis reveals that words of certain syntactic categories have higher importance while the categories vary across language pairs, which can inspire better design principles of NMT architectures for multi-lingual translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1089/>Multilingual Neural Machine Translation with Language Clustering</a></strong><br><a href=/people/x/xu-tan/>Xu Tan</a>
|
<a href=/people/j/jiale-chen/>Jiale Chen</a>
|
<a href=/people/d/di-he/>Di He</a>
|
<a href=/people/y/yingce-xia/>Yingce Xia</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1089><div class="card-body p-3 small">Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> or use a separate <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> that clusters languages into different groups and trains one multilingual model for each <a href=https://en.wikipedia.org/wiki/Cluster_analysis>cluster</a>. We study two methods for language clustering : (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the <a href=https://en.wikipedia.org/wiki/Embedding>embedding vectors</a> of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1096/>A Lexicon-Based Graph Neural Network for Chinese NER<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/y/yicheng-zou/>Yicheng Zou</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1096><div class="card-body p-3 small">Recurrent neural networks (RNN) used for Chinese named entity recognition (NER) that sequentially track character and word information have achieved great success. However, the characteristic of chain structure and the lack of global semantics determine that RNN-based models are vulnerable to <a href=https://en.wikipedia.org/wiki/Ambiguity>word ambiguities</a>. In this work, we try to alleviate this problem by introducing a lexicon-based graph neural network with global semantics, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and long-range dependency. Based on the multiple graph-based interactions among characters, potential words, and the whole-sentence semantics, word ambiguities can be effectively tackled. Experiments on four NER datasets show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvements against other baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1101/>A Bayesian Approach for Sequence Tagging with Crowds<span class=acl-fixed-case>B</span>ayesian Approach for Sequence Tagging with Crowds</a></strong><br><a href=/people/e/edwin-simpson/>Edwin Simpson</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1101><div class="card-body p-3 small">Current methods for sequence tagging, a core task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, are data hungry, which motivates the use of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> as a cheap way to obtain labelled data. However, annotators are often unreliable and current aggregation methods can not capture common types of span annotation error. To address this, we propose a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian method</a> for aggregating sequence tags that reduces errors by modelling sequential dependencies between the annotations as well as the ground-truth labels. By taking a Bayesian approach, we account for uncertainty in the model due to both annotator errors and the lack of data for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>, showing that our sequential model outperforms the previous state of the art, and that Bayesian approaches outperform non-Bayesian alternatives. We also find that our approach can reduce crowdsourcing costs through more effective <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>, as it better captures uncertainty in the sequence labels when there are few annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1104.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1104/>Look-up and Adapt : A One-shot Semantic Parser</a></strong><br><a href=/people/z/zhichu-lu/>Zhichu Lu</a>
|
<a href=/people/f/forough-arabshahi/>Forough Arabshahi</a>
|
<a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1104><div class="card-body p-3 small">Computing devices have recently become capable of interacting with their end users via <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. However, they can only operate within a limited supported domain of discourse and fail drastically when faced with an out-of-domain utterance, mainly due to the limitations of their <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> maintains a <a href=https://en.wikipedia.org/wiki/Memory_(computing)>memory</a> consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> works by looking up a similar utterance from the <a href=https://en.wikipedia.org/wiki/Memory>memory</a> and adapting its <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8 % on one-shot parsing under two different evaluation settings compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1111.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1111/>Analytical Methods for Interpretable Ultradense Word Embeddings</a></strong><br><a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schtze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1111><div class="card-body p-3 small">Word embeddings are useful for a wide variety of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, but they lack <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation : Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in <a href=https://en.wikipedia.org/wiki/Closed-form_expression>closed form</a>, is hyperparameter-free and thus more robust than Densifier. We evaluate the three <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1115.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1115/>Neural Linguistic Steganography</a></strong><br><a href=/people/z/zachary-ziegler/>Zachary Ziegler</a>
|
<a href=/people/y/yuntian-deng/>Yuntian Deng</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1115><div class="card-body p-3 small">Whereas traditional <a href=https://en.wikipedia.org/wiki/Cryptography>cryptography</a> encrypts a secret message into an unintelligible form, <a href=https://en.wikipedia.org/wiki/Steganography>steganography</a> conceals that communication is taking place by encoding a secret message into a cover signal. Language is a particularly pragmatic cover signal due to its benign occurrence and independence from any one medium. Traditionally, linguistic steganography systems encode <a href=https://en.wikipedia.org/wiki/Cipher>secret messages</a> in existing text via synonym substitution or word order rearrangements. Advances in neural language models enable previously impractical generation-based techniques. We propose a <a href=https://en.wikipedia.org/wiki/Steganography>steganography technique</a> based on <a href=https://en.wikipedia.org/wiki/Arithmetic_coding>arithmetic coding</a> with large-scale neural language models. We find that our approach can generate realistic looking cover sentences as evaluated by humans, while at the same time preserving <a href=https://en.wikipedia.org/wiki/Computer_security>security</a> by matching the cover message distribution with the language model distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1118/>Rewarding Coreference Resolvers for Being Consistent with World Knowledge</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/a/ana-valeria-gonzalez/>Ana Valeria Gonzalez</a>
|
<a href=/people/d/daniel-herschcovich/>Daniel Herschcovich</a>
|
<a href=/people/c/chen-qiu/>Chen Qiu</a>
|
<a href=/people/a/anders-sandholm/>Anders Sandholm</a>
|
<a href=/people/m/michael-ringaard/>Michael Ringaard</a>
|
<a href=/people/a/anders-sogaard/>Anders Sgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1118><div class="card-body p-3 small">Unresolved coreference is a bottleneck for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1121/>Measure Country-Level Socio-Economic Indicators with Streaming News : An Empirical Study</a></strong><br><a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/x/xiaoxi-zhao/>Xiaoxi Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1121><div class="card-body p-3 small">Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the <a href=https://en.wikipedia.org/wiki/Unemployment>unemployment rate</a>, an indicator widely used by economists and policymakers. We argue that events reported in <a href=https://en.wikipedia.org/wiki/Streaming_media>streaming news</a> can be used as <a href=https://en.wikipedia.org/wiki/Microscope>micro-sensors</a> for measuring <a href=https://en.wikipedia.org/wiki/Socioeconomics>socio-economic conditions</a>. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>. We empirically demonstrate strong correlation between ECIM values to several representative indicators in socio-economic research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1122/>Towards Extracting <a href=https://en.wikipedia.org/wiki/Medical_family_history>Medical Family History</a> from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Interactions</a> : A New Dataset and Baselines</a></strong><br><a href=/people/m/mahmoud-azab/>Mahmoud Azab</a>
|
<a href=/people/s/stephane-dadian/>Stephane Dadian</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/l/larry-an/>Larry An</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1122><div class="card-body p-3 small">We introduce a new dataset consisting of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language interactions</a> annotated with <a href=https://en.wikipedia.org/wiki/Family_history_(medicine)>medical family histories</a>, obtained during interactions with a <a href=https://en.wikipedia.org/wiki/Genetic_counseling>genetic counselor</a> and through <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, following a questionnaire created by experts in the domain. We describe the data collection process and the annotations performed by medical professionals, including illness and personal attributes (name, age, gender, family relationships) for the patient and their family members. An initial system that performs argument identification and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> shows promising results average <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.87 on complex sentences on the targeted relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1124 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1124.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1124/>Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation<span class=acl-fixed-case>D</span>irichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation</a></strong><br><a href=/people/m/min-zeng/>Min Zeng</a>
|
<a href=/people/y/yisen-wang/>Yisen Wang</a>
|
<a href=/people/y/yuan-luo/>Yuan Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1124><div class="card-body p-3 small">Variational encoder-decoders have achieved well-recognized performance in the dialogue generation task. Existing works simply assume the Gaussian priors of the latent variable, which are incapable of representing complex latent variables effectively. To address the issues, we propose to use the <a href=https://en.wikipedia.org/wiki/Dirichlet_distribution>Dirichlet distribution</a> with flexible structures to characterize the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> in place of the traditional <a href=https://en.wikipedia.org/wiki/Normal_distribution>Gaussian distribution</a>, called Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder model (Dir-VHRED). Based on which, we further find that there is redundancy among the dimensions of <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>, and the lengths and sentence patterns of the responses can be strongly correlated to each dimension of the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>. Therefore, controllable responses can be generated through specifying the value of each dimension of the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>. Experimental results on benchmarks show that our proposed Dir-VHRED yields substantial improvements on negative log-likelihood, word-embedding-based and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1125 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1125/>Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe Budzianowski</a>
|
<a href=/people/r/richard-turner/>Richard Turner</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1125><div class="card-body p-3 small">Dialogue systems benefit greatly from optimizing on detailed annotations, such as <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed utterances</a>, internal dialogue state representations and dialogue act labels. However, collecting these <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> is expensive and time-consuming, holding back development in the area of dialogue modelling. In this paper, we investigate semi-supervised learning methods that are able to reduce the amount of required intermediate labelling. We find that by leveraging un-annotated data instead, the amount of turn-level annotations of dialogue state can be significantly reduced when building a neural dialogue system. Our analysis on the MultiWOZ corpus, covering a range of domains and topics, finds that annotations can be reduced by up to 30 % while maintaining equivalent system performance. We also describe and evaluate the first end-to-end dialogue model created for the MultiWOZ corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1126/>A Progressive Model to Enable Continual Learning for Semantic Slot Filling</a></strong><br><a href=/people/y/yilin-shen/>Yilin Shen</a>
|
<a href=/people/x/xiangyu-zeng/>Xiangyu Zeng</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1126><div class="card-body p-3 small">Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on precollected data, it is crucial to continually improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> after deployment to learn users&#8217; new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on all data or fine tune the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component ; and meanwhile enables this new <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>component</a> to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24 % and 3.03 % on two benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1127 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1127.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1127/>CASA-NLU : Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots<span class=acl-fixed-case>CASA</span>-<span class=acl-fixed-case>NLU</span>: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots</a></strong><br><a href=/people/a/arshit-gupta/>Arshit Gupta</a>
|
<a href=/people/p/peng-zhang/>Peng Zhang</a>
|
<a href=/people/g/garima-lalwani/>Garima Lalwani</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1127><div class="card-body p-3 small">Natural Language Understanding (NLU) is a core component of <a href=https://en.wikipedia.org/wiki/Dialog_(software)>dialog systems</a>. It typically involves two tasks-Intent Classification (IC) and Slot Labeling (SL), which are then followed by a dialogue management (DM) component. Such NLU systems cater to utterances in isolation, thus pushing the problem of <a href=https://en.wikipedia.org/wiki/Context_management>context management</a> to DM. However, <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> is critical to the correct prediction of intents in a conversation. Prior work on contextual NLU has been limited in terms of the types of contextual signals used and the understanding of their impact on the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In this work, we propose a context-aware self-attentive NLU (CASA-NLU) model that uses multiple signals over a variable context window, such as previous intents, slots, dialog acts and utterances, in addition to the current user utterance. CASA-NLU outperforms a recurrent contextual NLU baseline on two conversational datasets, yielding a gain of up to 7 % on the IC task. Moreover, a non-contextual variant of CASA-NLU achieves state-of-the-art performance on standard public datasets-SNIPS and ATIS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1130/>Modeling Multi-Action Policy for Task-Oriented Dialogues</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/p/piero-molino/>Piero Molino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1130><div class="card-body p-3 small">Dialogue management (DM) plays a key role in the quality of the interaction with the user in a task-oriented dialogue system. In most existing approaches, the agent predicts only one DM policy action per turn. This significantly limits the expressive power of the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>conversational agent</a> and introduces unwanted turns of interactions that may challenge users&#8217; patience. Longer conversations also lead to more errors and the <a href=https://en.wikipedia.org/wiki/System>system</a> needs to be more robust to handle them. In this paper, we compare the performance of several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the task of predicting multiple acts for each turn. A novel policy model is proposed based on a recurrent cell called gated Continue-Act-Slots (gCAS) that overcomes the limitations of the existing models. Experimental results show that <a href=https://en.wikipedia.org/wiki/GCAS>gCAS</a> outperforms other approaches. The datasets and code are available at https://leishu02.github.io/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1131.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1131" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1131/>An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction</a></strong><br><a href=/people/s/stefan-larson/>Stefan Larson</a>
|
<a href=/people/a/anish-mahendran/>Anish Mahendran</a>
|
<a href=/people/j/joseph-j-peper/>Joseph J. Peper</a>
|
<a href=/people/c/christopher-clarke/>Christopher Clarke</a>
|
<a href=/people/a/andrew-lee/>Andrew Lee</a>
|
<a href=/people/p/parker-hill/>Parker Hill</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/k/kevin-leach/>Kevin Leach</a>
|
<a href=/people/m/michael-a-laurenzano/>Michael A. Laurenzano</a>
|
<a href=/people/l/lingjia-tang/>Lingjia Tang</a>
|
<a href=/people/j/jason-mars/>Jason Mars</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1131><div class="card-body p-3 small">Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that includes queries that are out-of-scopei.e., queries that do not fall into any of the system&#8217;s supported intents. This poses a new challenge because models can not assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> along with several different out-of-scope identification schemes. We find that while the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1132" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1132/>Automatically Learning Data Augmentation Policies for Dialogue Tasks</a></strong><br><a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1132><div class="card-body p-3 small">Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image&#8217;s semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the <a href=https://en.wikipedia.org/wiki/Controller_(computing)>controller</a> to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy&#8217;s required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1134 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1134/>Multilingual word translation using auxiliary languages</a></strong><br><a href=/people/h/hagai-taitelbaum/>Hagai Taitelbaum</a>
|
<a href=/people/g/gal-chechik/>Gal Chechik</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1134><div class="card-body p-3 small">Current multilingual word translation methods are focused on jointly learning <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> from each language to a shared space. The actual <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, however, is still performed as an isolated bilingual task. In this study we propose a multilingual translation procedure that uses all the learned <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> to translate a word from one language to another. For each source word, we first search for the most relevant auxiliary languages. We then use the <a href=https://en.wikipedia.org/wiki/Translation>translations</a> to these languages to form an improved representation of the source word. Finally, this <a href=https://en.wikipedia.org/wiki/Representation_(systemics)>representation</a> is used for the actual <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to the target language. Experiments on a standard multilingual word translation benchmark demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state of the art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1135 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1135.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1135/>Towards Better Modeling <a href=https://en.wikipedia.org/wiki/Hierarchical_structure>Hierarchical Structure</a> for Self-Attention with Ordered Neurons</a></strong><br><a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/j/jinfeng-zhang/>Jinfeng Zhang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1135><div class="card-body p-3 small">Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks RNNs outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1137/>Simpler and Faster Learning of Adaptive Policies for <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>Simultaneous Translation</a></a></strong><br><a href=/people/b/baigong-zheng/>Baigong Zheng</a>
|
<a href=/people/r/renjie-zheng/>Renjie Zheng</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1137><div class="card-body p-3 small">Simultaneous translation is widely useful but remains challenging. Previous work falls into two main categories : (a) fixed-latency policies such as Ma et al. (2019) and (b) adaptive policies such as Gu et al. The former are simple and effective, but have to aggressively predict future content due to diverging source-target word order ; the latter do not anticipate, but suffer from unstable and inefficient training. To combine the merits of both approaches, we propose a simple supervised-learning framework to learn an adaptive policy from oracle READ / WRITE sequences generated from parallel text. At each step, such an oracle sequence chooses to WRITE the next target word if the available source sentence context provides enough information to do so, otherwise READ the next source word. Experiments on German = English show that our method, without retraining the underlying NMT model, can learn flexible policies with better BLEU scores and similar latencies compared to previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1138/>Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER<span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/p/phillip-keung/>Phillip Keung</a>
|
<a href=/people/y/yichao-lu/>Yichao Lu</a>
|
<a href=/people/v/vikas-bhardwaj/>Vikas Bhardwaj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1138><div class="card-body p-3 small">Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated state-of-the-art performance on various NLP tasks. Recent work with the multilingual version of BERT has shown that the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performs surprisingly well in cross-lingual settings, even when only labeled English data is used to finetune the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. We improve upon multilingual BERT&#8217;s zero-resource cross-lingual performance via <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a>. We report the magnitude of the improvement on the multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that language-adversarial training encourages BERT to align the embeddings of English documents and their translations, which may be the cause of the observed performance gains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1143.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1143" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1143/>Handling Syntactic Divergence in Low-resource Machine Translation</a></strong><br><a href=/people/c/chunting-zhou/>Chunting Zhou</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1143><div class="card-body p-3 small">Despite impressive empirical successes of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> on standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>, limited parallel data impedes the application of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>NMT models</a> to many language pairs. Data augmentation methods such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> make it possible to use monolingual data to help alleviate these issues, but <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1145 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1145/>Self-Attention with Structural Position Representations</a></strong><br><a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1145><div class="card-body p-3 small">Although self-attention networks (SANs) have advanced the state-of-the-art on various NLP tasks, one criticism of SANs is their ability of encoding positions of input words (Shaw et al., 2018). In this work, we propose to augment SANs with structural position representations to model the latent structure of the input sentence, which is complementary to the standard sequential positional representations. Specifically, we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese-to-English and WMT14 English-to-German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1146/>Exploiting <a href=https://en.wikipedia.org/wiki/Multilingualism>Multilingualism</a> through Multistage Fine-Tuning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1146><div class="card-body p-3 small">This paper highlights the impressive utility of multi-parallel corpora for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k440k) parallel corpus for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a>. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 39 BLEU score gains over a simple one-to-one model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1147 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1147/>Unsupervised Domain Adaptation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Domain-Aware Feature Embeddings</a></strong><br><a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1147><div class="card-body p-3 small">The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with <a href=https://en.wikipedia.org/wiki/Back_translation>back translation</a> can further improve the performance of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1148 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1148/>A Regularization-based Framework for Bilingual Grammar Induction</a></strong><br><a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1148><div class="card-body p-3 small">Grammar induction aims to discover <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structures</a> from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>unannotated sentences</a>. In this paper, we propose a framework in which the learning process of the grammar model of one language is influenced by knowledge from the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of another language. Unlike previous work on multilingual grammar induction, our approach does not rely on any external resource, such as <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a>, <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignments</a> or <a href=https://en.wikipedia.org/wiki/Phylogenetic_tree>linguistic phylogenetic trees</a>. We propose three <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization methods</a> that encourage similarity between model parameters, dependency edge scores, and parse trees respectively. We deploy our methods on a state-of-the-art unsupervised discriminative parser and evaluate it on both transfer grammar induction and bilingual grammar induction. Empirical results on multiple languages show that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> outperform strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1150/>Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model<span class=acl-fixed-case>K</span>orean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model</a></strong><br><a href=/people/h/hyun-je-song/>Hyun-Je Song</a>
|
<a href=/people/s/seong-bae-park/>Seong-Bae Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1150><div class="card-body p-3 small">Korean morphological analysis has been considered as a sequence of morpheme processing and POS tagging. Thus, a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline model</a> of the <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> has been adopted widely by previous studies. However, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> has a problem that it can not utilize interactions among the tasks. This paper formulates Korean morphological analysis as a combination of the tasks and presents a tied sequence-to-sequence multi-task model for training the two tasks simultaneously without any explicit <a href=https://en.wikipedia.org/wiki/Regularization_(linguistics)>regularization</a>. The experiments prove the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1152 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1152.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1152/>Improving Generative Visual Dialog by Answering Diverse Questions</a></strong><br><a href=/people/v/vishvak-murahari/>Vishvak Murahari</a>
|
<a href=/people/p/prithvijit-chattopadhyay/>Prithvijit Chattopadhyay</a>
|
<a href=/people/d/dhruv-batra/>Dhruv Batra</a>
|
<a href=/people/d/devi-parikh/>Devi Parikh</a>
|
<a href=/people/a/abhishek-das/>Abhishek Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1152><div class="card-body p-3 small">Prior work on training generative Visual Dialog models with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> ((Das et al., ICCV 2017) has explored a Q-Bot-A-Bot image-guessing game and shown that this &#8216;self-talk&#8217; approach can lead to improved performance at the downstream dialog-conditioned image-guessing task. However, this improvement saturates and starts degrading after a few rounds of <a href=https://en.wikipedia.org/wiki/Interaction>interaction</a>, and does not lead to a better Visual Dialog model. We find that this is due in part to repeated interactions between <a href=https://en.wikipedia.org/wiki/Q-Bot>Q-Bot</a> and A-BOT during <a href=https://en.wikipedia.org/wiki/Self-talk>self-talk</a>, which are not informative with respect to the <a href=https://en.wikipedia.org/wiki/Image>image</a>. To improve this, we devise a simple auxiliary objective that incentivizes <a href=https://en.wikipedia.org/wiki/Q-Bot>Q-Bot</a> to ask diverse questions, thus reducing repetitions and in turn enabling A-Bot to explore a larger <a href=https://en.wikipedia.org/wiki/State_space>state space</a> during <a href=https://en.wikipedia.org/wiki/Real-time_computing>RL</a> i.e. be exposed to more visual concepts to talk about, and varied questions to answer. We evaluate our approach via a host of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> and <a href=https://en.wikipedia.org/wiki/Humanities>human studies</a>, and demonstrate that it leads to better <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a>, i.e. dialog that is more diverse (i.e. less repetitive), consistent (i.e. has fewer conflicting exchanges), fluent (i.e., more human-like), and detailed, while still being comparably image-relevant as prior work and ablations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1153/>Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding</a></strong><br><a href=/people/q/quynh-do/>Quynh Do</a>
|
<a href=/people/j/judith-gaspers/>Judith Gaspers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1153><div class="card-body p-3 small">A typical cross-lingual transfer learning approach boosting model performance on a language is to pre-train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on all available <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised data</a> from another language. However, in large-scale systems this leads to high <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training times</a> and <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational requirements</a>. In addition, characteristic differences between the source and target languages raise a natural question of whether source data selection can improve the <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a>. In this paper, we address this question and propose a simple but effective language model based source-language data selection method for cross-lingual transfer learning in large-scale spoken language understanding. The experimental results show that with data selection i) source data and hence training speed is reduced significantly and ii) <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance is improved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1154/>Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations</a></strong><br><a href=/people/p/po-yao-huang/>Po-Yao Huang</a>
|
<a href=/people/x/xiaojun-chang/>Xiaojun Chang</a>
|
<a href=/people/a/alexander-g-hauptmann/>Alexander Hauptmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1154><div class="card-body p-3 small">With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for fine-grained alignments between sentences and images. We introduce a new <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30 K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields a significant performance gain over other methods in all of the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1155.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1155/>Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering</a></strong><br><a href=/people/s/soravit-changpinyo/>Soravit Changpinyo</a>
|
<a href=/people/b/bo-pang/>Bo Pang</a>
|
<a href=/people/p/piyush-sharma/>Piyush Sharma</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1155><div class="card-body p-3 small">Object detection plays an important role in current solutions to vision and language tasks like image captioning and visual question answering. However, popular models like Faster R-CNN rely on a costly process of annotating ground-truths for both the bounding boxes and their corresponding semantic labels, making it less amenable as a primitive task for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. In this paper, we examine the effect of decoupling box proposal and featurization for <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>down-stream tasks</a>. The key insight is that this allows us to leverage a large amount of labeled annotations that were previously unavailable for standard object detection benchmarks. Empirically, we demonstrate that this leads to effective <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and improved image captioning and visual question answering models, as measured on publicly-available benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1159.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1159/>Robust Navigation with Language Pretraining and Stochastic Sampling</a></strong><br><a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/q/qiaolin-xia/>Qiaolin Xia</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1159><div class="card-body p-3 small">Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-to-Room benchmark with 6 % absolute gain over the previous best result (47 %-53 %) on the Success Rate weighted by Path Length metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1161 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1161.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1161/>Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/y/yi-pei-chen/>Yi-Pei Chen</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1161><div class="card-body p-3 small">Understanding text often requires identifying meaningful <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent spans</a> such as <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a> and <a href=https://en.wikipedia.org/wiki/Verb_phrase>verb phrases</a>. In this work, we show that we can effectively recover these types of labels using the learned phrase vectors from deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster span representations to induce span labels. Additionally, we improve the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s labeling accuracy by integrating latent code learning into the training procedure. We evaluate this approach empirically through unsupervised labeled constituency parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street Journal (WSJ) dataset and is competitive to prior work that requires additional human annotations, improving over a previous state-of-the-art system that depends on ground-truth part-of-speech tags by 5 absolute F1 points (19 % relative error reduction).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1163.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1163/>Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog</a></strong><br><a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a>
|
<a href=/people/k/karishma-mandyam/>Karishma Mandyam</a>
|
<a href=/people/r/rushin-shah/>Rushin Shah</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1163><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> for parsing compositional utterances into Task Oriented Parse (TOP), a <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree representation</a> that has intents and slots as labels of nesting tree nodes. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is span-based : it scores labels of the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree nodes</a> covering each token span independently, but then decodes a valid <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> globally. In contrast to previous sequence decoding approaches and other span-based parsers, we (1) improve the training speed by removing the need to run the decoder at training time ; and (2) introduce edge scores, which model relations between parent and child labels, to mitigate the independence assumption between node labels and improve accuracy. Our best <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> outperforms previous methods on the TOP dataset of mixed-domain task-oriented utterances in both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and training speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1166.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1166" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1166/>Controlling Text Complexity in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/s/sweta-agrawal/>Sweta Agrawal</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1166><div class="card-body p-3 small">This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> makes it possible to train multi-task sequence to sequence models that can translate and simplify text jointly. We show that these multi-task models outperform <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline approaches</a> that translate and simplify text independently.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1169/>Cross-Lingual Machine Reading Comprehension</a></strong><br><a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1169><div class="card-body p-3 small">Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data. In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task which is straightforward to adopt. However, to exactly align the answer into source language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in bilingual context, and then utilize the learned knowledge to improve <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available : https://github.com/ymcui/Cross-Lingual-MRC</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1171.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1171/>Neural Duplicate Question Detection without Labeled Training Data</a></strong><br><a href=/people/a/andreas-ruckle/>Andreas Rckl</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1171><div class="card-body p-3 small">Supervised training of neural models to duplicate question detection in community Question Answering (CQA) requires large amounts of labeled question pairs, which can be costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methodsweak supervision using the title and body of a question, and the automatic generation of duplicate questionsand show that both can achieve improved performances even though they do not require any labeled data. We provide a comparison of popular training strategies and show that our proposed approaches are more effective in many cases because they can utilize larger amounts of data from the CQA forums. Finally, we show that weak supervision with question title and body information is also an effective method to train CQA answer selection models without direct answer supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1173.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1173/>Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection</a></strong><br><a href=/people/n/nina-poerner/>Nina Poerner</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schtze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1173><div class="card-body p-3 small">We address the problem of Duplicate Question Detection (DQD) in low-resource domain-specific Community Question Answering forums. Our multi-view framework MV-DASE combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis, using unlabeled data only. In our experiments, the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> includes generic and domain-specific averaged word embeddings, domain-finetuned BERT and the Universal Sentence Encoder. We evaluate MV-DASE on the CQADupStack corpus and on additional low-resource Stack Exchange forums. Combining the strengths of different encoders, we significantly outperform BM25, all single-view systems as well as a recent supervised domain-adversarial DQD method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1174.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1174/>Multi-label Categorization of Accounts of Sexism using a Neural Framework</a></strong><br><a href=/people/p/pulkit-parikh/>Pulkit Parikh</a>
|
<a href=/people/h/harika-abburi/>Harika Abburi</a>
|
<a href=/people/p/pinkesh-badjatiya/>Pinkesh Badjatiya</a>
|
<a href=/people/r/radhika-krishnan/>Radhika Krishnan</a>
|
<a href=/people/n/niyati-chhaya/>Niyati Chhaya</a>
|
<a href=/people/m/manish-gupta/>Manish Gupta</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1174><div class="card-body p-3 small">Sexism, an <a href=https://en.wikipedia.org/wiki/Injustice>injustice</a> that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in utilizing such data to study and counter sexism better. The existing work on <a href=https://en.wikipedia.org/wiki/Sexism>sexism classification</a>, which is different from sexism detection, has certain limitations in terms of the categories of <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as BERT with distributional and linguistic word embeddings using a flexible, hierarchical architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. The best proposed method outperforms several <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> as well as traditional machine learning baselines by an appreciable margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1175/>The Trumpiest Trump? Identifying a Subjects Most Characteristic Tweets</a></strong><br><a href=/people/c/charuta-pethe/>Charuta Pethe</a>
|
<a href=/people/s/steven-skiena/>Steve Skiena</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1175><div class="card-body p-3 small">The sequence of documents produced by any given author varies in style and content, but some documents are more typical or representative of the source than others. We quantify the extent to which a given short text is characteristic of a specific person, using a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets from fifteen celebrities. Such analysis is useful for generating excerpts of high-volume Twitter profiles, and understanding how <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representativeness</a> relates to tweet popularity. We first consider the related task of binary author detection (is x the author of text T?), and report a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>test accuracy</a> of 90.37 % for the best of five approaches to this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We then use these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to compute characterization scores among all of an author&#8217;s texts. A user study shows human evaluators agree with our characterization model for all 15 celebrities in our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, each with p-value 0.05. We use these <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> to show surprisingly strong correlations between characterization scores and the popularity of the associated texts. Indeed, we demonstrate a statistically significant correlation between this score and tweet popularity (likes / replies / retweets) for 13 of the 15 celebrities in our study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1176.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1176/>Finding Microaggressions in the Wild : A Case for Locating Elusive Phenomena in Social Media Posts</a></strong><br><a href=/people/l/luke-breitfeller/>Luke Breitfeller</a>
|
<a href=/people/e/emily-ahn/>Emily Ahn</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1176><div class="card-body p-3 small">Microaggressions are subtle, often veiled, manifestations of <a href=https://en.wikipedia.org/wiki/Bias>human biases</a>. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> in <a href=https://en.wikipedia.org/wiki/Communication>communication</a> has made it difficult for researchers to analyze their exact nature, and to quantify and extract <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> based on a small subset of data that we have. We then create two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> : one with examples of diverse types of <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a> recollected by their targets, and another with gender-based microaggressions in public conversations on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We introduce a new, more objective, criterion for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and an active-learning based procedure that increases the likelihood of surfacing posts containing <a href=https://en.wikipedia.org/wiki/Microaggression>microaggressions</a>. Finally, we analyze the trends that emerge from these new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1178" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1178/>Learning Invariant Representations of Social Media Users</a></strong><br><a href=/people/n/nicholas-andrews/>Nicholas Andrews</a>
|
<a href=/people/m/marcus-bishop/>Marcus Bishop</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1178><div class="card-body p-3 small">The evolution of social media users&#8217; behavior over time complicates user-level comparison tasks such as <a href=https://en.wikipedia.org/wiki/Verification_and_validation>verification</a>, <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>, and <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a>. As a result, naive approaches may fail to generalize to new users or even to future observations of previously known users. In this paper, we propose a novel procedure to learn a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> from short episodes of user activity on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> to a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> in which the distance between points captures the similarity of the corresponding users&#8217; invariant features. We fit the model by optimizing a surrogate metric learning objective over a large corpus of unlabeled social media content. Once learned, the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> may be applied to users not seen at training time and enables efficient comparisons of users in the resulting <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>. We present a comprehensive evaluation to validate the benefits of the proposed approach using <a href=https://en.wikipedia.org/wiki/Data>data</a> from <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1179.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1179/>(Male, Bachelor) and (Female, Ph. D) have different connotations : Parallelly Annotated Stylistic Language Dataset with Multiple Personas<span class=acl-fixed-case>P</span>h.<span class=acl-fixed-case>D</span>) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/v/varun-gangal/>Varun Gangal</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1179><div class="card-body p-3 small">Stylistic variation in text needs to be studied with different aspects including the writer&#8217;s personal traits, <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>interpersonal relations</a>, <a href=https://en.wikipedia.org/wiki/Rhetoric>rhetoric</a>, and more. Despite recent attempts on computational modeling of the variation, the lack of parallel corpora of style language makes it difficult to systematically control the stylistic change as well as evaluate such models. We release PASTEL, the parallel and annotated stylistic language dataset, that contains ~41 K parallel sentences (8.3 K parallel stories) annotated across different personas. Each persona has different styles in conjunction : gender, age, country, <a href=https://en.wikipedia.org/wiki/Politics>political view</a>, <a href=https://en.wikipedia.org/wiki/Education>education</a>, <a href=https://en.wikipedia.org/wiki/Ethnic_group>ethnic</a>, and time-of-writing. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is collected from human annotators with solid control of input denotation : not only preserving original meaning between text, but promoting stylistic diversity to annotators. We test the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> on two interesting applications of <a href=https://en.wikipedia.org/wiki/Style_language>style language</a>, where PASTEL helps design appropriate experiment and evaluation. First, in predicting a target style (e.g., male or female in gender) given a text, multiple styles of PASTEL make other external style variables controlled (or fixed), which is a more accurate experimental design. Second, a simple <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> with our parallel text outperforms the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a> using nonparallel text in style transfer. Our dataset is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1180.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1180/>Movie Plot Analysis via Turning Point Identification</a></strong><br><a href=/people/p/pinelopi-papalampidi/>Pinelopi Papalampidi</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1180><div class="card-body p-3 small">According to screenwriting theory, turning points (e.g., change of plans, major setback, climax) are crucial narrative moments within a screenplay : they define the plot structure, determine its progression and segment the screenplay into thematic units (e.g., setup, complications, aftermath). We propose the task of turning point identification in <a href=https://en.wikipedia.org/wiki/Film>movies</a> as a means of analyzing their <a href=https://en.wikipedia.org/wiki/Narrative_structure>narrative structure</a>. We argue that turning points and the segmentation they provide can facilitate processing long, complex narratives, such as <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a>, for summarization and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. We introduce a dataset consisting of <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a> and <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot synopses</a> annotated with turning points and present an end-to-end neural network model that identifies turning points in <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot synopses</a> and projects them onto scenes in <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms strong baselines based on state-of-the-art <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> and the expected position of turning points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1182/>Deep Ordinal Regression for Pledge Specificity Prediction</a></strong><br><a href=/people/s/shivashankar-subramanian/>Shivashankar Subramanian</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1182><div class="card-body p-3 small">Many pledges are made in the course of an <a href=https://en.wikipedia.org/wiki/Political_campaign>election campaign</a>, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual annotations. In this paper we collate a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of manifestos from eleven <a href=https://en.wikipedia.org/wiki/Elections_in_Australia>Australian federal election cycles</a>, with over 12,000 sentences annotated with <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a> (e.g., rhetorical vs detailed pledge) on a <a href=https://en.wikipedia.org/wiki/Scale_(social_sciences)>fine-grained scale</a>. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1185.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1185" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1185/>Are You for Real? Detecting Identity Fraud via Dialogue Interactions</a></strong><br><a href=/people/w/weikang-wang/>Weikang Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/q/qian-li/>Qian Li</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/z/zhifei-li/>Zhifei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1185><div class="card-body p-3 small">Identity fraud detection is of great importance in many real-world scenarios such as the <a href=https://en.wikipedia.org/wiki/Financial_services>financial industry</a>. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in <a href=https://en.wikipedia.org/wiki/Loan>loan applications</a> and propose to solve this problem with a novel interactive dialogue system which consists of two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>. One is the knowledge graph (KG) constructor organizing the <a href=https://en.wikipedia.org/wiki/Personal_data>personal information</a> for each loan applicant. The other is structured dialogue management that can dynamically generate a series of questions based on the personal KG to ask the applicants and determine their <a href=https://en.wikipedia.org/wiki/Identity_(social_science)>identity states</a>. We also present a heuristic user simulator based on <a href=https://en.wikipedia.org/wiki/Problem_analysis>problem analysis</a> to evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Experiments have shown that the trainable dialogue system can effectively detect <a href=https://en.wikipedia.org/wiki/Fraud>fraudsters</a>, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexible, which can help promote real-world applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1186/>Hierarchy Response Learning for Neural Conversation Generation</a></strong><br><a href=/people/b/bo-zhang/>Bo Zhang</a>
|
<a href=/people/x/xiaoming-zhang/>Xiaoming Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1186><div class="card-body p-3 small">The neural encoder-decoder models have shown great promise in neural conversation generation. However, they can not perceive and express the intention effectively, and hence often generate dull and generic responses. Unlike past work that has focused on diversifying the output at word-level or discourse-level with a flat model to alleviate this problem, we propose a hierarchical generation model to capture the different levels of diversity using the conditional variational autoencoders. Specifically, a hierarchical response generation (HRG) framework is proposed to capture the conversation intention in a natural and coherent way. It has two modules, namely, an expression reconstruction model to capture the hierarchical correlation between expression and intention, and an expression attention model to effectively combine the expressions with contents. Finally, the training procedure of <a href=https://en.wikipedia.org/wiki/Homeostasis>HRG</a> is improved by introducing reconstruction loss. Experiment results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate the responses with more appropriate content and expression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1187/>Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</a></strong><br><a href=/people/z/zhibin-liu/>Zhibin Liu</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1187><div class="card-body p-3 small">Two types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>, triples from <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge graphs</a> and texts from documents, have been studied for <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge aware open domain conversation generation</a>, in which graph paths can narrow down vertex candidates for <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge selection decision</a>, and texts can provide rich information for response generation. Fusion of a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> and texts might yield mutually reinforcing advantages, but there is less study on that. To address this challenge, we propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. For knowledge selection on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, we formulate it as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous works. To fully leverage long text information that differentiates our <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> from others, we improve a state of the art <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning algorithm</a> with machine reading comprehension technology. We demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a> on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in comparison with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1189" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1189/>Towards Knowledge-Based Recommender Dialog System</a></strong><br><a href=/people/q/qibin-chen/>Qibin Chen</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/y/yichang-zhang/>Yichang Zhang</a>
|
<a href=/people/m/ming-ding/>Ming Ding</a>
|
<a href=/people/y/yukuo-cen/>Yukuo Cen</a>
|
<a href=/people/h/hongxia-yang/>Hongxia Yang</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1189><div class="card-body p-3 small">In this paper, we propose a novel end-to-end framework called KBRD, which stands for Knowledge-Based Recommender Dialog System. It integrates the <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender system</a> and the dialog generation system. The dialog generation system can enhance the performance of the <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation system</a> by introducing information about users&#8217; preferences, and the <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender system</a> can improve that of the dialog generation system by providing recommendation-aware vocabulary bias. Experimental results demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has significant advantages over the baselines in both the evaluation of dialog generation and <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a>. A series of analyses show that the two <a href=https://en.wikipedia.org/wiki/System>systems</a> can bring mutual benefits to each other, and the introduced <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> contributes to both their performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1191 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1191/>Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration</a></strong><br><a href=/people/z/zhufeng-pan/>Zhufeng Pan</a>
|
<a href=/people/k/kun-bai/>Kun Bai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/l/lianqiang-zhou/>Lianqiang Zhou</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1191><div class="card-body p-3 small">In multi-turn dialogue, utterances do not always take the full form of sentences. These incomplete utterances will greatly reduce the performance of open-domain dialogue systems. Restoring more incomplete utterances from context could potentially help the <a href=https://en.wikipedia.org/wiki/System>systems</a> generate more relevant responses. To facilitate the study of incomplete utterance restoration for open-domain dialogue systems, a large-scale multi-turn dataset Restoration-200 K is collected and manually labeled with the explicit relation between an utterance and its context. We also propose a pick-and-combine model to restore the incomplete utterance from its context. Experimental results demonstrate that the annotated dataset and the proposed approach significantly boost the response quality of both single-turn and multi-turn dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1193/>Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots</a></strong><br><a href=/people/j/jia-chen-gu/>Jia-Chen Gu</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a>
|
<a href=/people/q/quan-liu/>Quan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1193><div class="card-body p-3 small">This paper proposes a dually interactive matching network (DIM) for presenting the personalities of dialogue agents in retrieval-based chatbots. This model develops from the interactive matching network (IMN) which models the matching degree between a context composed of multiple utterances and a response candidate. Compared with previous <a href=https://en.wikipedia.org/wiki/Persona>persona fusion approach</a> which enhances the representation of a context by calculating its similarity with a given <a href=https://en.wikipedia.org/wiki/Persona>persona</a>, the DIM model adopts a dual matching architecture, which performs interactive matching between responses and contexts and between responses and personas respectively for ranking response candidates. Experimental results on PERSONA-CHAT dataset show that the DIM model outperforms its baseline model, i.e., <a href=https://en.wikipedia.org/wiki/IMN>IMN</a> with persona fusion, by a margin of 14.5 % and outperforms the present <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art model</a> by a margin of 27.7 % in terms of top-1 accuracy hits@1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1195 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1195/>Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1195><div class="card-body p-3 small">End-to-end sequence generation is a popular technique for developing open domain dialogue systems, though they suffer from the safe response problem. Researchers have attempted to tackle this problem by incorporating <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> with the returns of <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval systems</a>. Recently, a skeleton-then-response framework has been shown promising results for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Nevertheless, how to precisely extract a <a href=https://en.wikipedia.org/wiki/Skeleton>skeleton</a> and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.<i>safe response problem</i>. Researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. Recently, a skeleton-then-response framework has been shown promising results for this task. Nevertheless, how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1199 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1199.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1199/>Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations</a></strong><br><a href=/people/r/ran-le/>Ran Le</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/m/mingyue-shang/>Mingyue Shang</a>
|
<a href=/people/z/zhenjun-you/>Zhenjun You</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1199><div class="card-body p-3 small">Previous research on dialogue systems generally focuses on the conversation between two participants, yet multi-party conversations which involve more than two participants within one session bring up a more complicated but realistic scenario. In real multi- party conversations, we can observe who is speaking, but the addressee information is not always explicit. In this paper, we aim to tackle the challenge of identifying all the miss- ing addressees in a conversation session. To this end, we introduce a novel who-to-whom (W2W) model which models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1201/>Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders<span class=acl-fixed-case>W</span>asserstein Autoencoders</a></strong><br><a href=/people/z/zhangming-chan/>Zhangming Chan</a>
|
<a href=/people/j/juntao-li/>Juntao Li</a>
|
<a href=/people/x/xiaopeng-yang/>Xiaopeng Yang</a>
|
<a href=/people/x/xiuying-chen/>Xiuying Chen</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1201><div class="card-body p-3 small">Variational autoencoders (VAEs) and Wasserstein autoencoders (WAEs) have achieved noticeable progress in open-domain response generation. Through introducing <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> in <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous space</a>, these models are capable of capturing utterance-level semantics, e.g., topic, syntactic properties, and thus can generate informative and diversified responses. In this work, we improve the WAE for response generation. In addition to the utterance-level information, we also model user-level information in latent continue space. Specifically, we embed user-level and utterance-level information into two <a href=https://en.wikipedia.org/wiki/Multimodal_distribution>multimodal distributions</a>, and combine these two <a href=https://en.wikipedia.org/wiki/Multimodal_distribution>multimodal distributions</a> into a mixed distribution. This mixed distribution will be used as the prior distribution of WAE in our proposed model, named as PersonaWAE. Experimental results on a large-scale real-world dataset confirm the superiority of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1203.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1203/>Recommendation as a Communication Game : Self-Supervised Bot-Play for Goal-oriented Dialogue</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/y/y-lan-boureau/>Y-Lan Boureau</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1203><div class="card-body p-3 small">Traditional recommendation systems produce static rather than interactive recommendations invariant to a user&#8217;s specific requests, clarifications, or current mood, and can suffer from the cold-start problem if their tastes are unknown. These issues can be alleviated by treating <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation</a> as an interactive dialogue task instead, where an expert recommender can sequentially ask about someone&#8217;s preferences, react to their requests, and recommend more appropriate items. In this work, we collect a goal-driven recommendation dialogue dataset (GoRecDial), which consists of 9,125 dialogue games and 81,260 conversation turns between pairs of human workers recommending movies to each other. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is specifically designed as a <a href=https://en.wikipedia.org/wiki/Cooperative_game_theory>cooperative game</a> between two players working towards a <a href=https://en.wikipedia.org/wiki/Goal>quantifiable common goal</a>. We leverage the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to develop an end-to-end dialogue system that can simultaneously converse and recommend. Models are first trained to imitate the behavior of human players without considering the task goal itself (supervised training). We then finetune our <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> on simulated bot-bot conversations between two paired pre-trained models (bot-play), in order to achieve the dialogue goal. Our experiments show that models finetuned with bot-play learn improved dialogue strategies, reach the dialogue goal more often when paired with a human, and are rated as more consistent by humans compared to <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> trained without bot-play. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and code are publicly available through the ParlAI framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1206.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1206/>How to Build User Simulators to Train RL-based Dialog Systems<span class=acl-fixed-case>RL</span>-based Dialog Systems</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/x/xuewei-wang/>Xuewei Wang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1206><div class="card-body p-3 small">User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the <a href=https://en.wikipedia.org/wiki/Simulation>simulator</a> directly impacts the <a href=https://en.wikipedia.org/wiki/Non-linear_gameplay>RL policy</a>. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these <a href=https://en.wikipedia.org/wiki/Simulation>simulators</a> both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1207.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1207/>Low-Rank HOCA : Efficient High-Order Cross-Modal Attention for Video Captioning<span class=acl-fixed-case>HOCA</span>: Efficient High-Order Cross-Modal Attention for Video Captioning</a></strong><br><a href=/people/t/tao-jin/>Tao Jin</a>
|
<a href=/people/s/siyu-huang/>Siyu Huang</a>
|
<a href=/people/y/yingming-li/>Yingming Li</a>
|
<a href=/people/z/zhongfei-zhang/>Zhongfei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1207><div class="card-body p-3 small">This paper addresses the challenging task of video captioning which aims to generate descriptions for <a href=https://en.wikipedia.org/wiki/Video>video data</a>. Recently, the attention-based encoder-decoder structures have been widely used in video captioning. In existing literature, the attention weights are often built from the information of an individual modality, while, the association relationships between multiple modalities are neglected. Motivated by this, we propose a video captioning model with High-Order Cross-Modal Attention (HOCA) where the attention weights are calculated based on the high-order correlation tensor to capture the frame-level cross-modal interaction of different modalities sufficiently. Furthermore, we novelly introduce Low-Rank HOCA which adopts <a href=https://en.wikipedia.org/wiki/Tensor_decomposition>tensor decomposition</a> to reduce the extremely large space requirement of HOCA, leading to a practical and efficient implementation in real-world applications. Experimental results on two benchmark datasets, MSVD and MSR-VTT, show that Low-rank HOCA establishes a new state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1210.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1210/>Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents</a></strong><br><a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1210><div class="card-body p-3 small">Images and text co-occur constantly on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, but explicit links between images and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a> (or other intra-document textual units) are often not present. We present <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1211/>UR-FUNNY : A Multimodal Language Dataset for Understanding Humor<span class=acl-fixed-case>UR</span>-<span class=acl-fixed-case>FUNNY</span>: A Multimodal Language Dataset for Understanding Humor</a></strong><br><a href=/people/m/md-kamrul-hasan/>Md Kamrul Hasan</a>
|
<a href=/people/w/wasifur-rahman/>Wasifur Rahman</a>
|
<a href=/people/a/amirali-bagher-zadeh/>AmirAli Bagher Zadeh</a>
|
<a href=/people/j/jianyuan-zhong/>Jianyuan Zhong</a>
|
<a href=/people/m/md-iftekhar-tanveer/>Md Iftekhar Tanveer</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/m/mohammed-ehsan-hoque/>Mohammed (Ehsan) Hoque</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1211><div class="card-body p-3 small">Humor is a unique and creative communicative behavior often displayed during <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a>. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding <a href=https://en.wikipedia.org/wiki/Humour>humor</a> from these three modalities falls within boundaries of multimodal language ; a recent research trend in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> that models <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> as it happens in face-to-face communication. Although humor detection is an established research area in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and accompanying studies, present a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1214/>A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/y/yangming-li/>Yangming Li</a>
|
<a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1214><div class="card-body p-3 small">Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1215.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1215/>Talk2Car : Taking Control of Your Self-Driving Car<span class=acl-fixed-case>T</span>alk2<span class=acl-fixed-case>C</span>ar: Taking Control of Your Self-Driving Car</a></strong><br><a href=/people/t/thierry-deruyttere/>Thierry Deruyttere</a>
|
<a href=/people/s/simon-vandenhende/>Simon Vandenhende</a>
|
<a href=/people/d/dusan-grujicic/>Dusan Grujicic</a>
|
<a href=/people/l/luc-van-gool/>Luc Van Gool</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1215><div class="card-body p-3 small">A long-term goal of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> is to have an agent execute commands communicated through <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. In many cases the <a href=https://en.wikipedia.org/wiki/Command_(computing)>commands</a> are grounded in a visual environment shared by the human who gives the command and the agent. Execution of the command then requires mapping the <a href=https://en.wikipedia.org/wiki/Command_(computing)>command</a> into the physical visual space, after which the appropriate action can be taken. In this paper we consider the former. Or more specifically, we consider the problem in an autonomous driving setting, where a passenger requests an action that can be associated with an object found in a street scene. Our work presents the Talk2Car dataset, which is the first object referral dataset that contains commands written in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> for <a href=https://en.wikipedia.org/wiki/Self-driving_car>self-driving cars</a>. We provide a detailed comparison with related datasets such as ReferIt, RefCOCO, RefCOCO+, RefCOCOg, Cityscape-Ref and CLEVR-Ref. Additionally, we include a performance analysis using strong <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a>. The results show that the proposed object referral task is a challenging one for which the models show promising results but still require additional research in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> and the intersection of these fields. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be found on our website : http://macchina-ai.eu/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1216/>Fact-Checking Meets Fauxtography : Verifying Claims About Images</a></strong><br><a href=/people/d/dimitrina-zlatkova/>Dimitrina Zlatkova</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/i/ivan-koychev/>Ivan Koychev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1216><div class="card-body p-3 small">The recent explosion of false claims in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a> in general has given rise to a lot of manual fact-checking initiatives. Unfortunately, the number of claims that need to be fact-checked is several orders of magnitude larger than what humans can handle manually. Thus, there has been a lot of research aiming at automating the process. Interestingly, previous work has largely ignored the growing number of claims about <a href=https://en.wikipedia.org/wiki/Image>images</a>. This is despite the fact that <a href=https://en.wikipedia.org/wiki/Visual_imagery>visual imagery</a> is more influential than <a href=https://en.wikipedia.org/wiki/Writing>text</a> and naturally appears alongside <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Here we aim at bridging this gap. In particular, we create a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this problem, and we explore a variety of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> modeling the claim, the <a href=https://en.wikipedia.org/wiki/Image>image</a>, and the relationship between the claim and the image. The evaluation results show sizable improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. We release our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, hoping to enable further research on fact-checking claims about <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1217 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1217/>Video Dialog via Progressive Inference and Cross-Transformer</a></strong><br><a href=/people/w/weike-jin/>Weike Jin</a>
|
<a href=/people/z/zhou-zhao/>Zhou Zhao</a>
|
<a href=/people/m/mao-gu/>Mao Gu</a>
|
<a href=/people/j/jun-xiao/>Jun Xiao</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1217><div class="card-body p-3 small">Video dialog is a new and challenging task, which requires the agent to answer questions combining video information with dialog history. And different from single-turn video question answering, the additional dialog history is important for video dialog, which often includes <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for the question. Existing visual dialog methods mainly use <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN</a> to encode the dialog history as a single vector representation, which might be rough and straightforward. Some more advanced methods utilize hierarchical structure, attention and memory mechanisms, which still lack an explicit reasoning process. In this paper, we introduce a novel progressive inference mechanism for video dialog, which progressively updates query information based on dialog history and video content until the agent think the information is sufficient and unambiguous. In order to tackle the multi-modal fusion problem, we propose a cross-transformer module, which could learn more fine-grained and comprehensive interactions both inside and between the modalities. And besides answer generation, we also consider question generation, which is more challenging but significant for a complete video dialog system. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on two large-scale datasets, and the extensive experiments show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1219" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1219/>Fusion of Detected Objects in Text for Visual Question Answering</a></strong><br><a href=/people/c/chris-alberti/>Chris Alberti</a>
|
<a href=/people/j/jeffrey-ling/>Jeffrey Ling</a>
|
<a href=/people/m/michael-collins/>Michael Collins</a>
|
<a href=/people/d/david-reitter/>David Reitter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1219><div class="card-body p-3 small">To advance models of multimodal context, we introduce a simple yet powerful neural architecture for <a href=https://en.wikipedia.org/wiki/Data>data</a> that combines <a href=https://en.wikipedia.org/wiki/Computer_vision>vision</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. The Bounding Boxes in Text Transformer (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark, achieving a new state-of-the-art with a 25 % relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. A reference implementation of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1222 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1222.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1222/>To Annotate or Not? Predicting Performance Drop under Domain Shift</a></strong><br><a href=/people/h/hady-elsahar/>Hady Elsahar</a>
|
<a href=/people/m/matthias-galle/>Matthias Gall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1222><div class="card-body p-3 small">Performance drop due to domain-shift is an endemic problem for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a> in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods (-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an <a href=https://en.wikipedia.org/wiki/Error_rate>error rate</a> as low as 2.15 % and 0.89 % for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> respectively.<tex-math>\\mathcal{H}</tex-math>-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1225 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1225/>A Deep Factorization of Style and Structure in Fonts</a></strong><br><a href=/people/n/nikita-srivatsan/>Nikita Srivatsan</a>
|
<a href=/people/j/jonathan-barron/>Jonathan Barron</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1225><div class="card-body p-3 small">We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1226 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1226/>Cross-lingual Semantic Specialization via Lexical Relation Induction</a></strong><br><a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vuli</a>
|
<a href=/people/g/goran-glavas/>Goran Glava</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1226><div class="card-body p-3 small">Semantic specialization integrates structured linguistic knowledge from external resources (such as lexical relations in WordNet) into pretrained distributional vectors in the form of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>. However, this technique can not be leveraged in many languages, because their structured external resources are typically incomplete or non-existent. To bridge this gap, we propose a novel method that transfers specialization from a resource-rich source language (English) to virtually any target language. Our specialization transfer comprises two crucial steps : 1) Inducing noisy constraints in the target language through automatic word translation ; and 2) Filtering the noisy constraints via a state-of-the-art relation prediction model trained on the source language constraints. This allows us to specialize any set of <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional vectors</a> in the target language with the refined <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a>. We prove the effectiveness of our method through intrinsic word similarity evaluation in 8 languages, and with 3 downstream tasks in 5 languages : lexical simplification, dialog state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> produces lists of WordNet-style lexical relations in resource-poor languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1227 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1227/>Modelling the interplay of <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> through <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a></a></strong><br><a href=/people/v/verna-dankers/>Verna Dankers</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/m/martha-lewis/>Martha Lewis</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1227><div class="card-body p-3 small">Metaphors allow us to convey <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> by connecting <a href=https://en.wikipedia.org/wiki/Experience>physical experiences</a> and <a href=https://en.wikipedia.org/wiki/Abstraction>abstract concepts</a>. The results of previous research in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> and <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> suggest that metaphorical phrases tend to be more emotionally evocative than their literal counterparts. In this paper, we investigate the relationship between <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> within a <a href=https://en.wikipedia.org/wiki/Software_framework>computational framework</a>, by proposing the first joint model of these <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a>. We experiment with several multitask learning architectures for this purpose, involving both hard and soft parameter sharing. Our results demonstrate that metaphor identification and emotion prediction mutually benefit from joint learning and our models advance the state of the art in both of these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1228.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1228/>How well do NLI models capture verb veridicality?<span class=acl-fixed-case>NLI</span> models capture verb veridicality?</a></strong><br><a href=/people/a/alexis-ross/>Alexis Ross</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1228><div class="card-body p-3 small">In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about <a href=https://en.wikipedia.org/wiki/Veridicality>veridicality</a> in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridicala bias which is amplified in BERT. We further show that, encouragingly, BERT&#8217;s inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the <a href=https://en.wikipedia.org/wiki/Complement_clause>complement clause</a> (to- vs. that-complements), and <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1230/>Negative Focus Detection via Contextual Attention Mechanism</a></strong><br><a href=/people/l/longxiang-shen/>Longxiang Shen</a>
|
<a href=/people/b/bowei-zou/>Bowei Zou</a>
|
<a href=/people/y/yu-hong/>Yu Hong</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a>
|
<a href=/people/a/aiti-aw/>AiTi Aw</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1230><div class="card-body p-3 small">Negation is a universal but complicated linguistic phenomenon, which has received considerable attention from the NLP community over the last decade, since a negated statement often carries both an explicit negative focus and implicit positive meanings. For the sake of understanding a negated statement, it is critical to precisely detect the negative focus in context. However, how to capture <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for negative focus detection is still an open challenge. To well address this, we come up with an attention-based neural network to model <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. In particular, we introduce a framework which consists of a Bidirectional Long Short-Term Memory (BiLSTM) neural network and a Conditional Random Fields (CRF) layer to effectively encode the order information and the long-range context dependency in a sentence. Moreover, we design two types of attention mechanisms, word-level contextual attention and topic-level contextual attention, to take advantage of contextual information across sentences from both the word perspective and the topic perspective, respectively. Experimental results on the SEM&#8217;12 shared task corpus show that our approach achieves the best performance on negative focus detection, yielding an absolute improvement of 2.11 % over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. This demonstrates the great effectiveness of the two types of contextual attention mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1231 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1231/>A Unified Neural Coherence Model</a></strong><br><a href=/people/h/han-cheol-moon/>Han Cheol Moon</a>
|
<a href=/people/m/muhammad-tasnim-mohiuddin/>Tasnim Mohiuddin</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/c/chi-xu/>Chi Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1231><div class="card-body p-3 small">Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often fail on harder <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> with more realistic application scenarios. In particular, the existing models underperform on tasks that require the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to be sensitive to local contexts such as candidate ranking in <a href=https://en.wikipedia.org/wiki/Dialogue>conversational dialogue</a> and in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> by a good margin, and establish a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1233/>Neural Generative Rhetorical Structure Parsing</a></strong><br><a href=/people/a/amandla-mabona/>Amandla Mabona</a>
|
<a href=/people/l/laura-rimell/>Laura Rimell</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1233><div class="card-body p-3 small">Rhetorical structure trees have been shown to be useful for several document-level tasks including <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. Previous approaches to RST parsing have used <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative models</a> ; however, these are less sample efficient than <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>, and RST parsing datasets are typically small. In this paper, we present the first <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> for RST parsing. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is a document-level RNN grammar (RNNG) with a <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up traversal order</a>. We show that, for our parser&#8217;s traversal order, previous beam search algorithms for RNNGs have a left-branching bias which is ill-suited for RST parsing. We develop a novel beam search algorithm that keeps track of both structure-and word-generating actions without exhibit-ing this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. Overall, our <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> outperforms a <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> with the same <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> by 2.6 <a href=https://en.wikipedia.org/wiki/F-number>F1points</a> and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1234 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1234/>Weak Supervision for Learning Discourse Structure</a></strong><br><a href=/people/s/sonia-badene/>Sonia Badene</a>
|
<a href=/people/k/kate-thompson/>Kate Thompson</a>
|
<a href=/people/j/jean-pierre-lorre/>Jean-Pierre Lorr</a>
|
<a href=/people/n/nicholas-asher/>Nicholas Asher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1234><div class="card-body p-3 small">This paper provides a detailed comparison of a data programming approach with (i) off-the-shelf, state-of-the-art deep learning architectures that optimize their representations (BERT) and (ii) handcrafted-feature approaches previously used in the discourse analysis literature. We compare these approaches on the task of learning discourse structure for <a href=https://en.wikipedia.org/wiki/Multi-party_system>multi-party dialogue</a>. The data programming paradigm offered by the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the generative step into probability distributions of the class labels given the data. We show that on our task the <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> outperforms both deep learning architectures as well as more traditional ML approaches when learning discourse structureit even outperforms the combination of deep learning methods and hand-crafted features. We also implement several strategies for decoding our generative model output in order to improve our results. We conclude that weak supervision methods hold great promise as a means for creating and improving data sets for discourse structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1235/>Predicting Discourse Structure using Distant Supervision from Sentiment</a></strong><br><a href=/people/p/patrick-huber/>Patrick Huber</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1235><div class="card-body p-3 small">Discourse parsing could not yet take full advantage of the neural NLP revolution, mostly due to the lack of annotated datasets. We propose a novel approach that uses distant supervision on an auxiliary task (sentiment classification), to generate abundant data for RST-style discourse structure prediction. Our approach combines a neural variant of multiple-instance learning, using document-level supervision, with an optimal CKY-style tree generation algorithm. In a series of experiments, we train a discourse parser (for only structure prediction) on our automatically generated dataset and compare it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> does not yet match the performance of a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is trained on one domain and tested / applied on another one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1239 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1239/>Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content</a></strong><br><a href=/people/s/sepideh-mesbah/>Sepideh Mesbah</a>
|
<a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/r/robert-jan-sips/>Robert-Jan Sips</a>
|
<a href=/people/m/manuel-valle-torre/>Manuel Valle Torre</a>
|
<a href=/people/c/christoph-lofi/>Christoph Lofi</a>
|
<a href=/people/a/alessandro-bozzon/>Alessandro Bozzon</a>
|
<a href=/people/g/geert-jan-houben/>Geert-Jan Houben</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1239><div class="card-body p-3 small">Social media provides a timely yet challenging data source for <a href=https://en.wikipedia.org/wiki/Adverse_drug_reaction>adverse drug reaction (ADR) detection</a>. Existing dictionary-based, semi-supervised learning approaches are intrinsically limited by the coverage and maintainability of laymen health vocabularies. In this paper, we introduce a data augmentation approach that leverages variational autoencoders to learn high-quality data distributions from a large unlabeled dataset, and subsequently, to automatically generate a large labeled training set from a small set of labeled samples. This allows for efficient social-media ADR detection with low training and re-training costs to adapt to the changes and emergence of informal medical laymen terms. An extensive evaluation performed on Twitter and Reddit data shows that our approach matches the performance of fully-supervised approaches while requiring only 25 % of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1242/>PullNet : Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text<span class=acl-fixed-case>P</span>ull<span class=acl-fixed-case>N</span>et: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</a></strong><br><a href=/people/h/haitian-sun/>Haitian Sun</a>
|
<a href=/people/t/tania-bedrax-weiss/>Tania Bedrax-Weiss</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1242><div class="card-body p-3 small">We consider open-domain question answering (QA) where answers are drawn from either a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., multi-hop) reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an <a href=https://en.wikipedia.org/wiki/Iterative_and_incremental_development>iterative process</a> to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or pull) operations on the corpus and/or KB. After the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraph</a> is complete, another graph CNN is used to extract the answer from the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraph</a>. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-only setting</a>.<fixed-case>iterative</fixed-case> process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or &#8220;pull&#8221;) operations on the corpus and/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1246/>A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs</a></strong><br><a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/m/masashi-shimbo/>Masashi Shimbo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1246><div class="card-body p-3 small">Bilinear diagonal models for knowledge graph embedding (KGE), such as DistMult and ComplEx, balance expressiveness and computational efficiency by representing relations as diagonal matrices. Although they perform well in predicting atomic relations, composite relations (relation paths) can not be modeled naturally by the product of relation matrices, as the product of diagonal matrices is commutative and hence invariant with the order of relations. In this paper, we propose a new bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE, <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>relation matrices</a> can be non-commutative, allowing composite relations to be modeled by <a href=https://en.wikipedia.org/wiki/Matrix_multiplication>matrix product</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is parameterized in a way that covers a spectrum ranging from diagonal to full relation matrices. A fast computation technique can be developed on the basis of the duality of the Fourier transform of circulant matrices.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1247 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1247/>Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss</a></strong><br><a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/z/zaiqing-nie/>Zaiqing Nie</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1247><div class="card-body p-3 small">We tackle the task of question generation over <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. Conventional methods for this task neglect two crucial research issues : 1) the given predicate needs to be expressed ; 2) the answer to the generated question needs to be definitive. In this paper, we strive toward the above two issues via incorporating diversified contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder model with multi-level copy mechanisms to generate such questions. Furthermore, the answer aware loss is introduced to make generated questions corresponding to more definitive answers. Experiments demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance. Meanwhile, such generated question is able to express the given predicate and correspond to a definitive answer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1248 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1248.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1248" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1248/>Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base</a></strong><br><a href=/people/t/tao-shen/>Tao Shen</a>
|
<a href=/people/x/xiubo-geng/>Xiubo Geng</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/d/daya-guo/>Daya Guo</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/g/guodong-long/>Guodong Long</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1248><div class="card-body p-3 small">We consider the problem of conversational question answering over a large-scale knowledge base. To handle huge entity vocabulary of a large-scale knowledge base, recent neural semantic parsing based approaches usually decompose the task into several subtasks and then solve them sequentially, which leads to following issues : 1) errors in earlier subtasks will be propagated and negatively affect downstream ones ; and 2) each subtask can not naturally share supervision signals with others. To tackle these issues, we propose an innovative multi-task learning framework where a pointer-equipped semantic parsing model is designed to resolve coreference in conversations, and naturally empower joint learning with a novel type-aware entity detection model. The proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> thus enables shared supervisions and alleviates the effect of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. Experiments on a large-scale conversational question answering dataset containing 1.6 M question answering pairs over 12.8 M entities show that the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> improves overall F1 score from 67 % to 79 % compared with previous state-of-the-art work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1249 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1249/>BiPaR : A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>P</span>a<span class=acl-fixed-case>R</span>: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels</a></strong><br><a href=/people/y/yimin-jing/>Yimin Jing</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/z/zhen-yan/>Zhen Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1249><div class="card-body p-3 small">This paper presents BiPaR, a bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support multilingual and cross-lingual reading comprehension. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages. We collect 3,667 bilingual parallel paragraphs from Chinese and English novels, from which we construct 14,668 parallel question-answer pairs via crowdsourced workers following a strict quality control procedure. We analyze BiPaR in depth and find that BiPaR offers good diversification in prefixes of questions, answer types and relationships between questions and passages. We also observe that answering questions of novels requires reading comprehension skills of <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, multi-sentence reasoning, and understanding of implicit causality, etc. With BiPaR, we build monolingual, multilingual, and cross-lingual MRC baseline models. Even for the relatively simple monolingual MRC on this dataset, experiments show that a strong BERT baseline is over 30 points behind human in terms of both EM and F1 score, indicating that BiPaR provides a challenging testbed for monolingual, multilingual and cross-lingual MRC on novels. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is available at https://multinlp.github.io/BiPaR/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1250 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1250" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1250/>Language Models as Knowledge Bases?</a></strong><br><a href=/people/f/fabio-petroni/>Fabio Petroni</a>
|
<a href=/people/t/tim-rocktaschel/>Tim Rocktschel</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/a/anton-bakhtin/>Anton Bakhtin</a>
|
<a href=/people/y/yuxiang-wu/>Yuxiang Wu</a>
|
<a href=/people/a/alexander-miller/>Alexander Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1250><div class="card-body p-3 small">Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as fill-in-the-blank cloze statements. Language models have many advantages over structured knowledge bases : they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to recall factual knowledge without any <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> demonstrates their potential as <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised open-domain QA systems</a>. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1251.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1251" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1251/>NumNet : Machine Reading Comprehension with <a href=https://en.wikipedia.org/wiki/Numerical_analysis>Numerical Reasoning</a><span class=acl-fixed-case>N</span>um<span class=acl-fixed-case>N</span>et: Machine Reading Comprehension with Numerical Reasoning</a></strong><br><a href=/people/q/qiu-ran/>Qiu Ran</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1251><div class="card-body p-3 small">Numerical reasoning, such as <a href=https://en.wikipedia.org/wiki/Addition>addition</a>, <a href=https://en.wikipedia.org/wiki/Subtraction>subtraction</a>, <a href=https://en.wikipedia.org/wiki/Sorting>sorting</a> and <a href=https://en.wikipedia.org/wiki/Counting>counting</a> is a critical skill in human&#8217;s reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56 % on the DROP dataset, outperforming all existing <a href=https://en.wikipedia.org/wiki/Machine_learning>machine reading comprehension models</a> by considering the <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical relations</a> among numbers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1253" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1253/>Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering</a></strong><br><a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1253><div class="card-body p-3 small">Text-based Question Generation (QG) aims at generating natural and relevant questions that can be answered by a given answer in some context. Existing QG models suffer from a semantic drift problem, i.e., the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the model-generated question drifts away from the given context and answer. In this paper, we first propose two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG model&#8217;s ability to mimic human annotators in generating QA training data. Experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves the new state-of-the-art performance w.r.t. traditional metrics, and also performs best on our QA-based evaluation metrics. Further, we investigate how to use our QG model to augment QA datasets and enable <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised QA</a>. We propose two ways to generate synthetic QA pairs : generate new questions from existing articles or collect QA pairs from new articles. We also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1255/>Incorporating External Knowledge into <a href=https://en.wikipedia.org/wiki/Machine_reading>Machine Reading</a> for Generative Question Answering</a></strong><br><a href=/people/b/bin-bi/>Bin Bi</a>
|
<a href=/people/c/chen-wu/>Chen Wu</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/j/jiangnan-xia/>Jiangnan Xia</a>
|
<a href=/people/c/chenliang-li/>Chenliang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1255><div class="card-body p-3 small">Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> for a given question with context. In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available : question, passage, vocabulary and knowledge. During the process of <a href=https://en.wikipedia.org/wiki/Question_answering>answer generation</a>, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1257.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1257" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1257/>Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension</a></strong><br><a href=/people/t/todor-mihaylov/>Todor Mihaylov</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1257><div class="card-body p-3 small">In this work, we propose to use linguistic annotations as a basis for a Discourse-Aware Semantic Self-Attention encoder that we employ for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> on narrative texts. We extract relations between discourse units, <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a>, and their arguments as well as coreferring mentions, using available annotation tools. Our empirical evaluation shows that the investigated structures improve the overall performance (up to +3.4 Rouge-L), especially intra-sentential and cross-sentential discourse relations, sentence-internal semantic role relations, and long-distance coreference relations. We show that dedicating self-attention heads to intra-sentential relations and relations connecting neighboring sentences is beneficial for finding answers to questions in longer contexts. Our findings encourage the use of discourse-semantic annotations to enhance the generalization capacity of self-attention models for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1258 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1258" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1258/>Revealing the Importance of Semantic Retrieval for <a href=https://en.wikipedia.org/wiki/Machine_reading>Machine Reading</a> at Scale</a></strong><br><a href=/people/y/yixin-nie/>Yixin Nie</a>
|
<a href=/people/s/songhe-wang/>Songhe Wang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1258><div class="card-body p-3 small">Machine Reading at Scale (MRS) is a challenging task in which a <a href=https://en.wikipedia.org/wiki/System>system</a> is given an input query and is asked to produce a precise output by reading information from a large knowledge base. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> has gained popularity with its natural combination of <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval (IR)</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine comprehension (MC)</a>. Advancements in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> have led to separated progress in both IR and MC ; however, very few studies have examined the relationship and combined design of <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval</a> and <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a> at different levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1259 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1259" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1259/>PubMedQA : A Dataset for Biomedical Research Question Answering<span class=acl-fixed-case>P</span>ub<span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>QA</span>: A Dataset for Biomedical Research Question Answering</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/z/zhengping-liu/>Zhengping Liu</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/x/xinghua-lu/>Xinghua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1259><div class="card-body p-3 small">We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes / no / maybe (e.g. : Do preoperative <a href=https://en.wikipedia.org/wiki/Statin>statins</a> reduce <a href=https://en.wikipedia.org/wiki/Atrial_fibrillation>atrial fibrillation</a> after coronary artery bypass grafting?) using the corresponding <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a>. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes / no / maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, compared to single human performance of 78.0 % accuracy and majority-baseline of 55.2 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1264 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1264.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1264/>Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Reinforcement Learning</a></a></strong><br><a href=/people/h/heng-wang/>Heng Wang</a>
|
<a href=/people/s/shuangyin-li/>Shuangyin Li</a>
|
<a href=/people/r/rong-pan/>Rong Pan</a>
|
<a href=/people/m/mingzhi-mao/>Mingzhi Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1264><div class="card-body p-3 small">Knowledge Graph (KG) reasoning aims at finding reasoning paths for relations, in order to solve the problem of incompleteness in KG. Many previous path-based methods like PRA and DeepPath suffer from lacking memory components, or stuck in training. Therefore, their performances always rely on well-pretraining. In this paper, we present a deep reinforcement learning based model named by AttnPath, which incorporates LSTM and Graph Attention Mechanism as the memory components. We define two metrics, Mean Selection Rate (MSR) and Mean Replacement Rate (MRR), to quantitatively measure how difficult it is to learn the query relations, and take advantages of them to fine-tune the model under the framework of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Meanwhile, a novel mechanism of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> is proposed by forcing an agent to walk forward every step to avoid the agent stalling at the same entity node constantly. Based on this operation, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> not only can get rid of the pretraining process, but also achieves state-of-the-art performance comparing with the other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on FB15K-237 and NELL-995 datasets with different <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. Extensive experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is effective and competitive with many current state-of-the-art methods, and also performs well in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1266 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1266/>DIVINE : A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning<span class=acl-fixed-case>DIVINE</span>: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning</a></strong><br><a href=/people/r/ruiping-li/>Ruiping Li</a>
|
<a href=/people/x/xiang-cheng/>Xiang Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1266><div class="card-body p-3 small">Knowledge graphs (KGs) often suffer from <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>sparseness</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>incompleteness</a>. Knowledge graph reasoning provides a feasible way to address such problems. Recent studies on knowledge graph reasoning have shown that reinforcement learning (RL) based methods can provide state-of-the-art performance. However, existing RL-based methods require numerous trials for <a href=https://en.wikipedia.org/wiki/Pathfinding>path-finding</a> and rely heavily on meticulous reward engineering to fit specific dataset, which is inefficient and laborious to apply to fast-evolving KGs. To this end, in this paper, we present DIVINE, a novel plug-and-play framework based on generative adversarial imitation learning for enhancing existing RL-based methods. DIVINE guides the path-finding process, and learns reasoning policies and reward functions self-adaptively through imitating the demonstrations automatically sampled from KGs. Experimental results on two benchmark datasets show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> improves the performance of existing RL-based methods while eliminating extra reward engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1269 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1269.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1269" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1269/>Collaborative Policy Learning for Open Knowledge Graph Reasoning</a></strong><br><a href=/people/c/cong-fu/>Cong Fu</a>
|
<a href=/people/t/tong-chen/>Tong Chen</a>
|
<a href=/people/m/meng-qu/>Meng Qu</a>
|
<a href=/people/w/woojeong-jin/>Woojeong Jin</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1269><div class="card-body p-3 small">In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoninga task that aims to reason for missing facts over a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> augmented by a <a href=https://en.wikipedia.org/wiki/Text_corpus>background text corpus</a>. A key challenge of the task is to filter out irrelevant facts extracted from corpus, in order to maintain an effective <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> during path inference. We propose a novel reinforcement learning framework to train two <a href=https://en.wikipedia.org/wiki/Collaborative_learning>collaborative agents</a> jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly ; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1272.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1272 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1272 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1272.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1272" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1272/>Keep Calm and Switch On ! Preserving Sentiment and Fluency in Semantic Text Exchange</a></strong><br><a href=/people/s/steven-y-feng/>Steven Y. Feng</a>
|
<a href=/people/a/aaron-w-li/>Aaron W. Li</a>
|
<a href=/people/j/jesse-hoey/>Jesse Hoey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1272><div class="card-body p-3 small">In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> and <a href=https://en.wikipedia.org/wiki/Virtual_assistant>virtual assistants</a>. We introduce a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> called SMERTI that combines <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity replacement</a>, similarity masking, and text infilling. We measure our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>&#8217;s success by its Semantic Text Exchange Score (STES): the ability to preserve the original text&#8217;s sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on <a href=https://en.wikipedia.org/wiki/Yelp>Yelp reviews</a>, <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon reviews</a>, and <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1280 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1280.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1280" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1280/>Interactive Language Learning by Question Answering</a></strong><br><a href=/people/x/xingdi-yuan/>Xingdi Yuan</a>
|
<a href=/people/m/marc-alexandre-cote/>Marc-Alexandre Ct</a>
|
<a href=/people/j/jie-fu/>Jie Fu</a>
|
<a href=/people/z/zhouhan-lin/>Zhouhan Lin</a>
|
<a href=/people/c/christopher-pal/>Chris Pal</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1280><div class="card-body p-3 small">Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> present <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task : Question Answering with Interactive Text (QAit). In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The <a href=https://en.wikipedia.org/wiki/Data>data</a> is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> for the QAit task that includes deep reinforcement learning agents. Experiments show that the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> presents a major challenge for machine reading systems, while humans solve it with relative ease.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1283 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1283/>Learning with Limited Data for Multilingual Reading Comprehension</a></strong><br><a href=/people/k/kyungjae-lee/>Kyungjae Lee</a>
|
<a href=/people/s/sunghyun-park/>Sunghyun Park</a>
|
<a href=/people/h/hojae-han/>Hojae Han</a>
|
<a href=/people/j/jinyoung-yeo/>Jinyoung Yeo</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a>
|
<a href=/people/j/juho-lee/>Juho Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1283><div class="card-body p-3 small">This paper studies the problem of supporting question answering in a new language with limited training resources. As an extreme scenario, when no such resource exists, one can (1) transfer labels from another language, and (2) generate labels from unlabeled data, using translator and automatic labeling function respectively. However, these approaches inevitably introduce noises to the training data, due to translation or generation errors, which require a judicious use of data with varying confidence. To address this challenge, we propose a weakly-supervised framework that quantifies such noises from automatically generated labels, to deemphasize or fix noisy data in training. On reading comprehension task, we demonstrate the effectiveness of our model on low-resource languages with varying similarity to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, namely, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1287 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1287" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1287/>Representation of Constituents in Neural Language Models : Coordination Phrase as a Case Study</a></strong><br><a href=/people/a/aixiu-an/>Aixiu An</a>
|
<a href=/people/p/peng-qian/>Peng Qian</a>
|
<a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1287><div class="card-body p-3 small">Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a> is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models&#8217; ability to represent <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituent-level features</a>, using coordinated noun phrases as a case study. We assess whether different neural language models trained on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a> represent phrase-level number and gender features, and use those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP / verb number agreement. This <a href=https://en.wikipedia.org/wiki/Behavior>behavior</a> is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with <a href=https://en.wikipedia.org/wiki/Gender_of_connectors_and_fasteners>gender agreement</a>. Models trained on large corpora perform best, and there is no obvious advantage for <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained using explicit syntactic supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1288 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1288.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1288/>Towards Zero-shot Language Modeling</a></strong><br><a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vuli</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1288><div class="card-body p-3 small">Can we construct a neural language model which is inductively biased towards learning <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through <a href=https://en.wikipedia.org/wiki/Laplace&#8217;s_method>Laplace&#8217;s method</a>. Based on a large and diverse sample of languages, the use of our <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> from <a href=https://en.wikipedia.org/wiki/Linguistic_description>typological databases</a>, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the <a href=https://en.wikipedia.org/wiki/Limited_series_(comics)>few-shot setting</a>, but ineffective in the <a href=https://en.wikipedia.org/wiki/Limited_series_(comics)>zero-shot setting</a>. Since the paucity of even plain digital text affects the majority of the world&#8217;s languages, we hope that these insights will broaden the scope of applications for <a href=https://en.wikipedia.org/wiki/Language_technology>language technology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1290 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1290/>Modeling Frames in Argumentation</a></strong><br><a href=/people/y/yamen-ajjour/>Yamen Ajjour</a>
|
<a href=/people/m/milad-alshomary/>Milad Alshomary</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1290><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a>, <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> is used to emphasize a specific aspect of a controversial topic while concealing others. When talking about <a href=https://en.wikipedia.org/wiki/Drug_liberalization>legalizing drugs</a>, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the right frame(s) to convince the audience to adopt the author&#8217;s stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a> that fit the audience&#8217;s cultural background and interests. This paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information and then identifies <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a> using <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. For evaluation purposes, we provide a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with 12, 326 debate-portal arguments, organized along the frames of the debates&#8217; topics. On this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, our approach outperforms different strong baselines, achieving an F1-score of 0.28.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1293 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1293.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1293/>Nonsense ! : Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials</a></strong><br><a href=/people/w/wonsuk-yang/>Wonsuk Yang</a>
|
<a href=/people/s/seungwon-yoon/>Seungwon Yoon</a>
|
<a href=/people/a/ada-carpenter/>Ada Carpenter</a>
|
<a href=/people/j/jong-c-park/>Jong Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1293><div class="card-body p-3 small">Annotation quality control is a critical aspect for building reliable corpora through <a href=https://en.wikipedia.org/wiki/Annotation>linguistic annotation</a>. In this study, we present a simple but powerful quality control method using two-step reason selection. We gathered sentential annotations of local acceptance and three related attributes through a <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing platform</a>. For each attribute, the reason for the choice of the attribute value is selected in a two-step manner. The options given for reason selection were designed to facilitate the detection of a nonsensical reason selection. We assume that a sentential annotation that contains a nonsensical reason is less reliable than the one without such reason. Our method, based solely on this assumption, is found to retain the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> with satisfactory quality out of the entire annotations mixed with those of low quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1294 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1294.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1294" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1294/>Evaluating Pronominal Anaphora in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> : An Evaluation Measure and a Test Suite</a></strong><br><a href=/people/p/prathyusha-jwalapuram/>Prathyusha Jwalapuram</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/i/irina-temnikova/>Irina Temnikova</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1294><div class="card-body p-3 small">The ongoing neural revolution in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, as only a few words end up being affected. Thus, specialized <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation measures</a> are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> to report correlations with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1295 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1295/>A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing</a></strong><br><a href=/people/z/zeyu-dai/>Zeyu Dai</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1295><div class="card-body p-3 small">We argue that external commonsense knowledge and linguistic constraints need to be incorporated into neural network models for mitigating data sparsity issues and further improving the performance of discourse parsing. Realizing that external knowledge and linguistic constraints may not always apply in understanding a particular context, we propose a regularization approach that tightly integrates these constraints with contexts for deriving word representations. Meanwhile, it balances attentions over contexts and constraints through adding a regularization term into the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a>. Experiments show that our knowledge regularization approach outperforms all previous systems on the benchmark dataset <a href=https://en.wikipedia.org/wiki/PDTB>PDTB</a> for discourse parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1299/>Enhancing Neural Data-To-Text Generation Models with External Background Knowledge</a></strong><br><a href=/people/s/shuang-chen/>Shuang Chen</a>
|
<a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/f/feng-jiang/>Feng Jiang</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1299><div class="card-body p-3 small">Recent neural models for data-to-text generation rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that writing knowledge can be acquired from the training data alone. However, when people are writing, they not only rely on the data but also consider <a href=https://en.wikipedia.org/wiki/Common_knowledge_(logic)>related knowledge</a>. In this paper, we enhance neural data-to-text models with external knowledge in a simple but effective way to improve the fidelity of generated text. Besides relying on parallel data and text as in previous work, our model attends to relevant external knowledge, encoded as a temporary memory, and combines this <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> with the context representation of data before generating words. This allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to infer relevant facts which are not explicitly stated in the data table from an external knowledge source. Experimental results on twenty-one Wikipedia infobox-to-text datasets show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, KBAtt, consistently improves a state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on most of the datasets. In addition, to quantify when and why <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>external knowledge</a> is effective, we design a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, KBGain, which shows a strong correlation with the observed performance boost. This result demonstrates the relevance of external knowledge and sparseness of original data are the main factors affecting system performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1300 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1300.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1300/>Reading Like HER : Human Reading Inspired Extractive Summarization<span class=acl-fixed-case>HER</span>: Human Reading Inspired Extractive Summarization</a></strong><br><a href=/people/l/ling-luo/>Ling Luo</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/f/feiyang-pan/>Feiyang Pan</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/q/qing-he/>Qing He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1300><div class="card-body p-3 small">In this work, we re-examine the problem of extractive text summarization for long documents. We observe that the process of extracting summarization of human can be divided into two stages : 1) a rough reading stage to look for sketched information, and 2) a subsequent careful reading stage to select key sentences to form the summary. By simulating such a two-stage process, we propose a novel approach for extractive summarization. We formulate the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a contextual-bandit problem and solve it with policy gradient. We adopt a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to encode gist of paragraphs for rough reading, and a decision making policy with an adapted termination mechanism for careful reading. Experiments on the CNN and DailyMail datasets show that our proposed method can provide high-quality summaries with varied length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1302/>NCLS : Neural Cross-Lingual Summarization<span class=acl-fixed-case>NCLS</span>: Neural Cross-Lingual Summarization</a></strong><br><a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/s/shaonan-wang/>Shaonan Wang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1302><div class="card-body p-3 small">Cross-lingual summarization (CLS) is the task to produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps : <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and <a href=https://en.wikipedia.org/wiki/Automatic_translation>translation</a>, leading to the problem of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. To handle that, we present an end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS), for the first time. Moreover, we propose to further improve NCLS by incorporating two related tasks, monolingual summarization and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, into the training process of CLS under <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Due to the lack of supervised CLS data, we propose a round-trip translation strategy to acquire two high-quality large-scale CLS datasets based on existing monolingual summarization datasets. Experimental results have shown that our NCLS achieves remarkable improvement over traditional pipeline methods on both English-to-Chinese and Chinese-to-English CLS human-corrected test sets. In addition, NCLS with <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> can further significantly improve the quality of generated summaries. We make our dataset and code publicly available here : http://www.nlpr.ia.ac.cn/cip/dataset.htm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1303/>Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1303><div class="card-body p-3 small">Sensational headlines are <a href=https://en.wikipedia.org/wiki/Headline>headlines</a> that capture people&#8217;s attention and generate <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>reader interest</a>. Conventional abstractive headline generation methods, unlike human writers, do not optimize for maximal reader attention. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that generates <a href=https://en.wikipedia.org/wiki/Sensationalism>sensational headlines</a> without labeled data. We first train a sensationalism scorer by classifying online headlines with many comments (clickbait) against a baseline of headlines generated from a summarization model. The score from the sensationalism scorer is used as the <a href=https://en.wikipedia.org/wiki/Reward_system>reward</a> for a reinforcement learner. However, maximizing the noisy sensationalism reward will generate unnatural phrases instead of sensational headlines. To effectively leverage this noisy reward, we propose a novel <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, Auto-tuned Reinforcement Learning (ARL), to dynamically balance <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> with <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a>. Human evaluation shows that 60.8 % of samples generated by our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> are sensational, which is significantly better than the Pointer-Gen baseline and other RL models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1305 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1305.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1305/>Surface Realisation Using Full Delexicalisation</a></strong><br><a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1305><div class="card-body p-3 small">Surface realisation (SR) maps a meaning representation to a sentence and can be viewed as consisting of three subtasks : <a href=https://en.wikipedia.org/wiki/Word_order>word ordering</a>, <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> and <a href=https://en.wikipedia.org/wiki/Contraction_(grammar)>contraction generation</a> (e.g., clitic attachment in Portuguese or elision in French). We propose a modular approach to surface realisation which models each of these components separately, and evaluate our approach on the 10 languages covered by the SR&#8217;18 Surface Realisation Shared Task shallow track. We provide a detailed evaluation of how <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological realisation</a> and <a href=https://en.wikipedia.org/wiki/Contraction_(grammar)>contractions</a> are handled by the model and an analysis of the differences in word ordering performance across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1308 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1308/>Mixture Content Selection for Diverse Sequence Generation</a></strong><br><a href=/people/j/jaemin-cho/>Jaemin Cho</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1308><div class="card-body p-3 small">Generating diverse sequences is important in many NLP applications such as question generation or <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different <a href=https://en.wikipedia.org/wiki/Mask_(computing)>binary masks</a> on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Diversity_index>diversity</a> and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training efficiency</a>, including state-of-the-art top-1 accuracy in both datasets, 6 % gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1309/>An End-to-End Generative Architecture for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>Paraphrase Generation</a></a></strong><br><a href=/people/q/qian-yang/>Qian Yang</a>
|
<a href=/people/z/zhouyuan-huo/>Zhouyuan Huo</a>
|
<a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/y/yong-cheng/>Yong Cheng</a>
|
<a href=/people/w/wenlin-wang/>Wenlin Wang</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1309><div class="card-body p-3 small">Generating high-quality paraphrases is a fundamental yet challenging natural language processing task. Despite the effectiveness of previous work based on <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>, there remain problems with <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> in <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, and often a failure to generate realistic sentences. To overcome these challenges, we propose the first end-to-end conditional generative architecture for generating <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> via adversarial training, which does not depend on extra linguistic information. Extensive experiments on four public datasets demonstrate the proposed method achieves state-of-the-art results, outperforming previous generative architectures on both automatic metrics (BLEU, <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>, and TER) and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1314.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1314/>Enhancing AMR-to-Text Generation with Dual Graph Representations<span class=acl-fixed-case>AMR</span>-to-Text Generation with Dual Graph Representations</a></strong><br><a href=/people/l/leonardo-f-r-ribeiro/>Leonardo F. R. Ribeiro</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1314><div class="card-body p-3 small">Generating text from <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph-based data</a>, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1316/>Toward a Task of Feedback Comment Generation for Writing Learning</a></strong><br><a href=/people/r/ryo-nagata/>Ryo Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1316><div class="card-body p-3 small">In this paper, we introduce a novel task called feedback comment generation a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non-native learners of English. There has been almost no work on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, showing that a simple neural retrieval-based method sets a baseline performance with an <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1318 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1318/>Deep Copycat Networks for Text-to-Text Generation</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1318><div class="card-body p-3 small">Most text-to-text generation tasks, for example <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarisation</a> and text simplification, require copying words from the input to the output. We introduce <a href=https://en.wikipedia.org/wiki/Copycat>Copycat</a>, a transformer-based pointer network for such tasks which obtains competitive results in abstractive text summarisation and generates more abstractive summaries. We propose a further extension of this architecture for automatic post-editing, where generation is conditioned over two inputs (source language and machine translation), and the model is capable of deciding where to copy information from. This approach achieves competitive performance when compared to state-of-the-art automated post-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing-overcorrecting translations-and that our novel mechanism for copying source language words improves the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1319 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1319/>Towards Controllable and Personalized Review Generation</a></strong><br><a href=/people/p/pan-li/>Pan Li</a>
|
<a href=/people/a/alexander-tuzhilin/>Alexander Tuzhilin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1319><div class="card-body p-3 small">In this paper, we propose a novel model RevGAN that automatically generates controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information. RevGAN utilizes the combination of three novel components, including self-attentive recursive autoencoders, conditional discriminators, and personalized decoders. We test its performance on the several real-world datasets, where our model significantly outperforms state-of-the-art generation models in terms of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence quality</a>, <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>, <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, and human evaluations. We also empirically show that the generated reviews could not be easily distinguished from the organically produced reviews and that they follow the same statistical linguistics laws.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1321 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1321.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1321" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1321/>Long and Diverse Text Generation with Planning-based Hierarchical Variational Model</a></strong><br><a href=/people/z/zhihong-shao/>Zhihong Shao</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/j/jiangtao-wen/>Jiangtao Wen</a>
|
<a href=/people/w/wenfei-xu/>Wenfei Xu</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1321><div class="card-body p-3 small">Existing neural methods for data-to-text generation are still struggling to produce long and diverse texts : they are insufficient to model input data dynamically during generation, to capture inter-sentence coherence, or to generate diversified expressions. To address these issues, we propose a Planning-based Hierarchical Variational Model (PHVM). Our model first plans a sequence of groups (each group is a subset of input items to be covered by a sentence) and then realizes each sentence conditioned on the planning result and the previously generated context, thereby decomposing long text generation into dependent sentence generation sub-tasks. To capture expression diversity, we devise a hierarchical latent structure where a global planning latent variable models the diversity of reasonable planning and a sequence of local latent variables controls sentence realization. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art baselines in long and diverse text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1325 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1325.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1325/>Domain Adaptive Text Style Transfer</a></strong><br><a href=/people/d/dianqi-li/>Dianqi Li</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a>
|
<a href=/people/m/ming-ting-sun/>Ming-Ting Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1325><div class="card-body p-3 small">Text style transfer without <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> has achieved some practical success. However, in the scenario where less data is available, these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> may yield poor performance. In this paper, we examine <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for text style transfer to leverage massively available data from other domains. These <a href=https://en.wikipedia.org/wiki/Data>data</a> may demonstrate domain shift, which impedes the benefits of utilizing such <a href=https://en.wikipedia.org/wiki/Data>data</a> for training. To address this challenge, we propose simple yet effective domain adaptive text style transfer models, enabling domain-adaptive information exchange. The proposed models presumably learn from the source domain to : (i) distinguish stylized information and generic content information ; (ii) maximally preserve content information ; and (iii) adaptively transfer the styles in a domain-aware manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1326 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1326.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1326" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1326/>Lets Ask Again : Refine Network for Automatic Question Generation</a></strong><br><a href=/people/p/preksha-nema/>Preksha Nema</a>
|
<a href=/people/a/akash-kumar-mohankumar/>Akash Kumar Mohankumar</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a>
|
<a href=/people/b/balaraman-ravindran/>Balaraman Ravindran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1326><div class="card-body p-3 small">In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question. It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer. An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of the above-mentioned qualities. In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement. To alleviate this shortcoming, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it. More specifically, we propose Refine Network (RefNet) which contains two <a href=https://en.wikipedia.org/wiki/Code>decoders</a>. The second <a href=https://en.wikipedia.org/wiki/Code>decoder</a> uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first <a href=https://en.wikipedia.org/wiki/Code>decoder</a>. In effect, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> refines the question generated by the first <a href=https://en.wikipedia.org/wiki/Code>decoder</a>, thereby making it more correct and complete. We evaluate RefNet on three datasets, viz., SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16 % on all of these datasets. Lastly, we show that we can improve the quality of the second decoder on specific <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, such as, <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> and answerability by explicitly rewarding revisions that improve on the corresponding <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> during training.<fixed-case>the above-mentioned qualities</fixed-case>. In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement. <fixed-case>To alleviate this shortcoming</fixed-case>, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it. More specifically, we propose Refine Network (RefNet) which contains two decoders. The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder. In effect, it refines the question generated by the first decoder, thereby making it more correct and complete. We evaluate RefNet on three datasets, <i>viz.</i>, SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16% on all of these datasets. Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training. The code has been made publicly available .</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1329 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1329.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1329/>Towards Realistic Practices In Low-Resource Natural Language Processing : The Development Set</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1329><div class="card-body p-3 small">Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions : Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> lead to overestimation or underestimation of performance? We repeat multiple experiments from recent work on neural models for low-resource NLP and compare results for <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> obtained by training with and without development sets. On average over languages, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>absolute accuracy</a> differs by up to 1.4 %. However, for some languages and tasks, differences are as big as 18.0 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our results highlight the importance of realistic experimental setups in the publication of low-resource NLP research results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1330/>Synchronously Generating Two Languages with Interactive Decoding</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/y/yuchen-liu/>Yuchen Liu</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1330><div class="card-body p-3 small">In this paper, we introduce a novel interactive approach to translate a source language into two different languages simultaneously and interactively. Specifically, the generation of one language relies on not only previously generated outputs by itself, but also the outputs predicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1331 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1331/>On NMT Search Errors and Model Errors : Cat Got Your Tongue?<span class=acl-fixed-case>NMT</span> Search Errors and Model Errors: Cat Got Your Tongue?</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1331><div class="card-body p-3 small">We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and <a href=https://en.wikipedia.org/wiki/Depth-first_search>depth-first search</a>. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50 % of the sentences, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1332 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1332.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1332/>Going on a vacation takes longer than Going for a walk : A Study of Temporal Commonsense Understanding</a></strong><br><a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1332><div class="card-body p-3 small">Understanding time is crucial for understanding events expressed in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Because people rarely say the obvious, it is often necessary to have <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> about various temporal aspects of events, such as <a href=https://en.wikipedia.org/wiki/Time>duration</a>, <a href=https://en.wikipedia.org/wiki/Frequency>frequency</a>, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20 %, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1333 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1333.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1333" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1333/>QAInfomax : Learning Robust Question Answering System by Mutual Information Maximization<span class=acl-fixed-case>QAI</span>nfomax: Learning Robust Question Answering System by Mutual Information Maximization</a></strong><br><a href=/people/y/yi-ting-yeh/>Yi-Ting Yeh</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1333><div class="card-body p-3 small">Standard accuracy metrics indicate that modern reading comprehension systems have achieved strong performance in many question answering datasets. However, the extent these <a href=https://en.wikipedia.org/wiki/System>systems</a> truly understand <a href=https://en.wikipedia.org/wiki/Language>language</a> remains unknown, and existing <a href=https://en.wikipedia.org/wiki/System>systems</a> are not good at distinguishing distractor sentences which look related but do not answer the question. To address this problem, we propose QAInfomax as a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension systems</a> by maximizing <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> among passages, a question, and its answer. QAInfomax helps regularize the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to not simply learn the superficial correlation for answering the questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1335 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1335.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1335" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1335/>How Reasonable are Common-Sense Reasoning Tasks : A Case-Study on the Winograd Schema Challenge and SWAG<span class=acl-fixed-case>W</span>inograd Schema Challenge and <span class=acl-fixed-case>SWAG</span></a></strong><br><a href=/people/p/paul-trichelair/>Paul Trichelair</a>
|
<a href=/people/a/ali-emami/>Ali Emami</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1335><div class="card-body p-3 small">Recent studies have significantly improved the state-of-the-art on common-sense reasoning (CSR) benchmarks like the <a href=https://en.wikipedia.org/wiki/Winograd_Schema_Challenge>Winograd Schema Challenge (WSC)</a> and SWAG. The question we ask in this paper is whether improved performance on these <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> represents genuine progress towards common-sense-enabled systems. We make case studies of both <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> and design protocols that clarify and qualify the results of previous work by analyzing threats to the validity of previous experimental designs. Our protocols account for several properties prevalent in common-sense benchmarks including size limitations, structural regularities, and variable instance difficulty.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1336 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1336" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1336/>Pun-GAN : Generative Adversarial Network for Pun Generation<span class=acl-fixed-case>GAN</span>: Generative Adversarial Network for Pun Generation</a></strong><br><a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/s/shunyao-li/>Shunyao Li</a>
|
<a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1336><div class="card-body p-3 small">In this paper, we focus on the task of generating a <a href=https://en.wikipedia.org/wiki/Pun>pun sentence</a> given a pair of <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>. A major challenge for pun generation is the lack of large-scale pun corpus to guide <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator</a> to produce <a href=https://en.wikipedia.org/wiki/Pun>pun sentences</a>, and a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> to distinguish between the generated <a href=https://en.wikipedia.org/wiki/Pun>pun sentences</a> and the real sentences with specific word senses. The output of the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> is then used as a reward to train the generator via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1337 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1337/>Multi-Task Learning with <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> for Question Generation</a></strong><br><a href=/people/w/wenjie-zhou/>Wenjie Zhou</a>
|
<a href=/people/m/minghua-zhang/>Minghua Zhang</a>
|
<a href=/people/y/yunfang-wu/>Yunfang Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1337><div class="card-body p-3 small">This paper explores the task of answer-aware questions generation. Based on the attention-based pointer generator model, we propose to incorporate an auxiliary task of language modeling to help question generation in a hierarchical multi-task learning structure. Our joint-learning model enables the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to learn a better representation of the input sequence, which will guide the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> to generate more coherent and fluent questions. On both SQuAD and MARCO datasets, our multi-task learning model boosts the performance, achieving state-of-the-art results. Moreover, human evaluation further proves the high quality of our generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1338 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1338.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1338" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1338/>Autoregressive Text Generation Beyond Feedback Loops</a></strong><br><a href=/people/f/florian-schmidt/>Florian Schmidt</a>
|
<a href=/people/s/stephan-mandt/>Stephan Mandt</a>
|
<a href=/people/t/thomas-hofmann/>Thomas Hofmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1338><div class="card-body p-3 small">Autoregressive state transitions, where predictions are conditioned on past predictions, are the predominant choice for both deterministic and stochastic sequential models. However, autoregressive feedback exposes the evolution of the hidden state trajectory to potential biases from well-known train-test discrepancies. In this paper, we combine a latent state space model with a CRF observation model. We argue that such autoregressive observation models form an interesting middle ground that expresses local correlations on the word level but keeps the state evolution non-autoregressive. On unconditional sentence generation we show performance improvements compared to RNN and GAN baselines while avoiding some prototypical failure modes of <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1339 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1339.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1339" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1339/>The Woman Worked as a Babysitter : On Biases in Language Generation</a></strong><br><a href=/people/e/emily-sheng/>Emily Sheng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/p/prem-natarajan/>Premkumar Natarajan</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1339><div class="card-body p-3 small">We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for <a href=https://en.wikipedia.org/wiki/Opinion_poll>regard</a>. To this end, we collect strategically-generated text from <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, so that we can analyze biases in unseen text. Together, these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1342 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1342.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1342/>Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks</a></strong><br><a href=/people/x/xingwei-tan/>Xingwei Tan</a>
|
<a href=/people/y/yi-cai/>Yi Cai</a>
|
<a href=/people/c/changxi-zhu/>Changxi Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1342><div class="card-body p-3 small">Aspect-level sentiment classification, which is a fine-grained sentiment analysis task, has received lots of attention these years. There is a phenomenon that people express both positive and negative sentiments towards an aspect at the same time. Such opinions with conflicting sentiments, however, are ignored by existing studies, which design <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> based on the absence of them. We argue that the exclusion of conflict opinions is problematic, for the reason that it represents an important style of human thinking dialectic thinking. If a real-world sentiment classification system ignores the existence of conflict opinions when it is designed, it will incorrectly mixed conflict opinions into other sentiment polarity categories in action. Existing models have problems when recognizing <a href=https://en.wikipedia.org/wiki/Consensus_decision-making>conflicting opinions</a>, such as data sparsity. In this paper, we propose a multi-label classification model with dual attention mechanism to address these problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1343 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1343/>Investigating Dynamic Routing in Tree-Structured LSTM for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>LSTM</span> for Sentiment Analysis</a></strong><br><a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/k/k-robert-lai/>K. Robert Lai</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1343><div class="card-body p-3 small">Deep neural network models such as <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>long short-term memory (LSTM)</a> and tree-LSTM have been proven to be effective for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a>, the bigger the improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1344 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1344.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1344/>A Label Informative Wide & Deep Classifier for Patents and Papers</a></strong><br><a href=/people/m/muyao-niu/>Muyao Niu</a>
|
<a href=/people/j/jie-cai/>Jie Cai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1344><div class="card-body p-3 small">In this paper, we provide a simple and effective <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> for classifying both patents and papers to the well-established Cooperative Patent Classification (CPC). We propose a label-informative classifier based on the Wide & Deep structure, where the Wide part encodes string-level similarities between texts and labels, and the Deep part captures semantic-level similarities via non-linear transformations. Our <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> trains on millions of patents, and transfers to papers by developing distant-supervised training set and domain-specific features. Extensive experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves comparable performance to the state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used in industry on both patents and papers. The output of this work should facilitate the searching, granting and filing of innovative ideas for patent examiners, attorneys and researchers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1347 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1347/>Delta-training : Simple Semi-Supervised Text Classification using Pretrained Word Embeddings</a></strong><br><a href=/people/h/hwiyeol-jo/>Hwiyeol Jo</a>
|
<a href=/people/c/ceyda-cinarel/>Ceyda Cinarel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1347><div class="card-body p-3 small">We propose a novel and simple <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for semi-supervised text classification. The method stems from the hypothesis that a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> with pretrained word embeddings always outperforms the same <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> with randomly initialized word embeddings, as empirically observed in NLP tasks. Our method first builds two sets of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> as a form of model ensemble, and then initializes their <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> differently : one using random, the other using pretrained word embeddings. We focus on different predictions between the two <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on unlabeled data while following the self-training framework. We also use early-stopping in meta-epoch to improve the performance of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Our method, Delta-training, outperforms the self-training and the co-training framework in 4 different text classification datasets, showing robustness against error accumulation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1348 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1348.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1348/>Visual Detection with Context for Document Layout Analysis</a></strong><br><a href=/people/c/carlos-soto/>Carlos Soto</a>
|
<a href=/people/s/shinjae-yoo/>Shinjae Yoo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1348><div class="card-body p-3 small">We present 1) a work in progress method to visually segment key regions of scientific articles using an object detection technique augmented with contextual features, and 2) a novel dataset of region-labeled articles. A continuing challenge in scientific literature mining is the difficulty of consistently extracting high-quality text from formatted PDFs. To address this, we adapt the object-detection technique Faster R-CNN for document layout detection, incorporating contextual information that leverages the inherently localized nature of article contents to improve the region detection performance. Due to the limited availability of high-quality region-labels for scientific articles, we also contribute a novel dataset of region annotations, the first version of which covers 9 region classes and 822 article pages. Initial experimental results demonstrate a 23.9 % absolute improvement in mean average precision over the baseline model by incorporating contextual features, and a processing speed 14x faster than a text-based technique. Ongoing work on further improvements is also discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1350 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1350/>Neural Topic Model with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/j/jia-leng/>Jia Leng</a>
|
<a href=/people/g/gabriele-pergola/>Gabriele Pergola</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1350><div class="card-body p-3 small">In recent years, advances in neural variational inference have achieved many successes in <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1351.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1351 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1351 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1351" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1351/>Modelling Stopping Criteria for Search Results using <a href=https://en.wikipedia.org/wiki/Poisson_point_process>Poisson Processes</a><span class=acl-fixed-case>P</span>oisson Processes</a></strong><br><a href=/people/a/alison-sneyd/>Alison Sneyd</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1351><div class="card-body p-3 small">Text retrieval systems often return large sets of documents, particularly when applied to large collections. Stopping criteria can reduce the number of these documents that need to be manually evaluated for <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> by predicting when a suitable level of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> has been achieved. In this work, a novel method for determining a stopping criterion is proposed that models the rate at which relevant documents occur using a <a href=https://en.wikipedia.org/wiki/Poisson_point_process>Poisson process</a>. This method allows a user to specify both a minimum desired level of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> to achieve and a desired probability of having achieved it. We evaluate our method on a <a href=https://en.wikipedia.org/wiki/Data_set>public dataset</a> and compare it with previous techniques for determining <a href=https://en.wikipedia.org/wiki/Stopping_time>stopping criteria</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1352 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1352/>Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval</a></strong><br><a href=/people/z/zeynep-akkalyoncu-yilmaz/>Zeynep Akkalyoncu Yilmaz</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/h/haotian-zhang/>Haotian Zhang</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1352><div class="card-body p-3 small">This paper applies BERT to ad hoc document retrieval on news articles, which requires addressing two challenges : relevance judgments in existing test collections are typically provided only at the document level, and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate <a href=https://en.wikipedia.org/wiki/Sentence_(law)>sentence-level evidence</a> to rank documents. Furthermore, we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance, and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1353.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1353 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1353 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1353.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1353/>The Challenges of Optimizing <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for Low Resource Cross-Language Information Retrieval</a></strong><br><a href=/people/c/constantine-lignos/>Constantine Lignos</a>
|
<a href=/people/d/daniel-cohen/>Daniel Cohen</a>
|
<a href=/people/y/yen-chieh-lien/>Yen-Chieh Lien</a>
|
<a href=/people/p/pratik-mehta/>Pratik Mehta</a>
|
<a href=/people/w/w-bruce-croft/>W. Bruce Croft</a>
|
<a href=/people/s/scott-miller/>Scott Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1353><div class="card-body p-3 small">When performing <a href=https://en.wikipedia.org/wiki/Cross-language_information_retrieval>cross-language information retrieval (CLIR)</a> for lower-resourced languages, a common approach is to retrieve over the output of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. However, there is no established guidance on how to optimize the resulting MT-IR system. In this paper, we examine the relationship between the performance of MT systems and both neural and term frequency-based IR models to identify how CLIR performance can be best predicted from MT quality. We explore performance at varying amounts of MT training data, byte pair encoding (BPE) merge operations, and across two IR collections and retrieval models. We find that the choice of IR collection can substantially affect the predictive power of MT tuning decisions and evaluation, potentially introducing dissociations between MT-only and overall CLIR performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1355 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1355" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1355/>GlossBERT : BERT for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a> with Gloss Knowledge<span class=acl-fixed-case>G</span>loss<span class=acl-fixed-case>BERT</span>: <span class=acl-fixed-case>BERT</span> for Word Sense Disambiguation with Gloss Knowledge</a></strong><br><a href=/people/l/luyao-huang/>Luyao Huang</a>
|
<a href=/people/c/chi-sun/>Chi Sun</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1355><div class="card-body p-3 small">Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lexical resources like <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incorporating gloss (sense definition) into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for WSD. However, compared with traditional word expert supervised methods, <a href=https://en.wikipedia.org/wiki/They>they</a> have not achieved much improvement. In this paper, we focus on how to better leverage <a href=https://en.wikipedia.org/wiki/Gloss_(annotation)>gloss knowledge</a> in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT based models for WSD. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1357 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1357.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1357/>Bridging the Defined and the Defining : Exploiting Implicit Lexical Semantic Relations in Definition Modeling</a></strong><br><a href=/people/k/koki-washio/>Koki Washio</a>
|
<a href=/people/s/satoshi-sekine/>Satoshi Sekine</a>
|
<a href=/people/t/tsuneaki-kato/>Tsuneaki Kato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1357><div class="card-body p-3 small">Definition modeling includes acquiring <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> from <a href=https://en.wikipedia.org/wiki/Dictionary_definition>dictionary definitions</a> and generating definitions of words. While the meanings of defining words are important in dictionary definitions, it is crucial to capture the lexical semantic relations between defined words and defining words. However, thus far, the utilization of such <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> has not been explored for definition modeling. In this paper, we propose definition modeling methods that use <a href=https://en.wikipedia.org/wiki/Lexical_semantics>lexical semantic relations</a>. To utilize implicit semantic relations in definitions, we use unsupervisedly obtained pattern-based word-pair embeddings that represent semantic relations of word pairs. Experimental results indicate that our methods improve the performance in learning embeddings from definitions, as well as definition generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1358 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1358.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1358" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1358/>Do nt Just Scratch the Surface : Enhancing Word Representations for <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> with Hanja<span class=acl-fixed-case>K</span>orean with Hanja</a></strong><br><a href=/people/k/kang-min-yoo/>Kang Min Yoo</a>
|
<a href=/people/t/taeuk-kim/>Taeuk Kim</a>
|
<a href=/people/s/sang-goo-lee/>Sang-goo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1358><div class="card-body p-3 small">We propose a simple yet effective approach for improving Korean word representations using additional linguistic annotation (i.e. Hanja). We employ cross-lingual transfer learning in training word representations by leveraging the fact that <a href=https://en.wikipedia.org/wiki/Hanja>Hanja</a> is closely related to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We evaluate the intrinsic quality of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> learned through our approach using the <a href=https://en.wikipedia.org/wiki/Analogy>word analogy</a> and similarity tests. In addition, we demonstrate their effectiveness on several <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a>, including a novel Korean news headline generation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1360 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1360" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1360/>Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1360><div class="card-body p-3 small">In countries that speak multiple main languages, mixing up different languages within a conversation is commonly called <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>. Previous works addressing this challenge mainly focused on word-level aspects such as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. However, in many cases, languages share common subwords, especially for closely related languages, but also for languages that are seemingly irrelevant. Therefore, we propose Hierarchical Meta-Embeddings (HME) that learn to combine multiple monolingual word-level and subword-level embeddings to create language-agnostic lexical representations. On the task of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> for English-Spanish code-switching data, our model achieves the state-of-the-art performance in the multilingual settings. We also show that, in cross-lingual settings, our model not only leverages closely related languages, but also learns from languages with different roots. Finally, we show that combining different subunits are crucial for capturing code-switching entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1361 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1361/>Fine-tune BERT with Sparse Self-Attention Mechanism<span class=acl-fixed-case>BERT</span> with Sparse Self-Attention Mechanism</a></strong><br><a href=/people/b/baiyun-cui/>Baiyun Cui</a>
|
<a href=/people/y/yingming-li/>Yingming Li</a>
|
<a href=/people/m/ming-chen/>Ming Chen</a>
|
<a href=/people/z/zhongfei-zhang/>Zhongfei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1361><div class="card-body p-3 small">In this paper, we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into self-attention mechanism to enhance the fine-tuning performance of BERT. In particular, sparsity is introduced into the self-attention by replacing <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax function</a> with a controllable sparse transformation when fine-tuning with BERT. It enables us to learn a structurally sparse attention distribution, which leads to a more interpretable representation for the whole input. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is evaluated on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference tasks</a>. The extensive experimental results across multiple datasets demonstrate its effectiveness and superiority to the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1362 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1362" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1362/>Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels<span class=acl-fixed-case>NER</span> Labeling with Noisy Labels</a></strong><br><a href=/people/l/lukas-lange/>Lukas Lange</a>
|
<a href=/people/m/michael-a-hedderich/>Michael A. Hedderich</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1362><div class="card-body p-3 small">In low-resource settings, the performance of supervised labeling models can be improved with automatically annotated or distantly supervised data, which is cheap to create but often noisy. Previous works have shown that significant improvements can be reached by injecting information about the confusion between clean and noisy labels in this additional training data into the classifier training. However, for noise estimation, these approaches either do not take the input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> (in our case word embeddings) into account, or they need to learn the noise modeling from scratch which can be difficult in a low-resource setting. We propose to cluster the training data using the input <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and then compute different <a href=https://en.wikipedia.org/wiki/Confusion_matrix>confusion matrices</a> for each <a href=https://en.wikipedia.org/wiki/Cluster_analysis>cluster</a>. To the best of our knowledge, our approach is the first to leverage feature-dependent noise modeling with pre-initialized confusion matrices. We evaluate on low-resource named entity recognition settings in several languages, showing that our methods improve upon other confusion-matrix based methods by up to 9 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1363 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1363/>A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation<span class=acl-fixed-case>P</span>rocrustes Analysis for Multilingual Word Translation</a></strong><br><a href=/people/h/hagai-taitelbaum/>Hagai Taitelbaum</a>
|
<a href=/people/g/gal-chechik/>Gal Chechik</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1363><div class="card-body p-3 small">In this paper we present a novel approach to simultaneously representing multiple languages in a common space. Procrustes Analysis (PA) is commonly used to find the optimal orthogonal word mapping in the <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual case</a>. The proposed Multi Pairwise Procrustes Analysis (MPPA) is a natural extension of the PA algorithm to multilingual word mapping. Unlike previous PA extensions that require a k-way dictionary, this approach requires only pairwise bilingual dictionaries that are much easier to construct.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1366 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1366.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1366/>Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training</a></strong><br><a href=/people/c/chih-te-lai/>Chih-Te Lai</a>
|
<a href=/people/y/yi-te-hong/>Yi-Te Hong</a>
|
<a href=/people/h/hong-you-chen/>Hong-You Chen</a>
|
<a href=/people/c/chi-jen-lu/>Chi-Jen Lu</a>
|
<a href=/people/s/shou-de-lin/>Shou-De Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1366><div class="card-body p-3 small">The objective of non-parallel text style transfer, or controllable text generation, is to alter specific attributes (e.g. sentiment, <a href=https://en.wikipedia.org/wiki/Mood_(psychology)>mood</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a>, <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a>, etc) of a given text while preserving its remaining attributes and content. Generative adversarial network (GAN) is a popular model to ensure the transferred sentences are realistic and have the desired target styles. However, training GAN often suffers from mode collapse problem, which causes that the transferred text is little related to the original text. In this paper, we propose a new GAN model with a word-level conditional architecture and a two-phase training procedure. By using a style-related condition architecture before generating a word, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is able to maintain style-unrelated words while changing the others. By separating the training procedure into reconstruction and transfer phases, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is able to learn a proper text generation process, which further improves the content preservation. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on polarity sentiment transfer and multiple-attribute transfer tasks. The empirical results show that our model achieves comparable evaluation scores in both transfer accuracy and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> but significantly outperforms other state-of-the-art models in content compatibility on three real-world datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1373 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1373/>Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training</a></strong><br><a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1373><div class="card-body p-3 small">One way to reduce network traffic in multi-node data-parallel stochastic gradient descent is to only exchange the largest gradients. However, doing so damages the <a href=https://en.wikipedia.org/wiki/Gradient>gradient</a> and degrades the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance. Transformer models degrade dramatically while the impact on <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a> is smaller. We restore gradient quality by combining the compressed global gradient with the node&#8217;s locally computed uncompressed gradient. Neural machine translation experiments show that Transformer convergence is restored while <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNNs</a> converge faster. With our method, training on 4 nodes converges up to 1.5x as fast as with uncompressed gradients and scales 3.5x relative to single-node training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1376 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1376.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1376" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1376/>PaLM : A Hybrid Parser and Language Model<span class=acl-fixed-case>P</span>a<span class=acl-fixed-case>LM</span>: A Hybrid Parser and Language Model</a></strong><br><a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1376><div class="card-body p-3 small">We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1378 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1378.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1378/>Global Reasoning over Database Structures for Text-to-SQL Parsing<span class=acl-fixed-case>SQL</span> Parsing</a></strong><br><a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1378><div class="card-body p-3 small">State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. % since their decisions are based on weak, local information only. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use <a href=https://en.wikipedia.org/wiki/Message_passing>message-passing</a> through a graph neural network to softly select a subset of <a href=https://en.wikipedia.org/wiki/Constant_(computer_programming)>database constants</a> for the output query, conditioned on the question. Moreover, we train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from 39.4 % to 47.4 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1379 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1379.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1379/>Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis</a></strong><br><a href=/people/h/hiroki-ouchi/>Hiroki Ouchi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1379><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a>, an unlabeled test set is used for <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>. Although this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> is underexplored in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Here we conduct an empirical study of <a href=https://en.wikipedia.org/wiki/Transductive_learning>transductive learning</a> for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1380 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1380" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1380/>Efficient Sentence Embedding using <a href=https://en.wikipedia.org/wiki/Discrete_cosine_transform>Discrete Cosine Transform</a></a></strong><br><a href=/people/n/nada-almarwani/>Nada Almarwani</a>
|
<a href=/people/h/hanan-aldarmaki/>Hanan Aldarmaki</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1380><div class="card-body p-3 small">Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. While more complex sequential or convolutional networks potentially yield superior <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance, the improvements in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of <a href=https://en.wikipedia.org/wiki/Discrete_cosine_transform>DCT</a> to preserve word order information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1381 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1381/>A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection</a></strong><br><a href=/people/k/kurt-junshean-espinosa/>Kurt Junshean Espinosa</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1381><div class="card-body p-3 small">We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute <a href=https://en.wikipedia.org/wiki/Event_(computing)>events</a>, from the relation graph. We define actions to construct events and use all the beams in a <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> to detect all event structures that may be overlapping and nested. The search process constructs <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more computationally efficient while yielding higher <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1383 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1383" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1383/>Pretrained Language Models for Sequential Sentence Classification</a></strong><br><a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/i/iz-beltagy/>Iz Beltagy</a>
|
<a href=/people/d/daniel-king/>Daniel King</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/d/daniel-s-weld/>Dan Weld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1383><div class="card-body p-3 small">As a step toward better document-level understanding, we explore <a href=https://en.wikipedia.org/wiki/Categorization>classification</a> of a sequence of sentences into their corresponding categories, a task that requires understanding sentences in context of the document. Recent successful <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for this task have used hierarchical models to contextualize sentence representations, and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent labels. In this work, we show that pretrained language models, BERT (Devlin et al., 2018) in particular, can be used for this task to capture contextual dependencies without the need for hierarchical encoding nor a CRF. Specifically, we construct a joint sentence representation that allows BERT Transformer layers to directly utilize contextual information from all words in all sentences. Our approach achieves state-of-the-art results on four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, including a new dataset of structured scientific abstracts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1386.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1386 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1386 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1386.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1386/>Summary Cloze : A New Task for Content Selection in Topic-Focused Summarization</a></strong><br><a href=/people/d/daniel-deutsch/>Daniel Deutsch</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1386><div class="card-body p-3 small">A key challenge in topic-focused summarization is determining what information should be included in the summary, a problem known as content selection. In this work, we propose a new method for studying content selection in topic-focused summarization called the summary cloze task. The goal of the summary cloze task is to generate the next sentence of a summary conditioned on the beginning of the summary, a topic, and a reference document(s). The main challenge is deciding what information in the references is relevant to the topic and partial summary and should be included in the summary. Although the cloze task does not address all aspects of the traditional summarization problem, the more narrow scope of the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> allows us to collect a large-scale datset of nearly 500k summary cloze instances from Wikipedia. We report experimental results on this new dataset using various extractive models and a two-step abstractive model that first extractively selects a small number of sentences and then abstractively summarizes them. Our results show that the topic and partial summary help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> identify relevant content, but the task remains a significant challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1387.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1387 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1387 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1387.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1387" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1387/>Text Summarization with Pretrained Encoders</a></strong><br><a href=/people/y/yang-liu/>Yang Liu</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1387><div class="card-body p-3 small">Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a document and obtain representations for its sentences. Our extractive model is built on top of this <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art results across the board in both extractive and abstractive settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1390 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1390.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1390/>Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1390><div class="card-body p-3 small">Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks : Firstly, the <a href=https://en.wikipedia.org/wiki/Pointer_(computer_programming)>pointer</a> is limited to copying the exact words while ignoring possible <a href=https://en.wikipedia.org/wiki/Inflection_point>inflections</a> or <a href=https://en.wikipedia.org/wiki/Abstraction_(computer_science)>abstractions</a>, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to edit pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>relation embedding</a>. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higher-quality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1391 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1391/>Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs</a></strong><br><a href=/people/b/bailin-wang/>Bailin Wang</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1391><div class="card-body p-3 small">Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a <a href=https://en.wikipedia.org/wiki/Denotation>denotation</a>. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> in the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain <a href=https://en.wikipedia.org/wiki/Structural_analysis>structural constraints</a> were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial abstract program and (2) refining it while modeling structured alignments with <a href=https://en.wikipedia.org/wiki/Differential_dynamic_programming>differential dynamic programming</a>. We obtain state-of-the-art performance on the WikiTableQuestions and WikiSQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1392.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1392 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1392 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1392/>Broad-Coverage Semantic Parsing as Transduction</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/x/xutai-ma/>Xutai Ma</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1392><div class="card-body p-3 small">We unify different broad-coverage semantic parsing tasks into a transduction parsing paradigm, and propose an attention-based neural transducer that incrementally builds meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the neural transducer can be effectively trained without relying on a pre-trained aligner. Experiments separately conducted on three broad-coverage semantic parsing tasks AMR, SDP and UCCA demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1397 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1397.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1397" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1397/>Looking Beyond Label Noise : Shifted Label Distribution Matters in Distantly Supervised Relation Extraction</a></strong><br><a href=/people/q/qinyuan-ye/>Qinyuan Ye</a>
|
<a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/m/maosen-zhang/>Maosen Zhang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1397><div class="card-body p-3 small">In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models can not automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23 % relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1398 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1398/>Easy First Relation Extraction with <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>Information Redundancy</a></a></strong><br><a href=/people/s/shuai-ma/>Shuai Ma</a>
|
<a href=/people/g/gang-wang/>Gang Wang</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/j/jinpeng-huai/>Jinpeng Huai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1398><div class="card-body p-3 small">Many existing relation extraction (RE) models make decisions globally using integer linear programming (ILP). However, it is nontrivial to make use of <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>integer linear programming</a> as a blackbox solver for <a href=https://en.wikipedia.org/wiki/RE_(complexity)>RE</a>. Its cost of time and memory may become unacceptable with the increase of data scale, and <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundant information</a> needs to be encoded cautiously for ILP. In this paper, we propose an easy first approach for relation extraction with information redundancies, embedded in the results produced by local sentence level extractors, during which conflict decisions are resolved with domain and uniqueness constraints. Information redundancies are leveraged to support both easy first collective inference for easy decisions in the first stage and ILP for hard decisions in a subsequent stage. Experimental study shows that our approach improves the <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of RE, and outperforms both ILP and neural network-based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1403 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1403" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1403/>Induction Networks for Few-Shot Text Classification</a></strong><br><a href=/people/r/ruiying-geng/>Ruiying Geng</a>
|
<a href=/people/b/binhua-li/>Binhua Li</a>
|
<a href=/people/y/yongbin-li/>Yongbin Li</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a>
|
<a href=/people/p/ping-jian/>Ping Jian</a>
|
<a href=/people/j/jian-sun/>Jian Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1403><div class="card-body p-3 small">Text classification tends to struggle when data is deficient or when <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> needs to adapt to unseen classes. In such challenging scenarios, recent studies have used <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a> to simulate the few-shot task, in which new queries are compared to a small support set at the sample-wise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>. In this way, we find the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1404 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1404" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1404/>Benchmarking Zero-shot Text Classification : Datasets, Evaluation and Entailment Approach</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/j/jamaal-hay/>Jamaal Hay</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1404><div class="card-body p-3 small">Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>event</a>, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include : i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects : the topic aspect includes sports and politics as labels ; the emotion aspect includes joy and anger ; the situation aspect includes medical assistance and water shortage. ii) We extend the existing evaluation setup (label-partially-unseen) given a dataset, train on some labels, test on all labels to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1409 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1409.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1409/>Judge the Judges : A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation</a></strong><br><a href=/people/c/cristina-garbacea/>Cristina Garbacea</a>
|
<a href=/people/s/samuel-carton/>Samuel Carton</a>
|
<a href=/people/s/shiyan-yan/>Shiyan Yan</a>
|
<a href=/people/q/qiaozhu-mei/>Qiaozhu Mei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1409><div class="card-body p-3 small">We conduct a large-scale, systematic study to evaluate the existing evaluation methods for <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find <a href=https://en.wikipedia.org/wiki/Lexical_diversity>lexical diversity</a> an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1410 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1410" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1410/>Sentence-BERT : Sentence Embeddings using Siamese BERT-Networks<span class=acl-fixed-case>BERT</span>: Sentence Embeddings using <span class=acl-fixed-case>S</span>iamese <span class=acl-fixed-case>BERT</span>-Networks</a></strong><br><a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1410><div class="card-body p-3 small">BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead : Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised tasks</a> like <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1411 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1411.Attachment.rar data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1411/>Learning Only from Relevant Keywords and Unlabeled Documents</a></strong><br><a href=/people/n/nontawat-charoenphakdee/>Nontawat Charoenphakdee</a>
|
<a href=/people/j/jongyeong-lee/>Jongyeong Lee</a>
|
<a href=/people/y/yiping-jin/>Yiping Jin</a>
|
<a href=/people/d/dittaya-wanvarie/>Dittaya Wanvarie</a>
|
<a href=/people/m/masashi-sugiyama/>Masashi Sugiyama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1411><div class="card-body p-3 small">We consider a document classification problem where document labels are absent but only relevant keywords of a target class and unlabeled documents are given. Although <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic methods</a> based on pseudo-labeling have been considered, theoretical understanding of this problem has still been limited. Moreover, previous methods can not easily incorporate well-developed techniques in supervised text classification. In this paper, we propose a theoretically guaranteed learning framework that is simple to implement and has flexible choices of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, e.g., linear models or <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We demonstrate how to optimize the area under the receiver operating characteristic curve (AUC) effectively and also discuss how to adjust it to optimize other well-known evaluation metrics such as the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and F1-measure. Finally, we show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> using <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1416 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1416.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1416/>Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation</a></strong><br><a href=/people/d/dong-qian/>Dong Qian</a>
|
<a href=/people/w/william-k-cheung/>William K. Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1416><div class="card-body p-3 small">While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the <a href=https://en.wikipedia.org/wiki/Airborne_early_warning_and_control>VAE</a>. In this paper, we propose to introduce a mutual information (MI) term between the input and its <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for the estimation of the MI and provide a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithm</a> based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1417 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1417.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1417" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1417/>Sampling Bias in Deep Active Classification : An Empirical Study</a></strong><br><a href=/people/a/ameya-prabhu/>Ameya Prabhu</a>
|
<a href=/people/c/charles-dognin/>Charles Dognin</a>
|
<a href=/people/m/maneesh-singh/>Maneesh Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1417><div class="card-body p-3 small">The exploding cost and time needed for data labeling and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a> are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> can help mitigate such bottlenecks. Previous works on <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> identify the problem of <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling bias</a> in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness of the selected samples by creating tiny high-quality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> for deep active text classification that outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>. We expect the presented work to be useful and informative for <a href=https://en.wikipedia.org/wiki/Data_compression>dataset compression</a> and for problems involving active, semi-supervised or online learning scenarios. Code and models are available at : https://github.com/drimpossible/Sampling-Bias-Active-Learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1418/>Do nt Take the Easy Way Out : Ensemble Based Methods for Avoiding Known Dataset Biases</a></strong><br><a href=/people/c/christopher-clark/>Christopher Clark</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1418><div class="card-body p-3 small">State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to be more robust to domain shift. Our method has two stages : we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a <a href=https://en.wikipedia.org/wiki/Robust_statistics>robust model</a> as part of an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1422 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1422.Attachment.rar data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1422" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1422/>Hierarchically-Refined Label Attention Network for Sequence Labeling</a></strong><br><a href=/people/l/leyang-cui/>Leyang Cui</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1422><div class="card-body p-3 small">CRF has been used as a powerful model for statistical sequence labeling. For neural sequence labeling, however, BiLSTM-CRF does not always lead to better results compared with BiLSTM-softmax local classification. This can be because the simple Markov label transition model of CRF does not give much information gain over strong <a href=https://en.wikipedia.org/wiki/Neural_coding>neural encoding</a>. For better representing label sequences, we investigate a hierarchically-refined label attention network, which explicitly leverages label embeddings and captures potential long-term label dependency by giving each word incrementally refined label distributions with hierarchical attention. Results on POS tagging, NER and CCG supertagging show that the proposed model not only improves the overall tagging accuracy with similar number of parameters, but also significantly speeds up the training and testing compared to BiLSTM-CRF.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1423 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1423.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1423" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1423/>Certified Robustness to Adversarial Word Substitutions</a></strong><br><a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/a/aditi-raghunathan/>Aditi Raghunathan</a>
|
<a href=/people/k/kerem-goksel/>Kerem Gksel</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1423><div class="card-body p-3 small">State-of-the-art <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP models</a> can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>) to input text. The number of possible <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> scales exponentially with text length, so <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> can not cover all <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models&#8217; robustness to these transformations, we measure <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75 % adversarial accuracy on both <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> on <a href=https://en.wikipedia.org/wiki/IMDB>IMDB</a> and natural language inference on SNLI ; in comparison, on <a href=https://en.wikipedia.org/wiki/IMDB>IMDB</a>, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12 % and 41 %, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1424 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1424/>Visualizing and Understanding the Effectiveness of BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/y/yaru-hao/>Yaru Hao</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1424><div class="card-body p-3 small">Language model pre-training, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and <a href=https://en.wikipedia.org/wiki/Trajectory_optimization>optimization trajectories</a> of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, which suggests that the <a href=https://en.wikipedia.org/wiki/Layers_(digital_image_editing)>layers</a> that are close to input learn more transferable representations of language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1425 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1425" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1425/>Topics to Avoid : Demoting Latent Confounds in <a href=https://en.wikipedia.org/wiki/Text_classification>Text Classification</a></a></strong><br><a href=/people/s/sachin-kumar/>Sachin Kumar</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1425><div class="card-body p-3 small">Despite impressive performance on many text classification tasks, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the <a href=https://en.wikipedia.org/wiki/Prediction>prediction task</a> (e.g., if the input text mentions Sweden, the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> predicts that the author&#8217;s native language is Swedish). We propose a method that represents the latent topical confounds and a model which unlearns confounding features by predicting both the label of the input text and the confound ; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> generalizes better and learns features that are indicative of the writing style rather than the content.<i>native language identification</i>. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author&#8217;s native language is Swedish). We propose a method that represents the latent topical confounds and a model which &#8220;unlearns&#8221; confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1426 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1426/>Learning to Ask for Conversational Machine Learning</a></strong><br><a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1426><div class="card-body p-3 small">Natural language has recently been explored as a new medium of supervision for training <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>. Here, we explore learning classification tasks using <a href=https://en.wikipedia.org/wiki/Language>language</a> in a conversational setting where the automated learner does not simply receive language input from a teacher, but can proactively engage the teacher by asking questions. We present a reinforcement learning framework, where the learner&#8217;s actions correspond to question types and the reward for asking a question is based on how the teacher&#8217;s response changes performance of the resulting machine learning model on the learning task. In this framework, learning good question-asking strategies corresponds to asking sequences of questions that maximize the cumulative (discounted) reward, and hence quickly lead to effective <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. Empirical analysis across three domains shows that learned question-asking strategies expedite classifier training by asking appropriate questions at different points in the learning process. The approach allows learning classifiers from a blend of strategies, including learning from observations, explanations and clarifications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1427 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1427.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1427/>Language Modeling for <a href=https://en.wikipedia.org/wiki/Code_switching>Code-Switching</a> : Evaluation, Integration of Monolingual Data, and Discriminative Training</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1427><div class="card-body p-3 small">We focus on the problem of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons : (1) lack of available large-scale code-switched data for training ; (2) lack of a replicable evaluation setup that is ASR directed yet isolates <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> performance from the other intricacies of the ASR system ; and (3) the reliance on <a href=https://en.wikipedia.org/wiki/Generative_model>generative modeling</a>. We tackle these three issues : we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we explore a variety of training protocols and verify the effectiveness of training with large amounts of monolingual data followed by fine-tuning with small amounts of code-switched data, for both the generative and discriminative cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1432 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1432" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1432/>Distributionally Robust Language Modeling</a></strong><br><a href=/people/y/yonatan-oren/>Yonatan Oren</a>
|
<a href=/people/s/shiori-sagawa/>Shiori Sagawa</a>
|
<a href=/people/t/tatsunori-b-hashimoto/>Tatsunori B. Hashimoto</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1432><div class="card-body p-3 small">Language models are generally trained on <a href=https://en.wikipedia.org/wiki/Data>data</a> spanning a wide range of topics (e.g., <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Review>reviews</a>, <a href=https://en.wikipedia.org/wiki/Fiction>fiction</a>), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over <a href=https://en.wikipedia.org/wiki/Model-driven_engineering>MLE</a> when the language models are trained on a mixture of Yelp reviews and news and tested only on <a href=https://en.wikipedia.org/wiki/Review>reviews</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1436 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1436/>ARAML : A Stable Adversarial Training Framework for Text Generation<span class=acl-fixed-case>ARAML</span>: A Stable Adversarial Training Framework for Text Generation</a></strong><br><a href=/people/p/pei-ke/>Pei Ke</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1436><div class="card-body p-3 small">Most of the existing generative adversarial networks (GAN) for text generation suffer from the instability of reinforcement learning training algorithms such as policy gradient, leading to unstable performance. To tackle this problem, we propose a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> called Adversarial Reward Augmented Maximum Likelihood (ARAML). During adversarial training, the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> assigns rewards to samples which are acquired from a <a href=https://en.wikipedia.org/wiki/Stationary_distribution>stationary distribution</a> near the data rather than the generator&#8217;s distribution. The generator is optimized with <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation</a> augmented by the discriminator&#8217;s rewards instead of policy gradient. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can outperform state-of-the-art text GANs with a more stable training process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1437 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1437.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1437" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1437/>FlowSeq : Non-Autoregressive Conditional Sequence Generation with Generative Flow<span class=acl-fixed-case>F</span>low<span class=acl-fixed-case>S</span>eq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</a></strong><br><a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/c/chunting-zhou/>Chunting Zhou</a>
|
<a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1437><div class="card-body p-3 small">Most sequence-to-sequence (seq2seq) models are <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive</a> ; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel processing</a> on hardware such as <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a>. However, directly modeling the <a href=https://en.wikipedia.org/wiki/Joint_probability_distribution>joint distribution</a> of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>. In this paper, we propose a simple, efficient, and effective <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for non-autoregressive sequence generation using <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable models</a>. Specifically, we turn to generative flow, an elegant technique to model complex distributions using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1438 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1438.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1438" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1438/>Compositional Generalization for Primitive Substitutions</a></strong><br><a href=/people/y/yuanpeng-li/>Yuanpeng Li</a>
|
<a href=/people/l/liang-zhao/>Liang Zhao</a>
|
<a href=/people/j/jianyu-wang/>Jianyu Wang</a>
|
<a href=/people/j/joel-hestness/>Joel Hestness</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1438><div class="card-body p-3 small">Compositional generalization is a basic mechanism in human language learning, but current <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> lack such ability. In this paper, we conduct fundamental research for encoding compositionality in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the <a href=https://en.wikipedia.org/wiki/Entropy>entropy</a> in each <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> to improve <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In the SCAN domain, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> boosts <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracies</a> from 14.0 % to 98.8 % in Jump task, and from 92.0 % to 99.7 % in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1439 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1439.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1439" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1439/>WikiCREM : A Large Unsupervised Corpus for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a><span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>CREM</span>: A Large Unsupervised Corpus for Coreference Resolution</a></strong><br><a href=/people/v/vid-kocijan/>Vid Kocijan</a>
|
<a href=/people/o/oana-maria-camburu/>Oana-Maria Camburu</a>
|
<a href=/people/a/ana-maria-cretu/>Ana-Maria Cretu</a>
|
<a href=/people/y/yordan-yordanov/>Yordan Yordanov</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/t/thomas-lukasiewicz/>Thomas Lukasiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1439><div class="card-body p-3 small">Pronoun resolution is a major area of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. However, large-scale training sets are still scarce, since manually labelling data is costly. In this work, we introduce WikiCREM (Wikipedia CoREferences Masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. We use a language-model-based approach for pronoun resolution in combination with our WikiCREM dataset. We compare a series of models on a collection of diverse and challenging coreference resolution problems, where we match or outperform previous state-of-the-art approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP, WinoBias, and WinoGender. We release our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to be used off-the-shelf for solving pronoun disambiguation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1440 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1440" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1440/>Identifying and Explaining Discriminative Attributes</a></strong><br><a href=/people/a/armins-stepanjans/>Armins Stepanjans</a>
|
<a href=/people/a/andre-freitas/>Andr Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1440><div class="card-body p-3 small">Identifying what is at the center of the meaning of a word and what discriminates it from other words is a fundamental natural language inference task. This paper describes an explicit word vector representation model (WVM) to support the identification of discriminative attributes. A core contribution of the paper is a quantitative and qualitative comparative analysis of different types of data sources and Knowledge Bases in the construction of explainable and explicit WVMs : (i) <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> built from dictionary definitions, (ii) entity-attribute-relationships graphs derived from images and (iii) commonsense knowledge graphs. Using a detailed quantitative and qualitative analysis, we demonstrate that these data sources have complementary semantic aspects, supporting the creation of explicit semantic vector spaces. The explicit vector spaces are evaluated using the task of discriminative attribute identification, showing comparable performance to the state-of-the-art systems in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> (F1-score = 0.69), while delivering full model transparency and explainability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1443.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1443 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1443 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1443/>Transformer Dissection : An Unified Understanding for Transformers Attention via the Lens of Kernel</a></strong><br><a href=/people/y/yao-hung-hubert-tsai/>Yao-Hung Hubert Tsai</a>
|
<a href=/people/s/shaojie-bai/>Shaojie Bai</a>
|
<a href=/people/m/makoto-yamada/>Makoto Yamada</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1443><div class="card-body p-3 small">Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>, and sequence prediction. At the core of the <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> is the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> via the lens of the <a href=https://en.wikipedia.org/wiki/Kernel_(linear_algebra)>kernel</a>. To be more precise, we realize that the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer&#8217;s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer&#8217;s attention. As an example, we propose a new variant of Transformer&#8217;s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with less <a href=https://en.wikipedia.org/wiki/Computation>computation</a>. In our experiments, we empirically study different kernel construction strategies on two widely used tasks : <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and sequence prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1444 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1444/>Learning to Learn and Predict : A Meta-Learning Approach for Multi-Label Classification</a></strong><br><a href=/people/j/jiawei-wu/>Jiawei Wu</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1444><div class="card-body p-3 small">Many tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> can be viewed as multi-label classification problems. However, most of the existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained with the standard cross-entropy loss function and use a <a href=https://en.wikipedia.org/wiki/Linear_prediction>fixed prediction policy</a> (e.g., a threshold of 0.5) for all the labels, which completely ignores the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> and dependencies among different labels. In this paper, we propose a meta-learning method to capture these complex label dependencies. More specifically, our method utilizes a meta-learner to jointly learn the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training policies</a> and <a href=https://en.wikipedia.org/wiki/Prediction>prediction policies</a> for different labels. The training policies are then used to train the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> with the cross-entropy loss function, and the <a href=https://en.wikipedia.org/wiki/Prediction>prediction policies</a> are further implemented for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Experimental results on fine-grained entity typing and text classification demonstrate that our proposed method can obtain more accurate multi-label classification results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1445 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1445.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1445/>Revealing the Dark Secrets of BERT<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/o/olga-kovaleva/>Olga Kovaleva</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1445><div class="card-body p-3 small">BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT&#8217;s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1446 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1446/>Machine Translation With Weakly Paired Documents</a></strong><br><a href=/people/l/lijun-wu/>Lijun Wu</a>
|
<a href=/people/j/jinhua-zhu/>Jinhua Zhu</a>
|
<a href=/people/d/di-he/>Di He</a>
|
<a href=/people/f/fei-gao/>Fei Gao</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/j/jianhuang-lai/>Jianhuang Lai</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1446><div class="card-body p-3 small">Neural machine translation, which achieves near human-level performance in some languages, strongly relies on the large amounts of parallel sentences, which hinders its applicability to low-resource language pairs. Recent works explore the possibility of unsupervised machine translation with <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only, leading to much lower <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> compared with the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised one</a>. Observing that weakly paired bilingual documents are much easier to collect than <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual sentences</a>, e.g., from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, news websites or books, in this paper, we investigate training translation models with weakly paired bilingual documents. Our <a href=https://en.wikipedia.org/wiki/Tactic_(method)>approach</a> contains two components. 1) We provide a simple approach to mine implicitly bilingual sentence pairs from document pairs which can then be used as supervised training signals. 2) We leverage the topic consistency of two weakly paired documents and learn the sentence translation model by constraining the word distribution-level alignments. We evaluate our method on weakly paired documents from Wikipedia on six tasks, the widely used WMT16 GermanEnglish, WMT13 SpanishEnglish and WMT16 RomanianEnglish translation tasks. We obtain 24.1/30.3, 28.1/27.6 and 30.1/27.6 BLEU points separately, outperforming previous results by more than 5 BLEU points in each direction and reducing the gap between unsupervised translation and supervised translation up to 50 %.<tex-math>\\leftrightarrow</tex-math>English, WMT13 Spanish<tex-math>\\leftrightarrow</tex-math>English and WMT16 Romanian<tex-math>\\leftrightarrow</tex-math>English translation tasks. We obtain 24.1/30.3, 28.1/27.6 and 30.1/27.6 BLEU points separately, outperforming previous results by more than 5 BLEU points in each direction and reducing the gap between unsupervised translation and supervised translation up to 50%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1448 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1448.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1448/>The Bottom-up Evolution of Representations in the Transformer : A Study with <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> Objectives</a></strong><br><a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1448><div class="card-body p-3 small">We seek to understand how the representations of individual tokens and the structure of the learned <a href=https://en.wikipedia.org/wiki/Feature_space>feature space</a> evolve between layers in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> under different learning objectives. We chose the Transformers for our analysis as they have been shown effective with various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top <a href=https://en.wikipedia.org/wiki/Multimedia_Messaging_Service>MLM layers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1449 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1449.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1449" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1449/>Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vuli</a>
|
<a href=/people/g/goran-glavas/>Goran Glava</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1449><div class="card-body p-3 small">Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1450 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1450/>Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings</a></strong><br><a href=/people/h/haozhou-wang/>Haozhou Wang</a>
|
<a href=/people/j/james-henderson/>James Henderson</a>
|
<a href=/people/p/paola-merlo/>Paola Merlo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1450><div class="card-body p-3 small">Distributed representations of words which map each word to a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous vector</a> have proven useful in capturing important linguistic information not only in a single language but also across different languages. Current <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised adversarial approaches</a> show that it is possible to build a mapping matrix that aligns two sets of monolingual word embeddings without high quality parallel data, such as a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a> or a sentence-aligned corpus. However, without an additional step of <a href=https://en.wikipedia.org/wiki/Refinement_(computing)>refinement</a>, the preliminary <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> learnt by these methods is unsatisfactory, leading to poor performance for typologically distant languages. In this paper, we propose a weakly-supervised adversarial training method to overcome this limitation, based on the intuition that mapping across languages is better done at the concept level than at the word level. We propose a concept-based adversarial training method which improves the performance of previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised adversarial methods</a> for most languages, and especially for typologically distant language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1451 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1451" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1451/>Aligning Cross-Lingual Entities with Multi-Aspect Information</a></strong><br><a href=/people/h/hsiu-wei-yang/>Hsiu-Wei Yang</a>
|
<a href=/people/y/yanyan-zou/>Yanyan Zou</a>
|
<a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1451><div class="card-body p-3 small">Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> from multilingual KGs into the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly outperforms existing <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1452.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1452 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1452 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1452/>Contrastive Language Adaptation for Cross-Lingual Stance Detection</a></strong><br><a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/j/james-glass/>James Glass</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1452><div class="card-body p-3 small">We study cross-lingual stance detection, which aims to leverage labeled data in one language to identify the relative perspective (or stance) of a given document with respect to a claim in a different target language. In particular, we introduce a novel contrastive language adaptation approach applied to memory networks, which ensures accurate alignment of stances in the source and target languages, and can effectively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1453 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1453.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1453" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1453/>Jointly Learning to Align and Translate with Transformer Models</a></strong><br><a href=/people/s/sarthak-garg/>Sarthak Garg</a>
|
<a href=/people/s/stephan-peitz/>Stephan Peitz</a>
|
<a href=/people/u/udhyakumar-nallasamy/>Udhyakumar Nallasamy</a>
|
<a href=/people/m/matthias-paulik/>Matthias Paulik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1453><div class="card-body p-3 small">The state of the art in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> is governed by <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural approaches</a>, which typically provide superior translation accuracy over <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical approaches</a>. However, on the closely related task of <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1455 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1455" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1455/>Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning</a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1455><div class="card-body p-3 small">Multi-hop QA requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. The recently proposed HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four different multi-hop reasoning paradigms (two bridge entity setups, checking multiple properties, and comparing two entities), making it challenging for a single neural network to handle all four. In this work, we present an interpretable, controller-based Self-Assembling Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique types of language reasoning. Based on a question, our layout controller RNN dynamically infers a series of reasoning modules to construct the entire <a href=https://en.wikipedia.org/wiki/Computer_network>network</a>. Empirically, we show that our dynamic, multi-hop modular network achieves significant improvements over the static, single-hop baseline (on both regular and adversarial evaluation). We further demonstrate the interpretability of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> via three analyses. First, the controller can softly decompose the multi-hop question into multiple single-hop sub-questions to promote compositional reasoning behavior of the main network. Second, the <a href=https://en.wikipedia.org/wiki/Controller_(computing)>controller</a> can predict layouts that conform to the layouts designed by human experts. Finally, the intermediate module can infer the entity that connects two distantly-located supporting facts by addressing the sub-question from the controller.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1459 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1459/>Taskmaster-1 : Toward a Realistic and Diverse Dialog Dataset</a></strong><br><a href=/people/b/bill-byrne/>Bill Byrne</a>
|
<a href=/people/k/karthik-krishnamoorthi/>Karthik Krishnamoorthi</a>
|
<a href=/people/c/chinnadhurai-sankar/>Chinnadhurai Sankar</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a>
|
<a href=/people/b/ben-goodrich/>Ben Goodrich</a>
|
<a href=/people/d/daniel-duckworth/>Daniel Duckworth</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/a/amit-dubey/>Amit Dubey</a>
|
<a href=/people/k/kyu-young-kim/>Kyu-Young Kim</a>
|
<a href=/people/a/andy-cedilnik/>Andy Cedilnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1459><div class="card-body p-3 small">A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedures</a> were used to create this <a href=https://en.wikipedia.org/wiki/Collection_(abstract_data_type)>collection</a>, each with unique advantages. The first involves a two-person, spoken Wizard of Oz (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is self-dialog in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API calls</a> and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in <a href=https://en.wikipedia.org/wiki/Writing>written vs. spoken language</a>, <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse patterns</a>, <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error handling</a> and other <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic phenomena</a> related to dialog system research, development and design.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1460 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1460.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1460/>Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>D</span>o<span class=acl-fixed-case>GO</span>): Strategies toward Curating and Annotating Large Scale Dialogue Data</a></strong><br><a href=/people/d/denis-peskov/>Denis Peskov</a>
|
<a href=/people/n/nancy-clarke/>Nancy Clarke</a>
|
<a href=/people/j/jason-krone/>Jason Krone</a>
|
<a href=/people/b/brigi-fodor/>Brigi Fodor</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a>
|
<a href=/people/a/adel-youssef/>Adel Youssef</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1460><div class="card-body p-3 small">The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as <a href=https://en.wikipedia.org/wiki/Virtual_assistant>virtual assistants</a> become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, <a href=https://en.wikipedia.org/wiki/Linguistic_diversity>linguistic diversity</a>, domain coverage, or <a href=https://en.wikipedia.org/wiki/Granularity>annotation granularity</a>. In this paper, we present <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> toward curating and annotating large scale goal oriented dialogue data. We introduce the MultiDoGO dataset to overcome these limitations. With a total of over 81 K dialogues harvested across six domains, MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable dialogue dataset currently available to the public. Over 54 K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the customer) is paired with a trained annotator (the agent). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> on the agent and customer utterances as well as slot labeling for each domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1461 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1461.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1461/>Build it Break it Fix it for Dialogue Safety : Robustness from Adversarial Human Attack</a></strong><br><a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/s/samuel-humeau/>Samuel Humeau</a>
|
<a href=/people/b/bharath-chintagunta/>Bharath Chintagunta</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1461><div class="card-body p-3 small">The detection of offensive language in the context of a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> has become an increasingly important application of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The detection of trolls in public forums (Galn-Garca et al., 2016), and the deployment of <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> in the <a href=https://en.wikipedia.org/wiki/Public_domain>public domain</a> (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to become robust to such human attacks by an iterative build it, break it, fix it scheme with humans and models in the loop. In detailed experiments we show this <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> is considerably more robust than previous <a href=https://en.wikipedia.org/wiki/System>systems</a>. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and can not be viewed as a single sentence offensive detection task as in most previous work. Our newly collected <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and methods are all made open source and publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1462.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1462 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1462 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1462/>GECOR : An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue<span class=acl-fixed-case>GECOR</span>: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue</a></strong><br><a href=/people/j/jun-quan/>Jun Quan</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/c/changjian-hu/>Changjian Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1462><div class="card-body p-3 small">Ellipsis and co-reference are common and ubiquitous especially in <a href=https://en.wikipedia.org/wiki/Dialogue>multi-turn dialogues</a>. In this paper, we treat the resolution of ellipsis and co-reference in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> as a problem of generating omitted or referred expressions from the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue context</a>. We therefore propose a unified end-to-end Generative Ellipsis and CO-reference Resolution model (GECOR) in the context of dialogue. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can generate a new pragmatically complete user utterance by alternating the generation and copy mode for each user utterance. A multi-task learning framework is further proposed to integrate the GECOR into an end-to-end task-oriented dialogue. In order to train both the GECOR and the multi-task learning framework, we manually construct a new dataset on the basis of the public dataset CamRest676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the GECOR model significantly outperforms the sequence-to-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with GECOR achieves a higher success rate of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1464 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1464" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1464/>Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</a></strong><br><a href=/people/c/chen-zhang/>Chen Zhang</a>
|
<a href=/people/q/qiuchi-li/>Qiuchi Li</a>
|
<a href=/people/d/dawei-song/>Dawei Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1464><div class="card-body p-3 small">Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on <a href=https://en.wikipedia.org/wiki/Information_technology>it</a>, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1466 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1466.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1466" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1466/>Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning</a></strong><br><a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/x/xin-li/>Xin Li</a>
|
<a href=/people/y/ying-wei/>Ying Wei</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/q/qiang-yang/>Qiang Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1466><div class="card-body p-3 small">Joint extraction of aspects and sentiments can be effectively formulated as a sequence labeling problem. However, such formulation hinders the effectiveness of <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> due to the lack of annotated sequence data in many domains. To address this issue, we firstly explore an unsupervised domain adaptation setting for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Prior work can only use common syntactic relations between <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a> and opinion words to bridge the domain gaps, which highly relies on external linguistic resources. To resolve it, we propose a novel Selective Adversarial Learning (SAL) method to align the inferred correlation vectors that automatically capture their latent relations. The SAL method can dynamically learn an alignment weight for each word such that more important words can possess higher alignment weights to achieve fine-grained (word-level) adaptation. Empirically, extensive experiments demonstrate the effectiveness of the proposed SAL method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1467 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1467.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1467/>CAN : Constrained Attention Networks for Multi-Aspect Sentiment Analysis<span class=acl-fixed-case>CAN</span>: Constrained Attention Networks for Multi-Aspect Sentiment Analysis</a></strong><br><a href=/people/m/mengting-hu/>Mengting Hu</a>
|
<a href=/people/s/shiwan-zhao/>Shiwan Zhao</a>
|
<a href=/people/l/li-zhang/>Li Zhang</a>
|
<a href=/people/k/keke-cai/>Keke Cai</a>
|
<a href=/people/z/zhong-su/>Zhong Su</a>
|
<a href=/people/r/renhong-cheng/>Renhong Cheng</a>
|
<a href=/people/x/xiaowei-shen/>Xiaowei Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1467><div class="card-body p-3 small">Aspect level sentiment classification is a fine-grained sentiment analysis task. To detect the sentiment towards a particular aspect in a sentence, previous studies have developed various attention-based methods for generating aspect-specific sentence representations. However, the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> may inherently introduce <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a> and downgrade the performance. In this paper, we propose constrained attention networks (CAN), a simple yet effective solution, to regularize the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for multi-aspect sentiment analysis, which alleviates the drawback of the <a href=https://en.wikipedia.org/wiki/Attention>attention mechanism</a>. Specifically, we introduce orthogonal regularization on multiple aspects and sparse regularization on each single aspect. Experimental results on two public datasets demonstrate the effectiveness of our approach. We further extend our approach to multi-task settings and outperform the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1468 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1468.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1468" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1468/>Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training</a></strong><br><a href=/people/g/giannis-karamanolakis/>Giannis Karamanolakis</a>
|
<a href=/people/d/daniel-hsu/>Daniel Hsu</a>
|
<a href=/people/l/luis-gravano/>Luis Gravano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1468><div class="card-body p-3 small">User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., <a href=https://en.wikipedia.org/wiki/Price>price</a>, <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised topic models</a> often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches fail to leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher) ; in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous <a href=https://en.wikipedia.org/wiki/Supervised_learning>weakly supervised approaches</a> (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1472 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1472.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1472/>Text-based inference of moral sentiment change</a></strong><br><a href=/people/j/jing-yi-xie/>Jing Yi Xie</a>
|
<a href=/people/r/renato-ferreira-pinto-junior/>Renato Ferreira Pinto Junior</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1472><div class="card-body p-3 small">We present a text-based framework for investigating moral sentiment change of the public via <a href=https://en.wikipedia.org/wiki/Longitudinal_study>longitudinal corpora</a>. Our framework is based on the premise that language use can inform people&#8217;s moral perception toward right or wrong, and we build our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> by exploring moral biases learned from diachronic word embeddings. We demonstrate how a parameter-free model supports inference of historical shifts in moral sentiment toward concepts such as <a href=https://en.wikipedia.org/wiki/Slavery>slavery</a> and <a href=https://en.wikipedia.org/wiki/Democracy>democracy</a> over centuries at three incremental levels : moral relevance, moral polarity, and fine-grained moral dimensions. We apply this methodology to visualizing moral time courses of individual concepts and analyzing the relations between psycholinguistic variables and rates of moral sentiment change at scale. Our work offers opportunities for applying <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> toward characterizing moral sentiment change in society.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1474.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1474 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1474 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1474" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1474/>Multilingual and Multi-Aspect Hate Speech Analysis</a></strong><br><a href=/people/n/nedjma-ousidhoum/>Nedjma Ousidhoum</a>
|
<a href=/people/z/zizheng-lin/>Zizheng Lin</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/d/dit-yan-yeung/>Dit-Yan Yeung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1474><div class="card-body p-3 small">Current research on hate speech analysis is typically oriented towards monolingual and single classification tasks. In this paper, we present a new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches. We evaluate our dataset in various classification settings, then we discuss how to leverage our annotations in order to improve hate speech detection and classification in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1476.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1476 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1476 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1476/>A Deep Neural Information Fusion Architecture for Textual Network Embeddings</a></strong><br><a href=/people/z/zenan-xu/>Zenan Xu</a>
|
<a href=/people/q/qinliang-su/>Qinliang Su</a>
|
<a href=/people/x/xiaojun-quan/>Xiaojun Quan</a>
|
<a href=/people/w/weijia-zhang/>Weijia Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1476><div class="card-body p-3 small">Textual network embeddings aim to learn a low-dimensional representation for every node in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> so that both the structural and textual information from the networks can be well preserved in the representations. Traditionally, the structural and textual embeddings were learned by <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that rarely take the mutual influences between them into account. In this paper, a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural architecture</a> is proposed to effectively fuse the two kinds of informations into one <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a>. The novelties of the proposed architecture are manifested in the aspects of a newly defined <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a>, the complementary information fusion method for structural and textual features, and the mutual gate mechanism for textual feature extraction. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the comparing methods on all three datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1477 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1477.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1477/>You Shall Know a User by the Company It Keeps : Dynamic Representations for Social Media Users in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/m/marco-del-tredici/>Marco Del Tredici</a>
|
<a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernndez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1477><div class="card-body p-3 small">Information about individuals can help to better understand what they say, particularly in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a>, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on Graph Attention Networks that captures this observation. It dynamically explores the <a href=https://en.wikipedia.org/wiki/Social_graph>social graph</a> of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to three different tasks, evaluate it against alternative <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and analyse the results extensively, showing that it significantly outperforms other current methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1482 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1482" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1482/>A Benchmark Dataset for Learning to Intervene in Online Hate Speech</a></strong><br><a href=/people/j/jing-qian/>Jing Qian</a>
|
<a href=/people/a/anna-bethke/>Anna Bethke</a>
|
<a href=/people/y/yinyin-liu/>Yinyin Liu</a>
|
<a href=/people/e/elizabeth-belding/>Elizabeth Belding</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1482><div class="card-body p-3 small">Countering <a href=https://en.wikipedia.org/wiki/Online_hate_speech>online hate speech</a> is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context</a>. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> to provide a benchmark for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1484 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1484.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1484" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1484/>CodeSwitch-Reddit : Exploration of Written Multilingual Discourse in Online Discussion Forums<span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>S</span>witch-<span class=acl-fixed-case>R</span>eddit: Exploration of Written Multilingual Discourse in Online Discussion Forums</a></strong><br><a href=/people/e/ella-rabinovich/>Ella Rabinovich</a>
|
<a href=/people/m/masih-sultani/>Masih Sultani</a>
|
<a href=/people/s/suzanne-stevenson/>Suzanne Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1484><div class="card-body p-3 small">In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into <a href=https://en.wikipedia.org/wiki/Code-switching>written code-switching</a> in <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion forums</a>. The released <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can further facilitate a range of research and practical activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1485.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1485 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1485 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1485/>Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity</a></strong><br><a href=/people/p/penghui-wei/>Penghui Wei</a>
|
<a href=/people/n/nan-xu/>Nan Xu</a>
|
<a href=/people/w/wenji-mao/>Wenji Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1485><div class="card-body p-3 small">Automatically verifying rumorous information has become an important and challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Social_media_analytics>social media analytics</a>. Previous studies reveal that people&#8217;s stances towards rumorous messages can provide indicative clues for identifying the veracity of rumors, and thus determining the stances of public reactions is a crucial preceding step for rumor veracity prediction. In this paper, we propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, which consists of two components. The bottom component of our framework classifies the stances of tweets in a conversation discussing a rumor via modeling the structural property based on a novel graph convolutional network. The top component predicts the rumor veracity by exploiting the temporal dynamics of stance evolution. Experimental results on two benchmark datasets show that our method outperforms previous methods in both rumor stance classification and veracity prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1487 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1487/>Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network</a></strong><br><a href=/people/s/shuqing-bian/>Shuqing Bian</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/y/yang-song/>Yang Song</a>
|
<a href=/people/t/tao-zhang/>Tao Zhang</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1487><div class="card-body p-3 small">Person-job fit has been an important task which aims to automatically match job positions with suitable candidates. Previous methods mainly focus on solving the match task in single-domain setting, which may not work well when labeled data is limited. We study the domain adaptation problem for person-job fit. We first propose a deep global match network for capturing the global semantic interactions between two sentences from a job posting and a candidate resume respectively. Furthermore, we extend the match network and implement domain adaptation in three levels, sentence-level representation, sentence-level match, and global match. Extensive experiment results on a large real-world dataset consisting of six domains have demonstrated the effectiveness of the proposed model, especially when there is not sufficient labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1488 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1488/>Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification</a></strong><br><a href=/people/h/hu-linmei/>Hu Linmei</a>
|
<a href=/people/t/tianchi-yang/>Tianchi Yang</a>
|
<a href=/people/c/chuan-shi/>Chuan Shi</a>
|
<a href=/people/h/houye-ji/>Houye Ji</a>
|
<a href=/people/x/xiaoli-li/>Xiaoli Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1488><div class="card-body p-3 small">Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on <a href=https://en.wikipedia.org/wiki/Longitudinal_study>long texts</a> and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1489.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1489 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1489 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1489/>Comparing and Developing Tools to Measure the Readability of Domain-Specific Texts</a></strong><br><a href=/people/e/elissa-redmiles/>Elissa Redmiles</a>
|
<a href=/people/l/lisa-maszkiewicz/>Lisa Maszkiewicz</a>
|
<a href=/people/e/emily-hwang/>Emily Hwang</a>
|
<a href=/people/d/dhruv-kuchhal/>Dhruv Kuchhal</a>
|
<a href=/people/e/everest-liu/>Everest Liu</a>
|
<a href=/people/m/miraida-morales/>Miraida Morales</a>
|
<a href=/people/d/denis-peskov/>Denis Peskov</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/r/rock-stevens/>Rock Stevens</a>
|
<a href=/people/k/kristina-gligoric/>Kristina Gligori</a>
|
<a href=/people/s/sean-kross/>Sean Kross</a>
|
<a href=/people/m/michelle-mazurek/>Michelle Mazurek</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daum III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1489><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Readability>readability</a> of a digital text can influence people&#8217;s ability to learn new things about a range topics from <a href=https://en.wikipedia.org/wiki/Web_resource>digital resources</a> (e.g., <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/WebMD>WebMD</a>). Readability also impacts <a href=https://en.wikipedia.org/wiki/Search_engine_results_page>search rankings</a>, and is used to evaluate the performance of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. Despite this, we lack a thorough understanding of how to validly measure <a href=https://en.wikipedia.org/wiki/Readability>readability</a> at scale, especially for domain-specific texts. In this work, we present a comparison of the <a href=https://en.wikipedia.org/wiki/Validity_(statistics)>validity</a> of well-known readability measures and introduce a novel approach, Smart Cloze, which is designed to address shortcomings of existing measures. We compare these approaches across four different corpora : crowdworker-generated stories, Wikipedia articles, security and privacy advice, and health information. On these corpora, we evaluate the convergent and content validity of each measure, and detail tradeoffs in score precision, domain-specificity, and participant burden. These results provide a foundation for more accurate readability measurements and better evaluation of new <a href=https://en.wikipedia.org/wiki/Natural-language_processing>natural-language-processing systems</a> and tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1490.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1490 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1490 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1490/>News2vec : News Network Embedding with Subnode Information<span class=acl-fixed-case>N</span>ews2vec: News Network Embedding with Subnode Information</a></strong><br><a href=/people/y/ye-ma/>Ye Ma</a>
|
<a href=/people/l/lu-zong/>Lu Zong</a>
|
<a href=/people/y/yikang-yang/>Yikang Yang</a>
|
<a href=/people/j/jionglong-su/>Jionglong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1490><div class="card-body p-3 small">With the development of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP technologies</a>, <a href=https://en.wikipedia.org/wiki/News>news</a> can be automatically categorized and labeled according to a variety of characteristics, at the same time be represented as low dimensional embeddings. However, it lacks a systematic approach that effectively integrates the inherited features and inter-textual knowledge of news to represent the collective information with a dense vector. With the aim of filling this gap, the News2vec model is proposed to allow the distributed representation of news taking into account its associated features. To describe the cross-document linkages between news, a <a href=https://en.wikipedia.org/wiki/Social_network>network</a> consisting of <a href=https://en.wikipedia.org/wiki/News>news</a> and its attributes is constructed. Moreover, the News2vec model treats the news node as a bag of features by developing the Subnode model. Based on the <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>biased random walk</a> and the skip-gram model, each <a href=https://en.wikipedia.org/wiki/News>news feature</a> is mapped to a vector, and the <a href=https://en.wikipedia.org/wiki/News>news</a> is thus represented as the sum of its features. This approach offers an easy solution to create embeddings for unseen news nodes based on its attributes. To evaluate our model, dimension reduction plots and correlation heat-maps are created to visualize the news vectors, together with the application of two downstream tasks, the stock movement prediction and news recommendation. By comparing with other established text / sentence embedding models, we show that News2vec achieves state-of-the-art performance on these news-related tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1491.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1491 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1491 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1491.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1491/>Recursive Context-Aware Lexical Simplification</a></strong><br><a href=/people/s/sian-gooding/>Sian Gooding</a>
|
<a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1491><div class="card-body p-3 small">This paper presents a novel architecture for recursive context-aware lexical simplification, REC-LS, that is capable of (1) making use of the wider context when detecting the words in need of simplification and suggesting alternatives, and (2) taking previous simplification steps into account. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and outperforms the current state-of-the-art systems in <a href=https://en.wikipedia.org/wiki/Lexical_simplification>lexical simplification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1493.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1493 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1493 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1493/>Neural News Recommendation with Heterogeneous User Behavior</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/m/mingxiao-an/>Mingxiao An</a>
|
<a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/j/jianqiang-huang/>Jianqiang Huang</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1493><div class="card-body p-3 small">News recommendation is important for <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news platforms</a> to help users find interested news and alleviate <a href=https://en.wikipedia.org/wiki/Information_overload>information overload</a>. Existing news recommendation methods usually rely on the news click history to model <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a>. However, these methods may suffer from the data sparsity problem, since the news click behaviors of many users in online news platforms are usually very limited. Fortunately, some other kinds of user behaviors such as <a href=https://en.wikipedia.org/wiki/Web_navigation>webpage browsing</a> and <a href=https://en.wikipedia.org/wiki/Web_search_query>search queries</a> can also provide useful clues of users&#8217; news reading interest. In this paper, we propose a neural news recommendation approach which can exploit heterogeneous user behaviors. Our approach contains two major <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, i.e., <a href=https://en.wikipedia.org/wiki/News_media>news representation</a> and <a href=https://en.wikipedia.org/wiki/User_interface>user representation</a>. In the news representation module, we learn representations of news from their titles via CNN networks, and apply attention networks to select important words. In the user representation module, we propose an attentive multi-view learning framework to learn unified representations of users from their heterogeneous behaviors such as <a href=https://en.wikipedia.org/wiki/Web_search_query>search queries</a>, clicked news and browsed webpages. In addition, we use word- and record-level attentions to select informative words and behavior records. Experiments on a real-world dataset validate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1495.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1495 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1495 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1495" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1495/>Event Representation Learning Enhanced with External Commonsense Knowledge</a></strong><br><a href=/people/x/xiao-ding/>Xiao Ding</a>
|
<a href=/people/k/kuo-liao/>Kuo Liao</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/z/zhongyang-li/>Zhongyang Li</a>
|
<a href=/people/j/junwen-duan/>Junwen Duan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1495><div class="card-body p-3 small">Prior work has proposed effective methods to learn <a href=https://en.wikipedia.org/wiki/Event_(computing)>event representations</a> that can capture syntactic and semantic information over <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a>, demonstrating their effectiveness for downstream tasks such as script event prediction. On the other hand, events extracted from raw texts lacks of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>, such as the intents and emotions of the event participants, which are useful for distinguishing event pairs when there are only subtle differences in their surface realizations. To address this issue, this paper proposes to leverage external commonsense knowledge about the intent and sentiment of the event. Experiments on three event-related tasks, i.e., event similarity, script event prediction and <a href=https://en.wikipedia.org/wiki/Stock_market_prediction>stock market prediction</a>, show that our model obtains much better event embeddings for the tasks, achieving 78 % improvements on hard similarity task, yielding more precise inferences on subsequent events under given contexts, and better accuracies in predicting the <a href=https://en.wikipedia.org/wiki/Volatility_(finance)>volatilities</a> of the stock market.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1497.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1497 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1497 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1497/>A Neural Citation Count Prediction Model based on Peer Review Text</a></strong><br><a href=/people/s/siqing-li/>Siqing Li</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/e/eddy-jing-yin/>Eddy Jing Yin</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1497><div class="card-body p-3 small">Citation count prediction (CCP) has been an important research task for automatically estimating the future impact of a scholarly paper. Previous studies mainly focus on extracting or mining useful features from the paper itself or the associated authors. An important kind of <a href=https://en.wikipedia.org/wiki/Signal_(IPC)>data signals</a>, <a href=https://en.wikipedia.org/wiki/Peer_review>peer review text</a>, has not been utilized for the CCP task. In this paper, we take the initiative to utilize <a href=https://en.wikipedia.org/wiki/Peer_review>peer review data</a> for the CCP task with a neural prediction model. Our focus is to learn a comprehensive semantic representation for peer review text for improving the <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> performance. To achieve this goal, we incorporate the abstract-review match mechanism and the cross-review match mechanism to learn deep features from peer review text. We also consider integrating hand-crafted features via a <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>wide component</a>. The deep and wide components jointly make the <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Extensive experiments have demonstrated the usefulness of the peer review data and the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> has been released online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1499.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1499 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1499 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1499/>Semi-supervised Text Style Transfer : Cross Projection in Latent Space</a></strong><br><a href=/people/m/mingyue-shang/>Mingyue Shang</a>
|
<a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/z/zhenxin-fu/>Zhenxin Fu</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1499><div class="card-body p-3 small">Text style transfer task requires the model to transfer a sentence of one style to another style while retaining its original content meaning, which is a challenging problem that has long suffered from the shortage of parallel data. In this paper, we first propose a semi-supervised text style transfer model that combines the small-scale parallel data with the large-scale nonparallel data. With these two types of training data, we introduce a <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection function</a> between the latent space of different styles and design two <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> to train it. We also introduce two other simple but effective <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised methods</a> to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1500 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1500" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1500/>Question Answering for Privacy Policies : Combining Computational and Legal Perspectives</a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a>
|
<a href=/people/s/shomir-wilson/>Shomir Wilson</a>
|
<a href=/people/t/thomas-norton/>Thomas Norton</a>
|
<a href=/people/n/norman-sadeh/>Norman Sadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1500><div class="card-body p-3 small">Privacy policies are long and complex documents that are difficult for users to read and understand. Yet, they have legal effects on how <a href=https://en.wikipedia.org/wiki/User_data>user data</a> can be collected, managed and used. Ideally, we would like to empower users to inform themselves about the issues that matter to them, and enable them to selectively explore these issues. We present PrivacyQA, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consisting of 1750 questions about the <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a> of <a href=https://en.wikipedia.org/wiki/Mobile_app>mobile applications</a>, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms <a href=https://en.wikipedia.org/wiki/Human_performance>human performance</a> by almost 0.3 <a href=https://en.wikipedia.org/wiki/F-number>F1</a> on PrivacyQA, suggesting considerable room for improvement for future systems. Further, we use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to categorically identify challenges to <a href=https://en.wikipedia.org/wiki/Question_answering>question answerability</a>, with domain-general implications for any <a href=https://en.wikipedia.org/wiki/Question_answering>question answering system</a>. The PrivacyQA corpus offers a challenging corpus for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, with genuine real world utility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1502 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1502/>Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks</a></strong><br><a href=/people/h/hailong-jin/>Hailong Jin</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/t/tiansi-dong/>Tiansi Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1502><div class="card-body p-3 small">This paper addresses the problem of inferring the fine-grained type of an entity from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. We convert this problem into the task of graph-based semi-supervised classification, and propose Hierarchical Multi Graph Convolutional Network (HMGCN), a novel Deep Learning architecture to tackle this problem. We construct three kinds of <a href=https://en.wikipedia.org/wiki/Connectivity_matrix>connectivity matrices</a> to capture different kinds of semantic correlations between entities. A recursive regularization is proposed to model the subClassOf relations between types in given <a href=https://en.wikipedia.org/wiki/Type_hierarchy>type hierarchy</a>. Extensive experiments with two large-scale public datasets show that our proposed method significantly outperforms four state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1504 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1504.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1504/>Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm</a></strong><br><a href=/people/m/moontae-lee/>Moontae Lee</a>
|
<a href=/people/s/sungjun-cho/>Sungjun Cho</a>
|
<a href=/people/d/david-bindel/>David Bindel</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1504><div class="card-body p-3 small">Despite great scalability on large data and their ability to understand correlations between topics, spectral topic models have not been widely used due to the absence of reliability in real data and lack of practical implementations. This paper aims to solidify the foundations of spectral topic inference and provide a practical implementation for anchor-based topic modeling. Beginning with vocabulary curation, we scrutinize every single inference step with other viable options. We also evaluate our matrix-based approach against popular alternatives including a tensor-based spectral method as well as probabilistic algorithms. Our quantitative and qualitative experiments demonstrate the power of Rectified Anchor Word algorithm in various real datasets, providing a complete guide to practical correlated topic modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1507 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1507/>Subword Language Model for Query Auto-Completion</a></strong><br><a href=/people/g/gyuwan-kim/>Gyuwan Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1507><div class="card-body p-3 small">Current neural query auto-completion (QAC) systems rely on character-level language models, but they slow down when queries are long. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Representing queries with <a href=https://en.wikipedia.org/wiki/Subword>subwords</a> shorten a decoding length significantly. To deal with issues coming from introducing subword language model, we develop a retrace algorithm and a reranking method by approximate marginalization. As a result, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves up to 2.5 times faster while maintaining a similar quality of generated results compared to the character-level baseline. Also, we propose a new evaluation metric, mean recoverable length (MRL), measuring how many upcoming characters the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could complete correctly. It provides more explicit meaning and eliminates the need for prefix length sampling for existing rank-based metrics. Moreover, we performed a comprehensive analysis with ablation study to figure out the importance of each component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1508 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1508/>Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph</a></strong><br><a href=/people/x/xinzhu-lin/>Xinzhu Lin</a>
|
<a href=/people/x/xiahui-he/>Xiahui He</a>
|
<a href=/people/q/qin-chen/>Qin Chen</a>
|
<a href=/people/h/huaixiao-tou/>Huaixiao Tou</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/t/ting-chen/>Ting Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1508><div class="card-body p-3 small">Symptom diagnosis is a challenging yet profound problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Most previous research focus on investigating the standard <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical records</a> for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark models</a> on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performance on the constructed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1510 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1510.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1510" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1510/>Encode, Tag, Realize : High-Precision Text Editing</a></strong><br><a href=/people/e/eric-malmi/>Eric Malmi</a>
|
<a href=/people/s/sebastian-krause/>Sebastian Krause</a>
|
<a href=/people/s/sascha-rothe/>Sascha Rothe</a>
|
<a href=/people/d/daniil-mirylenka/>Daniil Mirylenka</a>
|
<a href=/people/a/aliaksei-severyn/>Aliaksei Severyn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1510><div class="card-body p-3 small">We propose LaserTagger-a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations : keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks : sentence fusion, sentence splitting, abstractive summarization, and <a href=https://en.wikipedia.org/wiki/Grammar>grammar correction</a>. LaserTagger achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1511 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1511/>Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation</a></strong><br><a href=/people/w/weichao-wang/>Weichao Wang</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/d/daling-wang/>Daling Wang</a>
|
<a href=/people/y/yifei-zhang/>Yifei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1511><div class="card-body p-3 small">Generating intriguing question is a key step towards building human-like open-domain chatbots. Although some recent works have focused on this task, compared with questions raised by humans, significant gaps remain in maintaining semantic coherence with post, which may result in generating dull or deviated questions. We observe that the answer has strong semantic coherence to its question and post, which can be used to guide question generation. Thus, we devise two methods to further enhance semantic coherence between post and question under the guidance of answer. First, the coherence score between generated question and answer is used as the <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reward function</a> in a reinforcement learning framework, to encourage the cases that are consistent with the answer in semantic. Second, we incorporate <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a> to explicitly control question generation in the direction of question-answer coherence. Extensive experiments show that our two methods outperform state-of-the-art baseline algorithms with large margins in raising semantic coherent questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1513 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1513.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1513/>A Topic Augmented Text Generation Model : Joint Learning of Semantics and Structural Features</a></strong><br><a href=/people/h/hongyin-tang/>Hongyin Tang</a>
|
<a href=/people/m/miao-li/>Miao Li</a>
|
<a href=/people/b/beihong-jin/>Beihong Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1513><div class="card-body p-3 small">Text generation is among the most fundamental tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we propose a text generation model that learns <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and structural features simultaneously. This model captures structural features by a sequential variational autoencoder component and leverages a topic modeling component based on <a href=https://en.wikipedia.org/wiki/Normal_distribution>Gaussian distribution</a> to enhance the recognition of text semantics. To make the reconstructed text more coherent to the topics, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> further adapts the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> of the topic modeling component for a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a>. The results of experiments over several datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms several states of the art models in terms of <a href=https://en.wikipedia.org/wiki/Perplexity>text perplexity</a> and <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>topic coherence</a>. Moreover, the latent representations learned by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is superior to others in a text classification task. Finally, given the input texts, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can generate meaningful texts which hold similar structures but under different topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1515 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1515.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1515" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1515/>Phrase Grounding by Soft-Label Chain Conditional Random Field</a></strong><br><a href=/people/j/jiacheng-liu/>Jiacheng Liu</a>
|
<a href=/people/j/julia-hockenmaier/>Julia Hockenmaier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1515><div class="card-body p-3 small">The phrase grounding task aims to ground each entity mention in a given caption of an <a href=https://en.wikipedia.org/wiki/Image>image</a> to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>dependencies</a> need to resort to <a href=https://en.wikipedia.org/wiki/Approximate_inference>approximate inference</a> or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define so-called Soft-Label Chain CRFs, and present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that enables convenient end-to-end training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity dependencies</a> captured by the CRF and from the soft-label training regime. Our code is available at<url>github.com/liujch1998/SoftLabelCCRF</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1518 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1518/>DEBUG : A Dense Bottom-Up Grounding Approach for Natural Language Video Localization<span class=acl-fixed-case>DEBUG</span>: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization</a></strong><br><a href=/people/c/chujie-lu/>Chujie Lu</a>
|
<a href=/people/l/long-chen/>Long Chen</a>
|
<a href=/people/c/chilie-tan/>Chilie Tan</a>
|
<a href=/people/x/xiaolin-li/>Xiaolin Li</a>
|
<a href=/people/j/jun-xiao/>Jun Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1518><div class="card-body p-3 small">In this paper, we focus on natural language video localization : localizing (ie, grounding) a natural language description in a long and untrimmed video sequence. All currently published models for addressing this problem can be categorized into two types : (i) <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>top-down approach</a> : it does classification and <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> for a set of pre-cut video segment candidates ; (ii) <a href=https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design>bottom-up approach</a> : it directly predicts probabilities for each video frame as the temporal boundaries (ie, start and end time point). However, both two approaches suffer several limitations : the former is computation-intensive for densely placed candidates, while the latter has trailed the performance of the top-down counterpart thus far. To this end, we propose a novel dense bottom-up framework : DEnse Bottom-Up Grounding (DEBUG). DEBUG regards all frames falling in the ground truth segment as foreground, and each foreground frame regresses the unique distances from its location to bi-directional ground truth boundaries. Extensive experiments on three challenging benchmarks (TACoS, Charades-STA, and ActivityNet Captions) show that DEBUG is able to match the speed of bottom-up models while surpassing the performance of the state-of-the-art top-down models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1521 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1521.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1521" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1521/>Open Domain Web Keyphrase Extraction Beyond Language Modeling</a></strong><br><a href=/people/l/lee-xiong/>Lee Xiong</a>
|
<a href=/people/c/chuan-hu/>Chuan Hu</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/d/daniel-campos/>Daniel Campos</a>
|
<a href=/people/a/arnold-overwijk/>Arnold Overwijk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1521><div class="card-body p-3 small">This paper studies <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a> in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> using visual presentations of documents and weak supervision from <a href=https://en.wikipedia.org/wiki/Web_search_query>search queries</a>. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> from the open domain data compared to a specific domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1525 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1525.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1525" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1525/>Adversarial Reprogramming of Text Classification Neural Networks</a></strong><br><a href=/people/p/paarth-neekhara/>Paarth Neekhara</a>
|
<a href=/people/s/shehzeen-hussain/>Shehzeen Hussain</a>
|
<a href=/people/s/shlomo-dubnov/>Shlomo Dubnov</a>
|
<a href=/people/f/farinaz-koushanfar/>Farinaz Koushanfar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1525><div class="card-body p-3 small">In this work, we develop methods to repurpose text classification neural networks for alternate tasks without modifying the <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> or parameters. We propose a context based vocabulary remapping method that performs a computationally inexpensive input transformation to reprogram a victim classification model for a new set of sequences. We propose algorithms for training such an input transformation in both white box and black box settings where the adversary may or may not have access to the victim model&#8217;s architecture and parameters. We demonstrate the application of our model and the vulnerability of neural networks by adversarially repurposing various text-classification models including LSTM, bi-directional LSTM and CNN for alternate classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1527 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1527/>On Efficient Retrieval of Top Similarity Vectors</a></strong><br><a href=/people/s/shulong-tan/>Shulong Tan</a>
|
<a href=/people/z/zhixin-zhou/>Zhixin Zhou</a>
|
<a href=/people/z/zhaozhuo-xu/>Zhaozhuo Xu</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1527><div class="card-body p-3 small">Retrieval of relevant vectors produced by <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> critically influences the efficiency in natural language processing (NLP) tasks. In this paper, we demonstrate an efficient method for searching vectors via a typical non-metric matching function : <a href=https://en.wikipedia.org/wiki/Inner_product_space>inner product</a>. Our method, which constructs an approximate Inner Product Delaunay Graph (IPDG) for top-1 Maximum Inner Product Search (MIPS), transforms retrieving the most suitable latent vectors into a graph search problem with great benefits of efficiency. Experiments on data representations learned for different machine learning tasks verify the outperforming effectiveness and efficiency of the proposed IPDG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1531 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1531/>Examining Gender Bias in <a href=https://en.wikipedia.org/wiki/Language>Languages</a> with Grammatical Gender</a></strong><br><a href=/people/p/pei-zhou/>Pei Zhou</a>
|
<a href=/people/w/weijia-shi/>Weijia Shi</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/k/kuan-hao-huang/>Kuan-Hao Huang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1531><div class="card-body p-3 small">Recent studies have shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> exhibit <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such <a href=https://en.wikipedia.org/wiki/Bias>bias</a> only in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. These analyses can not be directly extended to <a href=https://en.wikipedia.org/wiki/Language>languages</a> that exhibit <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>morphological agreement</a> on gender, such as <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. In this paper, we propose new metrics for evaluating <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> of these languages and further demonstrate evidence of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in bilingual embeddings which align these languages with <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Finally, we extend an existing approach to mitigate gender bias in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> of these <a href=https://en.wikipedia.org/wiki/Language>languages</a> under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1532 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1532.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1532/>Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation</a></strong><br><a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1532><div class="card-body p-3 small">Words in different languages rarely cover the exact same <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>. This work characterizes differences in meaning between words across languages using semantic relations that have been used to relate the meaning of English words. However, because of translation ambiguity, semantic relations are not always preserved by translation. We introduce a cross-lingual relation classifier trained only with English examples and a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a>. Our <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> relies on a novel attention-based distillation approach to account for translation ambiguity when transferring knowledge from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to cross-lingual settings. On new English-Chinese and English-Hindi test sets, the resulting models largely outperform baselines that more naively rely on bilingual embeddings or dictionaries for cross-lingual transfer, and approach the performance of fully supervised systems on English tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1534 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1534.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1534/>Do NLP Models Know Numbers? Probing Numeracy in Embeddings<span class=acl-fixed-case>NLP</span> Models Know Numbers? Probing Numeracy in Embeddings</a></strong><br><a href=/people/e/eric-wallace/>Eric Wallace</a>
|
<a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1534><div class="card-body p-3 small">The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokensthey embed them as distributed vectors. Is this enough to capture <a href=https://en.wikipedia.org/wiki/Numeracy>numeracy</a>? We begin by investigating the numerical reasoning capabilities of a state-of-the-art <a href=https://en.wikipedia.org/wiki/Question_answering>question answering model</a> on the DROP dataset. We find this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> excels on questions that require <a href=https://en.wikipedia.org/wiki/Numerical_analysis>numerical reasoning</a>, i.e., <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> already captures <a href=https://en.wikipedia.org/wiki/Numeracy>numeracy</a>. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of <a href=https://en.wikipedia.org/wiki/Numeracy>numeracy</a> is naturally present in standard <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. For example, GloVe and <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> accurately encode <a href=https://en.wikipedia.org/wiki/Magnitude_(mathematics)>magnitude</a> for numbers up to 1,000. Furthermore, character-level embeddings are even more preciseELMo captures <a href=https://en.wikipedia.org/wiki/Numeracy>numeracy</a> the best for all pre-trained methodsbut BERT, which uses sub-word units, is less exact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1535 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1535" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1535/>A Split-and-Recombine Approach for Follow-up Query Analysis</a></strong><br><a href=/people/q/qian-liu/>Qian Liu</a>
|
<a href=/people/b/bei-chen/>Bei Chen</a>
|
<a href=/people/h/haoyan-liu/>Haoyan Liu</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/l/lei-fang/>Lei Fang</a>
|
<a href=/people/b/bin-zhou/>Bin Zhou</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1535><div class="card-body p-3 small">Context-dependent semantic parsing has proven to be an important yet challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. To leverage the advances in context-independent semantic parsing, we propose to perform follow-up query analysis, aiming to restate context-dependent natural language queries with <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. It is parser-independent and able to handle multifarious follow-up scenarios in different domains. Experiments on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline by a large margin of nearly 8 %. The superiority on <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> results verifies the feasibility of follow-up query analysis. We also explore the extensibility of STAR on the SQA dataset, which is very promising.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1537 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1537" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1537/>Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions<span class=acl-fixed-case>SQL</span> Query Generation for Cross-Domain Context-Dependent Questions</a></strong><br><a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/h/heyang-er/>Heyang Er</a>
|
<a href=/people/s/sungrok-shim/>Sungrok Shim</a>
|
<a href=/people/e/eric-xue/>Eric Xue</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1537><div class="card-body p-3 small">We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to <a href=https://en.wikipedia.org/wiki/Propagation_of_uncertainty>error propagation</a>. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at.<url>https://github.com/ryanzhumich/sparc_atis_pytorch</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1539 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1539/>Cloze-driven Pretraining of Self-attention Networks</a></strong><br><a href=/people/a/alexei-baevski/>Alexei Baevski</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a>
|
<a href=/people/y/yinhan-liu/>Yinhan Liu</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1539><div class="card-body p-3 small">We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1540 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1540/>Bridging the Gap between Relevance Matching and <a href=https://en.wikipedia.org/wiki/Semantic_matching>Semantic Matching</a> for Short Text Similarity Modeling</a></strong><br><a href=/people/j/jinfeng-rao/>Jinfeng Rao</a>
|
<a href=/people/l/linqing-liu/>Linqing Liu</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/p/peng-shi/>Peng Shi</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1540><div class="card-body p-3 small">A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user&#8217;s query. On the other hand, many NLP problems, such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and paraphrase identification, can be considered variants of <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a>, which is to measure the <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a> between two pieces of short texts. While at a high level both <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>relevance</a> and <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a> require modeling textual similarity, many existing techniques for one can not be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1542 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1542/>Transfer Fine-Tuning : A BERT Case Study<span class=acl-fixed-case>BERT</span> Case Study</a></strong><br><a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1542><div class="card-body p-3 small">A semantic equivalence assessment is defined as a task that assesses <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic equivalence</a> in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> crucial for research on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>&#8217;s performance improves by increasing its model size, the required <a href=https://en.wikipedia.org/wiki/Computational_power>computational power</a> is an obstacle preventing practical applications from adopting the <a href=https://en.wikipedia.org/wiki/Technology>technology</a>. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, which is a property desirable for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1548 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1548" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1548/>Modeling Graph Structure in Transformer for Better AMR-to-Text Generation<span class=acl-fixed-case>AMR</span>-to-Text Generation</a></strong><br><a href=/people/j/jie-zhu/>Jie Zhu</a>
|
<a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/m/muhua-zhu/>Muhua Zhu</a>
|
<a href=/people/l/longhua-qian/>Longhua Qian</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1548><div class="card-body p-3 small">Recent studies on AMR-to-text generation often formalize the task as a sequence-to-sequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequences. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware self-attention approach to better model the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e. the Transformer. In particular, a few different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> are explored to learn <a href=https://en.wikipedia.org/wiki/Representation_(arts)>structural representations</a> between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> on the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1553 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1553/>Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification</a></strong><br><a href=/people/p/pengcheng-yang/>Pengcheng Yang</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/j/jun-xie/>Jun Xie</a>
|
<a href=/people/q/qi-su/>Qi Su</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1553><div class="card-body p-3 small">The task of unsupervised sentiment modification aims to reverse the sentiment polarity of the input text while preserving its semantic content without any parallel data. Most previous work follows a two-step process. They first separate the content from the original sentiment, and then directly generate text with the target sentiment only based on the content produced by the first step. However, the second step bears both the target sentiment addition and content reconstruction, thus resulting in a lack of specific information like proper nouns in the generated text. To remedy this, we propose a specificity-driven cascading approach in this work, which can effectively increase the specificity of the generated text and further improve content preservation. In addition, we propose a more reasonable <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment modification</a>. The experiments show that our approach outperforms competitive baselines by a large margin, which achieves 11 % and 38 % relative improvements of the overall metric on the Yelp and Amazon datasets, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1556.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1556 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1556 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1556.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1556/>From the Token to the Review : A Hierarchical Multimodal approach to <a href=https://en.wikipedia.org/wiki/Opinion_mining>Opinion Mining</a></a></strong><br><a href=/people/a/alexandre-garcia/>Alexandre Garcia</a>
|
<a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/f/florence-dalche-buc/>Florence dAlch-Buc</a>
|
<a href=/people/s/slim-essid/>Slim Essid</a>
|
<a href=/people/c/chloe-clavel/>Chlo Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1556><div class="card-body p-3 small">The task of predicting fine grained user opinion based on spontaneous spoken language is a key problem arising in the development of Computational Agents as well as in the development of social network based opinion miners. Unfortunately, gathering reliable data on which a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can be trained is notoriously difficult and existing works rely only on coarsely labeled opinions. In this work we aim at bridging the gap separating fine grained opinion models already developed for <a href=https://en.wikipedia.org/wiki/Written_language>written language</a> and coarse grained models developed for spontaneous multimodal opinion mining. We take advantage of the implicit hierarchical structure of opinions to build a joint fine and coarse grained opinion model that exploits different views of the opinion expression. The resulting model shares some properties with attention-based models and is shown to provide competitive results on a recently released multimodal fine grained annotated corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1558 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1558/>Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification</a></strong><br><a href=/people/m/mengting-hu/>Mengting Hu</a>
|
<a href=/people/y/yike-wu/>Yike Wu</a>
|
<a href=/people/s/shiwan-zhao/>Shiwan Zhao</a>
|
<a href=/people/h/honglei-guo/>Honglei Guo</a>
|
<a href=/people/r/renhong-cheng/>Renhong Cheng</a>
|
<a href=/people/z/zhong-su/>Zhong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1558><div class="card-body p-3 small">Cross-domain sentiment classification has drawn much attention in recent years. Most existing approaches focus on learning domain-invariant representations in both the source and target domains, while few of them pay attention to the domain-specific information. Despite the non-transferability of the domain-specific information, simultaneously learning domain-dependent representations can facilitate the learning of domain-invariant representations. In this paper, we focus on aspect-level cross-domain sentiment classification, and propose to distill the domain-invariant sentiment features with the help of an orthogonal domain-dependent task, i.e. aspect detection, which is built on the aspects varying widely in different domains. We conduct extensive experiments on three public datasets and the experimental results demonstrate the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1562 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1562" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1562/>Rethinking Attribute Representation and Injection for Sentiment Classification</a></strong><br><a href=/people/r/reinald-kim-amplayo/>Reinald Kim Amplayo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1562><div class="card-body p-3 small">Text attributes, such as user and product information in <a href=https://en.wikipedia.org/wiki/Review>product reviews</a>, have been used to improve the performance of sentiment classification models. The de facto standard method is to incorporate them as additional biases in the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>, and more performance gains are achieved by extending the model architecture. In this paper, we show that the above <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is the least effective way to represent and inject attributes. To demonstrate this hypothesis, unlike previous models with complicated architectures, we limit our base model to a simple BiLSTM with attention classifier, and instead focus on how and where the attributes should be incorporated in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We propose to represent <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> as chunk-wise importance weight matrices and consider four locations in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> (i.e., embedding, encoding, attention, classifier) to inject attributes. Experiments show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a>, contradicting prior work. We also outperform the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> despite our use of a simple base model. Finally, we show that these <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> transfer well to other <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. Model implementation and datasets are released here : https://github.com/rktamplayo/CHIM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1563.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1563 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1563 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1563/>A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis</a></strong><br><a href=/people/c/chuang-fan/>Chuang Fan</a>
|
<a href=/people/h/hongyu-yan/>Hongyu Yan</a>
|
<a href=/people/j/jiachen-du/>Jiachen Du</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a>
|
<a href=/people/r/ruibin-mao/>Ruibin Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1563><div class="card-body p-3 small">Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and <a href=https://en.wikipedia.org/wiki/Common_knowledge>common knowledge</a>. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08 % in <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1564.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1564 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1564 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1564.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1564/>Automatic Argument Quality Assessment-New Datasets and Methods</a></strong><br><a href=/people/a/assaf-toledo/>Assaf Toledo</a>
|
<a href=/people/s/shai-gretz/>Shai Gretz</a>
|
<a href=/people/e/edo-cohen-karlik/>Edo Cohen-Karlik</a>
|
<a href=/people/r/roni-friedman/>Roni Friedman</a>
|
<a href=/people/e/elad-venezian/>Elad Venezian</a>
|
<a href=/people/d/dan-lahav/>Dan Lahav</a>
|
<a href=/people/m/michal-jacovi/>Michal Jacovi</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1564><div class="card-body p-3 small">We explore the task of automatic assessment of argument quality. To that end, we actively collected 6.3k arguments, more than a factor of five compared to previously examined data. Each argument was explicitly and carefully annotated for its quality. In addition, 14k pairs of arguments were annotated independently, identifying the higher quality argument in each pair. In spite of the inherent subjective nature of the task, both <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a> led to surprisingly consistent results. We release the labeled datasets to the community. Furthermore, we suggest <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> based on a recently released <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, for argument ranking as well as for argument-pair classification. In the former <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, our results are comparable to <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> ; in the latter <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> our results significantly outperform earlier <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1567.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1567 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1567 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1567/>Sequential Learning of Convolutional Features for Effective Text Classification</a></strong><br><a href=/people/a/avinash-madasu/>Avinash Madasu</a>
|
<a href=/people/v/vijjini-anvesh-rao/>Vijjini Anvesh Rao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1567><div class="card-body p-3 small">Text classification has been one of the major problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. With the advent of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, convolutional neural network (CNN) has been a popular solution to this task. However, CNNs which were first proposed for <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>, face many crucial challenges in the context of <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>, namely in their elementary blocks : convolution filters and <a href=https://en.wikipedia.org/wiki/Max_pooling>max pooling</a>. These challenges have largely been overlooked by the most existing CNN models proposed for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. In this paper, we present an experimental study on the fundamental blocks of CNNs in text categorization. Based on this critique, we propose Sequential Convolutional Attentive Recurrent Network (SCARN). The proposed SCARN model utilizes both the advantages of recurrent and convolutional structures efficiently in comparison to previously proposed recurrent convolutional models. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on different text classification datasets across tasks like <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and question classification. Extensive experiments establish that SCARN outperforms other recurrent convolutional architectures with significantly less parameters. Furthermore, SCARN achieves better performance compared to equally large various <a href=https://en.wikipedia.org/wiki/Deep_learning>deep CNN</a> and LSTM architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1571.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1571 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1571 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1571.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1571" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1571/>Simple and Effective Noisy Channel Modeling for Neural Machine Translation</a></strong><br><a href=/people/k/kyra-yee/>Kyra Yee</a>
|
<a href=/people/y/yann-dauphin/>Yann Dauphin</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1571><div class="card-body p-3 small">Previous work on neural noisy channel modeling relied on <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable models</a> that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT&#8217;17 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1572.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1572 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1572 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1572.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1572" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1572/>MultiFiT : Efficient Multi-lingual Language Model Fine-tuning<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>F</span>i<span class=acl-fixed-case>T</span>: Efficient Multi-lingual Language Model Fine-tuning</a></strong><br><a href=/people/j/julian-eisenschlos/>Julian Eisenschlos</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/p/piotr-czapla/>Piotr Czapla</a>
|
<a href=/people/m/marcin-kadras/>Marcin Kadras</a>
|
<a href=/people/s/sylvain-gugger/>Sylvain Gugger</a>
|
<a href=/people/j/jeremy-howard/>Jeremy Howard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1572><div class="card-body p-3 small">Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1573.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1573 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1573 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1573.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1573" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1573/>Hint-Based Training for Non-Autoregressive Machine Translation</a></strong><br><a href=/people/z/zhuohan-li/>Zhuohan Li</a>
|
<a href=/people/z/zi-lin/>Zi Lin</a>
|
<a href=/people/d/di-he/>Di He</a>
|
<a href=/people/f/fei-tian/>Fei Tian</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1573><div class="card-body p-3 small">Due to the unparallelizable nature of the autoregressive factorization, AutoRegressive Translation (ART) models have to generate tokens sequentially during decoding and thus suffer from high inference latency. Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time, but could only achieve inferior translation accuracy. In this paper, we proposed a novel approach to leveraging the hints from hidden states and word alignments to help the training of NART models. The results achieve significant improvement over previous NART models for the WMT14 En-De and De-En datasets and are even comparable to a strong LSTM-based ART baseline but one order of magnitude faster in inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1577.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1577 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1577 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1577/>Quantifying the Semantic Core of Gender Systems</a></strong><br><a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/d/damian-blasi/>Damian Blasi</a>
|
<a href=/people/l/lawrence-wolf-sonkin/>Lawrence Wolf-Sonkin</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1577><div class="card-body p-3 small">Many of the world&#8217;s languages employ <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender</a> on the <a href=https://en.wikipedia.org/wiki/Lexeme>lexeme</a>. For instance, in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, house casa is feminine, whereas the word for paper papel is masculine. To a speaker of a <a href=https://en.wikipedia.org/wiki/Genderless_language>genderless language</a>, this categorization seems to exist with neither <a href=https://en.wikipedia.org/wiki/Rhyme>rhyme</a> nor reason. But, is the association of <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> to <a href=https://en.wikipedia.org/wiki/Gender>gender classes</a> truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1578.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1578 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1578 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1578/>Perturbation Sensitivity Analysis to Detect Unintended Model Biases</a></strong><br><a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/b/ben-hutchinson/>Ben Hutchinson</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1578><div class="card-body p-3 small">Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> and <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> should ideally produce scores that are independent of the identity of such entities mentioned in text and their <a href=https://en.wikipedia.org/wiki/Association_(psychology)>social associations</a>. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models a sentiment model and a toxicity model applied on online comments in English language from four different genres.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1582.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1582 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1582 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1582/>Event Detection with Multi-Order Graph Convolution and Aggregated Attention</a></strong><br><a href=/people/h/haoran-yan/>Haoran Yan</a>
|
<a href=/people/x/xiaolong-jin/>Xiaolong Jin</a>
|
<a href=/people/x/xiangbin-meng/>Xiangbin Meng</a>
|
<a href=/people/j/jiafeng-guo/>Jiafeng Guo</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1582><div class="card-body p-3 small">Syntactic relations are broadly used in many NLP tasks. For event detection, syntactic relation representations based on dependency tree can better capture the interrelations between candidate trigger words and related entities than sentence representations. But, existing studies only use <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order syntactic relations</a> (i.e., the arcs) in dependency trees to identify trigger words. For this reason, this paper proposes a new method for event detection, which uses a dependency tree based graph convolution network with aggregative attention to explicitly model and aggregate multi-order syntactic representations in sentences. Experimental comparison with state-of-the-art baselines shows the superiority of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1584.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1584 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1584 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1584.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1584" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1584/>HMEAE : Hierarchical Modular Event Argument Extraction<span class=acl-fixed-case>HMEAE</span>: Hierarchical Modular Event Argument Extraction</a></strong><br><a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/z/ziqi-wang/>Ziqi Wang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1584><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event extraction methods</a> classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1587.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1587 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1587 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1587/>Split or Merge : Which is Better for Unsupervised RST Parsing?<span class=acl-fixed-case>RST</span> Parsing?</a></strong><br><a href=/people/n/naoki-kobayashi/>Naoki Kobayashi</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/k/kengo-nakamura/>Kengo Nakamura</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1587><div class="card-body p-3 small">Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a>. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> based on span merging achieved the best score, around 0.8 F_1 score, which is close to the scores of the previous supervised parsers.<tex-math>_1</tex-math> score, which is close to the scores of the previous supervised parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1588.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1588 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1588 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1588" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1588/>BERT for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> : Baselines and Analysis<span class=acl-fixed-case>BERT</span> for Coreference Resolution: Baselines and Analysis</a></strong><br><a href=/people/m/mandar-joshi/>Mandar Joshi</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1588><div class="card-body p-3 small">We apply BERT to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, achieving a new state of the art on the GAP (+11.5 F1) and OntoNotes (+3.9 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO), but that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. We will release all code and trained models upon publication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1591.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1591 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1591 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1591/>What Part of the <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> Does This? Understanding LSTMs by Measuring and Dissecting Neurons<span class=acl-fixed-case>LSTM</span>s by Measuring and Dissecting Neurons</a></strong><br><a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a>
|
<a href=/people/y/yaoliang-yu/>Yaoliang Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1591><div class="card-body p-3 small">Memory neurons of long short-term memory (LSTM) networks encode and process information in powerful yet mysterious ways. While there has been work to analyze their behavior in carrying low-level information such as linguistic properties, how they directly contribute to label prediction remains unclear. We find inspiration from biologists and study the affinity between individual neurons and labels, propose a novel <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to quantify the sensitivity of neurons to each label, and conduct experiments to show the validity of our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>. We discover that some neurons are trained to specialize on a subset of labels, and while dropping an arbitrary neuron has little effect on the overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the model, dropping label-specialized neurons predictably and significantly degrades prediction accuracy on the associated label. We further examine the consistency of neuron-label affinity across different models. These observations provide insight into the inner mechanisms of LSTMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1594.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1594 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1594 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1594/>Text Genre and Training Data Size in Human-like Parsing</a></strong><br><a href=/people/j/john-hale/>John Hale</a>
|
<a href=/people/a/adhiguna-kuncoro/>Adhiguna Kuncoro</a>
|
<a href=/people/k/keith-hall/>Keith Hall</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/j/jonathan-brennan/>Jonathan Brennan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1594><div class="card-body p-3 small">Domain-specific training typically makes <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> work better. We show that this extends to cognitive modeling as well by relating the states of a neural phrase-structure parser to electrophysiological measures from human participants. These measures were recorded as participants listened to a spoken recitation of the same <a href=https://en.wikipedia.org/wiki/Literature>literary text</a> that was supplied as input to the neural parser. Given more training data, the system derives a better <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive model</a> but only when the training examples come from the same textual genre. This finding is consistent with the idea that humans adapt syntactic expectations to particular genres during <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension</a> (Kaan and Chun, 2018 ; Branigan and Pickering, 2017).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1596.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1596 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1596 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1596.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1596/>Sunny and Dark Outside? ! Improving Answer Consistency in VQA through Entailed Question Generation<span class=acl-fixed-case>VQA</span> through Entailed Question Generation</a></strong><br><a href=/people/a/arijit-ray/>Arijit Ray</a>
|
<a href=/people/k/karan-sikka/>Karan Sikka</a>
|
<a href=/people/a/ajay-divakaran/>Ajay Divakaran</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a>
|
<a href=/people/g/giedrius-burachas/>Giedrius Burachas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1596><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> lack <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a>. For instance, if a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> answers red to What color is the balloon?, it might answer no if asked, Is the balloon red?. These responses violate simple notions of <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a> and raise questions about how effectively VQA models ground language. In this work, we introduce a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, ConVQA, and metrics that enable quantitative evaluation of <a href=https://en.wikipedia.org/wiki/Consistency>consistency</a> in <a href=https://en.wikipedia.org/wiki/Quality_assurance>VQA</a>. For a given observable fact in an <a href=https://en.wikipedia.org/wiki/Image_(mathematics)>image</a> (e.g. the balloon&#8217;s color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA&#8217;s answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the Con-VQA datasets and is a strong baseline for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1600 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1600 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1600" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1600/>A Span-Extraction Dataset for Chinese Machine Reading Comprehension<span class=acl-fixed-case>C</span>hinese Machine Reading Comprehension</a></strong><br><a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/l/li-xiao/>Li Xiao</a>
|
<a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/w/wentao-ma/>Wentao Ma</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1600><div class="card-body p-3 small">Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In this paper, we introduce a Span-Extraction dataset for Chinese machine reading comprehension to add <a href=https://en.wikipedia.org/wiki/Language_diversity>language diversities</a> in this area. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> as well as anonymous submissions for demonstrating the difficulties in this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. With the release of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> could further accelerate the Chinese machine reading comprehension research. Resources are available : https://github.com/ymcui/cmrc2018</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1601 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1601/>MICRON : Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering<span class=acl-fixed-case>MICRON</span>: Multigranular Interaction for Contextualizing <span class=acl-fixed-case>R</span>epresentati<span class=acl-fixed-case>ON</span> in Non-factoid Question Answering</a></strong><br><a href=/people/h/hojae-han/>Hojae Han</a>
|
<a href=/people/s/seungtaek-choi/>Seungtaek Choi</a>
|
<a href=/people/h/haeju-park/>Haeju Park</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1601><div class="card-body p-3 small">This paper studies the problem of non-factoid question answering, where the answer may span over multiple sentences. Existing solutions can be categorized into representation- and interaction-focused approaches. We combine their complementary strength, by a hybrid approach allowing multi-granular interactions, but represented at word level, enabling an easy integration with strong word-level signals. Specifically, we propose MICRON : Multigranular Interaction for Contextualizing RepresentatiON, a novel approach which derives contextualized uni-gram representation from <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a>. Our contributions are as follows : First, we enable multi-granular matches between question and answer n-grams. Second, by contextualizing word representation with surrounding n-grams, MICRON can naturally utilize word-based signals for query term weighting, known to be effective in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. We validate MICRON in two public non-factoid question answering datasets : WikiPassageQA and InsuranceQA, showing our model achieves the state of the art among baselines with reported performances on both datasets.<tex-math>n</tex-math>-grams. Second, by contextualizing word representation with surrounding n-grams, MICRON can naturally utilize word-based signals for query term weighting, known to be effective in information retrieval. We validate MICRON in two public non-factoid question answering datasets: WikiPassageQA and InsuranceQA, showing our model achieves the state of the art among baselines with reported performances on both datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1602 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1602/>Machine Reading Comprehension Using Structural Knowledge Graph-aware Network</a></strong><br><a href=/people/d/delai-qiu/>Delai Qiu</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/x/xinwei-feng/>Xinwei Feng</a>
|
<a href=/people/x/xiangwen-liao/>Xiangwen Liao</a>
|
<a href=/people/w/wenbin-jiang/>Wenbin Jiang</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1602><div class="card-body p-3 small">Leveraging external knowledge is an emerging trend in machine comprehension task. Previous work usually utilizes <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> such as <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> as external knowledge, and extracts triples from them to enhance the initial representation of the machine comprehension context. However, such <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can not capture the structural information in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. To this end, we propose a Structural Knowledge Graph-aware Network(SKG) model, constructing sub-graphs for entities in the machine comprehension context. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> dynamically updates the representation of the knowledge according to the structural information of the constructed sub-graph. Experiments show that SKG achieves state-of-the-art performance on the ReCoRD dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1604 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1604.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1604/>Improving Answer Selection and Answer Triggering using Hard Negatives</a></strong><br><a href=/people/s/sawan-kumar/>Sawan Kumar</a>
|
<a href=/people/s/shweta-garg/>Shweta Garg</a>
|
<a href=/people/k/kartik-mehta/>Kartik Mehta</a>
|
<a href=/people/n/nikhil-rasiwasia/>Nikhil Rasiwasia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1604><div class="card-body p-3 small">In this paper, we establish the effectiveness of using hard negatives, coupled with a siamese network and a suitable <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a>, for the tasks of answer selection and answer triggering. We show that the choice of <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling strategy</a> is key for achieving improved performance on these tasks. Evaluating on recent answer selection datasets-InsuranceQA, SelQA, and an internal QA dataset, we show that using hard negatives with relatively simple model architectures (bag of words and LSTM-CNN) drives significant performance gains. On InsuranceQA, this <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> alone improves over previously reported results by a minimum of 1.6 points in P@1. Using <a href=https://en.wikipedia.org/wiki/Negative_(photography)>hard negatives</a> with a Transformer encoder provides a further improvement of 2.3 points. Further, we propose to use quadruplet loss for answer triggering, with the aim of producing globally meaningful similarity scores. We show that quadruplet loss function coupled with the selection of hard negatives enables <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words models</a> to improve <a href=https://en.wikipedia.org/wiki/Score_(statistics)>F1 score</a> by 2.3 points over previous baselines, on SelQA answer triggering dataset. Our results provide key insights into answer selection and answer triggering tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1607 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1607.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1607/>Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model</a></strong><br><a href=/people/t/tsung-yuan-hsu/>Tsung-Yuan Hsu</a>
|
<a href=/people/c/chi-liang-liu/>Chi-Liang Liu</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1607><div class="card-body p-3 small">Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on <a href=https://en.wikipedia.org/wiki/Multilingualism>multi-lingual corpus</a>. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learn in zero-shot setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1611 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1611.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1611/>Polly Want a Cracker : Analyzing Performance of Parroting on Paraphrase Generation Datasets</a></strong><br><a href=/people/h/hong-ren-mao/>Hong-Ren Mao</a>
|
<a href=/people/h/hung-yi-lee/>Hung-Yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1611><div class="card-body p-3 small">Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> commonly used for <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. Our findings illustrate that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could be seemingly adept at generating <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a>, despite only making trivial changes to the input sentence or even none at all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1612 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1612.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1612/>Query-focused Sentence Compression in <a href=https://en.wikipedia.org/wiki/Time_complexity>Linear Time</a></a></strong><br><a href=/people/a/abram-handler/>Abram Handler</a>
|
<a href=/people/b/brendan-oconnor/>Brendan OConnor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1612><div class="card-body p-3 small">Search applications often display shortened sentences which must contain certain query terms and must fit within the space constraints of a user interface. This work introduces a new transition-based sentence compression technique developed for such settings. Our query-focused method constructs length and lexically constrained compressions in <a href=https://en.wikipedia.org/wiki/Time_complexity>linear time</a>, by growing a <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>subgraph</a> in the dependency parse of a sentence. This theoretically efficient approach achieves an 11x empirical speedup over baseline ILP methods, while better reconstructing gold constrained shortenings. Such speedups help query-focused applications, because users are measurably hindered by interface lags. Additionally, our technique does not require an <a href=https://en.wikipedia.org/wiki/Linear_programming>ILP solver</a> or a <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1614.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1614 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1614 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1614/>Generating Highly Relevant Questions</a></strong><br><a href=/people/j/jiazuo-qiu/>Jiazuo Qiu</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1614><div class="card-body p-3 small">The neural seq2seq based question generation (QG) is prone to generating generic and undiversified questions that are poorly relevant to the given passage and target answer. In this paper, we propose two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> to address the issue. (1) By a partial copy mechanism, we prioritize words that are morphologically close to words in the input passage when generating questions ; (2) By a QA-based reranker, from the n-best list of question candidates, we select questions that are preferred by both the QA and QG model. Experiments and analyses demonstrate that the proposed two methods substantially improve the relevance of generated questions to passages and answers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1619 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1619.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1619" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1619/>An Empirical Comparison on Imitation Learning and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> for Paraphrase Generation</a></strong><br><a href=/people/w/wanyu-du/>Wanyu Du</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1619><div class="card-body p-3 small">Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a <a href=https://en.wikipedia.org/wiki/Code>decoder</a>, <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> which maximizes the likelihood of tokens always suffers from the <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>. Although both <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how <a href=https://en.wikipedia.org/wiki/RL_(complexity)>RL</a> and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model. Experiments on the benchmark datasets show that (1) imitation learning is constantly better than <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> ; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1623 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1623/>Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization</a></strong><br><a href=/people/s/siyao-li/>Siyao Li</a>
|
<a href=/people/d/deren-lei/>Deren Lei</a>
|
<a href=/people/p/pengda-qin/>Pengda Qin</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1623><div class="card-body p-3 small">Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward Rouge-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of Rouge-L, we explore the practicability of utilizing the <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a> to measure the matching degrees. With <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN / Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1627.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1627 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1627 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1627.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1627/>What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition</a></strong><br><a href=/people/t/ting-yun-chang/>Ting-Yun Chang</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1627><div class="card-body p-3 small">Contextualized word embeddings have boosted many NLP tasks compared with traditional static word embeddings. However, the word with a specific sense may have different contextualized embeddings due to its various contexts. To further investigate what contextualized word embeddings capture, this paper analyzes whether they can indicate the corresponding sense definitions and proposes a general framework that is capable of explaining word meanings given contextualized word embeddings for better interpretation. The experiments show that both ELMo and BERT embeddings can be well interpreted via a readable textual form, and the findings may benefit the research community for a better understanding of what the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> capture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1629 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1629.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1629/>WIQA : A dataset for What if... reasoning over procedural text<span class=acl-fixed-case>WIQA</span>: A dataset for What if... reasoning over procedural text</a></strong><br><a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1629><div class="card-body p-3 small">We introduce WIQA, the first large-scale dataset of What if... questions over procedural text. WIQA contains a collection of paragraphs, each annotated with multiple influence graphs describing how one change affects another, and a large (40k) collection of What if...? multiple-choice questions derived from these. For example, given a paragraph about beach erosion, would stormy weather hasten or decelerate erosion? WIQA contains three kinds of questions : perturbations to steps mentioned in the paragraph ; external (out-of-paragraph) perturbations requiring commonsense knowledge ; and irrelevant (no effect) perturbations. We find that state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve 73.8 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, well below the <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human performance</a> of 96.3 %. We analyze the challenges, in particular tracking chains of influences, and present the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> as an open challenge to the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1633.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1633 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1633 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1633" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1633/>Mask-Predict : Parallel Decoding of Conditional Masked Language Models</a></strong><br><a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/y/yinhan-liu/>Yinhan Liu</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1633><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1636.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1636 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1636 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1636/>A Modular Architecture for Unsupervised Sarcasm Generation</a></strong><br><a href=/people/a/abhijit-mishra/>Abhijit Mishra</a>
|
<a href=/people/t/tarun-tater/>Tarun Tater</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1636><div class="card-body p-3 small">In this paper, we propose a novel framework for sarcasm generation ; the system takes a literal negative opinion as input and translates it into a sarcastic version. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> does not require any <a href=https://en.wikipedia.org/wiki/Paired_data>paired data</a> for training. Sarcasm emanates from context-incongruity which becomes apparent as the sentence unfolds. Our framework introduces incongruity into the literal input version through modules that : (a) filter factual content from the input opinion, (b) retrieve incongruous phrases related to the filtered facts and (c) synthesize sarcastic text from the incongruous filtered and incongruous phrases. The framework employs reinforced neural sequence to sequence learning and information retrieval and is trained only using unlabeled non-sarcastic and sarcastic opinions. Since no labeled dataset exists for such a task, for evaluation, we manually prepare a benchmark dataset containing <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>literal opinions</a> and their <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcastic paraphrases</a>. Qualitative and quantitative performance analyses on the data reveal our system&#8217;s superiority over baselines built using known unsupervised statistical and neural machine translation and style transfer techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1640.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1640 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1640 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1640" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1640/>Detect Camouflaged Spam Content via StoneSkipping : Graph and Text Joint Embedding for Chinese Character Variation Representation<span class=acl-fixed-case>S</span>tone<span class=acl-fixed-case>S</span>kipping: Graph and Text Joint Embedding for <span class=acl-fixed-case>C</span>hinese Character Variation Representation</a></strong><br><a href=/people/z/zhuoren-jiang/>Zhuoren Jiang</a>
|
<a href=/people/z/zhe-gao/>Zhe Gao</a>
|
<a href=/people/g/guoxiu-he/>Guoxiu He</a>
|
<a href=/people/y/yangyang-kang/>Yangyang Kang</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/q/qiong-zhang/>Qiong Zhang</a>
|
<a href=/people/l/luo-si/>Luo Si</a>
|
<a href=/people/x/xiaozhong-liu/>Xiaozhong Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1640><div class="card-body p-3 small">The task of Chinese text spam detection is very challenging due to both glyph and phonetic variations of Chinese characters. This paper proposes a novel framework to jointly model Chinese variational, semantic, and contextualized representations for Chinese text spam detection task. In particular, a Variation Family-enhanced Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation graph. The VFGE can learn both the <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embeddings</a> of the Chinese characters (local) and the latent variation families (global). Furthermore, an enhanced bidirectional language model, with a combination gate function and an aggregation learning function, is proposed to integrate the graph and text information while capturing the sequential information. Extensive experiments have been conducted on both SMS and review datasets, to show the proposed method outperforms a series of state-of-the-art models for Chinese spam detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1642.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1642 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1642 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1642.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1642/>An Improved Neural Baseline for Temporal Relation Extraction</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/s/sanjay-subramanian/>Sanjay Subramanian</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1642><div class="card-body p-3 small">Determining temporal relations (e.g., before or after) between events has been a challenging natural language understanding task, partly due to the difficulty to generate large amounts of high-quality training data. Consequently, neural approaches have not been widely used on <a href=https://en.wikipedia.org/wiki/Information_technology>it</a>, or showed only moderate improvements. This paper proposes a new neural system that achieves about 10 % absolute improvement in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> over the previous best <a href=https://en.wikipedia.org/wiki/System>system</a> (25 % error reduction) on two benchmark datasets. The proposed system is trained on the state-of-the-art MATRES dataset and applies contextualized word embeddings, a Siamese encoder of a temporal common sense knowledge base, and global inference via integer linear programming (ILP). We suggest that the new approach could serve as a strong baseline for future research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1645.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1645 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1645 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1645.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1645/>Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas</a></strong><br><a href=/people/k/kosuke-akimoto/>Kosuke Akimoto</a>
|
<a href=/people/t/takuya-hiraoka/>Takuya Hiraoka</a>
|
<a href=/people/k/kunihiko-sadamasa/>Kunihiko Sadamasa</a>
|
<a href=/people/m/mathias-niepert/>Mathias Niepert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1645><div class="card-body p-3 small">Most existing relation extraction approaches exclusively target <a href=https://en.wikipedia.org/wiki/Binary_relation>binary relations</a>, and n-ary relation extraction is relatively unexplored. Current state-of-the-art n-ary relation extraction method is based on a supervised learning approach and, therefore, may suffer from the lack of sufficient relation labels. In this paper, we propose a novel approach to cross-sentence n-ary relation extraction based on universal schemas. To alleviate the sparsity problem and to leverage inherent decomposability of n-ary relations, we propose to learn relation representations of lower-arity facts that result from decomposing higher-arity facts. The proposed method computes a score of a new n-ary fact by aggregating scores of its decomposed lower-arity facts. We conduct experiments with <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for ternary relation extraction and empirically show that our method improves the n-ary relation extraction performance compared to previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1646.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1646 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1646 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1646/>Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition</a></strong><br><a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/b/bin-dong/>Bin Dong</a>
|
<a href=/people/s/shanshan-jiang/>Shanshan Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1646><div class="card-body p-3 small">Current region-based NER models only rely on fully-annotated training data to learn effective region encoder, which often face the training data bottleneck. To alleviate this problem, this paper proposes Gazetteer-Enhanced Attentive Neural Networks, which can enhance region-based NER by learning name knowledge of entity mentions from easily-obtainable gazetteers, rather than only from fully-annotated data. Specially, we first propose an attentive neural network (ANN), which explicitly models the mention-context association and therefore is convenient for integrating externally-learned knowledge. Then we design an auxiliary gazetteer network, which can effectively encode name regularity of mentions only using <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteers</a>. Finally, the learned gazetteer network is incorporated into ANN for better <a href=https://en.wikipedia.org/wiki/Network_topology>NER</a>. Experiments show that our ANN can achieve the state-of-the-art performance on ACE2005 named entity recognition benchmark. Besides, incorporating gazetteer network can further improve the performance and significantly reduce the requirement of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1650.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1650 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1650 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1650/>ner and pos when nothing is capitalized<span class=acl-fixed-case>ner and pos when nothing is capitalized</span></a></strong><br><a href=/people/s/stephen-mayhew/>Stephen Mayhew</a>
|
<a href=/people/t/tatiana-tsygankova/>Tatiana Tsygankova</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1650><div class="card-body p-3 small">For those languages which use it, <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a> is an important signal for the fundamental NLP tasks of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition (NER)</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>Part of Speech (POS) tagging</a>. In fact, it is such a strong signal that <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on these tasks drops sharply in common lowercased scenarios, such as noisy web text or machine translation outputs. In this work, we perform a systematic analysis of solutions to this problem, modifying only the casing of the train or test data using lowercasing and truecasing methods. While prior work and first impressions might suggest training a caseless model, or using a truecaser at test time, we show that the most effective strategy is a concatenation of cased and lowercased training data, producing a single <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with high performance on both cased and uncased text. As shown in our experiments, this result holds across tasks and input representations. Finally, we show that our proposed solution gives an 8 % F1 improvement in mention detection on noisy out-of-domain Twitter data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1653.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1653 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1653 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1653/>Revealing and Predicting Online Persuasion Strategy with Elementary Units</a></strong><br><a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/r/ryo-egawa/>Ryo Egawa</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1653><div class="card-body p-3 small">In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persuasive strategy directly. However, existing research lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategy</a> using <a href=https://en.wikipedia.org/wiki/European_Union>EUs</a>. Our contributions are as follows : (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) revealing both intuitive and non-intuitive strategic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing baseline neural models that identify the EU boundary and type. Our observations imply that <a href=https://en.wikipedia.org/wiki/European_Union>EUs</a> definitively characterize online persuasion strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1654.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1654 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1654 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1654" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1654/>A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis</a></strong><br><a href=/people/q/qingnan-jiang/>Qingnan Jiang</a>
|
<a href=/people/l/lei-chen/>Lei Chen</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a>
|
<a href=/people/x/xiang-ao/>Xiang Ao</a>
|
<a href=/people/m/min-yang/>Min Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1654><div class="card-body p-3 small">Aspect-based sentiment analysis (ABSA) has attracted increasing attention recently due to its broad applications. In existing ABSA datasets, most sentences contain only one aspect or multiple aspects with the same sentiment polarity, which makes ABSA task degenerate to sentence-level sentiment analysis. In this paper, we present a new large-scale Multi-Aspect Multi-Sentiment (MAMS) dataset, in which each sentence contains at least two different aspects with different sentiment polarities. The release of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> would push forward the research in this field. In addition, we propose simple yet effective CapsNet and CapsNet-BERT models which combine the strengths of recent NLP advances. Experiments on our new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the state-of-the-art baseline methods</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1655.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1655 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1655 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1655/>Learning with Noisy Labels for Sentence-level Sentiment Classification</a></strong><br><a href=/people/h/hao-wang/>Hao Wang</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/c/chaozhuo-li/>Chaozhuo Li</a>
|
<a href=/people/y/yan-yang/>Yan Yang</a>
|
<a href=/people/t/tianrui-li/>Tianrui Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1655><div class="card-body p-3 small">Deep neural networks (DNNs) can fit (or even over-fit) the training data very well. If a DNN model is trained using data with noisy labels and tested on data with clean labels, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> may perform poorly. This paper studies the problem of learning with noisy labels for sentence-level sentiment classification. We propose a novel DNN model called NetAb (as shorthand for <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural Networks</a> with Ab-networks) to handle noisy labels during training. NetAb consists of two <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>, one with a noise transition layer for dealing with the input noisy labels and the other for predicting &#8216;clean&#8217; labels. We train the two <a href=https://en.wikipedia.org/wiki/Neural_network>networks</a> using their respective <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> in a mutual reinforcement manner. Experimental results demonstrate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1656.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1656 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1656 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1656.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D19-1656/>DENS : A Dataset for Multi-class Emotion Analysis<span class=acl-fixed-case>DENS</span>: A Dataset for Multi-class Emotion Analysis</a></strong><br><a href=/people/c/chen-liu/>Chen Liu</a>
|
<a href=/people/m/muhammad-osama/>Muhammad Osama</a>
|
<a href=/people/a/anderson-de-andrade/>Anderson De Andrade</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1656><div class="card-body p-3 small">We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Narrative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives avail- able on <a href=https://en.wikipedia.org/wiki/Wattpad>Wattpad</a>, annotated using <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a>. A number of statistics and baseline benchmarks are provided for the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Of the tested techniques, we find that the <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> of a pre-trained BERT model achieves the best results, with an average micro-F1 score of 60.4 %. Our results show that the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provides a novel opportunity in emotion analysis that requires moving beyond existing sentence-level techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1657.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1657 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1657 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1657/>Multi-Task Stance Detection with Sentiment and Stance Lexicons</a></strong><br><a href=/people/y/yingjie-li/>Yingjie Li</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1657><div class="card-body p-3 small">Stance detection aims to detect whether the opinion holder is in support of or against a given target. Recent works show improvements in stance detection by using either the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> or <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a>. In this paper, we propose a multi-task framework that incorporates target-specific attention mechanism and at the same time takes sentiment classification as an auxiliary task. Moreover, we used a sentiment lexicon and constructed a stance lexicon to provide guidance for the attention layer. Experimental results show that the proposed model significantly outperforms state-of-the-art deep learning methods on the SemEval-2016 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1658.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1658 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1658 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1658/>A Robust Self-Learning Framework for Cross-Lingual Text Classification</a></strong><br><a href=/people/x/xin-luna-dong/>Xin Dong</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1658><div class="card-body p-3 small">Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry. In this paper, we present an elegantly simple robust self-learning framework to include unlabeled non-English samples in the fine-tuning process of pretrained multilingual representation models. We leverage a multilingual model&#8217;s own predictions on unlabeled non-English data in order to obtain additional information that can be used during further <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1660.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1660 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1660 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1660/>Label Embedding using Hierarchical Structure of Labels for Twitter Classification<span class=acl-fixed-case>T</span>witter Classification</a></strong><br><a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/k/kiminobu-makino/>Kiminobu Makino</a>
|
<a href=/people/y/yuka-takei/>Yuka Takei</a>
|
<a href=/people/h/hiroki-okamoto/>Hiroki Okamoto</a>
|
<a href=/people/j/jun-goto/>Jun Goto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1660><div class="card-body p-3 small">Twitter is used for various <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> such as <a href=https://en.wikipedia.org/wiki/Emergency_management>disaster monitoring</a> and <a href=https://en.wikipedia.org/wiki/Electronic_news-gathering>news material gathering</a>. In these <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, each Tweet is classified into pre-defined classes. These <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>classes</a> have a semantic relationship with each other and can be classified into a <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a>, which is regarded as important information. Label texts of pre-defined classes themselves also include important clues for <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a>. Therefore, we propose a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> that can consider the <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a> of labels and label texts themselves. We conducted evaluation over the Text REtrieval Conference (TREC) 2018 Incident Streams (IS) track dataset, and we found that our method outperformed the methods of the conference participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1664.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1664 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1664 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1664.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1664" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1664/>In Plain Sight : <a href=https://en.wikipedia.org/wiki/Media_bias>Media Bias</a> Through the Lens of Factual Reporting</a></strong><br><a href=/people/l/lisa-fan/>Lisa Fan</a>
|
<a href=/people/m/marshall-white/>Marshall White</a>
|
<a href=/people/e/eva-sharma/>Eva Sharma</a>
|
<a href=/people/r/ruisi-su/>Ruisi Su</a>
|
<a href=/people/p/prafulla-kumar-choubey/>Prafulla Kumar Choubey</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1664><div class="card-body p-3 small">The increasing prevalence of political bias in <a href=https://en.wikipedia.org/wiki/News_media>news media</a> calls for greater public awareness of it, as well as robust methods for its detection. While prior work in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has primarily focused on the lexical bias captured by linguistic attributes such as <a href=https://en.wikipedia.org/wiki/Word_choice>word choice</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of <a href=https://en.wikipedia.org/wiki/Information_bias>informational bias</a> : factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> annotated with 1,727 bias spans and find evidence that <a href=https://en.wikipedia.org/wiki/Information_bias>informational bias</a> appears in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> more frequently than lexical bias. We further study our <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> to observe how <a href=https://en.wikipedia.org/wiki/Information_bias>informational bias</a> surfaces in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> by different media outlets. Lastly, a baseline model for informational bias prediction is presented by fine-tuning BERT on our labeled data, indicating the challenges of the task and future directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1666.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1666 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1666 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D19-1666.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-1666" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-1666/>Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts<span class=acl-fixed-case>A</span>merican Football Broadcasts</a></strong><br><a href=/people/j/jack-merullo/>Jack Merullo</a>
|
<a href=/people/l/luke-yeh/>Luke Yeh</a>
|
<a href=/people/a/abram-handler/>Abram Handler</a>
|
<a href=/people/a/alvin-grissom-ii/>Alvin Grissom II</a>
|
<a href=/people/b/brendan-oconnor/>Brendan OConnor</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1666><div class="card-body p-3 small">Sports broadcasters inject drama into <a href=https://en.wikipedia.org/wiki/Sports_commentator>play-by-play commentary</a> by building team and player narratives through <a href=https://en.wikipedia.org/wiki/Subjectivity>subjective analyses</a> and <a href=https://en.wikipedia.org/wiki/Anecdote>anecdotes</a>. Prior studies based on <a href=https://en.wikipedia.org/wiki/Small_data>small datasets</a> and manual coding show that such theatrics evince commentator bias in <a href=https://en.wikipedia.org/wiki/Broadcasting_of_sports_events>sports broadcasts</a>. To examine this phenomenon, we assemble <a href=https://en.wikipedia.org/wiki/American_football>FOOTBALL</a>, which contains 1,455 broadcast transcripts from American football games across six decades that are automatically annotated with 250 K player mentions and linked with <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>racial metadata</a>. We identify major confounding factors for researchers examining racial bias in FOOTBALL, and perform a computational analysis that supports conclusions from prior social science studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1667.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1667 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1667 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1667/>Charge-Based Prison Term Prediction with Deep Gating Network</a></strong><br><a href=/people/h/huajie-chen/>Huajie Chen</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/w/wei-dai/>Wei Dai</a>
|
<a href=/people/z/zehui-dai/>Zehui Dai</a>
|
<a href=/people/y/yadong-ding/>Yadong Ding</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1667><div class="card-body p-3 small">Judgment prediction for <a href=https://en.wikipedia.org/wiki/Legal_case>legal cases</a> has attracted much research efforts for its practice use, of which the ultimate goal is prison term prediction. While existing work merely predicts the total prison term, in reality a defendant is often charged with multiple crimes. In this paper, we argue that charge-based prison term prediction (CPTP) not only better fits realistic needs, but also makes the total prison term prediction more accurate and interpretable. We collect the first large-scale structured data for <a href=https://en.wikipedia.org/wiki/CPTP>CPTP</a> and evaluate several competitive <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Based on the observation that fine-grained feature selection is the key to achieving good performance, we propose the Deep Gating Network (DGN) for charge-specific feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1672.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1672 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1672 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1672/>What Matters for Neural Cross-Lingual Named Entity Recognition : An Empirical Analysis</a></strong><br><a href=/people/x/xiaolei-huang/>Xiaolei Huang</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1672><div class="card-body p-3 small">Building named entity recognition (NER) models for languages that do not have much training data is a challenging task. While recent work has shown promising results on cross-lingual transfer from high-resource languages, it is unclear what knowledge is transferred. In this paper, we first propose a simple and efficient neural architecture for cross-lingual NER. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves competitive performance with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. We further explore how <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> works for cross-lingual NER on two transferable factors : sequential order and multilingual embedding. Our results shed light on future research for improving cross-lingual NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1674.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1674 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1674 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1674/>Generating Natural Anagrams : Towards <a href=https://en.wikipedia.org/wiki/Language_generation>Language Generation</a> Under Hard Combinatorial Constraints</a></strong><br><a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/s/sho-takase/>Sho Takase</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1674><div class="card-body p-3 small">An <a href=https://en.wikipedia.org/wiki/Anagram>anagram</a> is a sentence or a phrase that is made by permutating the characters of an input sentence or a phrase. For example, Trims cash is an anagram of Christmas. Existing automatic anagram generation methods can find possible combinations of words form an <a href=https://en.wikipedia.org/wiki/Anagram>anagram</a>. However, they do not pay much attention to the naturalness of the generated <a href=https://en.wikipedia.org/wiki/Anagram>anagrams</a>. In this paper, we show that simple <a href=https://en.wikipedia.org/wiki/Depth-first_search>depth-first search</a> can yield natural anagrams when it is combined with modern neural language models. Human evaluation results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can generate significantly more natural anagrams than baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1675.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1675 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1675 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1675/>STANCY : Stance Classification Based on Consistency Cues<span class=acl-fixed-case>STANCY</span>: Stance Classification Based on Consistency Cues</a></strong><br><a href=/people/k/kashyap-popat/>Kashyap Popat</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1675><div class="card-body p-3 small">Controversial claims are abundant in <a href=https://en.wikipedia.org/wiki/Mass_media>online media</a> and <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion forums</a>. A better understanding of such claims requires analyzing them from different perspectives. Stance classification is a necessary step for inferring these <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspectives</a> in terms of supporting or opposing the claim. In this work, we present a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> for stance classification leveraging BERT representations and augmenting them with a novel consistency constraint. Experiments on the Perspectrum dataset, consisting of claims and users&#8217; perspectives from various debate websites, demonstrate the effectiveness of our approach over state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-1676.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-1676 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-1676 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-1676/>Cross-lingual intent classification in a low resource industrial setting</a></strong><br><a href=/people/t/talaat-khalil/>Talaat Khalil</a>
|
<a href=/people/k/kornel-kielczewski/>Kornel Kieczewski</a>
|
<a href=/people/g/georgios-christos-chouliaras/>Georgios Christos Chouliaras</a>
|
<a href=/people/a/amina-keldibek/>Amina Keldibek</a>
|
<a href=/people/m/maarten-versteegh/>Maarten Versteegh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-1676><div class="card-body p-3 small">This paper explores different approaches to multilingual intent classification in a low resource setting. Recent advances in multilingual text representations promise cross-lingual transfer for <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a>. We investigate the potential for this transfer in an applied industrial setting and compare to multilingual classification using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translated text</a>. Our results show that while the recently developed methods show promise, practical application calls for a combination of techniques for useful results.</div></div></div><hr><div id=d19-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/D19-2/>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2001/>Dive into Deep Learning for Natural Language Processing</a></strong><br><a href=/people/h/haibin-lin/>Haibin Lin</a>
|
<a href=/people/x/xingjian-shi/>Xingjian Shi</a>
|
<a href=/people/l/leonard-lausen/>Leonard Lausen</a>
|
<a href=/people/a/aston-zhang/>Aston Zhang</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/s/sheng-zha/>Sheng Zha</a>
|
<a href=/people/a/alexander-smola/>Alexander Smola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2001><div class="card-body p-3 small">Deep learning has become the dominant approach to NLP problems, especially when applied on large scale corpora. Recent progress on unsupervised pre-training techniques such as BERT, ELMo, GPT-2, and language modeling in general, when applied on large corpora, is shown to be effective in improving a wide variety of downstream tasks. These techniques push the limits of available hardware, requiring specialized frameworks optimized for GPU, ASIC, and distributed cloud-based training.\n\nA few complexities pose challenges to scale these models and algorithms effectively. Compared to other areas where deep learning is applied, these NLP models contain a variety of moving parts: text normalization and tokenization, word representation at subword-level and word-level, variable-length models such as RNN and attention, and sequential decoder based on beam search, among others.\n\nIn this hands-on tutorial, we take a closer look at the challenges from these complexities and see how with proper tooling with Apache MXNet and GluonNLP, we can overcome these challenges and achieve state-of-the-art results for real-world problems. GluonNLP is a powerful new toolkit that combines MXNet&#8217;s speed, the flexibility of Gluon, and an extensive new library automating the most laborious aspects of deep learning for NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2002/>Processing and Understanding Mixed Language Data</a></strong><br><a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2002><div class="card-body p-3 small">Multilingual communities exhibit code-mixing, that is, mixing of two or more socially stable languages in a single conversation, sometimes even in a single utterance. This phenomenon has been widely studied by linguists and interaction scientists in the spoken language of such communities. However, with the prevalence of social media and other informal interactive platforms, code-switching is now also ubiquitously observed in user-generated text. As multilingual communities are more the norm from a global perspective, it becomes essential that code-switched text and speech are adequately handled by language technologies and NUIs.\n\nCode-mixing is extremely prevalent in all multilingual societies. Current studies have shown that as much as 20% of user generated content from some geographies, like South Asia, parts of Europe, and Singapore, are code-mixed. Thus, it is very important to handle code-mixed content as a part of NLP systems and applications for these geographies.\n\nIn the past 5 years, there has been an active interest in computational models for code-mixing with a substantive research outcome in terms of publications, datasets and systems. However, it is not easy to find a single point of access for a complete and coherent overview of the research. This tutorial is expecting to fill this gap and provide new researchers in the area with a foundation in both linguistic and computational aspects of code-mixing. We hope that this then becomes a starting point for those who wish to pursue research, design, development and deployment of code-mixed systems in multilingual societies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2003/>Data Collection and End-to-End Learning for Conversational <span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iigo Casanueva</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vuli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2003><div class="card-body p-3 small">A fundamental long-term goal of conversational AI is to merge two main dialogue system paradigms into a standalone multi-purpose system. Such a system should be capable of conversing about arbitrary topics (Paradigm 1: open-domain dialogue systems), and simultaneously assist humans with completing a wide range of tasks with well-defined semantics such as restaurant search and booking, customer service applications, or ticket bookings (Paradigm 2: task-based dialogue systems).\n\nThe recent developmental leaps in conversational AI technology are undoubtedly linked to more and more sophisticated deep learning algorithms that capture patterns in increasing amounts of data generated by various data collection mechanisms. The goal of this tutorial is therefore twofold. First, it aims at familiarising the research community with the recent advances in algorithmic design of statistical dialogue systems for both open-domain and task-based dialogue paradigms. The focus of the tutorial is on recently introduced end-to-end learning for dialogue systems and their relation to more common modular systems. In theory, learning end-to-end from data offers seamless and unprecedented portability of dialogue systems to a wide spectrum of tasks and languages. From a practical point of view, there are still plenty of research challenges and opportunities remaining: in this tutorial we analyse this gap between theory and practice, and introduce the research community with the main advantages as well as with key practical limitations of current end-to-end dialogue learning.\n\nThe critical requirement of each statistical dialogue system is the data at hand. The system cannot provide assistance for the task without having appropriate task-related data to learn from. Therefore, the second major goal of this tutorial is to provide a comprehensive overview of the current approaches to data collection for dialogue, and analyse the current gaps and challenges with diverse data collection protocols, as well as their relation to and current limitations of data-driven end-to-end dialogue modeling. We will again analyse this relation and limitations both from research and industry perspective, and provide key insights on the application of state-of-the-art methodology into industry-scale conversational AI systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2004/>Bias and Fairness in Natural Language Processing</a></strong><br><a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2004><div class="card-body p-3 small">Recent advances in data-driven machine learning techniques (e.g., deep neural networks) have revolutionized many natural language processing applications. These approaches automatically learn how to make decisions based on the statistics and diagnostic information from large amounts of training data. Despite the remarkable accuracy of machine learning in various applications, learning algorithms run the risk of relying on societal biases encoded in the training data to make predictions. This often occurs even when gender and ethnicity information is not explicitly provided to the system because learning algorithms are able to discover implicit associations between individuals and their demographic information based on other variables such as names, titles, home addresses, etc. Therefore, machine learning algorithms risk potentially encouraging unfair and discriminatory decision making and raise serious privacy concerns. Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models might have the undesirable effect of magnifying harmful stereotypes or implicit biases that rely on sensitive demographic attributes.\n\nIn this tutorial, we will review the history of bias and fairness studies in machine learning and language processing and present recent community effort in quantifying and mitigating bias in natural language processing models for a wide spectrum of tasks, including word embeddings, co-reference resolution, machine translation, and vision-and-language tasks. In particular, we will focus on the following topics:\n\n+ Definitions of fairness and bias.\n\n+ Data, algorithms, and models that propagate and even amplify social bias to NLP applications and metrics to quantify these biases.\n\n+ Algorithmic solutions; learning objective; design principles to prevent social bias in NLP systems and their potential drawbacks.\n\nThe tutorial will bring researchers and practitioners to be aware of this issue, and encourage the research community to propose innovative solutions to promote fairness in NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2005/>Discreteness in Neural Natural Language Processing</a></strong><br><a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2005><div class="card-body p-3 small">This tutorial provides a comprehensive guide to the process of discreteness in neural NLP.\n\nAs a gentle start, we will briefly introduce the background of deep learning based NLP, where we point out the ubiquitous discreteness of natural language and its challenges in neural information processing. Particularly, we will focus on how such discreteness plays a role in the input space, the latent space, and the output space of a neural network. In each part, we will provide examples, discuss machine learning techniques, as well as demonstrate NLP applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2006/>Graph-based Deep Learning in Natural Language Processing</a></strong><br><a href=/people/s/shikhar-vashishth/>Shikhar Vashishth</a>
|
<a href=/people/n/naganand-yadati/>Naganand Yadati</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2006><div class="card-body p-3 small">This tutorial aims to introduce recent advances in graph-based deep learning techniques such as Graph Convolutional Networks (GCNs) for Natural Language Processing (NLP). It provides a brief introduction to deep learning methods on non-Euclidean domains such as graphs and justifies their relevance in NLP. It then covers recent advances in applying graph-based deep learning methods for various NLP tasks, such as semantic role labeling, machine translation, relationship extraction, and many more.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D19-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-2007/>Semantic Specialization of Distributional Word Vectors</a></strong><br><a href=/people/g/goran-glavas/>Goran Glava</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vuli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-2007><div class="card-body p-3 small">Distributional word vectors have become an indispensable component of most state-of-art NLP models. As a major artefact of the underlying distributional hypothesis, distributional word vector spaces conflate various paradigmatic and syntagmatic lexico-semantic relations. For example, relations such as synonymy/similarity (e.g., car-automobile) or lexical entailment (e.g., car-vehicle) often cannot be distinguished from antonymy (e.g., black-white), meronymy (e.g., car-wheel) or broader thematic relatedness (e.g., car-driver) based on the distances in the distributional vector space. This inherent property of distributional spaces often harms performance in downstream applications, since different lexico-semantic relations support different classes of NLP applications. For instance, Semantic Similarity provides guidance for Paraphrasing, Dialogue State Tracking, and Text Simplification, Lexical Entailment supports Natural Language Inference and Taxonomy Induction, whereas broader thematic relatedness yields gains for Named Entity Recognition, Parsing, and Text Classification and Retrieval.\n\nA plethora of methods have been proposed to emphasize specific lexico-semantic relations in a reshaped (i.e., specialized) vector space. A common solution is to move beyond purely unsupervised word representation learning and include external lexico-semantic knowledge, in a process commonly referred to as semantic specialization. In this tutorial, we provide a thorough overview of specialization methods, covering: 1) joint specialization methods, which augment distributional learning objectives with external linguistic constraints, 2) post-processing retrofitting models, which fine-tune pre-trained distributional vectors to better reflect external linguistic constraints, and 3) the most recently proposed post-specialization methods that generalize the perturbations of the post-processing methods to the whole distributional space. In addition to providing a comprehensive overview of specialization methods, we will introduce the most recent developments, such as (among others): handling asymmetric relations (e.g., hypernymy-hyponymy) in Euclidean and hyperbolic spaces by accounting for vector magnitude as well as for vector distance; cross-lingual transfer of semantic specialization for languages without external lexico-semantic resources; downstream effects of specializing distributional vector spaces; injecting external knowledge into unsupervised pretraining architectures such as ELMo or BERT.</div></div></div><hr><div id=d19-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D19-3/>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3000/>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</a></strong><br><a href=/people/s/sebastian-pado/>Sebastian Pad</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3003/>ALTER : Auxiliary Text Rewriting Tool for Natural Language Generation<span class=acl-fixed-case>ALTER</span>: Auxiliary Text Rewriting Tool for Natural Language Generation</a></strong><br><a href=/people/q/qiongkai-xu/>Qiongkai Xu</a>
|
<a href=/people/c/chenchen-xu/>Chenchen Xu</a>
|
<a href=/people/l/lizhen-qu/>Lizhen Qu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3003><div class="card-body p-3 small">In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a>, text simplification, fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3005/>Automatic Taxonomy Induction and Expansion</a></strong><br><a href=/people/n/nicolas-rodolfo-fauceglia/>Nicolas Rodolfo Fauceglia</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/s/sarthak-dash/>Sarthak Dash</a>
|
<a href=/people/m/md-faisal-mahbub-chowdhury/>Md. Faisal Mahbub Chowdhury</a>
|
<a href=/people/n/nandana-mihindukulasooriya/>Nandana Mihindukulasooriya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3005><div class="card-body p-3 small">The Knowledge Graph Induction Service (KGIS) is an end-to-end knowledge induction system. One of its main capabilities is to automatically induce <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomies</a> from input documents using a hybrid approach that takes advantage of linguistic patterns, <a href=https://en.wikipedia.org/wiki/Semantic_Web>semantic web</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. KGIS allows the user to semi-automatically curate and expand the induced taxonomy through a component called Smart SpreadSheet by exploiting <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>. In this paper, we describe these taxonomy induction and expansion features of <a href=https://en.wikipedia.org/wiki/Geographic_information_system>KGIS</a>. A screencast video demonstrating the system is available in https://ibm.box.com/v/emnlp-2019-demo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3010/>EGG : a toolkit for research on Emergence of lanGuage in Games<span class=acl-fixed-case>EGG</span>: a toolkit for research on Emergence of lan<span class=acl-fixed-case>G</span>uage in Games</a></strong><br><a href=/people/e/eugene-kharitonov/>Eugene Kharitonov</a>
|
<a href=/people/r/rahma-chaabouni/>Rahma Chaabouni</a>
|
<a href=/people/d/diane-bouchacourt/>Diane Bouchacourt</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3010><div class="card-body p-3 small">There is renewed interest in simulating <a href=https://en.wikipedia.org/wiki/Language_emergence>language emergence</a> among <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural agents</a> that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the <a href=https://en.wikipedia.org/wiki/Evolution_of_language>evolution of human language</a>. However, optimizing deep architectures connected by a <a href=https://en.wikipedia.org/wiki/Communication_channel>discrete communication channel</a> (such as that in which <a href=https://en.wikipedia.org/wiki/Language>language</a> emerges) is technically challenging. We introduce EGG, a <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> that greatly simplifies the implementation of emergent-language communication games. EGG&#8217;s modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3011/>Entity resolution for noisy ASR transcripts<span class=acl-fixed-case>ASR</span> transcripts</a></strong><br><a href=/people/a/arushi-raghuvanshi/>Arushi Raghuvanshi</a>
|
<a href=/people/v/vijay-ramakrishnan/>Vijay Ramakrishnan</a>
|
<a href=/people/v/varsha-embar/>Varsha Embar</a>
|
<a href=/people/l/lucien-carroll/>Lucien Carroll</a>
|
<a href=/people/k/karthik-raghunathan/>Karthik Raghunathan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3011><div class="card-body p-3 small">Large vocabulary domain-agnostic Automatic Speech Recognition (ASR) systems often mistranscribe domain-specific words and phrases. Since these generic ASR systems are the first component of most voice assistants in production, building Natural Language Understanding (NLU) systems that are robust to these errors can be a challenging task. In this paper, we focus on handling ASR errors in <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>, specifically <a href=https://en.wikipedia.org/wiki/Personal_name>person names</a>, for a voice-based collaboration assistant. We demonstrate an effective method for resolving person names that are mistranscribed by black-box ASR systems, using character and phoneme-based information retrieval techniques and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>, which improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by 40.8 % on our <a href=https://en.wikipedia.org/wiki/Production_system_(computer_science)>production system</a>. We provide a live interactive demo to further illustrate the nuances of this problem and the effectiveness of our solution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3012/>EUSP : An Easy-to-Use Semantic Parsing PlatForm<span class=acl-fixed-case>EUSP</span>: An Easy-to-Use Semantic Parsing <span class=acl-fixed-case>P</span>lat<span class=acl-fixed-case>F</span>orm</a></strong><br><a href=/people/b/bo-an/>Bo An</a>
|
<a href=/people/c/chen-bo/>Chen Bo</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3012><div class="card-body p-3 small">Semantic parsing aims to map <a href=https://en.wikipedia.org/wiki/Utterance>natural language utterances</a> into structured meaning representations. We present a modular platform, EUSP (Easy-to-Use Semantic Parsing PlatForm), that facilitates developers to build semantic parser from scratch. Instead of requiring a large amount of training data or complex grammar knowledge, in our platform developers can build grammar-based semantic parser or neural-based semantic parser through configure files which specify the modules and components that compose semantic parsing system. A high quality grammar-based semantic parsing system only requires domain lexicons rather than costly training data for a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>. Furthermore, we provide a browser-based method to generate the semantic parsing system to minimize the difficulty of development. Experimental results show that the neural-based semantic parser system achieves competitive performance on semantic parsing task, and grammar-based semantic parsers significantly improve the performance of a business search engine.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3015/>HARE : a Flexible Highlighting Annotator for Ranking and Exploration<span class=acl-fixed-case>HARE</span>: a Flexible Highlighting Annotator for Ranking and Exploration</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3015><div class="card-body p-3 small">Exploration and analysis of potential data sources is a significant challenge in the application of NLP techniques to novel information domains. We describe HARE, a system for highlighting relevant information in document collections to support <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a> and <a href=https://en.wikipedia.org/wiki/Triage>triage</a>, which provides tools for post-processing and qualitative analysis for model development and tuning. We apply HARE to the use case of narrative descriptions of mobility information in clinical data, and demonstrate its utility in comparing candidate embedding features. We provide a web-based interface for annotation visualization and document ranking, with a modular backend to support interoperability with existing annotation tools. Our system is available online at https://github.com/OSU-slatelab/HARE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3018/>INMT : Interactive Neural Machine Translation Prediction<span class=acl-fixed-case>INMT</span>: Interactive Neural Machine Translation Prediction</a></strong><br><a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3018><div class="card-body p-3 small">In this paper, we demonstrate an Interactive Machine Translation interface, that assists human translators with on-the-fly hints and suggestions. This makes the end-to-end translation process faster, more efficient and creates high-quality translations. We augment the OpenNMT backend with a mechanism to accept the user input and generate conditioned translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3020/>Journalist-in-the-Loop : Continuous Learning as a Service for Rumour Analysis</a></strong><br><a href=/people/t/twin-karmakharm/>Twin Karmakharm</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3020><div class="card-body p-3 small">Automatically identifying <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and assessing their veracity is an important task with downstream applications in <a href=https://en.wikipedia.org/wiki/Journalism>journalism</a>. A significant challenge is how to keep rumour analysis tools up-to-date as new information becomes available for particular <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a> that spread in a <a href=https://en.wikipedia.org/wiki/Social_network>social network</a>. This paper presents a novel open-source web-based rumour analysis tool that can continuous learn from journalists. The system features a rumour annotation service that allows journalists to easily provide feedback for a given social media post through a <a href=https://en.wikipedia.org/wiki/Web_application>web-based interface</a>. The feedback allows the <a href=https://en.wikipedia.org/wiki/System>system</a> to improve an underlying state-of-the-art neural network-based rumour classification model. The <a href=https://en.wikipedia.org/wiki/System>system</a> can be easily integrated as a service into existing tools and platforms used by journalists using a <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>REST API</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3021/>LIDA : Lightweight Interactive Dialogue Annotator<span class=acl-fixed-case>LIDA</span>: Lightweight Interactive Dialogue Annotator</a></strong><br><a href=/people/e/edward-collins/>Edward Collins</a>
|
<a href=/people/n/nikolai-rozanov/>Nikolai Rozanov</a>
|
<a href=/people/b/bingbing-zhang/>Bingbing Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3021><div class="card-body p-3 small">Dialogue systems have the potential to change how people interact with machines but are highly dependent on the quality of the data used to train them. It is therefore important to develop good dialogue annotation tools which can improve the speed and quality of dialogue data annotation. With this in mind, we introduce LIDA, an <a href=https://en.wikipedia.org/wiki/Annotation>annotation tool</a> designed specifically for conversation data. As far as we know, LIDA is the first dialogue annotation system that handles the entire dialogue annotation pipeline from raw text, as may be the output of transcription services, to structured conversation data. Furthermore it supports the integration of arbitrary machine learning mod-els as annotation recommenders and also has a dedicated interface to resolve inter-annotator disagreements such as after crowdsourcing an-notations for a dataset. LIDA is fully open source, documented and publicly available. [ https://github.com/Wluper/lida ] Screen Cast : https://vimeo.com/329824847</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3022/>LINSPECTOR WEB : A Multilingual Probing Suite for Word Representations<span class=acl-fixed-case>LINSPECTOR</span> <span class=acl-fixed-case>WEB</span>: A Multilingual Probing Suite for Word Representations</a></strong><br><a href=/people/m/max-eichler/>Max Eichler</a>
|
<a href=/people/g/gozde-gul-sahin/>Gzde Gl ahin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3022><div class="card-body p-3 small">We present LINSPECTOR WEB, an open source multilingual inspector to analyze word representations. Our system provides researchers working in low-resource settings with an easily accessible web based probing tool to gain quick insights into their <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> especially outside of the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. To do this we employ 16 simple linguistic probing tasks such as <a href=https://en.wikipedia.org/wiki/Grammatical_gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_case>case marking</a>, and <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> for a diverse set of 28 languages. We support probing of static word embeddings along with pretrained AllenNLP models that are commonly used for NLP downstream tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, natural language inference and dependency parsing. The results are visualized in a <a href=https://en.wikipedia.org/wiki/Polar_chart>polar chart</a> and also provided as a table. LINSPECTOR WEB is available as an offline tool or at https://linspector.ukp.informatik.tu-darmstadt.de.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3025 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3025/>Memory Grounded Conversational Reasoning</a></strong><br><a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/p/pararth-shah/>Pararth Shah</a>
|
<a href=/people/r/rajen-subba/>Rajen Subba</a>
|
<a href=/people/a/anuj-kumar/>Anuj Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3025><div class="card-body p-3 small">We demonstrate a conversational system which engages the user through a multi-modal, multi-turn dialog over the user&#8217;s memories. The <a href=https://en.wikipedia.org/wiki/System>system</a> can perform QA over memories by responding to <a href=https://en.wikipedia.org/wiki/User_(computing)>user queries</a> to recall specific attributes and associated media (e.g. photos) of past episodic memories. The <a href=https://en.wikipedia.org/wiki/System>system</a> can also make proactive suggestions to surface related events or facts from past memories to make conversations more engaging and natural. To implement such a system, we collect a new corpus of memory grounded conversations, which comprises human-to-human role-playing dialogs given synthetic memory graphs with simulated attributes. Our proof-of-concept system operates on these synthetic memory graphs, however it can be trained and applied to real-world user memory data (e.g. photo albums, etc.) We present the architecture of the proposed conversational system, and example queries that the <a href=https://en.wikipedia.org/wiki/System>system</a> supports.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3027 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3027/>MY-AKKHARA : A Romanization-based Burmese (Myanmar) Input Method<span class=acl-fixed-case>MY</span>-<span class=acl-fixed-case>AKKHARA</span>: A <span class=acl-fixed-case>R</span>omanization-based <span class=acl-fixed-case>B</span>urmese (<span class=acl-fixed-case>M</span>yanmar) Input Method</a></strong><br><a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3027><div class="card-body p-3 small">MY-AKKHARA is a method used to input <a href=https://en.wikipedia.org/wiki/Burmese_language>Burmese texts</a> encoded in the <a href=https://en.wikipedia.org/wiki/Unicode>Unicode standard</a>, based on commonly accepted <a href=https://en.wikipedia.org/wiki/Latin_script>Latin transcription</a>. By using this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a>, arbitrary Burmese strings can be accurately inputted with 26 lowercase Latin letters. Meanwhile, the 26 uppercase Latin letters are designed as shortcuts of lowercase letter sequences. The frequency of Burmese characters is considered in MY-AKKHARA to realize an efficient keystroke distribution on a <a href=https://en.wikipedia.org/wiki/QWERTY>QWERTY keyboard</a>. Given that the <a href=https://en.wikipedia.org/wiki/Unicode>Unicode standard</a> has not been extensively used in digitization of Burmese, we hope that MY-AKKHARA can contribute to the widespread use of <a href=https://en.wikipedia.org/wiki/Unicode>Unicode</a> in Myanmar and can provide a platform for smart input methods for <a href=https://en.wikipedia.org/wiki/Burmese_language>Burmese</a> in the future. An implementation of MY-AKKHARA running in <a href=https://en.wikipedia.org/wiki/Microsoft_Windows>Windows</a> is released at http://www2.nict.go.jp/astrec-att/member/ding/my-akkhara.html</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3031 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3031/>PolyResponse : A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking<span class=acl-fixed-case>P</span>oly<span class=acl-fixed-case>R</span>esponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking</a></strong><br><a href=/people/m/matthew-henderson/>Matthew Henderson</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vuli</a>
|
<a href=/people/i/inigo-casanueva/>Iigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Pawe Budzianowski</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/s/sam-coope/>Sam Coope</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrki</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3031><div class="card-body p-3 small">We present PolyResponse, a conversational search engine that supports task-oriented dialogue. It is a retrieval-based approach that bypasses the complex multi-component design of traditional task-oriented dialogue systems and the use of explicit semantics in the form of task-specific ontologies. The PolyResponse engine is trained on hundreds of millions of examples extracted from real conversations : it learns what responses are appropriate in different conversational contexts. It then ranks a large index of text and visual responses according to their similarity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3032 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3032/>PyOpenDial : A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules<span class=acl-fixed-case>P</span>y<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>D</span>ial: A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules</a></strong><br><a href=/people/y/youngsoo-jang/>Youngsoo Jang</a>
|
<a href=/people/j/jongmin-lee/>Jongmin Lee</a>
|
<a href=/people/j/jaeyoung-park/>Jaeyoung Park</a>
|
<a href=/people/k/kyeng-hun-lee/>Kyeng-Hun Lee</a>
|
<a href=/people/p/pierre-lison/>Pierre Lison</a>
|
<a href=/people/k/kee-eung-kim/>Kee-Eung Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3032><div class="card-body p-3 small">We present PyOpenDial, a Python-based domain-independent, open-source toolkit for spoken dialogue systems. Recent advances in core components of dialogue systems, such as <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>, <a href=https://en.wikipedia.org/wiki/Dialogue_management>dialogue management</a>, and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>language generation</a>, harness <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to achieve state-of-the-art performance. The original OpenDial, implemented in <a href=https://en.wikipedia.org/wiki/Java_(programming_language)>Java</a>, provides a plugin architecture to integrate external modules, but lacks Python bindings, making it difficult to interface with popular deep learning frameworks such as <a href=https://en.wikipedia.org/wiki/Tensorflow>Tensorflow</a> or <a href=https://en.wikipedia.org/wiki/PyTorch>PyTorch</a>. To this end, we re-implemented OpenDial in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> and extended the toolkit with a number of novel functionalities for neural dialogue state tracking and action planning. We describe the overall <a href=https://en.wikipedia.org/wiki/Systems_architecture>architecture</a> and its extensions, and illustrate their use on an example where the system response model is implemented with a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3034 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3034/>SEAGLE : A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval<span class=acl-fixed-case>SEAGLE</span>: A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval</a></strong><br><a href=/people/f/fabian-david-schmidt/>Fabian David Schmidt</a>
|
<a href=/people/m/markus-dietsche/>Markus Dietsche</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3034><div class="card-body p-3 small">We introduce Seagle, a platform for comparative evaluation of semantic text encoding models on information retrieval (IR) tasks. Seagle implements (1) word embedding aggregators, which represent texts as algebraic aggregations of pretrained word embeddings and (2) pretrained semantic encoders, and allows for their comparative evaluation on arbitrary (monolingual and cross-lingual) IR collections. We benchmark Seagle&#8217;s models on monolingual document retrieval and cross-lingual sentence retrieval. Seagle functionality can be exploited via an easy-to-use web interface and its modular backend (micro-service architecture) can easily be extended with additional semantic search models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3035 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3035/>A Stylometry Toolkit for Latin Literature<span class=acl-fixed-case>L</span>atin Literature</a></strong><br><a href=/people/t/thomas-j-bolt/>Thomas J. Bolt</a>
|
<a href=/people/j/jeffrey-h-flynt/>Jeffrey H. Flynt</a>
|
<a href=/people/p/pramit-chaudhuri/>Pramit Chaudhuri</a>
|
<a href=/people/j/joseph-p-dexter/>Joseph P. Dexter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3035><div class="card-body p-3 small">Computational stylometry has become an increasingly important aspect of <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary criticism</a>, but many humanists lack the technical expertise or language-specific NLP resources required to exploit computational methods. We demonstrate a stylometry toolkit for analysis of Latin literary texts, which is freely available at www.qcrit.org/stylometry. Our <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> generates data for a diverse range of literary features and has an intuitive point-and-click interface. The features included have proven effective for multiple <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary studies</a> and are calculated using custom <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> without the need for <a href=https://en.wikipedia.org/wiki/Parsing>syntactic parsing</a>. As such, the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> models one approach to the user-friendly generation of stylometric data, which could be extended to other premodern and non-English languages underserved by standard NLP resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3036 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-3036/>A <a href=https://en.wikipedia.org/wiki/Summarization>Summarization System</a> for Scientific Documents</a></strong><br><a href=/people/s/shai-erera/>Shai Erera</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a>
|
<a href=/people/g/guy-feigenblat/>Guy Feigenblat</a>
|
<a href=/people/o/ora-peled-nakash/>Ora Peled Nakash</a>
|
<a href=/people/o/odellia-boni/>Odellia Boni</a>
|
<a href=/people/h/haggai-roitman/>Haggai Roitman</a>
|
<a href=/people/d/doron-cohen/>Doron Cohen</a>
|
<a href=/people/b/bar-weiner/>Bar Weiner</a>
|
<a href=/people/y/yosi-mass/>Yosi Mass</a>
|
<a href=/people/o/or-rivlin/>Or Rivlin</a>
|
<a href=/people/g/guy-lev/>Guy Lev</a>
|
<a href=/people/a/achiya-jerbi/>Achiya Jerbi</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/c/charles-jochim/>Charles Jochim</a>
|
<a href=/people/m/martin-gleize/>Martin Gleize</a>
|
<a href=/people/f/francesca-bonin/>Francesca Bonin</a>
|
<a href=/people/f/francesca-bonin/>Francesca Bonin</a>
|
<a href=/people/d/david-konopnicki/>David Konopnicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3036><div class="card-body p-3 small">We present a novel <a href=https://en.wikipedia.org/wiki/System>system</a> providing summaries for Computer Science publications. Through a qualitative user study, we identified the most valuable scenarios for discovery, exploration and understanding of scientific documents. Based on these findings, we built a system that retrieves and summarizes scientific documents for a given information need, either in form of a free-text query or by choosing categorized values such as scientific tasks, <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> and more. Our <a href=https://en.wikipedia.org/wiki/System>system</a> ingested 270,000 papers, and its summarization module aims to generate concise yet detailed summaries. We validated our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> with <a href=https://en.wikipedia.org/wiki/Expert_witness>human experts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3039 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3039/>TEASPN : Framework and Protocol for Integrated Writing Assistance Environments<span class=acl-fixed-case>TEASPN</span>: Framework and Protocol for Integrated Writing Assistance Environments</a></strong><br><a href=/people/m/masato-hagiwara/>Masato Hagiwara</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3039><div class="card-body p-3 small">Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with <a href=https://en.wikipedia.org/wiki/Writing_system>writing software</a>. We propose TEASPN, a <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> and an open-source framework for achieving integrated writing assistance environments. The <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> standardizes the way <a href=https://en.wikipedia.org/wiki/Computer-aided_software_engineering>writing software</a> communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> with low cost. As a result, users can enjoy the integrated experience in their favorite <a href=https://en.wikipedia.org/wiki/Writing_system>writing software</a>. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-3041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-3041 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-3041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D19-3041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D19-3041/>UER : An Open-Source Toolkit for Pre-training Models<span class=acl-fixed-case>UER</span>: An Open-Source Toolkit for Pre-training Models</a></strong><br><a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/h/hui-chen/>Hui Chen</a>
|
<a href=/people/j/jinbin-zhang/>Jinbin Zhang</a>
|
<a href=/people/w/wayne-xin-zhao/>Xin Zhao</a>
|
<a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/h/haotang-deng/>Haotang Deng</a>
|
<a href=/people/q/qi-ju/>Qi Ju</a>
|
<a href=/people/x/xiaoyong-du/>Xiaoyong Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-3041><div class="card-body p-3 small">Existing works, including <a href=https://en.wikipedia.org/wiki/ELMO>ELMO</a> and <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, have revealed the importance of pre-training for NLP tasks. While there does not exist a single pre-training model that works best in all cases, it is of necessity to develop a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> that is able to deploy various pre-training models efficiently. For this purpose, we propose an assemble-on-demand pre-training toolkit, namely Universal Encoder Representations (UER). UER is loosely coupled, and encapsulated with <a href=https://en.wikipedia.org/wiki/Modular_programming>rich modules</a>. By assembling modules on demand, users can either reproduce a state-of-the-art pre-training model or develop a pre-training model that remains unexplored. With UER, we have built a model zoo, which contains pre-trained models based on different corpora, encoders, and targets (objectives). With proper pre-trained models, we could achieve new state-of-the-art results on a range of downstream datasets.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright &nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>